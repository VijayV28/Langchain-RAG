{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add3279d-431b-4e52-bfd1-359337532bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6907b3-b118-44fc-9649-ad6a7f13a0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from data_retriever import data_lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01b0f7a-e85e-4816-abef-3fa2e9824e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 16:31:03.111 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run d:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "d:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `CohereEmbeddings` was deprecated in LangChain 0.0.30 and will be removed in 0.3.0. An updated version of the class exists in the langchain-cohere package and should be used instead. To use it run `pip install -U langchain-cohere` and import as `from langchain_cohere import CohereEmbeddings`.\n",
      "  warn_deprecated(\n",
      "d:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\pydantic\\_internal\\_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "* 'smart_union' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://vijayv2807/cohere-llmu-data already exists, loading from the storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `CohereRerank` was deprecated in LangChain 0.0.30 and will be removed in 0.3.0. An updated version of the class exists in the langchain-cohere package and should be used instead. To use it run `pip install -U langchain-cohere` and import as `from langchain_cohere import CohereRerank`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "db, retriever, compression_retriever = data_lake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268eb434-16c6-4e6e-94f6-b690d8888c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.deeplake.DeepLake at 0x1c6c7ad42b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fa03dd-2a8e-4fa9-8fa0-f138236d73ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['DeepLake', 'CohereEmbeddings'], vectorstore=<langchain_community.vectorstores.deeplake.DeepLake object at 0x000001C6C7AD42B0>, search_type='mmr', search_kwargs={'k': 25, 'fetch_k': 8, 'distance_metric': 'cos'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164242c1-4118-4353-a6a2-c8da22cb2702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContextualCompressionRetriever(base_compressor=CohereRerank(client=<cohere.client.Client object at 0x000001C6C95E7E20>, top_n=5, model='rerank-english-v3.0', cohere_api_key=None, user_agent='langchain'), base_retriever=VectorStoreRetriever(tags=['DeepLake', 'CohereEmbeddings'], vectorstore=<langchain_community.vectorstores.deeplake.DeepLake object at 0x000001C6C7AD42B0>, search_type='mmr', search_kwargs={'k': 25, 'fetch_k': 8, 'distance_metric': 'cos'}))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compression_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d47ed5-51ba-4ab7-a3c5-84af28b029ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3a468c-f399-4fa0-9f53-c62254d6a9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83a2986-0a25-4221-9aca-5a2d3a3090ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory():\n",
    "    memory = ConversationBufferWindowMemory(\n",
    "        memory_key=\"chat_history\", k=5, return_messages=True, output_key=\"answer\"\n",
    "    )\n",
    "    return memory\n",
    "memory = memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0217b393-88d8-4984-bef0-b521bd570839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferWindowMemory(output_key='answer', return_messages=True, memory_key='chat_history')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b04914-932e-4719-828a-92f47ae5bb46",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\Projects\\LLM Git Repos\\Langchain-RAG\\scripts\\chat_memory.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m qa \u001b[39m=\u001b[39m ConversationalRetrievalChain\u001b[39m.\u001b[39mfrom_llm(\n\u001b[1;32m----> 2\u001b[0m     llm\u001b[39m=\u001b[39mllm,\n\u001b[0;32m      3\u001b[0m     retriever\u001b[39m=\u001b[39mcompression_retriever,\n\u001b[0;32m      4\u001b[0m     memory\u001b[39m=\u001b[39mmemory,\n\u001b[0;32m      5\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m      6\u001b[0m     chain_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstuff\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     return_source_documents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=compression_retriever,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    chain_type=\"stuff\",\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be39c91-fa3d-475a-99b8-d1ac2a139b10",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatOpenAI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\Projects\\LLM Git Repos\\Langchain-RAG\\scripts\\chat_memory.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llm \u001b[39m=\u001b[39m ChatOpenAI(model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-4o-mini\u001b[39m\u001b[39m\"\u001b[39m, temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ChatOpenAI' is not defined"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0938579a-5198-4e81-8b40-c36143c56934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca08f924-2e2c-4f13-99b6-676b07d5a76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb708eb5-0526-4699-9ca9-23c098ace7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=compression_retriever,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    chain_type=\"stuff\",\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f98a5cd-7d24-42df-9932-941a4081240d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "# RESPONSE Here is a list of potential FAQs based on the provided text: - **Q: What does the Cohere Platform offer to developers and organizations?** A: The Cohere Platform offers an API that provides access to advanced Large Language Models (LLMs) without requiring machine learning expertise. It simplifies the process by handling data curation, model development, training, and serving. - **Q: What are the key language processing capabilities of the Cohere Platform?** A: The platform offers two main capabilities: text generation and text embedding. Text generation involves completing a prompt with a stream of generated text, like writing a haiku. Text embedding returns a numerical representation of the semantic meaning of a text input, useful for sentiment analysis and other measurements. - **Q: How does the Cohere Platform benefit developers?** A: Developers can focus on creating valuable applications without getting bogged down by the complexities of building language processing capabilities from scratch. The platform handles the heavy lifting, allowing developers to save time and effort. - **Q: What is the difference between text generation and text embedding?** A: Text generation is about creating new text based on a prompt, like generating a haiku or continuing a story. Text embedding, on the other hand, translates text into a series of numbers that represent its semantic meaning, enabling quantitative analysis and understanding of the text's context. - **Q: Can I use the Cohere Platform for sentiment analysis?** A: Yes, the text embedding capability of the Cohere Platform is particularly useful for sentiment analysis. By converting text into numerical representations, you can quantitatively analyze and understand the sentiment or emotional tone expressed in a given piece of text. - **Q: Does the Cohere Platform require machine learning expertise to use?** A: No, the platform is designed to abstract away the complexities of machine learning. Developers can\n",
      "\n",
      "An overview of what we'll cover in the code walkthrough\n",
      "With Cohere Chat, in particular, you get the complete suite of tools needed to build a high-quality RAG application in the shortest time possible. Some of the highlights (see a more comprehensive list) of Cohere’s RAG capabilities include:\n",
      "Query generation: With Cohere’s RAG solution, you get an LLM that’s trained for query generation. It takes a user message and transforms it into queries that are more relevant and optimized for the retrieval process.\n",
      "Retrieval models: Cohere Embed helps you build a high-quality semantic search system that retrieves the most relevant documents using embeddings. Cohere Rerank, on the other hand, helps you boost the results further by reranking the search results based on relevance.\n",
      "Fine-grained citation: Each grounded response includes fine-grained citations linking back to the source documents. This makes the response easily verifiable and builds trust with the user.\n",
      "Cohere Chat provides a complete set of tools needed to build a high-quality RAG application\n",
      "Set Up Amazon Bedrock and SageMaker\n",
      "First, we set up the clients for Bedrock (to be used for Chat and Embed) and SageMaker (to be used for Rerank) using the same steps as in the previous chapters. Here we name the clients co_br for Bedrock and co_sm for SageMaker.\n",
      "# ! pip install cohere cohere-aws boto3 hnswlib unstructured -q\n",
      "import os import cohere import boto3 import cohere_aws from cohere_aws import Client\n",
      "Setup Bedrock\n",
      "To use Bedrock, we create a BedrockClient by passing the necessary AWS credentials.\n",
      "# Create Bedrock client via the native Cohere SDK # Contact your AWS administrator for the credentials co_br = cohere.BedrockClient( aws_region=\"YOUR_AWS_REGION\", aws_access_key=\"YOUR_AWS_ACCESS_KEY_ID\", aws_secret_key=\"YOUR_AWS_SECRET_ACCESS_KEY\", aws_session_token=\"YOUR_AWS_SESSION_TOKEN\", ) \n",
      "Setup SageMaker\n",
      "\n",
      "generate_text(\"\"\" Given the following text, write down a list of potential frequently asked questions (FAQ), together with the answers. The Cohere Platform provides an API for developers and organizations to access cutting-edge LLMs without needing machine learning know-how. The platform handles all the complexities of curating massive amounts of text data, model development, distributed training, model serving, and more. This means that developers can focus on creating value on the applied side rather than spending time and effort on the capability-building side. There are two key types of language processing capabilities that the Cohere Platform provides — text generation and text embedding — and each is served by a different type of model. With text generation, we enter a piece of text, or prompt, and get back a stream of text as a completion to the prompt. One example is asking the model to write a haiku (the prompt) and getting an originally written haiku in return (the completion). With text embedding, we enter a piece of text and get back a list of numbers that represents its semantic meaning (we’ll see what “semantic” means in a section below). This is useful for use cases that involve “measuring” what a passage of text represents, for example, in analyzing its sentiment. \"\"\")\n",
      "\n",
      "Use Case PatternsQualified\n",
      "We’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\n",
      "Cohere’s Command model is an instruction-following text generation model trained to follow user commands. It is trained to be instantly useful in practical applications, covering a wide range of use cases.\n",
      "Having said that, if you are a developer just starting with this technology, it can be daunting to try to comprehend what’s possible. Large language models (LLMs) like the Command model are general-purpose and can be applied in infinite ways, but if one can’t recognize the patterns where they can be useful, it can feel overwhelming.\n",
      "In this chapter, we’ll go through several broad use case categories for the Command model. Though they won’t cover all the possible ways that you can use the model, they are good starting points for understanding the patterns of tasks where the model works well.\n",
      "We’ll go through the following use cases:\n",
      "Writing\n",
      "Question Answering\n",
      "Brainstorming\n",
      "Transforming\n",
      "Summarizing\n",
      "Rewriting\n",
      "Extracting\n",
      "Classifying\n",
      "Setting Up\n",
      "The examples in this post will be shown in Python. For each use case, we’ll look at some ideas on how a prompt can be constructed and the associated model settings. This blog post comes with a Google Colaboratory notebook that lets you get hands-on with the code.\n",
      "First, let’s install the Cohere package, get the Cohere API key, and set up the client.\n",
      "! pip install cohere import cohere co = cohere.Client(\"COHERE_API_KEY\") # Your Cohere API key \n",
      "Let’s also define a function to take a prompt and a temperature value and then call the Chat endpoint, which is how we can access the Command model.​​ We set a default temperature value of 0, which nudges the response to be more predictable and less random. This function returns the text response generated by the model.\n",
      "\n",
      "Conclusion\n",
      "Semantic search applications, enabled by text embeddings, offer a significantly more effective approach to retrieving and analyzing information. Cohere's Embed model can do this across over 100 languages. Its application in fields like financial analysis, as demonstrated in this chapter, shows how it can transform data retrieval and processing tasks, saving time and improving accuracy.\n",
      "In Chapter 5, we’ll switch to Amazon SageMaker and look at an example using the Cohere Rerank model on SageMaker.\n",
      "Human: What is Cohere mainly used for?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = qa.invoke(\"What is Cohere mainly used for?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5202fd69-363a-4151-b8b6-7831cde2515b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is Cohere mainly used for?',\n",
       " 'chat_history': [],\n",
       " 'answer': 'Cohere is mainly used for providing access to advanced Large Language Models (LLMs) through an API, enabling developers and organizations to implement language processing capabilities such as text generation and text embedding without requiring machine learning expertise. It simplifies the process of building applications that involve natural language understanding and generation.',\n",
       " 'source_documents': [Document(metadata={'title': 'Prompt Engineering Basics', 'source': 'https://cohere.com/llmu/prompt-engineering-basics', 'relevance_score': 0.9980657}, page_content=\"# RESPONSE Here is a list of potential FAQs based on the provided text: - **Q: What does the Cohere Platform offer to developers and organizations?** A: The Cohere Platform offers an API that provides access to advanced Large Language Models (LLMs) without requiring machine learning expertise. It simplifies the process by handling data curation, model development, training, and serving. - **Q: What are the key language processing capabilities of the Cohere Platform?** A: The platform offers two main capabilities: text generation and text embedding. Text generation involves completing a prompt with a stream of generated text, like writing a haiku. Text embedding returns a numerical representation of the semantic meaning of a text input, useful for sentiment analysis and other measurements. - **Q: How does the Cohere Platform benefit developers?** A: Developers can focus on creating valuable applications without getting bogged down by the complexities of building language processing capabilities from scratch. The platform handles the heavy lifting, allowing developers to save time and effort. - **Q: What is the difference between text generation and text embedding?** A: Text generation is about creating new text based on a prompt, like generating a haiku or continuing a story. Text embedding, on the other hand, translates text into a series of numbers that represent its semantic meaning, enabling quantitative analysis and understanding of the text's context. - **Q: Can I use the Cohere Platform for sentiment analysis?** A: Yes, the text embedding capability of the Cohere Platform is particularly useful for sentiment analysis. By converting text into numerical representations, you can quantitatively analyze and understand the sentiment or emotional tone expressed in a given piece of text. - **Q: Does the Cohere Platform require machine learning expertise to use?** A: No, the platform is designed to abstract away the complexities of machine learning. Developers can\"),\n",
       "  Document(metadata={'title': 'Retrieval-Augmented Generation (RAG) Using Cohere on AWS', 'source': 'https://cohere.com/llmu/co-aws-rag', 'relevance_score': 0.98372257}, page_content='An overview of what we\\'ll cover in the code walkthrough\\nWith Cohere Chat, in particular, you get the complete suite of tools needed to build a high-quality RAG application in the shortest time possible. Some of the highlights (see a more comprehensive list) of Cohere’s RAG capabilities include:\\nQuery generation: With Cohere’s RAG solution, you get an LLM that’s trained for query generation. It takes a user message and transforms it into queries that are more relevant and optimized for the retrieval process.\\nRetrieval models: Cohere Embed helps you build a high-quality semantic search system that retrieves the most relevant documents using embeddings. Cohere Rerank, on the other hand, helps you boost the results further by reranking the search results based on relevance.\\nFine-grained citation: Each grounded response includes fine-grained citations linking back to the source documents. This makes the response easily verifiable and builds trust with the user.\\nCohere Chat provides a complete set of tools needed to build a high-quality RAG application\\nSet Up Amazon Bedrock and SageMaker\\nFirst, we set up the clients for Bedrock (to be used for Chat and Embed) and SageMaker (to be used for Rerank) using the same steps as in the previous chapters. Here we name the clients co_br for Bedrock and co_sm for SageMaker.\\n# ! pip install cohere cohere-aws boto3 hnswlib unstructured -q\\nimport os import cohere import boto3 import cohere_aws from cohere_aws import Client\\nSetup Bedrock\\nTo use Bedrock, we create a BedrockClient by passing the necessary AWS credentials.\\n# Create Bedrock client via the native Cohere SDK # Contact your AWS administrator for the credentials co_br = cohere.BedrockClient( aws_region=\"YOUR_AWS_REGION\", aws_access_key=\"YOUR_AWS_ACCESS_KEY_ID\", aws_secret_key=\"YOUR_AWS_SECRET_ACCESS_KEY\", aws_session_token=\"YOUR_AWS_SESSION_TOKEN\", ) \\nSetup SageMaker'),\n",
       "  Document(metadata={'title': 'Prompt Engineering Basics', 'source': 'https://cohere.com/llmu/prompt-engineering-basics', 'relevance_score': 0.9822877}, page_content='generate_text(\"\"\" Given the following text, write down a list of potential frequently asked questions (FAQ), together with the answers. The Cohere Platform provides an API for developers and organizations to access cutting-edge LLMs without needing machine learning know-how. The platform handles all the complexities of curating massive amounts of text data, model development, distributed training, model serving, and more. This means that developers can focus on creating value on the applied side rather than spending time and effort on the capability-building side. There are two key types of language processing capabilities that the Cohere Platform provides — text generation and text embedding — and each is served by a different type of model. With text generation, we enter a piece of text, or prompt, and get back a stream of text as a completion to the prompt. One example is asking the model to write a haiku (the prompt) and getting an originally written haiku in return (the completion). With text embedding, we enter a piece of text and get back a list of numbers that represents its semantic meaning (we’ll see what “semantic” means in a section below). This is useful for use cases that involve “measuring” what a passage of text represents, for example, in analyzing its sentiment. \"\"\")'),\n",
       "  Document(metadata={'title': 'Use Case Patterns', 'source': 'https://cohere.com/llmu/use-case-patterns', 'relevance_score': 0.9818753}, page_content='Use Case PatternsQualified\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nCohere’s Command model is an instruction-following text generation model trained to follow user commands. It is trained to be instantly useful in practical applications, covering a wide range of use cases.\\nHaving said that, if you are a developer just starting with this technology, it can be daunting to try to comprehend what’s possible. Large language models (LLMs) like the Command model are general-purpose and can be applied in infinite ways, but if one can’t recognize the patterns where they can be useful, it can feel overwhelming.\\nIn this chapter, we’ll go through several broad use case categories for the Command model. Though they won’t cover all the possible ways that you can use the model, they are good starting points for understanding the patterns of tasks where the model works well.\\nWe’ll go through the following use cases:\\nWriting\\nQuestion Answering\\nBrainstorming\\nTransforming\\nSummarizing\\nRewriting\\nExtracting\\nClassifying\\nSetting Up\\nThe examples in this post will be shown in Python. For each use case, we’ll look at some ideas on how a prompt can be constructed and the associated model settings. This blog post comes with a Google Colaboratory notebook that lets you get hands-on with the code.\\nFirst, let’s install the Cohere package, get the Cohere API key, and set up the client.\\n! pip install cohere import cohere co = cohere.Client(\"COHERE_API_KEY\") # Your Cohere API key \\nLet’s also define a function to take a prompt and a temperature value and then call the Chat endpoint, which is how we can access the Command model.\\u200b\\u200b We set a default temperature value of 0, which nudges the response to be more predictable and less random. This function returns the text response generated by the model.'),\n",
       "  Document(metadata={'title': 'Semantic Search Using Cohere Embed on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-search', 'relevance_score': 0.96771675}, page_content=\"Conclusion\\nSemantic search applications, enabled by text embeddings, offer a significantly more effective approach to retrieving and analyzing information. Cohere's Embed model can do this across over 100 languages. Its application in fields like financial analysis, as demonstrated in this chapter, shows how it can transform data retrieval and processing tasks, saving time and improving accuracy.\\nIn Chapter 5, we’ll switch to Amazon SageMaker and look at an example using the Cohere Rerank model on SageMaker.\")]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6cf5dd-8b2a-443b-9f7b-0f2c2858c8c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatPromptTemplate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\Projects\\LLM Git Repos\\Langchain-RAG\\scripts\\chat_memory.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Prompt To Generate Search Query For Retriever\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m search_query \u001b[39m=\u001b[39m ChatPromptTemplate\u001b[39m.\u001b[39mfrom_messages(\n\u001b[0;32m      3\u001b[0m     [\n\u001b[0;32m      4\u001b[0m         MessagesPlaceholder(variable_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchat_history\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m      5\u001b[0m         (\u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m{input}\u001b[39;00m\u001b[39m\"\u001b[39m),\n\u001b[0;32m      6\u001b[0m         (\n\u001b[0;32m      7\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mGiven the above conversation, generate a search query to look up to get information relevant to the conversation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m         ),\n\u001b[0;32m     10\u001b[0m     ]\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[39m# Chain that takes conversation history and returns documents.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m retriever_chain \u001b[39m=\u001b[39m create_history_aware_retriever(\n\u001b[0;32m     15\u001b[0m     llm\u001b[39m=\u001b[39mllm, retriever\u001b[39m=\u001b[39mcompression_retriever, prompt\u001b[39m=\u001b[39msearch_query\n\u001b[0;32m     16\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ChatPromptTemplate' is not defined"
     ]
    }
   ],
   "source": [
    "# Prompt To Generate Search Query For Retriever\n",
    "search_query = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain that takes conversation history and returns documents.\n",
    "retriever_chain = create_history_aware_retriever(\n",
    "    llm=llm, retriever=compression_retriever, prompt=search_query\n",
    ")\n",
    "\n",
    "# Prompt To Get Response From LLM Based on Chat History\n",
    "fetch_answer = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user's questions based on the below context:\\\\n\\\\n{context}\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain for passing a list of Documents to a model\n",
    "document_chain = create_stuff_documents_chain(llm=llm, prompt=fetch_answer)\n",
    "\n",
    "# Create retrieval chain that retrieves documents and then passes them on to the LLM\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c19ec8-2cb1-4415-94a6-62d3cdf5ca3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ConversationalRetrievalChain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\Projects\\LLM Git Repos\\Langchain-RAG\\scripts\\chat_memory.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmemory\u001b[39;00m \u001b[39mimport\u001b[39;00m ConversationBufferWindowMemory\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_openai\u001b[39;00m \u001b[39mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mretrieval\u001b[39;00m \u001b[39mimport\u001b[39;00m create_retrieval_chain\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhistory_aware_retriever\u001b[39;00m \u001b[39mimport\u001b[39;00m create_history_aware_retriever\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcombine_documents\u001b[39;00m \u001b[39mimport\u001b[39;00m create_stuff_documents_chain\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain\\chains\\retrieval.py:71\u001b[0m\n\u001b[0;32m     63\u001b[0m     retrieval_chain \u001b[39m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m         RunnablePassthrough\u001b[39m.\u001b[39massign(\n\u001b[0;32m     65\u001b[0m             context\u001b[39m=\u001b[39mretrieval_docs\u001b[39m.\u001b[39mwith_config(run_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mretrieve_documents\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     66\u001b[0m         )\u001b[39m.\u001b[39massign(answer\u001b[39m=\u001b[39mcombine_docs_chain)\n\u001b[0;32m     67\u001b[0m     )\u001b[39m.\u001b[39mwith_config(run_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mretrieval_chain\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     69\u001b[0m     \u001b[39mreturn\u001b[39;00m retrieval_chain\n\u001b[1;32m---> 71\u001b[0m qa \u001b[39m=\u001b[39m ConversationalRetrievalChain\u001b[39m.\u001b[39mfrom_llm( llm\u001b[39m=\u001b[39mllm,\n\u001b[0;32m     72\u001b[0m retriever\u001b[39m=\u001b[39mcompression_retriever,\n\u001b[0;32m     73\u001b[0m memory\u001b[39m=\u001b[39mmemory,\n\u001b[0;32m     74\u001b[0m verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     75\u001b[0m chain_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstuff\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     76\u001b[0m return_source_documents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     77\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ConversationalRetrievalChain' is not defined"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
    "\n",
    "from data_retriever import data_lake\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192757b2-47b2-42ab-926a-956ac641bb27",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ConversationalRetrievalChain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\Projects\\LLM Git Repos\\Langchain-RAG\\scripts\\chat_memory.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmemory\u001b[39;00m \u001b[39mimport\u001b[39;00m ConversationBufferWindowMemory\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_openai\u001b[39;00m \u001b[39mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mretrieval\u001b[39;00m \u001b[39mimport\u001b[39;00m create_retrieval_chain\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhistory_aware_retriever\u001b[39;00m \u001b[39mimport\u001b[39;00m create_history_aware_retriever\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcombine_documents\u001b[39;00m \u001b[39mimport\u001b[39;00m create_stuff_documents_chain\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain\\chains\\retrieval.py:71\u001b[0m\n\u001b[0;32m     63\u001b[0m     retrieval_chain \u001b[39m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m         RunnablePassthrough\u001b[39m.\u001b[39massign(\n\u001b[0;32m     65\u001b[0m             context\u001b[39m=\u001b[39mretrieval_docs\u001b[39m.\u001b[39mwith_config(run_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mretrieve_documents\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     66\u001b[0m         )\u001b[39m.\u001b[39massign(answer\u001b[39m=\u001b[39mcombine_docs_chain)\n\u001b[0;32m     67\u001b[0m     )\u001b[39m.\u001b[39mwith_config(run_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mretrieval_chain\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     69\u001b[0m     \u001b[39mreturn\u001b[39;00m retrieval_chain\n\u001b[1;32m---> 71\u001b[0m qa \u001b[39m=\u001b[39m ConversationalRetrievalChain\u001b[39m.\u001b[39mfrom_llm( llm\u001b[39m=\u001b[39mllm,\n\u001b[0;32m     72\u001b[0m retriever\u001b[39m=\u001b[39mcompression_retriever,\n\u001b[0;32m     73\u001b[0m memory\u001b[39m=\u001b[39mmemory,\n\u001b[0;32m     74\u001b[0m verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     75\u001b[0m chain_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstuff\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     76\u001b[0m return_source_documents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     77\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ConversationalRetrievalChain' is not defined"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
    "\n",
    "from data_retriever import data_lake\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db3422c-0730-448e-bba7-3fd5a0f7f877",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ConversationalRetrievalChain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\Projects\\LLM Git Repos\\Langchain-RAG\\scripts\\chat_memory.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mretrieval\u001b[39;00m \u001b[39mimport\u001b[39;00m create_retrieval_chain\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain\\chains\\retrieval.py:71\u001b[0m\n\u001b[0;32m     63\u001b[0m     retrieval_chain \u001b[39m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m         RunnablePassthrough\u001b[39m.\u001b[39massign(\n\u001b[0;32m     65\u001b[0m             context\u001b[39m=\u001b[39mretrieval_docs\u001b[39m.\u001b[39mwith_config(run_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mretrieve_documents\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     66\u001b[0m         )\u001b[39m.\u001b[39massign(answer\u001b[39m=\u001b[39mcombine_docs_chain)\n\u001b[0;32m     67\u001b[0m     )\u001b[39m.\u001b[39mwith_config(run_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mretrieval_chain\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     69\u001b[0m     \u001b[39mreturn\u001b[39;00m retrieval_chain\n\u001b[1;32m---> 71\u001b[0m qa \u001b[39m=\u001b[39m ConversationalRetrievalChain\u001b[39m.\u001b[39mfrom_llm( llm\u001b[39m=\u001b[39mllm,\n\u001b[0;32m     72\u001b[0m retriever\u001b[39m=\u001b[39mcompression_retriever,\n\u001b[0;32m     73\u001b[0m memory\u001b[39m=\u001b[39mmemory,\n\u001b[0;32m     74\u001b[0m verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     75\u001b[0m chain_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstuff\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     76\u001b[0m return_source_documents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     77\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ConversationalRetrievalChain' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.chains.retrieval import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918481db-3f3b-4502-8f93-b4359b9d9793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1196aa0e-f5fc-4f08-bc6b-42eaa484708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
    "\n",
    "from data_retriever import data_lake\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b8b40-2d2b-4cc0-8061-7644721a62e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory():\n",
    "    memory = ConversationBufferWindowMemory(\n",
    "        memory_key=\"chat_history\", k=5, return_messages=True, output_key=\"answer\"\n",
    "    )\n",
    "    return memory\n",
    "memory = memory()\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "\n",
    "# Prompt To Generate Search Query For Retriever\n",
    "search_query = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain that takes conversation history and returns documents.\n",
    "retriever_chain = create_history_aware_retriever(\n",
    "    llm=llm, retriever=compression_retriever, prompt=search_query\n",
    ")\n",
    "\n",
    "# Prompt To Get Response From LLM Based on Chat History\n",
    "fetch_answer = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user's questions based on the below context:\\\\n\\\\n{context}\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain for passing a list of Documents to a model\n",
    "document_chain = create_stuff_documents_chain(llm=llm, prompt=fetch_answer)\n",
    "\n",
    "# Create retrieval chain that retrieves documents and then passes them on to the LLM\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3713ee71-da2f-4521-ad18-07e5708408db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06727790-807b-4143-b249-7c0408aa19b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\Projects\\LLM Git Repos\\Langchain-RAG\\scripts\\chat_memory.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m response \u001b[39m=\u001b[39m retrieval_chain\u001b[39m.\u001b[39minvoke({\n\u001b[1;32m----> 2\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mchat_history\u001b[39m\u001b[39m\"\u001b[39m: memory\u001b[39m.\u001b[39;49mmemory_variables(),\n\u001b[0;32m      3\u001b[0m \u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mWhat is Cohere mainly used for?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m })\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\n",
    "\"chat_history\": memory.memory_variables(),\n",
    "\"input\":\"What is Cohere mainly used for?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d3646c-3f88-4e6b-83b3-4a6b3485d137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferWindowMemory(output_key='answer', return_messages=True, memory_key='chat_history')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f261ae30-c5ce-455d-bd2a-e0fa2e79cebe",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ConversationBufferWindowMemory' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\Projects\\LLM Git Repos\\Langchain-RAG\\scripts\\chat_memory.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m con_memory \u001b[39m=\u001b[39m memory()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'ConversationBufferWindowMemory' object is not callable"
     ]
    }
   ],
   "source": [
    "con_memory = memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c1f288-5d23-414e-9b5e-61c76e53e250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory():\n",
    "    memory = ConversationBufferWindowMemory(\n",
    "        memory_key=\"chat_history\", k=5, return_messages=True, output_key=\"answer\"\n",
    "    )\n",
    "    return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55ec0db-f3e8-4f06-8de9-50db4c7fca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_memory = memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dad6aa-8f29-443f-bb9c-9508818700ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferWindowMemory(output_key='answer', return_messages=True, memory_key='chat_history')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729fc83d-0ba1-46d4-9269-9025334d1e88",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "variable chat_history should be a list of base messages, got {'chat_history': []} of type <class 'dict'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32md:\\Projects\\LLM Git Repos\\Langchain-RAG\\scripts\\chat_memory.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[39m=\u001b[39m retrieval_chain\u001b[39m.\u001b[39;49minvoke({\n\u001b[0;32m      2\u001b[0m \u001b[39m\"\u001b[39;49m\u001b[39mchat_history\u001b[39;49m\u001b[39m\"\u001b[39;49m: con_memory\u001b[39m.\u001b[39;49mload_memory_variables({}),\n\u001b[0;32m      3\u001b[0m \u001b[39m\"\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m\"\u001b[39;49m\u001b[39mWhat is Cohere mainly used for?\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m      4\u001b[0m })\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain_core\\runnables\\base.py:5094\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5088\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[0;32m   5089\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   5090\u001b[0m     \u001b[39minput\u001b[39m: Input,\n\u001b[0;32m   5091\u001b[0m     config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   5092\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5093\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Output:\n\u001b[1;32m-> 5094\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbound\u001b[39m.\u001b[39minvoke(\n\u001b[0;32m   5095\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[0;32m   5096\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5097\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs},\n\u001b[0;32m   5098\u001b[0m     )\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain_core\\runnables\\base.py:2876\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2874\u001b[0m context\u001b[39m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m   2875\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2876\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m context\u001b[39m.\u001b[39mrun(step\u001b[39m.\u001b[39minvoke, \u001b[39minput\u001b[39m, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2877\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2878\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m context\u001b[39m.\u001b[39mrun(step\u001b[39m.\u001b[39minvoke, \u001b[39minput\u001b[39m, config)\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain_core\\runnables\\passthrough.py:495\u001b[0m, in \u001b[0;36mRunnableAssign.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[0;32m    490\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    491\u001b[0m     \u001b[39minput\u001b[39m: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[0;32m    492\u001b[0m     config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    494\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m--> 495\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_invoke, \u001b[39minput\u001b[39m, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain_core\\runnables\\base.py:1785\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m     context \u001b[39m=\u001b[39m copy_context()\n\u001b[0;32m   1782\u001b[0m     context\u001b[39m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m   1783\u001b[0m     output \u001b[39m=\u001b[39m cast(\n\u001b[0;32m   1784\u001b[0m         Output,\n\u001b[1;32m-> 1785\u001b[0m         context\u001b[39m.\u001b[39mrun(\n\u001b[0;32m   1786\u001b[0m             call_func_with_variable_args,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m             func,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m             \u001b[39minput\u001b[39m,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m             config,\n\u001b[0;32m   1790\u001b[0m             run_manager,\n\u001b[0;32m   1791\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1792\u001b[0m         ),\n\u001b[0;32m   1793\u001b[0m     )\n\u001b[0;32m   1794\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1795\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain_core\\runnables\\config.py:427\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[39mif\u001b[39;00m run_manager \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    426\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[1;32m--> 427\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain_core\\runnables\\passthrough.py:482\u001b[0m, in \u001b[0;36mRunnableAssign._invoke\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_invoke\u001b[39m(\n\u001b[0;32m    470\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    471\u001b[0m     \u001b[39minput\u001b[39m: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    474\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    475\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[0;32m    476\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m    477\u001b[0m         \u001b[39minput\u001b[39m, \u001b[39mdict\u001b[39m\n\u001b[0;32m    478\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mThe input to RunnablePassthrough.assign() must be a dict.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    480\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[0;32m    481\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m,\n\u001b[1;32m--> 482\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmapper\u001b[39m.\u001b[39minvoke(\n\u001b[0;32m    483\u001b[0m             \u001b[39minput\u001b[39m,\n\u001b[0;32m    484\u001b[0m             patch_config(config, callbacks\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mget_child()),\n\u001b[0;32m    485\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    486\u001b[0m         ),\n\u001b[0;32m    487\u001b[0m     }\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain_core\\runnables\\base.py:3580\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   3575\u001b[0m     \u001b[39mwith\u001b[39;00m get_executor_for_config(config) \u001b[39mas\u001b[39;00m executor:\n\u001b[0;32m   3576\u001b[0m         futures \u001b[39m=\u001b[39m [\n\u001b[0;32m   3577\u001b[0m             executor\u001b[39m.\u001b[39msubmit(_invoke_step, step, \u001b[39minput\u001b[39m, config, key)\n\u001b[0;32m   3578\u001b[0m             \u001b[39mfor\u001b[39;00m key, step \u001b[39min\u001b[39;00m steps\u001b[39m.\u001b[39mitems()\n\u001b[0;32m   3579\u001b[0m         ]\n\u001b[1;32m-> 3580\u001b[0m         output \u001b[39m=\u001b[39m {key: future\u001b[39m.\u001b[39mresult() \u001b[39mfor\u001b[39;00m key, future \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(steps, futures)}\n\u001b[0;32m   3581\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[0;32m   3582\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain_core\\runnables\\base.py:3580\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3575\u001b[0m     \u001b[39mwith\u001b[39;00m get_executor_for_config(config) \u001b[39mas\u001b[39;00m executor:\n\u001b[0;32m   3576\u001b[0m         futures \u001b[39m=\u001b[39m [\n\u001b[0;32m   3577\u001b[0m             executor\u001b[39m.\u001b[39msubmit(_invoke_step, step, \u001b[39minput\u001b[39m, config, key)\n\u001b[0;32m   3578\u001b[0m             \u001b[39mfor\u001b[39;00m key, step \u001b[39min\u001b[39;00m steps\u001b[39m.\u001b[39mitems()\n\u001b[0;32m   3579\u001b[0m         ]\n\u001b[1;32m-> 3580\u001b[0m         output \u001b[39m=\u001b[39m {key: future\u001b[39m.\u001b[39;49mresult() \u001b[39mfor\u001b[39;00m key, future \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(steps, futures)}\n\u001b[0;32m   3581\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[0;32m   3582\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\concurrent\\futures\\_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    457\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 458\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    459\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfn(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture\u001b[39m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain_core\\runnables\\base.py:3564\u001b[0m, in \u001b[0;36mRunnableParallel.invoke.<locals>._invoke_step\u001b[1;34m(step, input, config, key)\u001b[0m\n\u001b[0;32m   3562\u001b[0m context \u001b[39m=\u001b[39m copy_context()\n\u001b[0;32m   3563\u001b[0m context\u001b[39m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m-> 3564\u001b[0m \u001b[39mreturn\u001b[39;00m context\u001b[39m.\u001b[39;49mrun(\n\u001b[0;32m   3565\u001b[0m     step\u001b[39m.\u001b[39;49minvoke,\n\u001b[0;32m   3566\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   3567\u001b[0m     child_config,\n\u001b[0;32m   3568\u001b[0m )\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain_core\\runnables\\base.py:5094\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5088\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[0;32m   5089\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   5090\u001b[0m     \u001b[39minput\u001b[39m: Input,\n\u001b[0;32m   5091\u001b[0m     config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   5092\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5093\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Output:\n\u001b[1;32m-> 5094\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbound\u001b[39m.\u001b[39minvoke(\n\u001b[0;32m   5095\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[0;32m   5096\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5097\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs},\n\u001b[0;32m   5098\u001b[0m     )\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain_core\\runnables\\branch.py:239\u001b[0m, in \u001b[0;36mRunnableBranch.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    237\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    238\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 239\u001b[0m         output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault\u001b[39m.\u001b[39minvoke(\n\u001b[0;32m    240\u001b[0m             \u001b[39minput\u001b[39m,\n\u001b[0;32m    241\u001b[0m             config\u001b[39m=\u001b[39mpatch_config(\n\u001b[0;32m    242\u001b[0m                 config, callbacks\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mget_child(tag\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbranch:default\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    243\u001b[0m             ),\n\u001b[0;32m    244\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    245\u001b[0m         )\n\u001b[0;32m    246\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    247\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain_core\\runnables\\base.py:2876\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2874\u001b[0m context\u001b[39m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m   2875\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2876\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m context\u001b[39m.\u001b[39mrun(step\u001b[39m.\u001b[39minvoke, \u001b[39minput\u001b[39m, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2877\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2878\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m context\u001b[39m.\u001b[39mrun(step\u001b[39m.\u001b[39minvoke, \u001b[39minput\u001b[39m, config)\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain_core\\prompts\\base.py:179\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtags:\n\u001b[0;32m    178\u001b[0m     config[\u001b[39m\"\u001b[39m\u001b[39mtags\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config[\u001b[39m\"\u001b[39m\u001b[39mtags\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtags\n\u001b[1;32m--> 179\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_with_config(\n\u001b[0;32m    180\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_format_prompt_with_error_handling,\n\u001b[0;32m    181\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[0;32m    182\u001b[0m     config,\n\u001b[0;32m    183\u001b[0m     run_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    184\u001b[0m )\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain_core\\runnables\\base.py:1785\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m     context \u001b[39m=\u001b[39m copy_context()\n\u001b[0;32m   1782\u001b[0m     context\u001b[39m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m   1783\u001b[0m     output \u001b[39m=\u001b[39m cast(\n\u001b[0;32m   1784\u001b[0m         Output,\n\u001b[1;32m-> 1785\u001b[0m         context\u001b[39m.\u001b[39mrun(\n\u001b[0;32m   1786\u001b[0m             call_func_with_variable_args,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m             func,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m             \u001b[39minput\u001b[39m,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m             config,\n\u001b[0;32m   1790\u001b[0m             run_manager,\n\u001b[0;32m   1791\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1792\u001b[0m         ),\n\u001b[0;32m   1793\u001b[0m     )\n\u001b[0;32m   1794\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1795\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain_core\\runnables\\config.py:427\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[39mif\u001b[39;00m run_manager \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    426\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[1;32m--> 427\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain_core\\prompts\\base.py:154\u001b[0m, in \u001b[0;36mBasePromptTemplate._format_prompt_with_error_handling\u001b[1;34m(self, inner_input)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_format_prompt_with_error_handling\u001b[39m(\u001b[39mself\u001b[39m, inner_input: Dict) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[0;32m    153\u001b[0m     _inner_input \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_input(inner_input)\n\u001b[1;32m--> 154\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_prompt(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_inner_input)\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain_core\\prompts\\chat.py:765\u001b[0m, in \u001b[0;36mBaseChatPromptTemplate.format_prompt\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mformat_prompt\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[0;32m    757\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt. Should return a PromptValue.\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \n\u001b[0;32m    759\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[39m        PromptValue.\u001b[39;00m\n\u001b[0;32m    764\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 765\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_messages(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    766\u001b[0m     \u001b[39mreturn\u001b[39;00m ChatPromptValue(messages\u001b[39m=\u001b[39mmessages)\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain_core\\prompts\\chat.py:1207\u001b[0m, in \u001b[0;36mChatPromptTemplate.format_messages\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1203\u001b[0m     result\u001b[39m.\u001b[39mextend([message_template])\n\u001b[0;32m   1204\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m   1205\u001b[0m     message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)\n\u001b[0;32m   1206\u001b[0m ):\n\u001b[1;32m-> 1207\u001b[0m     message \u001b[39m=\u001b[39m message_template\u001b[39m.\u001b[39mformat_messages(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1208\u001b[0m     result\u001b[39m.\u001b[39mextend(message)\n\u001b[0;32m   1209\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain_core\\prompts\\chat.py:231\u001b[0m, in \u001b[0;36mMessagesPlaceholder.format_messages\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m value \u001b[39m=\u001b[39m (\n\u001b[0;32m    226\u001b[0m     kwargs\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariable_name, [])\n\u001b[0;32m    227\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptional\n\u001b[0;32m    228\u001b[0m     \u001b[39melse\u001b[39;00m kwargs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariable_name]\n\u001b[0;32m    229\u001b[0m )\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mlist\u001b[39m):\n\u001b[1;32m--> 231\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    232\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvariable \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariable_name\u001b[39m}\u001b[39;00m\u001b[39m should be a list of base messages, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m of type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    234\u001b[0m     )\n\u001b[0;32m    235\u001b[0m value \u001b[39m=\u001b[39m convert_to_messages(value)\n\u001b[0;32m    236\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_messages:\n",
      "\u001b[1;31mValueError\u001b[0m: variable chat_history should be a list of base messages, got {'chat_history': []} of type <class 'dict'>"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\n",
    "\"chat_history\": con_memory.load_memory_variables({}),\n",
    "\"input\":\"What is Cohere mainly used for?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5aa902-dbf5-4579-935b-4a2d7095978a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\Projects\\LLM Git Repos\\Langchain-RAG\\scripts\\chat_memory.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m response \u001b[39m=\u001b[39m retrieval_chain\u001b[39m.\u001b[39minvoke({\n\u001b[1;32m----> 2\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mchat_history\u001b[39m\u001b[39m\"\u001b[39m: con_memory\u001b[39m.\u001b[39;49mmemory_variables(),\n\u001b[0;32m      3\u001b[0m \u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mWhat is Cohere mainly used for?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m })\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\n",
    "\"chat_history\": con_memory.memory_variables(),\n",
    "\"input\":\"What is Cohere mainly used for?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7219ae72-7741-4427-8e0b-faea1e8cb2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\n",
    "\"chat_history\": con_memory.memory_variables,\n",
    "\"input\":\"What is Cohere mainly used for?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d3a06-fedd-434e-8424-e1909891b86e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': ['chat_history'],\n",
       " 'input': 'What is Cohere mainly used for?',\n",
       " 'context': [Document(metadata={'title': 'Introduction to Cohere on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-bedrock', 'relevance_score': 0.99497885}, page_content='Note: the list of features will continue to evolve and expand. To keep up with the latest, visit the Bedrock website.\\nCohere Models on Amazon Bedrock\\nList of Models\\nThe most up-to-date list of available Cohere’s models is on the Bedrock website, but here is the list at the time of writing:\\nGenerative models:\\nCommand R+\\nCommand R\\nCommand\\nCommand Light\\nEmbeddings models:\\nEmbed (English)\\nEmbed (Multilingual)\\nPricing Mechanism\\nThe pricing for Cohere models on Bedrock at the time of writing falls under two types:\\nOn-demand and batch\\nThis applies to default Cohere models on Bedrock. Pricing is based on tokens and broken down into:\\nPrice per 1,000 input tokens\\nPrice per 1,000 output tokens\\nCustomization (fine-tuning)\\nThis applies to fine-tuned Cohere models on Bedrock. Pricing is broken down into:\\nPrice to train 1,000 tokens\\nPrice to store each custom model per month\\nPrice to infer from a custom model per model unit per hour (with no-commit Provisioned Throughput pricing)\\nTo get the most updated pricing mechanism with further details, visit the Bedrock pricing page.\\nExample Use Cases\\nThe following are some example use cases that you can build on top of Cohere’s models on Bedrock.\\nGenerative AI: Build applications that can write product descriptions, help draft emails, suggest example press releases, and much more.\\nChat: Combine the power of text generation with a conversational interface. Build powerful chatbots and interactive knowledge assistants that give users accurate answers from connected enterprise data, citing sources if required.\\nSemantic Search and Retrieval: Easily build powerful search solutions in English and over 100 other languages using Cohere’s industry-leading Embed model. Cohere provides the Embed model, connectors to common enterprise data sources, and customization tools to maximize search relevance by domain.'),\n",
       "  Document(metadata={'title': 'Prompt Engineering Basics', 'source': 'https://cohere.com/llmu/prompt-engineering-basics', 'relevance_score': 0.9931229}, page_content=\"# RESPONSE Here is a list of potential FAQs based on the provided text: - **Q: What does the Cohere Platform offer to developers and organizations?** A: The Cohere Platform offers an API that provides access to advanced Large Language Models (LLMs) without requiring machine learning expertise. It simplifies the process by handling data curation, model development, training, and serving. - **Q: What are the key language processing capabilities of the Cohere Platform?** A: The platform offers two main capabilities: text generation and text embedding. Text generation involves completing a prompt with a stream of generated text, like writing a haiku. Text embedding returns a numerical representation of the semantic meaning of a text input, useful for sentiment analysis and other measurements. - **Q: How does the Cohere Platform benefit developers?** A: Developers can focus on creating valuable applications without getting bogged down by the complexities of building language processing capabilities from scratch. The platform handles the heavy lifting, allowing developers to save time and effort. - **Q: What is the difference between text generation and text embedding?** A: Text generation is about creating new text based on a prompt, like generating a haiku or continuing a story. Text embedding, on the other hand, translates text into a series of numbers that represent its semantic meaning, enabling quantitative analysis and understanding of the text's context. - **Q: Can I use the Cohere Platform for sentiment analysis?** A: Yes, the text embedding capability of the Cohere Platform is particularly useful for sentiment analysis. By converting text into numerical representations, you can quantitatively analyze and understand the sentiment or emotional tone expressed in a given piece of text. - **Q: Does the Cohere Platform require machine learning expertise to use?** A: No, the platform is designed to abstract away the complexities of machine learning. Developers can\"),\n",
       "  Document(metadata={'title': 'Introduction to Cohere on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-bedrock', 'relevance_score': 0.9897514}, page_content=\"Introduction to Cohere on Amazon BedrockQualified\\nContents\\nIntroduction\\nCohere's Model Deployment Options\\nWhat Is Amazon Bedrock?\\nFeatures of Amazon Bedrock\\nCohere Models on Amazon Bedrock \\nList of Models\\nPricing Mechanism\\nExample Use Cases\\nGetting Started with Amazon Bedrock \\nStep 1: Set Up Amazon Bedrock\\nStep 2: Choose Your Models\\nStep 3: Test on the Playground\\nConclusion\\nIntroduction\\nThe most common way to access Cohere’s large language models (LLMs) is through the Cohere platform, which is fully managed by Cohere and accessible through an API.\\nBut that’s not the only way to access Cohere’s models. In an enterprise setting, companies might require more control over where and how the models are hosted. The good news is that Cohere provides enterprises with flexibility and choice around how they want to deploy the models.\\nIn this module, Cohere on AWS, you’ll learn how to deploy Cohere’s LLMs through Amazon Web Services (AWS). In this first chapter of the module, you’ll get an overview of Amazon Bedrock, AWS’s fully managed service for deploying foundational models (FMs).\\nCohere's Model Deployment Options\\nSpecifically, Cohere offers four types of deployment options:\\nCohere’s SaaS Platform and API\\nThis is the fastest and easiest way to start using Cohere’s models. The models are hosted on Cohere infrastructure and available on our public SaaS platform (which provides an API data opt-out), which is fully managed by Cohere.\\nCloud AI Services (Managed)\\nThese managed services enable enterprises to access Cohere’s models while easily integrating the service and dealing with sensitive data. In this scenario, Cohere’s models are hosted on the cloud provider’s infrastructure. Cohere is cloud-agnostic, meaning you can deploy our models through any cloud provider. Examples include Amazon Bedrock, Amazon SageMaker, Google Vertex AI, OCI Generative AI, and Azure AI.\\nPrivate Deployment - Cloud\"),\n",
       "  Document(metadata={'title': 'Introduction to Text Generation', 'source': 'https://cohere.com/llmu/introduction-to-text-generation', 'relevance_score': 0.98577297}, page_content=\"Introduction to Text GenerationQualified\\nChatbots brought large language models (LLMs) into the mainstream. LLMs have been around for a few years, but their adoption was largely limited to the AI community. The launch of AI-powered consumer chatbots has made LLMs accessible to the everyday user, and now they're a hot topic in tech and enterprise circles alike.\\nThis text generation module teaches you how to build LLM chatbots using Cohere’s Chat endpoint.\\nCommand Model\\nCommand is Cohere’s flagship LLM. It generates a response given a prompt or message from a user. It is trained to follow user commands and to be instantly useful in practical business applications, like summarization, copywriting, extraction, and question answering.\\nCommand has been trained with a large volume of multi-turn conversations to ensure that it excels at the various nuances associated with conversational language. It ranks at the top of the Holistic Evaluation of Language Models (HELM) benchmark, an evaluation leaderboard comparing large language models on a wide number of tasks (March ‘23 results).\\nCommand R and Command R+ Models\\nCommand R and Command R+ are designed to be the market leading family of models in the ‘scalable’ category that balance high efficiency with strong accuracy to enable enterprises to move from proof of concept into production-grade AI.\\nHere are some key features of Command R:\\nHigh-performance RAG: Retrieval-augmented generation (RAG) enables enterprises to give the model access to private knowledge that it otherwise would not have.\\nAccess to tools: Tool use enables enterprise developers to turn Command R into an engine for powering the automation of tasks and workflows that require using internal infrastructure like databases and software tools, as well as external tools like CRMs, search engines, and more. Command R+ supports Multi-Step Tool Use which allows the model to combine multiple tools over multiple steps to accomplish difficult tasks.\"),\n",
       "  Document(metadata={'title': 'Prompt Engineering Basics', 'source': 'https://cohere.com/llmu/prompt-engineering-basics', 'relevance_score': 0.9795506}, page_content='generate_text(\"\"\" Given the following text, write down a list of potential frequently asked questions (FAQ), together with the answers. The Cohere Platform provides an API for developers and organizations to access cutting-edge LLMs without needing machine learning know-how. The platform handles all the complexities of curating massive amounts of text data, model development, distributed training, model serving, and more. This means that developers can focus on creating value on the applied side rather than spending time and effort on the capability-building side. There are two key types of language processing capabilities that the Cohere Platform provides — text generation and text embedding — and each is served by a different type of model. With text generation, we enter a piece of text, or prompt, and get back a stream of text as a completion to the prompt. One example is asking the model to write a haiku (the prompt) and getting an originally written haiku in return (the completion). With text embedding, we enter a piece of text and get back a list of numbers that represents its semantic meaning (we’ll see what “semantic” means in a section below). This is useful for use cases that involve “measuring” what a passage of text represents, for example, in analyzing its sentiment. \"\"\")')],\n",
       " 'answer': 'Cohere is mainly used for accessing advanced Large Language Models (LLMs) that provide capabilities for text generation and text embedding. This allows developers and organizations to build applications for various use cases, such as generating product descriptions, drafting emails, creating chatbots, and performing semantic search and retrieval. The platform simplifies the process of utilizing these models without requiring machine learning expertise, enabling users to focus on creating valuable applications.'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
