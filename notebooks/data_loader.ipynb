{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe76c5c6-d087-475f-b272-bff4b7c5e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_community.utilities import ApifyWrapper\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d4c994-b920-4d62-9970-554b2cc182de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the API keys\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df4d3ef-9052-4ebc-a386-ca99aa6ba975",
   "metadata": {},
   "outputs": [],
   "source": [
    "apify = ApifyWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0189a98-14d2-47c5-8582-023fb6b83a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = apify.call_actor(\n",
    "    actor_id=\"apify/website-content-crawler\",\n",
    "    run_input={\n",
    "        \"startUrls\": [{\"url\": \"https://cohere.com/llmu\"}],\n",
    "        \"maxCrawlDepth\": 2,\n",
    "    },\n",
    "    dataset_mapping_function=lambda dataset_item: Document(\n",
    "        page_content=(\n",
    "            dataset_item[\"text\"] if dataset_item[\"text\"] else \"No content available\"\n",
    "        ),\n",
    "        metadata={\n",
    "            \"title\": dataset_item[\"metadata\"][\"title\"],\n",
    "            \"source\": dataset_item[\"url\"],\n",
    "        },\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a415a-323d-4c37-a580-41d41c97ba5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ApifyDatasetLoader(apify_client=<apify_client.client.ApifyClient object at 0x0000023B3EC5E230>, dataset_id='cq8YtWSRERjS0G9Qq', dataset_mapping_function=<function <lambda> at 0x0000023B3EC41990>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146cde23-3f02-4402-8caa-faaebeba0bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce872c1b-8ab0-4da7-aee7-fe33692272e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'LLM University | Cohere', 'source': 'https://cohere.com/llmu'}, page_content='LLM University | CohereQualified\\nWelcome to LLM University, your premier learning destination for mastering Enterprise AI technologies. Designed for developers and technical professionals, our hub offers comprehensive resources, expert-led courses, and step-by-step guides to help you start building quickly and stay ahead in the rapidly evolving AI landscape.\\nJoin us to enhance your skills, drive innovation, and unlock the full potential of AI in your enterprise.'),\n",
       " Document(metadata={'title': 'Reranking', 'source': 'https://cohere.com/llmu/reranking'}, page_content=\"RerankingQualified\\nThis chapter uses the same notebook as the previous chapter.\\nIn previous chapters, you learned keyword search and dense retrieval, and you were able to apply them by querying a large Wikipedia article dataset. You noticed that keyword search performed well with some queries, and not so well with others. Dense retrieval, on the other hand, worked well with all the queries.\\nFor both, keyword search and dense retrieval, and in fact, for any other search mechanism we use, there is a very powerful method called reranking, which can enhance it. With Cohere, you can perform reranking using the Rerank endpoint.\\nReranking works as follows: For each pair (query, response), it assigns a relevance score. As the name hints, relevance scores are high for pairs in which the response is relevant to the query, and low otherwise. In this chapter, you’ll learn how to use Reranking to improve the wikipedia search results you found previously in this module.\\nUsing Rerank to Improve Keyword Search\\nReranking is a very powerful method which can significantly boost any existing search system. In short, reranking takes a query and a response, and outputs a relevance score between them. In that way, one can use any search system to surface a number of documents that can potentially contain the answer to a query, and then sort them using the Rerank endpoint.\\nThe results from any search system get reranked based on their relevance to the query\\nRemember that the results we obtained for the query “Who was the first person to win two Nobel prizes” using the keyword_search function were the following (for the full text, please check out the Colab notebook):\\nQuery: “Who was the first person to win two Nobel prizes?”\\nResponses:\\nNeutrino\\nWestern culture\\nReality television\\nThese could contain the answer somewhere in the document, but they are certainly not the best documents for this query. Let’s dig in a bit more, and find the first 100 results. To save space, I’ll only note the top 20 titles.\\nNeutrino\\nWestern culture\\nReality television\\nPeter Mullan\\nIndiana Pacers\\nWilliam Regal\\nNobel Prize\\nNobel Prize\\nNobel Prize\\nNoble gas\\nNobel Prize in Literature\\nD.C. United\\nNobel Prize in Literature\\n2021-2022 Manchester United F.C. season\\nNobel Prize\\nNobel Prize\\nZach LaVine\\n2011 Formula One World Championship\\n2021-2022 Manchester United F.C. season\\nChristians\\nOk, there’s a high chance that the answer is there. Let’s see if reranking can help us find it. The following function calls the Rerank endpoint. Its inputs are the query, the responses, and the number of responses we’d like to retrieve.\\ndef rerank_responses(query, responses, num_responses=3): reranked_responses = co.rerank( query = query, documents = responses, top_n = num_responses, model = 'rerank-english-v3.0', return_documents=True ) return reranked_responses \\nRerank will output the result, as well as the relevance score. Let’s look at the top 3 results.\\nQuery: “Who was the first person to win two Nobel prizes?”\\nResponses:\\nNobel Prize: “Five people have received two Nobel Prizes. Marie Curie received the …”\\nRelevance score: 1.00\\nNobel Prize: “In terms of the most prestigious awards in STEM fields, only a small …”\\nRelevance score: 0.97\\nNobel Prize in Literature: “There are also prizes for honouring the lifetime achievement of writers …”\\nRelevance score: 0.87\\nWell, that certainly improved the keyword search results! Even though the third result doesn’t work, the first two retrieved the correct article that contains the answer. Notice that the relevance score for both is close to 1.\\nConclusion\\nReranking is a very useful method to find the most relevant responses to a particular query. It is very useful as a way to improve keyword search for dense retrieval. In this lab, we used it to vastly improve the results of keyword search, by first using keyword search to retrieve 100 potential documents that may contain the answer, and then using the Rerank endpoint to retrieve the top 3 among those. We encourage you to try reranking to improve the other searches we performed in the previous labs, and check your results!\"),\n",
       " Document(metadata={'title': 'Generating Answers', 'source': 'https://cohere.com/llmu/generating-answers'}, page_content='Generating AnswersQualified\\nThis chapter uses the same notebook as the previous chapter.\\nIn this module, you’ve learned how to search and retrieve information from large databases in very effective ways. In this chapter, you’ll learn how to combine this with a generative model, in order to get an answer in sentence format, instead of a list of search results.\\nLarge Language Models, as you know, are very good at answering questions, but they are prone to some limitations, such as incorrect information, or even hallucinations. A good way to fix this is to enhance an LLM with a search mechanism.\\nIn short, this combination is done in the following way:\\nGiven a query, the search mechanism retrieves one or more documents containing the answer.\\nThese documents are given to the large language model, and it is instructed to generate an answer based on that information.\\nI like to imagine this the following way. If I have a question about thermodynamics, I can pick a random friend of mine, and ask them that question. They may or may not get the answer wrong. But if I go and search a few chapters in books about thermodynamics, I give them to my friend, and then I ask them to answer the question based on that, they are much more likely to answer the question correctly.\\nIn this chapter, we\\'ll compare two ways a large language model can answer a question. The first one is by feeding the question to the generative model, and obtaining an answer. This will generate an answer that could be correct, but it may not. This is equivalent to asking your friend a thermodynamics question.\\nA generative model receives a query and outputs a response. This response may be inaccurate.\\nThe second one starts by using a search system to retrieve documents where the answer to the query is likely to appear. Then we feed the question and the documents to a generative model, and prompt it to answer the question using these documents. This yields a more accurate response.\\nThe query is first given to a search system, which retrieves documents which are likely to contain the answer. Then the query and the documents (context) are fed to the generative model, for a more accurate response.\\nGenerating Answers (Without Search)\\nLet’s first use a generative model to answer a slightly harder question — without search. We are trying to find out how many people won more than one Nobel prize. So, we ask the model the following query.\\nQuery: “How many people have won more than one Nobel prize?”\\nThe answer to this question is five: Marie Curie, Linus Pauling, John Bardeen, Frederick Sanger, and Karl Barry Sharpless.\\nThe way to ask this to the model is with the following line of code, which calls the co.chat endpoint.\\nprediction_without_search = [ co.chat( message=query, max_tokens=50, ) for _ in range(5) ] \\nWe call the endpoint five times to get five responses. The max_tokens parameter determines the length of the answer (which is why some answers appear truncated).\\nResponses:\\nMarie Skłodowska-Curie, a Polish and naturalized French physicist and chemist, won the Nobel Prize twice. She received the Nobel Prize in Physics in 1903 along with her husband Pierre Curie and Henri Becquerel for their pioneering work\\nMarie Skłodowska-Curie, a Polish physicist and chemist, won the Nobel Prize twice: once in physics and once in chemistry. She was awarded the Nobel Prize in Physics in 1903 along with her husband, Pierre Curie, and\\nMarie Skłodowska-Curie, a Polish and naturalized French physicist and chemist, won the Nobel Prize in Physics in 1903 and the Nobel Prize in Chemistry in 1911. She is the only person to have won multiple\\nMarie Skłodowska-Curie, a Polish and naturalized French physicist and chemist, won the Nobel Prize twice. She received the Nobel Prize in Physics in 1903 along with her husband, Pierre Curie, and Henri Becquerel for their\\nMarie Skłodowska-Curie, a Polish and naturalized French physicist and chemist, is the only person to have won multiple Nobel prizes. She won the Nobel Prize in Physics in 1903 and the Nobel Prize in Chemistry in 19\\nThese answers sound like they could be correct, but they’re all wrong. One reason for this is that transformers are good at talking and understanding sentiment and nuisances of the language, etc., but not so good at storing information. As a matter of fact, storing information inside the nodes of the neural network is not something that we can (or should!) fully trust.\\nInstead, let’s first search for the answer using what we’ve learned in the previous sections of this post.\\nSearching Answers\\nIn order to find the answer to this question in the Wikipedia dataset (the one we’ve been working with throughout this post), we can use the same dense_retrieval function that we used before. For simplicity, we will only use dense retrieval without Rerank, but we invite you to add it to the lab and see how the results improve!\\nresponses = dense_retrieval(query, num_results=20) \\nThis retrieves the top 20 articles, with their corresponding paragraphs. Here are the top three (remember that the search is done by finding the most similar paragraphs to the query, so some articles may appear several times with different paragraphs).\\nResponses:\\nNobel Peace Prize: “, the Peace prize has been awarded to 110 individuals and 27 organizations …”\\nNobel Prize: “The strict rule against awarding a prize to more than three people is also controversial …”\\nNobel Prize: “The prize ceremonies take place annually …”\\nNext, we’ll feed these 20 paragraphs to a generative model, and instruct it to answer the question in sentence format.\\nGenerating an Answer from the Search Results\\nIn order to get the generative model to answer a question based on a certain context, we need to create a prompt. And in this prompt, we need to give it a command and a context. The context will be the concatenation of all the paragraphs retrieved in the search step, which we can obtain using this line of code:\\ncontext = [r[\\'text\\'] for r in responses] \\nThe array context contains a lot of text, and, given the good results we’ve been obtaining with search mechanisms, we are fairly confident that somewhere in this text lies the answer to our original question. Now, we invoke the Chat endpoint. The prompt we’ll use is the following.\\nprompt = f\"\"\" Use the information provided below to answer the questions at the end. If the answer to the question is not contained in the provided information, say \"The answer is not in the context\". --- Context information: {context} --- Question: How many people have won more than one Nobel prize? \"\"\" \\nIn other words, we’ve prompted the model to answer the question, but only from information coming from the context array. And if the information is not there, we are prompting the model to state that the answer is not in the context. The following line of code will run the prompt. As before, we generate 5 answers, and max_tokens controls the length of each answer.\\nprediction_with_search = [ co.chat( message=prompt, max_tokens=50) for _ in range(5)] \\nThe five responses we get are the following (just like before, they are truncated):\\nFive people have won multiple Nobel prizes, according to the information provided. However, the source also states that another individual, Lê Đức Thọ, refused to accept the prize, and is therefore not counted among the winners. This brings the total number of multiple\\nFive people have won multiple Nobel prizes, according to the information provided. However, the source also states that another individual, Lê Đức Thọ, refused to accept the prize, and is therefore not counted among the winners. This brings the total number of multiple\\nFive people have won multiple Nobel prizes, according to the information provided. However, the source also states that another individual, Lê Đức Thọ, refused to accept the prize, and is therefore not counted among the winners. This brings the total number of multiple\\nFive people have won multiple Nobel prizes, according to the information provided. However, the source also states that another individual, Linus Pauling, was awarded two prizes but these were of different types, in different years. Marie Curie is the only person to\\nFive people have won multiple Nobel prizes, according to the information provided. However, the source also states that another individual, Lê Đức Thọ, refused to accept the prize, and is therefore not counted among the winners. This brings the total number of multiple\\nAs you can see, this improved the quality of the answer. It got the right number of people who received more than one Nobel prize, which is 5.\\nConclusion\\nGenerative models are prone to hallucinations. For example, when asked a question, they may answer with an incorrect answer. In this chapter you learned to power a generative model with search, in order to generate more accurate answers and reduce the chance of hallucinations.'),\n",
       " Document(metadata={'title': 'Constructing Prompts', 'source': 'https://cohere.com/llmu/constructing-prompts'}, page_content='Constructing PromptsQualified\\nContents\\nIntroduction\\nSetting Up\\nInstruction\\nSpecifics\\nContext\\nFormat\\nExamples\\nChain of Thought\\nSteps\\nConclusion\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nWhen working with large language models (LLMs), the prompt is the key to getting the desired response. A well-designed prompt will result in useful and accurate responses from a model and will considerably improve your experience interacting with it.\\nPrompts can be as simple as a one-liner, or they can be as complex as multiple layers of specific information. The more specific your command is, the more likely you will get exactly what you need from the model. We’ll look at some tips and ideas for constructing the commands in your prompt to help you get to your intended outcome. We’ll focus on the broad patterns without going into the long-tail list of techniques and tricks.\\nSetup\\nFirst, let’s install the Cohere Python SDK, get the Cohere API key, and set up the client.\\n! pip install cohere import cohere co = cohere.Client(\"COHERE_API_KEY\") # Your Cohere API key \\nLet’s also define a function to take a prompt and a temperature value and then call the Chat endpoint, which is how we can access the Command model.\\u200b\\u200b Here, we select the model type to be command.\\nWe set a default temperature value of 0, which nudges the response to be more predictable and less random. Throughout this article, you’ll see different temperature values being used in different situations. Increasing the temperature value tells the model to generate less predictable responses and instead be more “creative.”\\nThis function prints the text response generated by the model.\\ndef generate_text(prompt, temp=0): response = co.chat_stream( message=prompt, model=\"command-r-plus\", temperature=temp) for event in response: if event.event_type == \"text-generation\": print(event.text, end=\\'\\') \\nInstruction\\nAdding basic instructions to a prompt\\nWhile prompts can morph into something very lengthy and complex, it doesn’t have to be that way all the time. At its core, prompting a Command model is about sending an instruction to a text generation model and getting a response back. Hence, the smallest unit of a perfectly complete prompt is a short line of instruction to the model.\\nLet’s say we want to generate a product description for a wireless headphone. Here’s an example prompt, where we create a variable for the user to input some text and merge that into the main prompt.\\nuser_input = \"a wireless headphone product named the CO-1T\" prompt = f\"\"\"Write a creative product description for {user_input}\"\"\" generate_text(prompt, temp=0.5) \\nThe model returns the following sample response, which does the job we asked for.\\nLooking for a wireless headphone option that won\\'t break the bank? Look no further than the CO-1T! These sleek headphones offer crystal clear sound and a comfortable, snug fit. With their Bluetooth capabilities, you can enjoy your favorite tunes without any distractions, whether you\\'re on the go or just relaxing at home. The CO-1T is also designed with a built-in microphone, making it easy to switch between listening to music and answering calls with minimal effort. But it doesn\\'t stop there – these headphones have an incredible battery life that will last you all day long, so you don\\'t have to worry about charging them constantly. Overall, the CO-1T offers a convenient, versatile, and affordable solution to all your wireless headphone needs. Meanwhile, the design is a stylish blend of black, gray, and silver that boasts a professional and modern look. \\nSpecifics\\nAdding specific details to a prompt\\nA simple and short prompt can get you started, but in most cases, you’ll need to add specificity to your instructions. A generic prompt will return a generic response, and in most cases, that’s not what we want. In the same way that specific instructions will help humans do our job well, a model needs to be supplied with specific details to guide its response.\\nGoing back to the previous prompt, the generated product description was great, but what if we wanted it to include specific things, such as its features, who it is designed for, and so on? We can adjust the prompt to take more inputs from the user, like so:\\nuser_input_product = \"a wireless headphone product named the CO-1T\" user_input_keywords = \\'\"bluetooth\", \"wireless\", \"fast charging\"\\' user_input_customer = \"a software developer who works in noisy offices\" user_input_describe = \"benefits of this product\" prompt = f\"\"\"Write a creative product description for {user_input_product}, \\\\ with the keywords {user_input_keywords} for {user_input_customer}, and describe {user_input_describe}.\"\"\" generate_text(prompt, temp=0.5) \\nIn the example above, we pack the additional details of the prompt in a single paragraph. Alternatively, we can also compose it to be more structured, like so:\\nuser_input_product = \"a wireless headphone product named the CO-1T\" user_input_keywords = \\'\"bluetooth\", \"wireless\", \"fast charging\"\\' user_input_customer = \"a software developer who works in noisy offices\" user_input_describe = \"benefits of this product\" prompt = f\"\"\"Write a creative product description for {user_input_product}. Keywords: {user_input_keywords} Audience: {user_input_customer} Describe: {user_input_describe}\"\"\" generate_text(prompt, temp=0.5) \\nAnd here’s an example response. This time, the product description is tailored more specifically to our desired target customer, includes the key features that we specified, and sprinkles benefit statements throughout — all coming from the instruction we added to the prompt.\\nAre you a software developer struggling to focus in the hustle and bustle of noisy offices? The CO-1T wireless headphones are the ideal solution for you! With its Bluetooth connectivity and wireless design, you can enjoy uninterrupted music and podcast streaming from your devices, whether you\\'re working or kicking back. One of the CO-1T\\'s standout features is its fast charging capabilities, ensuring that you\\'re never without your trusted noise-canceling companion. A quick recharge can provide you with a bustling day\\'s worth of power, so you\\'re never left lacking when inspiration (or your power bar) runs low. Designed with functionality and form in mind, the CO-1T delivers crisp and clear audio while its ergonomic design showcases a sleek and stylish look. Comfortable ear pads that won\\'t irritate or pinch you guarantee comfort over long periods of use. Take control of your audio experience and stay focused on the tasks at hand with the CO-1T wireless headphones. Whether you\\'re tuning out distractions or simply kicking back with some music, this Bluetooth headphone is the ideal companion for programmers and audiophiles alike. \\nThere are many other angles to add specificity to a prompt. Here are some examples:\\nStyle: Telling the model to provide a response that follows a certain style or framework. For example, instead of asking the model to “Generate an ad copy for a wireless headphone product” in the generic sense, we ask it to follow a certain style, such as “Generate an ad copy for a wireless headphone product, following the AIDA Framework – Attention, Interest, Desire, Action.”\\nTone: Adding a line mentioning how the tone of a piece of text should be, such as professional, inspirational, fun, serious, and so on. For example, “Tone: casual”\\nPersona: Telling the model to act like a certain persona helps to add originality and quality to the response. For example, “You are a world-class content marketer. Write a product description for…”\\nLength: Telling the model to generate text with a specific length, be it in words, paragraphs, and others. This helps guide the model to be verbose, concise, or somewhere in between. For example, “Write in three paragraphs the benefits of …”\\nContext\\nAdding contextual information to a prompt\\nWhile LLMs excel in text generation tasks, they struggle in context-aware scenarios. Here’s an example. If you were to ask the model for the top qualities to look for in wireless headphones, it will duly generate a solid list of points. But if you were to ask it for the top qualities of the CO-1T headphone, it will not be able to provide an accurate response because it doesn’t know about it (CO-1T is a hypothetical product we just made up for illustration purposes).\\nIn real applications, being able to add context to a prompt is key because this is what enables personalized generative AI for a team or company. It makes many use cases possible, such as intelligent assistants, customer support, and productivity tools, that retrieve the right information from a wide range of sources and add it to the prompt.\\nThis is a whole topic on its own, but to provide some idea, this demo shows an example of information retrieval in action. In this article though, we’ll assume that the right information is already retrieved and added to the prompt.\\nHere’s an example where we ask the model to list the features of the CO-1T wireless headphone without any additional context:\\nuser_input =\"What are the key features of the CO-1T wireless headphone\" prompt = user_input generate_text(prompt, temp=0) \\nThis generates a response that the model makes up since it doesn’t have any information to refer to.\\nThe CO-1T wireless headphone is a high-quality, comfortable, and durable headphone that is designed for use with a variety of devices. It features a sleek and modern design, a comfortable and secure fit, and a high-quality sound. The CO-1T is also equipped with a variety of features, including a built-in microphone, a multi-function button, and a rechargeable battery. \\nAnd here’s the same request to the model, this time with the product description of the product added as context.\\ncontext = \"\"\"Think back to the last time you were working without any distractions in the office. That\\'s right...I bet it\\'s been a while. \\\\ With the newly improved CO-1T noise-cancelling Bluetooth headphones, you can work in peace all day. Designed in partnership with \\\\ software developers who work around the mayhem of tech startups, these headphones are finally the break you\\'ve been waiting for. With \\\\ fast charging capacity and wireless Bluetooth connectivity, the CO-1T is the easy breezy way to get through your day without being \\\\ overwhelmed by the chaos of the world.\"\"\" user_input = \"What are the key features of the CO-1T wireless headphone\" prompt = f\"\"\"{context} Given the information above, answer this question: {user_input}\"\"\" generate_text(prompt, temp=0) \\nNow, the model accurately lists the features of the model.\\nThe CO-1T wireless headphones have noise-cancellation, fast charging capacity, and Bluetooth connectivity. \\nFormat\\nAdding output format requirements to a prompt\\nSo far, we saw how to get the model to generate responses that follow certain styles or include specific information. But we can also get the model to generate responses in a certain format. Let’s look at a couple of them: markdown tables and JSON strings.\\nHere, the task is to extract information from a list of invoices. Instead of providing the information in plain text, we can prompt the model to generate a table that contains all the information required.\\nprompt=\"\"\"Turn the following information into a table with columns Invoice Number, Merchant Name, and Account Number. Bank Invoice: INVOICE #0521 MERCHANT ALLBIRDS ACC XXX3846 Bank Invoice: INVOICE #6781 MERCHANT SHOPPERS ACC XXX9877 Bank Invoice: INVOICE #0777 MERCHANT CN TOWER ACC XXX3846 \"\"\" generate_text(prompt, temp=0) \\nThe response will come in the form of a markdown table.\\nHere is a table formatted with the columns of Invoice Number, Merchant Name, and Account Number filling in the previous information: | Invoice Number | Merchant Name | Account Number | | --- | --- | --- | | INVOICE #0521 | ALLBIRDS | XXX3846 | | INVOICE #6781 | SHOPPERS | XXX9877 | | INVOICE #0777 | CN TOWER | XXX3846 | Let me know if you\\'d like me to make any modifications to this table or provide you with any further assistance! \\nAnother useful format is JSON, which we can modify the prompt as follows.\\nprompt=\"\"\"Turn the following information into a JSON string with the following keys: Invoice Number, Merchant Name, and Account Number. Bank Invoice: INVOICE #0521 MERCHANT ALLBIRDS ACC XXX3846 Bank Invoice: INVOICE #6781 MERCHANT SHOPPERS ACC XXX9877 Bank Invoice: INVOICE #0777 MERCHANT CN TOWER ACC XXX3846 \"\"\" generate_text(prompt, temp=0) \\nThis returns the following response.\\nCertainly, here is the JSON format of the three bank invoices: ```json [ { \"invoice_number\": \"INVOICE #0521\", \"merchant_name\": \"ALLBIRDS\", \"account_number\": \"XXX3846\" }, { \"invoice_number\": \"INVOICE #6781\", \"merchant_name\": \"SHOPPERS\", \"account_number\": \"XXX9877\" }, { \"invoice_number\": \"INVOICE #0777\", \"merchant_name\": \"CN TOWER\", \"account_number\": \"XXX3846\" } ] \\nExamples\\nAdding examples to a prompt\\nAll our prompts so far use what is called zero-shot prompting, which means that we are providing instruction without any example. But in many cases, it is extremely helpful to provide examples to the model to guide its response. This is called few-shot prompting.\\nFew-shot prompting is especially useful when we want the model response to follow a certain style or format. Also, sometimes it is hard to explain what you want in an instruction, and easier to show examples.\\nLet’s use an example task, where a model should take a request coming from a human and rephrase it into the most accurate utterance that an AI virtual assistant should use. The example data is taken from this paper (Einolghozati, et al. 2020).\\nWe’ll use this example request: “Send a message to Alison to ask if she can pick me up tonight to go to the concert together”. Given that request, we should expect the rephrased utterance to be something like: “Can you pick me up tonight to go to the concert together?”\\nFirst, let’s generate a response without giving the model an example. Here’s the prompt:\\nprompt=\"\"\"Turn the following message to a virtual assistant into the correct action: Send a message to Alison to ask if she can pick me up tonight to go to the concert together\"\"\" generate_text(prompt, temp=0) \\nThe response we get is not wrong, but it doesn’t follow the style that we need, which is a simple one-line rephrasing of the original request. Instead, it generates an email!\\nOk, I will send a message to Alison asking if she can pick you up tonight to go to the concert together. Please provide me with the details of your inquiry so I can assist you better! \\nNow, let’s modify the prompt by adding a few examples of how we expect the output to be.\\nuser_input = \"Send a message to Alison to ask if she can pick me up tonight to go to the concert together\" prompt=f\"\"\"Turn the following message to a virtual assistant into the correct action: Message: Ask my aunt if she can go to the JDRF Walk with me October 6th Action: can you go to the jdrf walk with me october 6th Message: Ask Eliza what should I bring to the wedding tomorrow Action: what should I bring to the wedding tomorrow Message: Send message to supervisor that I am sick and will not be in today Action: I am sick and will not be in today Message: {user_input}\"\"\" generate_text(prompt, temp=0) \\nThis time, the style of the response is exactly how we want it.\\ncan you pick me up tonight to go to the concert together? \\nChain of Thought\\nOne specific way to provide examples in a prompt is to show responses that include a reasoning step. This way, we are asking the model to “think” first rather than going straight to the response. In tasks involving mathematical questions, for example, there is a huge difference between directly giving the answer and adding a reasoning step in between.\\nThis concept is called chain of thought prompting, introduced by Wei et al. Let’s look at an example from the paper which illustrates this idea.\\nFirst let’s look at a prompt without a chain of thought. It contains one example of a question followed by the answer, without any intermediate calculation step. It also contains the new question we want to answer.\\nprompt=f\"\"\"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. \\\\ How many tennis balls does he have now? A: The answer is 11. --- Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? A: The answer is 9. --- Q: A box has 10 balls and a half of the balls are red balls. How many red balls are in the box if 4 red balls are added? A:\"\"\" generate_text(prompt, temp=0) \\nWe get the following response, which is an incorrect answer (12). And notice that the response gives the answer first, similar to the example shown.\\nThe answer is 12. There are 5 balls that are red and 5 balls that are not red, and adding 4 red balls would make it 9 red and 6 not red. \\nNow, let’s repeat that, this time with a chain of thought. Now, the example answer contains a reasoning step, describing the calculation logic to get to the final answer, before giving the final answer.\\nprompt=f\"\"\"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. \\\\ How many tennis balls does he have now? A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. \\\\ The answer is 11. --- Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? A: The cafeteria started with 23 apples. They used 20 to make lunch, so they have 23 - 20 = 3 apples. They bought 6 more apples, so they have 3 + 6 = 9 apples. \\\\ The answer is 9. --- Q: A box has 12 balls and a half of the balls are red balls. How many red balls are in the box if 4 red balls are added? A:\"\"\" generate_text(prompt, temp=0) \\nAnd we get the correct answer this time (10), with the response following the style of the example given.\\nThere are 12 / 2 = 6 red balls in the box. When 4 more red balls are added, the number of red balls becomes 6 + 4 = 10. The answer is 10. \\nSteps\\nAdding generation steps to a prompt\\nTo steer the model toward generating higher-quality responses, it can be helpful to add instructions for the model to generate intermediate steps before generating the final output. The information generated during these steps helps enrich the model’s context before it generates the final response.\\nThere could be another scenario where we specifically need the response to contain specific information. In this case, we can specify what we need in the prompt.\\nLet’s use an example of generating startup ideas. We can get the model to directly generate an idea for a given industry, like so:\\nuser_input = \"education\" prompt = f\"\"\"Generate a startup idea for this industry: {user_input}\"\"\" generate_text(prompt, temp=0.5) \\nThis generates the following response, which is reasonable, but perhaps not exactly what we are looking for.\\nHere is a startup idea for the education industry: \"MathsMadeEasy\" Description: \"MathsMadeEasy\" is an innovative K-12 ed-tech startup that aims to revolutionize the way young students learn mathematics. With a vision to eliminate math-related anxiety and foster a deep understanding of mathematical concepts, our platform offers a holistic learning experience that engages, empowers, and excites students on their mathematical journey. Approach & Features: 1. Gamified Learning: MathsMadeEasy infuses gaming elements into its interactive math lessons, turning repetitions into fun challenges and keeping students motivated to achieve their goals. 2. Personalized Learning: Using adaptive algorithms... \\nAlternatively, we can ask the model to generate information in steps, such as describing the problem to be solved and the target audience experiencing this problem.\\nuser_input = \"education\" prompt = f\"\"\"Generate a startup idea for this industry: {user_input} First, describe the problem to be solved. Next, describe the target audience of this startup idea. Next, describe the startup idea and how it solves the problem for the target audience. Next, provide a name for the given startup. Use the following format: Industry: <the given industry> The Problem: <the given problem> Audience: <the given target audience> Startup Idea: <the given idea> Startup Name: <the given name>\"\"\" generate_text(prompt, temp=0.9) \\nThis provides a richer description of the startup idea.\\nIndustry: Education The Problem: Many people today struggle to develop meaningful relationships where they can cultivate connections and get support for their educational needs. Audience: Recent college graduates and young professionals seeking community and educational support. Startup Idea: Online community forum and networking platform specifically curating mentorship and guidance on continuing education, relevant career skills, and personal development. Startup Name: \"Nurture Nest\" Our platform offers a diverse range of discussion forums... \\nConclusion\\nIn this article, we looked at some techniques for constructing prompts for the Command model. A prompt can be as simple as a single line of instruction, though the more specific the prompt is, the higher the level of quality and accuracy you can expect from the response. Each building block added a prompt provides a different type of lever to enhance the quality of the response.'),\n",
       " Document(metadata={'title': 'Use Case Patterns', 'source': 'https://cohere.com/llmu/use-case-patterns'}, page_content='Use Case PatternsQualified\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nCohere’s Command model is an instruction-following text generation model trained to follow user commands. It is trained to be instantly useful in practical applications, covering a wide range of use cases.\\nHaving said that, if you are a developer just starting with this technology, it can be daunting to try to comprehend what’s possible. Large language models (LLMs) like the Command model are general-purpose and can be applied in infinite ways, but if one can’t recognize the patterns where they can be useful, it can feel overwhelming.\\nIn this chapter, we’ll go through several broad use case categories for the Command model. Though they won’t cover all the possible ways that you can use the model, they are good starting points for understanding the patterns of tasks where the model works well.\\nWe’ll go through the following use cases:\\nWriting\\nQuestion Answering\\nBrainstorming\\nTransforming\\nSummarizing\\nRewriting\\nExtracting\\nClassifying\\nSetting Up\\nThe examples in this post will be shown in Python. For each use case, we’ll look at some ideas on how a prompt can be constructed and the associated model settings. This blog post comes with a Google Colaboratory notebook that lets you get hands-on with the code.\\nFirst, let’s install the Cohere package, get the Cohere API key, and set up the client.\\n! pip install cohere import cohere co = cohere.Client(\"COHERE_API_KEY\") # Your Cohere API key \\nLet’s also define a function to take a prompt and a temperature value and then call the Chat endpoint, which is how we can access the Command model.\\u200b\\u200b We set a default temperature value of 0, which nudges the response to be more predictable and less random. This function returns the text response generated by the model.\\ndef generate_text(prompt, temp=0): response = co.chat_stream( message=prompt, model=\"command-r\", temperature=temp, preamble=\"\") for event in response: if event.event_type == \"text-generation\": print(event.text, end=\\'\\') \\nOur examples will revolve around a company’s activities for launching a new wireless headphone product, such as getting the word out, managing customer interactions, and so on. For this, let’s define a text snippet containing the product description. We’ll be utilizing this snippet in several examples throughout this article.\\nproduct=\"\"\"The CO-1T is a wireless headphone product that uses Bluetooth technology to connect to your devices. \\\\ It has a long battery life and can be quickly charged using the included USB cable. The headphone is \\\\ lightweight and comfortable, ideal for long periods of use. It has a built-in microphone for making calls, \\\\ and a button to control the volume. The CO-1T is a great choice for anyone looking for a wireless headphone \\\\ product with great battery life.\"\"\" \\nWriting\\nWe’ll start with the most general type of use case, which is writing.\\nHere we can ask the model to write freeform text, for example, with this prompt: “Create an email about the launch of the wireless headphone product”. But that alone might not be very useful in practical applications because the generated text can go in very different directions. We may want to add more context and specificity to the prompt.\\nHere’s how we can do that. Let’s say we’re building an application for users to enter some bullet points and get a complete email written. We can set up the prompt in the following way: create a variable for the user to input some text and merge that, together with the product description, into the main prompt.\\nuser_input =\"\"\" - announce product launch - create a call to action - mention live chat for support \"\"\" prompt = f\"\"\"{product} Create an email about the product above mentioning the following: {user_input} \"\"\" response = generate_text(prompt, temp=0.5) print(response) \\nAdding this context helps to guide the model in the direction we want it to go. Here is a sample response:\\nWe are thrilled to announce the launch of our newest wireless headphones, the CO-1T! These headphones are designed to offer a seamless and convenient listening experience with the use of Bluetooth technology, giving you the freedom to connect to your devices wirelessly. The CO-1T is crafted to be comfortable and lightweight, making it the perfect companion for your daily commutes or workouts. With a long-lasting battery life, you\\'ll never have to worry about running out of charge during your day. Plus, a quick charge feature ensures you\\'re always ready to go. Simply use the included USB cable for efficient charging. Control your volume and manage your calls effortlessly with the built-in microphone and easy-to-use button controls. Experience the convenience of wireless connectivity and immerse yourself in crystal-clear audio. We believe our product will provide an exceptional listening experience and enhance your daily routine. Try it out today and discover the convenience of wireless freedom! If you have any questions or need further assistance, our team is here to help. Feel free to reach out to us through our live chat feature on our website, and we\\'ll be happy to assist you. Thank you, and we hope you enjoy the CO-1T wireless headphones! \\nIn this example, we changed the temperature value to 0.5. And throughout this article, you’ll see different temperature values being used in different situations. Increasing the temperature value tells the model to generate less predictable responses and instead be more “creative”. And since this task does require some form of creativity (expanding brief keywords into a full passage), we increased the temperature value. There is no one right value for a use case, so this is a setting you should try and iterate to get the best outcome for your task.\\nWe have looked at one example, but writing as a use case can mean many different things. So instead of beginning your prompt with “Create,” you can try other ways to extract different forms of content and formats that your application needs, such as using “Explain,” “Generate,” “Write,” “Provide,” “Design,” “Describe,” and so on.\\nQuestion Answering\\nThis use case is about answering a question that a user asks, be it in a single-turn, question answering scenario or a multi-turn, chatbot setting.\\nQuestion answering can take place in either a closed or open setting. In a closed-book question answering setting, we rely on the model to answer questions based on the general knowledge from which it has been trained. Here’s one example:\\nuser_input =\"What features should I consider when choosing a wireless headphone\" prompt = user_input response = generate_text(prompt, temp=0.5) print(response) \\nSample response:\\nThere are several features to consider when choosing a wireless headphone that best suits your needs: 1. Sound Quality: This is a primary factor for many people. Look for headphones with clear and balanced audio reproduction, good bass, and a wide frequency response. Some brands are known for their audio expertise and use high-quality drivers for superior sound. 2. Noise Cancellation: Wireless headphones with active noise cancellation (ANC) technology are excellent for reducing ambient noise and enhancing focus on your music or calls. They use microphones to detect external sound and produce an opposite signal to cancel it out. 3. Connectivity: Ensure the wireless headphones have Bluetooth compatibility with your devices, such as smartphones, tablets, or laptops. Some headphones also support multi-device pairing, allowing seamless connection switching. 4. Battery Life: Longer battery life ensures you won\\'t be frequently charged. Look for headphones that offer at least 15-20 hours of continuous playback on a single charge. Also, check the charging time and consider if the headphone supports fast charging. 5. Comfort and Design: Consider the comfort of the ear pads and the overall design. Look for ergonomic designs, adjustable headbands, and lightweight construction for extended wear. Also, choose between in-ear, on-ear, or over-the-ear models based on your preference and portability. 6. Controls and Microphone: Easy-to-use controls for playback, volume, and calls are essential. A good microphone quality is crucial for clear calls and voice commands. Some headphones have touch-sensitive controls on the earcups, while others have physical buttons. 7. Voice Assistant Integration: If you\\'re into voice commands, look for headphones with built-in support for virtual assistants like Siri or Google Assistant. This allows you to control various functions hands-free. 8. Multi-Point Connection: This feature lets you connect your headphones simultaneously to multiple devices. This way, you can switch between your laptop and smartphone effortlessly. 9. Water and Sweat Resistance: If you plan to use your headphones during workouts or in rainy weather, consider a model with an IPX rating for water resistance. 10. Price and Warranty: Determine your budget and check the warranty period offered by the manufacturer for any potential issues. Remember to read reviews and guides from reputable sources and consider trying them out, if possible, to ensure a comfortable and enjoyable listening experience. \\nThis works well for a question that is open-ended and doesn’t have one correct answer. But let’s say we have a customer asking for specific information about the wireless headphone, such as its feature set. For this, we need to turn to an extractive question answering setting.\\nIn this setting, we can get the model to refer to specific knowledge bases to help it do its job well. This way, we can design a system that can handle questions that require factual responses.\\nHere, a customer asks a product question. We can append the customer’s question and the product description to the prompt, as follows.\\nuser_input =\"How do I control the sound levels\" prompt = f\"\"\"{product} Answer this question based on the context provided above: {user_input}\"\"\" generate_text(prompt, temp=0) \\nAnd the model picks up the relevant information from the prompt to provide the correct answer.\\nSample response:\\nYou can control the volume using the button built into the headphones. \\nBrainstorming\\nAnother form of writing is brainstorming, where we want the model to generate a list of options based on a given prompt. This can be for writing outlines, generating ideas, providing critical analysis, and so on. This use case forces the model to go broad and cover different perspectives of a situation.\\nIn this example, we want the model to act as an assistant to a customer support agent in identifying possible ways to troubleshoot a technical problem that a customer is facing.\\nuser_input = \"I can\\'t get the Bluetooth connection working\" prompt = f\"\"\"{product} A customer provided the following complaint about this product: {user_input}. Provide a bulleted list of possible ways to troubleshoot so we can advise the customer accordingly. \"\"\" response = generate_text(prompt, temp=0.3) print(response) \\nSample response:\\nHere is a list of possible ways to troubleshoot the customer\\'s problem with the Bluetooth connection for the CO-1T wireless headphones: - Suggest forgetting the existing Bluetooth connection and reinstalling it from scratch - Recommend updating the software of the device they are trying to connect to - Inquiry about whether the customer\\'s device is compatible with the headphone\\'s Bluetooth profile - Suggest a factory reset of the headphones themselves, but warn that this option should only be used if other troubleshooting steps fail. Let me know if you would like me to provide more options to troubleshoot potential problems with the CO-1T headphones. \\nTransforming\\nThe first thing that comes to mind when thinking about generative models is their ability to write a fresh piece of text, but one aspect that is rather understated is their ability to synthesize an existing piece of text.\\nOne example is transforming a passage of text into a different form, making it reusable for different purposes.\\nFor example, creating a list of Frequently Asked Questions (FAQs) about wireless headphones is crucial, but it requires some effort to create. We can cut short this process by getting the model to generate a list of FAQs based on the product description, as follows:\\nprompt =f\"\"\"Turn the following product description into a list of frequently asked questions (FAQ). Product description: {product} \"\"\" response = generate_text(prompt, temp=0) print(response) \\nSample response:\\nHere is the product description turned into an FAQ format: FAQ: Q: How does the CO-1T connect to my devices? A: The CO-1T uses Bluetooth technology to connect wirelessly to your smartphones, laptops, or other compatible devices. Q: What is the battery life like? A: The CO-1T has an excellent battery life that will last you long periods of use. It also charges quickly using the included USB cable, so you won\\'t have to wait long to get back to listening. Q: Is it comfortable to wear? A: Absolutely! The CO-1T headphones are lightweight and designed to be comfortable for extended periods. They are perfect for listening to music, taking calls, or using them for work. Q: Does it have a microphone and volume control? A: Yes, the headphone has a built-in microphone, making it easy to take calls on the go. There\\'s also a button that allows you to adjust the volume according to your preference. Q: Why should I choose the CO-1T over other wireless headphones? A: Besides the excellent battery life and comfortable design, the CO-1T is a great choice for anyone looking for a reliable and convenient wireless headphone option at an affordable price. It\\'s quick to charge, easy to use, and perfect for everyday use. \\nSummarizing\\nOne popular use case for synthesizing text is summarization. Here we take a long passage of text and summarize it to its essence. These can be articles, conversation transcripts, reports, meeting notes, and so on.\\nIn this example, we create a prompt to summarize a list of customer reviews about the wireless headphone.\\nuser_input =\"\"\"Customer reviews of the CO-1T wireless headphones: \"The CO-1T is a great pair of headphones! The design is sleek and modern, and the headphones are \\\\ very comfortable to wear. The sound quality is excellent, and I can hear every detail of my music. \\\\ The built-in microphone means I can make calls without having to take my phone out of my pocket. I \\\\ highly recommend the CO-1T to anyone looking for a great pair of wireless headphones!\" \"I\\'m very disappointed with the CO-1T. The design is nice, but the battery life is terrible. I can \\\\ only use them for a few hours before they need to be recharged. This is very inconvenient, and I\\'m \\\\ not sure if I can recommend them to anyone.\" \"The CO-1T is a mixed bag. The speaker quality is great, but the built-in microphone\\'s quality is \\\\ poor. I can hear every detail of my music, but my voice sounds distorted when I make calls. The \\\\ design is nice, and the headphones are comfortable to wear, but the battery life is only average. \\\\ I\\'m not sure if I can recommend them to anyone.\" \"\"\" prompt = f\"\"\"Summarize the following. {user_input} \"\"\" response = generate_text(prompt, temp=0) print(response) \\nExample response:\\nThe CO-1T wireless headphones have a sleek, modern design and are comfortable to wear. They offer excellent sound quality, with clear and detailed audio. However, the reviews are mixed when it comes to the built-in microphone\\'s quality, with some users reporting distortion. The battery life is a major downside, lasting only a few hours on a single charge. Overall, the CO-1T seems like a decent option for those prioritizing sound quality and comfort, but the mediocre battery life and inconsistent microphone quality might be dealbreakers for some. \\nRewriting\\nRewriting text is another useful use case where you need to modify some aspects of the text while maintaining its overall meaning. One example is changing the tone of a piece of text to tailor it to a specific audience. Here we want to rewrite the product description so it’s more relatable to students.\\nuser_input = \"college students\" prompt = f\"\"\"Create a version of this product description that\\'s tailored towards {user_input}. {product}\"\"\" response = generate_text(prompt, temp=0.5) print(response) \\nSample response:\\nThe CO-1T headphones are the ultimate wireless companion for any college student. With Bluetooth capabilities, you can easily connect to your laptop, phone, or tablet without the hassle of tangling cords. Study for long hours without discomfort or interruption - the lightweight design ensures comfort and the long battery life keeps you connected throughout the day. Quick charge the batteries using the included USB cable when you\\'re running short on time, and use the built-in microphone to stay connected with your friends and family. Control your audio easily with the volume button, and focus on what really matters - acing your courses! \\nAnother extremely useful way of looking at text synthesis is information extraction. Here we leverage the model’s ability to capture the context of a piece of text to extract the right information as specified by the prompt.\\nHere is an example of an email that a customer is, unfortunately, asking for a refund for the wireless headphone. We can have the model process this email by getting it to extract information, such as the product name, refund reason, and pick-up address. \\nuser_input =\"\"\"I am writing to request a refund for a recent CO-1T purchase I made on your platform. \\\\ Unfortunately, the produce has not met my expectations due to its poor battery life. \\\\ Please arrange for the pick-up at this address: to 171 John Street, Toronto ON, M5T 1X2.\"\"\" prompt =f\"\"\"Extract the product, refund reason and pick-up address from this email: {user_input} \"\"\" response = generate_text(prompt, temp=0) print(response) \\nSample response:\\nProduct: CO-1T Refund Reason: Poor battery life Pick-up Address: 171 John Street, Toronto ON, M5T 1X2 \\nClassifying\\nOne of the most widely deployed use cases in NLP is text classification. Here, the task is to classify a piece of text into one of a few predefined classes. In this example, we want to classify incoming customer messages into one of three categories: Order, Support, or Refunds.\\nWe can create the prompt as follows.\\nuser_input =\"\"\"The battery drains fast\"\"\" prompt = f\"\"\"The following is a user message to a customer support agent. Classify the message into one of the following categories: Order, Support, or Refunds. {user_input} \"\"\" response = generate_text(prompt, temp=0) print(response) \\nHere’s a sample response where the generative model correctly classifies the text into the right category:\\nBased on the information provided in the message, this inquiry falls under the category of \"Support.\" This is an issue that the customer is encountering with a product that they have already purchased, and they are seeking assistance and guidance from the customer support agent regarding this issue. The customer is reporting a problem with a product they purchased and is asking for help. \\nAlternatively, the Classify endpoint provides a simple API for running text classification. The endpoint leverages Cohere’s embeddings models and makes it easy to add training examples and even create custom models that are specifically tailored to your task.\\nHere’s how we can use the Classify endpoint. It requires a minimum of two examples per class, which is passed as an argument to the API call. We have six examples altogether – two for each class.\\nfrom cohere import ClassifyExample response = co.classify( model=\\'embed-english-v3.0\\', inputs=[user_input], examples=[ClassifyExample(text=\"I can\\\\\\'t connect to the bluetooth\", label=\"Support\"), ClassifyExample(text=\"Why is max volume so low\", label=\"Support\"), ClassifyExample(text=\"When will my order arrive\", label=\"Order\"), ClassifyExample(text=\"How much is the shipping cost\", label=\"Order\"), ClassifyExample(text=\"What is your refund policy\", label=\"Refunds\"), ClassifyExample(text=\"How do I return my product\", label=\"Refunds\")]) print(response.classifications[0].predictions[0]) \\nSample response:\\nConclusion\\nThis chapter is a starting point for understanding the range of use cases you can build with the Cohere Command model. Take these use cases as the initial set of examples to further experiment on what’s possible.'),\n",
       " Document(metadata={'title': 'Chaining Prompts', 'source': 'https://cohere.com/llmu/chaining-prompts'}, page_content='Chaining PromptsQualified\\nContents\\nIntroduction\\nSequential\\nParallel\\nSampling\\nExploration\\nLoop\\nPerformance Considerations\\nFinal Thoughts\\nIntroduction\\nIn the previous two chapters, we looked at various ways to prompt the Command model. One thing similar about all of those examples is that they utilize a single prompt to fulfill a particular task. However, there may be other tasks where this is insufficient, and we instead have to chain a few prompts together to complete a task successfully.\\nWe can think of the previous two chapters as looking at prompts as a single unit, and this chapter will look at prompts as a combination of these units. The diagram below summarizes these three chapters.\\nA comparison of this blog post and the two previous ones (blog #1 and blog #2)\\nCombining multiple prompts to accomplish a single goal will be the focus of this chapter. We’ll look at some ideas and patterns for chaining prompts, which could come in handy when building a generative AI application.\\nSequential\\nLet’s begin with probably the most common approach for chaining prompts – in a sequence.\\nChaining prompts in a sequence\\nThe key idea is that if the task provided in a single prompt becomes too lengthy or complex and contains many different instructions, the response may not capture the details and granularity required. In this case, it would make sense to break this task into smaller subtasks that can be run one after the other. The response of one subtask becomes the prompt to the other subtask, and this chain continues until the overall task is complete.\\nLet\\'s use story generation as an example. Suppose we have a task to generate a long story containing a dialog guided by a set of information – characters, story beats, and locations. Now, we could stuff all this information into a single prompt, but that might dilute the key details we want the dialog to contain.\\nBesides, we don’t want to write all these additional details in the first place because we’d like the model to generate them for us. What we want is to provide a short summary of what we want the story to be about, which becomes our sole prompt.\\nThe diagram below summarizes the chains involved in generating the eventual dialog. First, a human enters a story summary, which becomes the prompt to generate a list of characters, which then becomes the prompt to generate the story beats, and so on, until we get to the dialog generation.\\nA summary of the different steps in the story generation task, running in a sequence\\nAnother added benefit of chaining prompts in sequences instead of using one single prompt is that we can implement human checks at each point in the sequence, and if a response does not fulfill a certain standard, we can trigger a re-run. This makes it easy to do evaluation and quality control over a complex generative AI task – by doing them in stages.\\nParallel\\nA sequential chain of prompts is needed when the subtasks depend on each other. But when they are independent, we can run them in parallel instead.\\nRunning prompts in parallel before combining them into a single output\\nLet’s say we are building an application that generates recipe ideas for a whole week and then generates a shopping list of ingredients for the user to buy. In this case, given a user input of, say, the number of meals or days, we can run the recipe generation step in parallel. The prompt might look something like the following:\\nprompt = f\\'Suggest a simple and quick recipe for {meal}. Write in JSON containing these keys \"Ingredients\" and \"Instructions\"\\' \\nNext, we’ll repeat the recipe generation across all meals. Once complete, we can consolidate the ingredients from each meal into a single shopping list that the user can use immediately.\\nprompt = f\"\"\"Consolidate the following ingredients into a single shopping list, without repetition: {ingredients}\"\"\" \\nSampling\\nIf a task involves logical reasoning, for example, in mathematical question answering, there may be more than one way to solve a given problem. In this case, we want to encourage the model to be more creative when exploring different solutions. But dialing up the creativity knob (i.e., “temperature”) also means that there’s a higher chance that the model will get the final answer wrong.\\nThe solution is, instead of prompting the same question to the model once, we can repeat that multiple times and choose the response with the highest majority.\\nSampling prompt responses followed by a majority vote\\nLet’s look at an example taken from a paper by Wang et al. that introduces the concept of self -consistency. \\nFirst, revisiting the previous blog post, we looked at the concept of chain-of-thought prompting introduced by Wei et al., where a model is prompted in such a way that it is encouraged to do a reasoning step before giving the final response. In those settings, however, the model is typically encouraged to do “greedy decoding,” which means biasing towards the correct and safe path. This can be done by adjusting settings like the temperature value.\\nWith self-consistency, we can build on top of the chain-of-thought approach by sampling from several paths instead of one. We also make the paths much more diverse by adjusting the settings towards being more “creative,” again using settings like temperature. We then do a majority vote out of all answers.\\nThe diagram below illustrates the self-consistency concept. It shows an example of comparing the result of a single solution, using the greedy decoding approach, and a solution that samples from multiple generations, using a more diverse decoding approach. The former didn’t get the answer correct, and the latter did.\\nAn example from Wei et al. illustrating the self-consistency concept\\nExploration\\nBuilding on the previous chain pattern, when a question is much more complex and requires more granular reasoning steps, it is helpful to break down the steps into pieces. At each step, we encourage the model to explore different solutions, vote for the best solution and only then continue to the next step.\\nThis is also useful in creative applications like story writing. Making the model explore different story ideas is more interesting than constraining it to a single path, elevating the final output\\'s quality.\\nExploring different solutions, one step at a time\\nLet’s use one concrete example from Yao et al., which introduces the concept of tree of thoughts. The diagram below illustrates how this concept differs from direct input-output prompting, chain-of-thought, and self-consistency with chain-of-thought. At each step, the model generates several diverse solutions. A separate prompt is then used to evaluate these solutions and vote for the best solution. The process repeats until the final step is completed.\\nAn illustration from Yao et al. of the Tree of Thoughts approach compared to three others\\nHere’s one of the examples shown in the paper with a creative writing task. The diagram below shows the initial input containing a list of four seemingly unrelated sentences – each talking about handstands, the smell of space, sign language, and people’s perceptions. The task is to weave them into one coherent passage. It must contain four short paragraphs, each ending with the given sentence.\\nA creative writing example from Yao et al. using Tree of Thoughts\\nIn this example, since the task is quite a challenging one, it makes sense to break down the task into two: writing the plan and writing the actual passage based on the winning plan. At each step, the model generates a few solutions, and then another prompt is used to evaluate and vote for the best solution, guiding the direction of the next step.\\nA couple of writing plan options are shown in the screenshot, and the winning option is the one that suggests using self-help as the theme to weave the sentences into a coherent passage.\\nLoop\\nIn some applications, we may need to re-run a generation step, given what happens in a subsequent step. One example is when that subsequent step is used to check if the response generated meets specific criteria, such as quality and format. This is where the loop pattern becomes useful.\\nLooping a task back to an earlier step\\nLet’s take a rephrasing task as an example. Say we have an application that takes a rude user comment and suggests a rephrasing that makes it more polite. Here, we want the LLM response to be polite and, at the same time, retain the original meaning of the comment.\\nThe prompt, taking in the user input to be rephrased, might look something like the following:\\nuser_input = \"I really don\\'t have time for this nonsense.\" prompt_rephrase = f\"\"\"Rephrase this user comment into something more polite: User comment: You don\\'t know what you\\'re talking about. Rephrased comment: I think there might be some inaccuracies in your statement. User comment: {user_input} Rephrased comment:\"\"\" \\nHere is an example response which rephrases the original user input:\\nI think we might need to set aside some time to discuss this properly. \\nNext, we create another prompt to check if the rephrased comment is similar enough to the original comment.\\nprompt_check = f\"\"\"Below is a rude comment that has been rephrased into a polite version. The rephrased comment must maintain a similar meaning to the original comment. Check if this is true. Answer with YES or NO. Original comment: Shut up, you\\'re always wrong. Rephrased comment: Please be quiet, check your facts again. Similar meaning: YES Original comment: I can\\'t stand you. Rephrased comment: Let\\'s discuss this. Similar meaning: NO Original comment: {user_input} Rephrased comment: {user_input_rephrased} Similar meaning:\"\"\" \\nAnd if the response is “NO,” we route the task back to the rephrasing prompt and repeat until we get a “YES” response.\\nPerformance Considerations\\nPrompt chaining is a powerful concept that makes complex use cases possible when a single prompt setup is insufficient. Having said that, prompt chaining should only be considered when it’s truly necessary. The overall performance considerations need to be taken into account. \\nOne such consideration is latency. The longer the chain, the longer it takes to complete a task from start to finish. If an application is latency-sensitive, it makes sense to minimize the number of chains as much as possible. The cost factor is another consideration when designing applications that rely on prompt chaining.\\nConclusion\\nIn this chapter, we looked at several prompt-chaining patterns and examples of how they can be applied to the Command model.\\nThis is a fascinating area of prompt engineering because it opens up so much room for creativity when solving problems with LLMs. Although it comes with some performance trade-offs, balancing these considerations presents an exciting challenge for building production-ready LLM-powered applications.'),\n",
       " Document(metadata={'title': 'Evaluating Outputs', 'source': 'https://cohere.com/llmu/evaluating-llm-outputs'}, page_content='Evaluating OutputsQualified\\nContents\\nIntroduction\\nHuman Evaluation \\nReference\\nScoring\\nA/B Testing\\nLLM-Generated Evaluation\\nWord-Level Metrics\\nConclusion\\nIntroduction\\nLarge language models (LLMs) offer exciting new ways to build applications that leverage natural language as the interface. However, as impressive as this technology may be, it is crucial to evaluate the generated outputs of LLMs to ensure the quality of an application.\\nEvaluating LLM outputs is especially important because the outputs produced by these models are probabilistic – meaning the same prompt does not necessarily produce the same outputs every time. Evaluations provide a way to measure the quality level of the outputs, ensuring a great user experience.\\nIn this blog post, we look at a few techniques for evaluating the outputs generated by an LLM.\\nReal User Feedback\\nThe gold standard for evaluation is gathering actual feedback from real application users. The best way to gain insights into an application’s quality and usefulness is by collecting feedback from users who interact with the application. In contrast, the rest of the methods we’ll discuss are all proxies for understanding real user experience and behaviors.\\nThe specific tactics for gathering user feedback can come in different forms, for example:\\nExplicit feedback: By implementing features to gather user feedback, such as thumbs up/down for an output, rating the output, and more\\nImplicit feedback: By observing user behaviors, such as considering ignored outputs as negatives, analyzing time spent on the output, and more\\nThis creates a flywheel for continuously improving an application. As more users start using the application, more data becomes available to inform the effectiveness of an application, providing signals on areas for improvement.\\nBut, of course, the challenge is that this can happen only after we deploy the application and users have been using it for a while. So, we must also perform evaluations before an application is deployed. For this, let’s look at alternative evaluation methods.\\nHuman Evaluation\\nThe next best option is to have human annotators evaluate the outputs of an application in the pre-deployment stage. A typical evaluation approach requires building a test dataset, and evaluation is performed against this test dataset.\\nAn example human evaluation comparing two LLM outputs.\\nLet’s look at a question-answering example. Here, we have one test data point: the prompt asks a question about a headphone product given a user review. The evaluation task is to rate the response generated by the LLM. \\nGiven the following user review, answer the question. User review: The CO-1T is a great pair of headphones! The sound quality is the best out there, and I can hear every detail of my music. Question: Why is the CO-1T a great wireless headphone? \\nAnd let’s suppose that this is the generated response.\\n\"Because the audio experience is unrivaled\" \\nThere are several ways to approach evaluation for this response, as follows.\\nReference\\nHere, the evaluator would compare each test data point against the ground truth of an ideal response.\\nWith our example, the ideal response might be, “Because the sound quality is the best out there.” Based on this, the evaluator provides a Yes/No judgment on whether the generated response provides an accurate response.\\nBut this approach requires the ground truth to be constructed beforehand. Given that no two use cases are quite the same, this means having to construct ground truth for each prompt and application. Moreover, the quality of the ground truth directly affects the evaluation outcome – if not constructed correctly, it can produce misleading results.\\nScoring\\nHere, the evaluator would evaluate the generated response by assigning a score, such as a rating between 0 and 10. There is no ground truth as a reference, so it’s up to the evaluator to provide a verdict on the quality of an output.\\nThe score can be a single score or a set of scores, and it can be broad or granular, depending on the use case. For example, a creative writing task might require more granular scoring for different output characteristics, such as fluency, interestingness, and conciseness.\\nAn evaluation criterion can be a score along a scale, and it can also be a check against a flag. For example, a summarization task might require checking whether the output is consistent and does not generate content that doesn\\'t exist in the actual document. A specialized task might require checking for specific rules, such as the number of syllables in a haiku.\\nA/B Testing\\nHere, the evaluator would be given a pair of LLM-generated responses and asked to rate the better response. This is useful for comparing an application’s quality over different time snapshots or different sets of configurations, such as prompts and parameters.\\nWith our example, let’s suppose the evaluator is tasked to compare the following two responses to our user question above.\\n1. \"Because the audio experience is unrivaled\" 2. \"Because the microphone has the best quality\" \\nWe can probably agree that the winning response should be the first one (“Because the audio experience is unrivaled\") as it answers the question accurately, while the second response (“Because the microphone has the best quality”) talks about the microphone’s quality instead of the sound quality.\\nThe challenge with human evaluation is that it cannot be scaled efficiently. The cost and time incurred are significantly higher than the alternative methods, which can be automated.\\nAnother challenge is that human evaluation is subjective – the verdict from one evaluator may not be the same as another. The example above is relatively straightforward, but in more challenging tasks, there will be more ambiguity and room for interpretation about what makes a response good or bad. Many factors can influence an evaluator’s verdict, such as expertise, style, and biases, impacting the evaluation outcome.\\nLLM-Generated Evaluation\\nAn alternative to human evaluation is to have an LLM to evaluate the output. With the same setup as above, this is done by having a prompt instructing the LLM to provide the verdict of a generated answer against the reference.\\nAn example LLM-generated evaluation comparing two LLM outputs.\\nAny human evaluation paradigms that we discussed (reference, scoring, and A/B testing) could be replicated using LLM-generated evaluation.\\nIn the example below, we use the Command model to perform an A/B testing evaluation for the same question-answering task. The model\\'s task is to choose the winning response between two responses to the question.\\n# Add text to evaluate ref_answer = \"Because the sound quality is the best out there\" gen_answers = [\"Because the audio experience is unrivaled\", \"Because the microphone has the best quality\"] # Run evaluation for gen_answer in gen_answers: prompt=f\"\"\"User review: The CO-1T is a great pair of headphones! The sound quality is the \\\\ best out there, and I can hear every detail of my music. Question: Why is the CO-1T a great headphone? Answer #1: {gen_answers[0]} Answer #2: {gen_answers[1]} Given the question about the user review, state whether Answer #1 or Answer #2 provides \\\\ the more accurate answer.\"\"\" response = co.generate(prompt=prompt,max_tokens=50, temperature=0).generations[0].text print(response) \\nThe generated verdict is “Answer #1”, which is what we expect the winning response should be.\\nThis approach is promising as it eliminates the cost and time constraints of human evaluation, but the jury is still out on whether it can surpass human evaluation in accuracy and quality. Its effectiveness on one task doesn’t guarantee that it will generalize to other tasks and domains, and the only way to know is by testing it on a specific application.\\nLLM-generated evaluation also faces the same subjectivity challenge as human evaluation. Many factors can affect a model’s evaluation outcome, such as the model’s overall capability, whether it’s being trained specifically to perform evaluations, the presence of training data that could introduce biases, and more.\\nWord-Level Metrics\\nAnother evaluation approach compares the reference and generated output at the word/token (or word/token group) level. Several evaluation metrics are available, such as BLEU, ROUGE, Perplexity, and BERTScore.\\nLet’s look at an example with ROUGE, originally created for evaluating summaries. It measures the number of matching “n-grams” between the reference and generated text. An N-gram is a contiguous sequence of `n` items in a text, where `n` can be 1, 2, and so on. To keep it simple, we’ll use `n=1`, also called “unigrams.” For example, in the sentence “I love cats,” the unigrams are “I,” “love,” and “cats.”\\nWe calculate the precision, recall, and F1-score of the n-grams of the question-answering task against a reference answer of “Because the sound quality is the best out there.”\\nPrecision is the ratio of the count of matching unigrams divided by the count of unigrams in the generated text\\nRecall is the ratio of the count of matching unigrams divided by the count of unigrams in the reference text\\nF1-score is calculated from precision and recall with the following formula: 2 * (precision * recall) / (precision + recall)\\nHere is an example using ROUGE:\\nfrom collections import Counter def rouge_1(reference, candidate): # Turn into unigrams reference_words = reference.split() candidate_words = candidate.split() # Compute the number of overlapping words reference_count = Counter(reference_words) candidate_count = Counter(candidate_words) overlap = sum(min(candidate_count[w], reference_count[w]) for w in candidate_count) # Compute precision, recall, and F1 score recall = overlap / len(reference_words) precision = overlap / len(candidate_words) f1 = 2 * (recall * precision) / (recall + precision) # Return resuls return {\"recall\": recall, \"precision\": precision, \"f1\": f1} for idx,gen_answer in enumerate(gen_answers): result = rouge_1(ref_answer, gen_answer) print(f\"Answer #{idx+1}\") print(f\"Precision: {result[\\'precision\\']:.2f}\") print(f\"Recall: {result[\\'recall\\']:.2f}\") print(f\"F1-Score: {result[\\'f1\\']:.2f}\") print(\"\\\\n\") \\nThis gives the following outcome.\\nAnswer #1 Precision: 0.50 Recall: 0.33 F1-Score: 0.40 Answer #2 Precision: 0.71 Recall: 0.56 F1-Score: 0.63 \\nHere, the second generated answer scored higher than the first in precision, recall, and F1-score, which is not the expected outcome. This is because it has more unigram overlaps with the reference answer, for example, with the words “best” and “quality.”\\nThis is an example where word-level metrics may fall short. They can be handy because they are easy to interpret and their implementation is fast and cheap, but they may not capture the overall meaning and accuracy when comparing two pieces of text.\\nConclusion\\nThis article looked at a few techniques for evaluating LLM outputs, from human evaluations to automated ones.\\nThere is a trade-off to be considered: on the one hand, automated evaluations are much more cost and time-efficient, which makes them practical options in some cases, such as in the early prototyping stages. On the other hand, human evaluations are still the gold standard for getting the strongest signal on an application\\'s accuracy and usefulness.\\nIt’s also worth noting that this article looks at evaluating LLM outputs in the general sense without making any assumptions about the actual task. In practice, there are other approaches not mentioned here that better suit specific tasks, such as code execution or information extraction.\\nUltimately, each evaluation approach has its potential pitfalls. An evaluation outcome can only be considered reliable if we have first understood and mitigated the associated limitations.'),\n",
       " Document(metadata={'title': 'Validating Outputs', 'source': 'https://cohere.com/llmu/validating-llm-outputs'}, page_content='Validating OutputsQualified\\nThis chapter was written in collaboration with the Guardrails AI team.\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nIn previous chapters, we covered prompt engineering techniques to elicit the desired responses from a large language model (LLM).\\nHowever, one key property of LLMs that’s different from traditional software is that the output is probabilistic in nature. The same input (i.e., the prompt) may not always produce the same response. While this property makes it possible to build entirely new classes of natural language applications, it also means that those applications require a mechanism for validating their outputs.\\nHere’s an example. In Chapter 1, we looked at a text extraction task of turning a list of bank invoices in a text document into a JSON object containing three fields: “Invoice Number,” “Merchant Name,” and “Account Number.” For brevity, we’ll turn it into a shorter version with the document containing just one invoice, as follows.\\nprompt=\"\"\"Turn the following information into a JSON string with the following keys: Invoice Number, Merchant Name, and Account Number. Bank Invoice: INVOICE #0521 MERCHANT ALLBIRDS ACC XXX3846 \"\"\" \\nThis produced an LLM response that followed exactly what we wanted, as shown below.\\n{ \"Invoice Number\": \"0521\", \"Merchant Name\": \"Allbirds\", \"Account Number\": \"XXXX3846\" } \\nBut how do we ensure we’ll get the same response every time? Perhaps another time, the output may miss some information, such as the returning incomplete information like this one.\\n{ \"Invoice Number\": \"0521\" } \\nThere are many other ways that a response may not match the expected structure. For example, what if the generated output contained fields we never specified? What if the document provided were more challenging to parse, causing an unclear response? What if we wanted to impose a rule that a field can only take up values within a specific range? In these cases and many others, we must add a step to validate the output.\\nIn the rest of this chapter, we’ll look at LLM output validation and examples of how to implement it using an open-source package called Guardrails AI.\\nWhen Is Output Validation Needed\\nTo build robust and production-ready LLM applications, the outputs need to be predictable. This helps to safeguard the application against unexpected behaviors and to ensure a reliable user experience. The following provides some example scenarios when output validation may be needed.\\nStructure compliance: Some LLM applications require their output to go beyond just freeform text and instead follow a specific structure containing specific types of information. We looked at a toy example earlier of a text extraction task that requires a JSON output that follows a certain format. We can extend this to synthetic data generation cases, where the generated data must meet certain criteria.\\nSafe responses: Due to their probabilistic nature, LLM applications require additional guardrails to ensure their outputs are safe, ethical, and privacy-preserving. For example, we may want to confirm that an output does not contain profanity. Alternatively, we may want to ensure that an output does not contain personally identifiable information (PII).\\nSemantic similarity: In some applications, we may require the output to be similar enough to a target. For example, in text summarization tasks, we want to ensure that the summary does not deviate too far from the original document. For this, with the help of text embeddings, we want to validate that the summary and the document are similar enough semantically.\\nValid choices: We may also want the LLM to generate valid outputs per given definitions or constraints. For example, creating an LLM chess player will require the LLM output to generate only valid moves for a given board state. We could also validate that a generated piece of code is syntactically correct.\\nQuality assurance: More generally, we may want to implement a validation step to ensure an LLM output meets a certain quality standard for a respective application’s use case and provides value to users.\\nWe can bake these types of validation into an application by implementing a set of validation steps followed by an automated action to fix any gaps found. Let’s see how we can do this.\\nOutput Validation with Guardrails AI\\nGuardrails AI is a Python package that enables developers to enhance the outputs of LLms by adding structural, type, and quality assurances. Guardrails helps developers with two key activities in a validation process:\\nValidation: Performing output validation for LLMs in a similar style to the Pydantic package, which is a data validation library for Python. Some examples are validating the structure of generated JSON output, identifying bias in generated text, and checking for bugs in generated code.\\nCorrection: Performing corrective actions based on the validation results, such as asking the LLM to re-generate the response or fixing the output directly.\\nValidation and correction are two key activities in a validation process\\nImplementation-wise, the following steps are involved in incorporating Guardrails into an LLM application:\\nCreate the output schema: The RAIL spec contains the overall schema of the LLM output, the type info for each field, the quality criteria for each field, and the corrective action to be taken if the quality criteria are not met. It also contains the prompt template and any custom code for implementing the schema. Alternatively, the output schema can also be defined using Pydantic, which we’ll use in our example later.\\nInitialize a Guard object based on the schema: The Guard class is the main entry point for using Guardrails. It is initialized using the output schema created in the previous step.\\nWrap an LLM call with the Guard object: The gd.Guard object wraps around an LLM API call to validate, structure, and correct the outputs.\\nThe key steps in output validation with Guardrails: creating the output schema, initializing a Guard object, and wrapping an LLM call with it\\nLet’s look at an example of using Guardrails in a text extraction task. The task is to extract the information from a doctor’s note into a JSON object. The following is the doctor’s note.\\ndoctors_notes = \"\"\"49 y/o Male with chronic macular rash to face & hair, worse in beard, eyebrows & nares. Itchy, flaky, slightly scaly. Moderate response to OTC steroid cream\"\"\" \\nSpecifically, we want our extracted information to contain the following fields:\\nPatient\\'s gender\\nPatient\\'s age\\nA list of symptoms, each with a severity rating and an affected area\\nA list of medications, each with information about the patient\\'s response to the medication\\nSetup\\nFirst, let’s install the packages required: cohere, guardrails-ai, and pydantic.\\npip install cohere guardrails-ai -q \\nWe\\'ll also need to download the validators required for this tutorial from Guardrails Hub, which is a place where you can find guardrails for common LLM validation use cases.\\n!guardrails hub install hub://guardrails/valid_range !guardrails hub install hub://guardrails/valid_choices \\nNext, import the necessary packages and create a Cohere client.\\nimport cohere import guardrails as gd from guardrails.hub import ValidRange, ValidChoices from pydantic import BaseModel, Field from rich import print from typing import List # Create a Cohere client co = cohere.Client(api_key=\"COHERE_API_KEY\") \\nDefine the Output Schema\\nNext, we define the output schema that defines what the LLM response should look like. As mentioned earlier, Guardrails provides an option to define the schema using Pydantic. We’ll use this option, and below is the schema we’ll use for the doctor notes extraction task.\\nclass Symptom(BaseModel): symptom: str = Field(..., description=\"Symptom that a patient is experiencing\") affected_area: str = Field( ..., description=\"What part of the body the symptom is affecting\", validators=[ValidChoices([\"Head\", \"Face\", \"Neck\", \"Chest\"], on_fail=\"reask\")] ) class CurrentMed(BaseModel): medication: str = Field(..., description=\"Name of the medication the patient is taking\") response: str = Field(..., description=\"How the patient is responding to the medication\") class PatientInfo(BaseModel): gender: str = Field(..., description=\"Patient\\'s gender\") age: int = Field(..., description=\"Patient\\'s age\", validators=[ValidRange(0, 100)]) symptoms: List[Symptom] = Field(..., description=\"Symptoms that the patient is experiencing\") current_meds: List[CurrentMed] = Field(..., description=\"Medications that the patient is currently taking\") \\nIn the schema, we defined a few “validators,” a Guardrails feature that lets us define the type of validation to perform. One example is ValidChoices, useful for situations where we want to enforce that a response can only be within a predefined list of items. In our example, in the symptom field, the value can only be one of head, neck, or chest. And if the generated response doesn’t fulfill this criteria, it will be re-prompted. This is shown by the on_fail setting that triggers a reask.\\nThe Guardrails documentation provides more information about the types of validators in Guardrails.\\nInitialize a Guard Object Based on the Schema\\nNext, we initialize a Guard object based on the schema we have defined.\\nFirst, we define the base instruction prompt for the LLM as follows.\\nPROMPT = \"\"\"Given the following doctor\\'s notes about a patient, please extract a dictionary that contains the patient\\'s information. ${doctors_notes} ${gr.complete_json_suffix_v2} \"\"\" \\nThen, we initialize a Guard object from the PatientInfo Pydantic model.\\n# Initialize a Guard object from the Pydantic model PatientInfo guard = gd.Guard.from_pydantic(PatientInfo, prompt=PROMPT) print(guard.base_prompt) \\nGuardrails then uses this information to construct the full prompt for the LLM, which looks like the following example.\\nGiven the following doctor\\'s notes about a patient, please extract a dictionary that contains the patient\\'s information. ${doctors_notes} Given below is XML that describes the information to extract from this document and the tags to extract it into. <output> <string name=\"gender\" description=\"Patient\\'s gender\"/> <integer name=\"age\" description=\"Patient\\'s age\" format=\"guardrails/valid_range: min=0 max=100\"/> <list name=\"symptoms\" description=\"Symptoms that the patient is experiencing\"> <object> <string name=\"symptom\" description=\"Symptom that a patient is experiencing\"/> <string name=\"affected_area\" description=\"What part of the body the symptom is affecting\" format=\"guardrails/valid_choices: choices=[\\'Head\\', \\'Face\\', \\'Neck\\', \\'Chest\\']\"/> </object> </list> <list name=\"current_meds\" description=\"Medications that the patient is currently taking\"> <object> <string name=\"medication\" description=\"Name of the medication the patient is taking\"/> <string name=\"response\" description=\"How the patient is responding to the medication\"/> </object> </list> </output> ONLY return a valid JSON object (no other text is necessary), where the key of the field in JSON is the `name` attribute of the corresponding XML, and the value is of the type specified by the corresponding XML\\'s tag. The JSON MUST conform to the XML format, including any types and format requests e.g. requests for lists, objects and specific types. Be correct and concise. Here are examples of simple (XML, JSON) pairs that show the expected behavior: - `<string name=\\'foo\\' format=\\'two-words lower-case\\' />` => `{\\'foo\\': \\'example one\\'}` - `<list name=\\'bar\\'><string format=\\'upper-case\\' /></list>` => `{\"bar\": [\\'STRING ONE\\', \\'STRING TWO\\', etc.]}` - `<object name=\\'baz\\'><string name=\"foo\" format=\"capitalize two-words\" /><integer name=\"index\" format=\"1-indexed\" /></object>` => `{\\'baz\\': {\\'foo\\': \\'Some String\\', \\'index\\': 1}}` \\nWrap an LLM Call with the Guard Object\\nWe’re ready to run an LLM call using the Cohere Generate endpoint. For this, we wrap the LLM call with the Guard object. This means it will take care of the validation and reasking (if any) until the final generated output fulfills the defined schema.\\n# Wrap the Cohere API call with the `guard` object response = guard( co.chat, prompt_params={\"doctors_notes\": doctors_notes}, model=\\'command-r-plus\\', temperature=0, ) # Print the validated output from the LLM print(response.validated_output) \\nAnd we get the final validated output as follows.\\n{ \\'gender\\': \\'Male\\', \\'age\\': 49, \\'symptoms\\': [{\\'symptom\\': \\'Chronic macular rash, itchy, flaky, slightly scaly\\', \\'affected_area\\': \\'Face\\'}], \\'current_meds\\': [{\\'medication\\': \\'OTC steroid cream\\', \\'response\\': \\'Moderate response\\'}] } \\nBehind the scenes, Guardrails performs the validation step on the output against the schema, raises any errors if there are mismatches, and triggers a reask. We can trace the execution steps as follows.\\nThe LLM call first returned the following response. However, notice that the affected_area field returned \"Face & Head\", which did not fall within the options we had defined earlier (any of \"Head\", \"Face\", \"Neck\", or \"Chest\").\\n{ \"gender\": \"Male\", \"age\": 49, \"symptoms\": [ { \"symptom\": \"Chronic macular rash, itchy, flaky, slightly scaly\", \"affected_area\": \"Face & Head\" } ], \"current_meds\": [ { \"medication\": \"OTC steroid cream\", \"response\": \"Moderate response\" } ] } \\nGuardrails captured this discrepancy by raising a FieldReAsk object containing the incorrect value, the error message, and other additional information.\\n{ \\'gender\\': \\'Male\\', \\'age\\': 49, \\'symptoms\\': [ { \\'symptom\\': \\'Chronic macular rash, itchy, flaky, slightly scaly\\', \\'affected_area\\': FieldReAsk( incorrect_value=\\'Face & Head\\', fail_results=[ FailResult( outcome=\\'fail\\', metadata=None, error_message=\"Value Face & Head is not in choices [\\'Head\\', \\'Face\\', \\'Neck\\', \\'Chest\\'].\", fix_value=None ) ], path=[\\'symptoms\\', 0, \\'affected_area\\'] ) } ], \\'current_meds\\': [ {\\'medication\\': \\'OTC steroid cream\\', \\'response\\': \\'Moderate response\\'} ] } \\nBased on this information, it triggered another LLM call to re-generate the response. Here is the full prompt.\\nI was given the following JSON response, which had problems due to incorrect values. { \"gender\": \"Male\", \"age\": 49, \"symptoms\": [ { \"symptom\": \"Chronic macular rash, itchy, flaky, slightly scaly\", \"affected_area\": { \"incorrect_value\": \"Face & Head\", \"error_messages\": [ \"Value Face & Head is not in choices [\\'Head\\', \\'Face\\', \\'Neck\\', \\'Chest\\'].\" ] } } ], \"current_meds\": [ { \"medication\": \"OTC steroid cream\", \"response\": \"Moderate response\" } ] } Help me correct the incorrect values based on the given error messages. Given below is XML that describes the information to extract from this document and the tags to extract it into. <output> <string name=\"gender\" description=\"Patient\\'s gender\"/> <integer name=\"age\" description=\"Patient\\'s age\" format=\"guardrails/valid_range: min=0 max=100\"/> <list name=\"symptoms\" description=\"Symptoms that the patient is experiencing\"> <object> <string name=\"symptom\" description=\"Symptom that a patient is experiencing\"/> <string name=\"affected_area\" description=\"What part of the body the symptom is affecting\" format=\"guardrails/valid_choices: choices=[\\'Head\\', \\'Face\\', \\'Neck\\', \\'Chest\\']\"/> </object> </list> <list name=\"current_meds\" description=\"Medications that the patient is currently taking\"> <object> <string name=\"medication\" description=\"Name of the medication the patient is taking\"/> <string name=\"response\" description=\"How the patient is responding to the medication\"/> </object> </list> </output> ONLY return a valid JSON object (no other text is necessary), where the key of the field in JSON is the `name` attribute of the corresponding XML, and the value is of the type specified by the corresponding XML\\'s tag. The JSON MUST conform to the XML format, including any types and format requests e.g. requests for lists, objects and specific types. Be correct and concise. If you are unsure anywhere, enter `null`. \\nGuardrails then generated the final validated output, which now completely fulfills the schema.\\n{ \"gender\": \"Male\", \"age\": 49, \"symptoms\": [ { \"symptom\": \"Chronic macular rash, itchy, flaky, slightly scaly\", \"affected_area\": \"Face\" } ], \"current_meds\": [ { \"medication\": \"OTC steroid cream\", \"response\": \"Moderate response\" } ] } \\nConclusion\\nIn this chapter, we looked at LLM output validation and how to implement it using Guardrails AI. Output validation is key to ensuring a generative AI application is robust and predictable enough to be deployed confidently.'),\n",
       " Document(metadata={'title': 'Getting Started with Retrieval-Augmented Generation', 'source': 'https://cohere.com/llmu/rag-start'}, page_content='Getting Started with Retrieval-Augmented GenerationQualified\\nWelcome to LLM University\\'s module on Retrieval-Augmented Generation (RAG)!\\nBy the end of this module, you will be able to build RAG-powered applications by leveraging various Cohere endpoints—Chat, Embed, and Rerank. You will also learn how to use quickstart connectors, which are pre-built implementations that connect a RAG application to over 80 enterprise datastores.\\nThis module consists of the following chapters:\\nIntroduction to RAG (this chapter): Learn the basics of RAG and how to get started with RAG via the Chat endpoint.\\nRAG with Chat, Embed, and Rerank: Learn how to build a RAG-powered chatbot using the Chat, Embed, and Rerank endpoints.\\nRAG with Connectors: Learn about connectors and how to build RAG applications using the web search connector.\\nRAG with Quickstart Connectors: Learn how to connect RAG applications to datastores by leveraging Cohere’s pre-built quickstart connectors.\\nRAG over Large-Scale Data: Learn how to build RAG applications over multiple datastores and long documents.\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nContents\\nWhat Is RAG?\\nRAG with Cohere\\nTry It with Coral\\nStep-by-Step Guide\\nSetup\\nDefine the Documents\\nGenerate the Response with Citations\\nConclusion\\nWhat Is RAG?\\nWhile LLMs are good at maintaining the context of the conversation and generating responses, they can be prone to hallucinate and include factually incorrect or incomplete information in their responses.\\nRetrieval-augmented generation (RAG) is a technique that enhances the performance of LLMs by incorporating external data sources. This approach significantly reduces the hallucination issue common in LLMs. RAG enables the model to access and utilize supplementary information from external documents, thereby improving the accuracy of its responses.\\nIn a previous module, we discussed how to build a chatbot using Cohere’s Chat endpoint. In this module, we’ll discuss the endpoint\\'s RAG capabilities. This means you can build chatbots that can connect to external documents, ground their responses on these documents, and produce inline citations in their responses.\\nThe chatbot provides helpful and verifiable responses through citations\\nHaving RAG in a chat paradigm means you can build context-aware applications that are able to both maintain the state of a conversation and generate grounded responses.\\nThe Chat endpoint adds RAG capabilities to the chat paradigm\\nRAG with Cohere\\nThe Cohere Chat endpoint comes with RAG features already integrated. This greatly simplifies the task of developing RAG-powered applications.\\nWith Cohere Chat, you get the complete set of building blocks needed to build a high-quality RAG application in the shortest time possible. We’ll cover them in depth throughout this module, but first, let’s take a quick look at some key capabilities of Cohere’s RAG solution.\\nChat interface: The RAG functionalities run on the Chat endpoint. That means everything is wrapped in a chat interface and powered by the Command model. Thus, you can build chatbots that have the full context of a conversation and are not limited to a single interaction.\\nQuery generation: With Cohere’s RAG solution, you also get an LLM that’s trained for query generation. It takes a user message and transforms it into queries that are more relevant and optimized for the retrieval process.\\nRetrieval models: Cohere Embed helps you build a high-quality semantic search system that retrieves the most relevant documents using embeddings. On top of that, Cohere Rerank helps you boost the results further by reranking the search results based on relevance.\\nResponse generation: Cohere’s RAG solution gives you an LLM that can provide the right responses to the user in different scenarios. A good RAG system should generate a grounded response based on relevant documents, but it should not do that every single time. The system also has to be able to determine whether or not any of the provided documents are relevant (and possibly decide that none are relevant), as well as decide that it can directly respond without needing any documents retrieved.\\nFine-grained citation: Each grounded response includes fine-grained citations linking back to the source documents. This makes the response easily verifiable and builds trust with the user.\\nConnector mode: What makes RAG work is having the data in the first place. In enterprises, data is typically spread across many platforms, and integrating data sources into a RAG system can be a huge challenge. Cohere Chat comes with a “connector mode,” which makes it easy to connect to multiple datastores.\\nQuickstart connectors: Cohere\\'s quickstart connectors allow you to quickly get up and running. These over 80 pre-built connectors are ready to use, including those for Google Drive, Slack, GitHub, Elastic, Pinecone, and more.\\nAutomated document handling: One common challenge in RAG is handling long documents at scale. The Cohere API provides an option for automating document handling, from chunking up to fitting them into a prompt.\\nDocument mode: For developers who want greater control over each component of a RAG system, Cohere Chat in document mode provides the modularity and flexibility needed to design such systems.\\nTry It with Coral\\nTo see Cohere-powered RAG in action, you can try Coral, which is a conversational AI toolkit for enterprises to build RAG-enabled knowledge assistants. Coral includes some document grounding functionalities out-of-the-box, such as web search results, specific domain grounding, and PDF document support.\\nUsers can engage Coral by entering a prompt to find answers from across their documents. Generated responses include citations of the information sources used, which verifies their accuracy and mitigates LLM hallucinations.\\nA screenshot of Coral, Cohere\\'s conversational AI toolkit for enterprises\\nStep-by-Step Guide\\nLet’s start our exploration of RAG with a quick example.\\nWe’ll walk through how to ground an LLM’s response with information from external documents and provide document citations along with it. In this example, we’ll use a static, short list of documents. Below is a diagram that provides an overview of our simple RAG system.\\nAn overview of what we\\'ll build\\nSetup\\nFirst, let’s install and import the cohere library, and then create a Cohere client using an API key.\\nimport cohere co = cohere.Client(\"COHERE_API_KEY\") \\nDefine the Documents\\nNext, we define the documents that we want to ground an LLM’s response with, formatted as a list. In our case, each document consists of two fields: title and text.\\nThe documents list includes a list of documents with a “text” field containing the information we want the model to use. The recommended length for the snippet of each document is relatively short, 300 words or less. We recommend using field names similar to the ones we’ve included in this example (i.e., “title” and “text”), but RAG is quite flexible with respect to how you structure the documents listings. You can give the fields any names you want, and you can pass in other fields as well, such as a “date” field. All field names and field values are passed to the model.\\ndocuments = [ { \"title\": \"Tall penguins\", \"text\": \"Emperor penguins are the tallest.\"}, { \"title\": \"Penguin habitats\", \"text\": \"Emperor penguins only live in Antarctica.\"}, { \"title\": \"What are animals?\", \"text\": \"Animals are different from plants.\"} ] \\nGenerate the Response with Citations\\nCohere’s RAG functionalities are part of the Chat endpoint, with the Command model as the underlying LLM. This allows developers to build chatbots that have the full context of a conversation and are not limited to a single interaction.\\nFirst, we define the message coming from the user. We’ll use a simple query, “What are the tallest living penguins?”, as an example.\\n# Get the user message message = \"What are the tallest living penguins?\" \\nThen, we pass this message as a message parameter to a Chat endpoint call. We also pass the list of documents as a documents parameter. By using the chat_stream method, the response is generated incrementally by token without having to wait for the full completion.\\n# Generate the response response = co.chat_stream(message=message, documents=documents) \\nFinally, we display the response from the model. The streamed response will return different types of objects, and for now, we are interested in the text-generation objects, which contain the generated text.\\nWe also display the citations and source documents, which we can get from the final object returned by the streamed response.\\n# Display the response citations = [] cited_documents = [] for event in response: if event.event_type == \"text-generation\": print(event.text, end=\"\") elif event.event_type == \"citation-generation\": citations.extend(event.citations) elif event.event_type == \"stream-end\": cited_documents = event.response.documents # Display the citations and source documents if citations: print(\"\\\\n\\\\nCITATIONS:\") for citation in citations: print(citation) print(\"\\\\nDOCUMENTS:\") for document in cited_documents: print(document) \\nAnd here’s the response generated by our RAG system. \\nThe tallest living penguins are emperor penguins, which are found only in Antarctica. CITATIONS: start=32 end=48 text=\\'emperor penguins\\' document_ids=[\\'doc_0\\'] start=66 end=85 text=\\'only in Antarctica.\\' document_ids=[\\'doc_1\\'] DOCUMENTS: {\\'id\\': \\'doc_0\\', \\'text\\': \\'Emperor penguins are the tallest.\\', \\'title\\': \\'Tall penguins\\'} {\\'id\\': \\'doc_1\\', \\'text\\': \\'Emperor penguins only live in Antarctica.\\', \\'title\\': \\'Penguin habitats\\'} \\nFirst, we get the actual text response from the model (see the output below).\\nThis is followed by a list of citations, which are references to specific source documents on our list that provided the information contained in specific passages within the text response. For example, the first citation indicates that the term “emperor penguins,” which appears between the 32nd and 48th characters of the response, came from the first document on the list (\\'doc_0\\').\\nFinally, we get the full list of the source documents used to generate the response.\\nConclusion\\nIn this chapter, you learned about RAG and how to get started with RAG using the Cohere Chat endpoint.\\nContinue to the next chapter to learn how to build a RAG-powered chatbot that leverages text embeddings using the Chat, Embed, and Rerank endpoints.\\nAbout Cohere’s LLM University\\nOur comprehensive curriculum aims to equip you with the skills to develop your own AI applications. We cater to learners from all backgrounds, covering everything from the basics to the most advanced topics in large language models (LLMs). Plus, you\\'ll have the opportunity to work on hands-on exercises, allowing you to build and deploy your very own solutions. Take a course today.'),\n",
       " Document(metadata={'title': 'How to Build a RAG-Powered Chatbot with Chat, Embed, and Rerank', 'source': 'https://cohere.com/llmu/rag-chatbot'}, page_content='How to Build a RAG-Powered Chatbot with Chat, Embed, and RerankQualified\\nIn this chapter, you’ll learn how to build a RAG-powered chatbot that leverages text embeddings using the Chat, Embed, and Rerank endpoints.\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nContents\\nStep-by-Step Guide \\nWhat We\\'ll Build\\nSetup\\nCreate the Vectorstore Component \\nLoad and Chunk the Documents\\nEmbed the Document Chunks\\nIndex Document Chunks\\nImplement Retrieval \\nDense Retrieval\\nReranking\\nProcess the Documents\\nTest Retrieval\\nCreate the Chatbot Component \\nGet the User Message\\nGenerate the Queries\\nRetrieve Relevant Chunks and Generate the Response\\nDisplay the Response with Citations\\nRun the Chatbot\\nConclusion\\nIn the previous chapter, you learned how to get started with RAG using the Chat endpoint.\\nIn this chapter, you’ll learn how to add RAG applications that leverage text embeddings and how to build a chatbot using RAG in document mode and multiple Cohere endpoints.\\nIn the previous chapter, we used a short list of simple documents. In real-world applications, however, developers typically need to work with a larger volume of documents that each vary in length.\\nThis is where text embeddings can help. With embeddings, we can split these documents into smaller chunks and build a semantic search system that can retrieve the most relevant chunks to a user query based on contextual meaning, and not just keyword-matching.\\nStep-by-Step Guide\\nThere are three RAG modes available with the Cohere Chat endpoint:\\nDocument mode: Specifying the documents for the model to use when generating a response\\nConnectors mode: Connecting the endpoint with an external service that handles all the logic of document retrieval\\nQuery-generation mode: Generating one or more queries given a user message\\nIn this chapter, you’ll learn how to use RAG in document mode, which will also involve the query-generation mode. In the next chapter, you’ll learn how to use RAG in connector mode. Refer to the RAG documentation for more details.\\nWhat We\\'ll Build\\nWe’ll build a chatbot that answers users’ questions about the contents in Cohere\\'s documentation on prompt engineering. Let’s examine the demo application\\'s high-level implementation plan (see the diagram below).\\nAn overview of what we\\'ll build\\nThe steps to building a RAG-powered chatbot are summarized below:\\nSetup phase:\\nStep 0: Ingest the documents – get documents, chunk, embed, and index\\nFor each user-chatbot interaction:\\nStep 1: Get the user message\\nStep 2: Call the Chat endpoint in query-generation mode\\nIf at least one query is generated:\\nStep 3: Retrieve and rerank relevant documents\\nStep 4: Call the Chat endpoint in document mode to generate a grounded response with citations\\nIf no query is generated:\\nStep 4: Call the Chat endpoint in normal mode to generate a response\\nThroughout the conversation:\\nAppend the user-chatbot interaction to the conversation thread\\nRepeat with every interaction\\nTo build a RAG system that can effectively handle a complex corpus of documents, we’ll need to use several Cohere API endpoints, including:\\nChat: For handling the main logic of the chatbot, including turning a user message into queries, generating responses, and producing citations\\nEmbed: For turning textual documents into their embeddings representation, later to be used in retrieval (we’ll use the latest Embed v3 model)\\nRerank: For reranking the retrieved documents according to their relevance to a query (we’ll use the latest Rerank 3 model)\\nThis demo application will use Cohere’s Chat, Embed, and Rerank endpoints\\nFor further reading, the API reference page contains a detailed description of the Chat endpoint’s input parameters and response objects.\\nSetup\\nFirst, let’s import the necessary libraries for this project. This includes cohere, hnswlib for the vector library, and unstructured for chunking the documents (more details on these later).\\npip install cohere hnswlib unstructured \\nThen, import the necessary modules from these libraries in addition to other required modules. Let’s also create a Cohere client.\\nimport cohere import uuid import hnswlib from typing import List, Dict from unstructured.partition.html import partition_html from unstructured.chunking.title import chunk_by_title co = cohere.Client(\"COHERE_API_KEY\") \\nCreate the Vectorstore Component\\nThe Vectorstore class handles the ingestion of documents into embeddings (or vectors) and the retrieval of relevant documents given a query.\\nThe Vectorstore component handles document ingestion and retrieval\\nAs an example, we’ll use the contents from Cohere\\'s documentation on prompt engineering. It consists of four web pages, each in the Python list raw_documents below. Each entry is identified by its title and URL.\\nraw_documents = [ { \"title\": \"Crafting Effective Prompts\", \"url\": \"https://docs.cohere.com/docs/crafting-effective-prompts\"}, { \"title\": \"Advanced Prompt Engineering Techniques\", \"url\": \"https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\"}, { \"title\": \"Prompt Truncation\", \"url\": \"https://docs.cohere.com/docs/prompt-truncation\"}, { \"title\": \"Preambles\", \"url\": \"https://docs.cohere.com/docs/preambles\"} ] \\nWe implement this in the Vectorstore class below, which takes the raw_documents list as input.\\nclass Vectorstore: def __init__(self, raw_documents: List[Dict[str, str]]): self.raw_documents = raw_documents self.docs = [] self.docs_embs = [] self.retrieve_top_k = 10 self.rerank_top_k = 3 self.load_and_chunk() self.embed() self.index() \\nWe also initialize a few instance attributes and methods. The attributes include self.raw_documents to represent the raw documents, self.docs to represent the chunked version of the documents, self.docs_embs to represent the embeddings of the chunked documents, and a couple of top_k parameters to be used for retrieval and reranking.\\nMeanwhile, the methods include load_and_chunk(), embed(), and index() for ingesting raw documents. As you’ll see, we will also specify a retrieve() method to retrieve relevant document chunks given a query.\\nThe document ingestion portion of the Documents component\\nLoad and Chunk the Documents\\nThe load_and_chunk() method loads the raw documents from the URL and breaks them into smaller chunks. Chunking for information retrieval is a broad topic in and of itself, with many strategies being discussed within the AI community. For our example, we’ll utilize the partition_html method from the unstructured library. Read its documentation for more information about its chunking approach.\\nEach chunk is turned into a dictionary with three fields:\\ntitle: The web page’s title\\ntext: The textual content of the chunk\\nurl: The web page’s URL\\nThis information will eventually be passed to the chatbot’s prompt for generating the response, so it’s crucial to populate relevant information into this dictionary. Note that we are not limited to these three fields. At a minimum, the Chat endpoint requires the text field, but beyond that, we can add custom fields that can provide more context about the document, such as subtitles, snippets, tags, and others.\\nThe resulting dictionaries are stored in the self.docs attribute.\\nclass Vectorstore: ... ... def load_and_chunk(self) -> None: \"\"\" Loads the text from the sources and chunks the HTML content. \"\"\" print(\"Loading documents...\") for raw_document in self.raw_documents: elements = partition_html(url=raw_document[\"url\"]) chunks = chunk_by_title(elements) for chunk in chunks: self.docs.append( { \"title\": raw_document[\"title\"], \"text\": str(chunk), \"url\": raw_document[\"url\"], } ) \\nEmbed the Document Chunks\\nThe embed() method generates embeddings of the chunked documents. We use the Embed endpoint and Cohere\\'s embed-english-v3.0 model. Since the endpoint has a limit of 96 documents per call, we send them in batches.\\nWith the Embed v3 model, we need to define an input_type, of which there are four options depending on the type of task. Using these input types ensures the highest possible quality for the respective tasks. Since our document chunks will be used for retrieval, we use search_document as the input_type.\\nThe resulting chunk embeddings are stored in the self.docs_embs attribute.\\nclass Vectorstore: ... ... def embed(self) -> None: \"\"\" Embeds the document chunks using the Cohere API. \"\"\" print(\"Embedding document chunks...\") batch_size = 90 self.docs_len = len(self.docs) for i in range(0, self.docs_len, batch_size): batch = self.docs[i : min(i + batch_size, self.docs_len)] texts = [item[\"text\"] for item in batch] docs_embs_batch = co.embed( texts=texts, model=\"embed-english-v3.0\", input_type=\"search_document\" ).embeddings self.docs_embs.extend(docs_embs_batch) \\nIndex Document Chunks\\nThe index() method indexes the document chunk embeddings. We build an index to store the embeddings in a structured and organized way in order to ensure efficient similarity search during retrieval.\\nThere are many options available for building an index. For production environments, typically a vector database (like Weaviate or MongoDB) is required to handle the continuous process of indexing documents and maintaining the index.\\nIn our example, however, we’ll keep it simple and use a vector library instead. We can choose from many open-source projects, such as Faiss, Annoy, ScaNN, or Hnswlib, which is the one we’ll use. These libraries store embeddings in in-memory indexes and implement approximate nearest neighbor (ANN) algorithms to make similarity search efficient.\\nThe resulting document chunk embeddings are stored in the self.idx attribute.\\nclass Vectorstore: ... ... def index(self) -> None: \"\"\" Indexes the documents for efficient retrieval. \"\"\" print(\"Indexing documents...\") self.idx = hnswlib.Index(space=\"ip\", dim=1024) self.idx.init_index(max_elements=self.docs_len, ef_construction=512, M=64) self.idx.add_items(self.docs_embs, list(range(len(self.docs_embs)))) print(f\"Indexing complete with {self.idx.get_current_count()} documents.\") \\nImplement Retrieval\\nThe retrieve() method uses semantic search to retrieve relevant document chunks given a query, and it has two steps: (1) dense retrieval, (2) reranking.\\nA more detailed view of document ingestion, retrieval, and reranking\\nDense Retrieval\\nWe implement a dense retrieval system that leverages embeddings to retrieve document chunks, offering significant improvements over basic keyword-matching approaches. Embeddings can capture the contextual meaning of a document, thus enabling the retrieval of highly relevant results to the given query.\\nWe embed the query using the same embed-english-v3.0 model that we used to embed the document chunks, but this time, we set input_type=”search_query”.\\nSearch is performed by the knn_query() method from the hnswlib library. Given a query, it returns the document chunks most similar to the query. We define the number of document chunks to return using the attribute self.retrieve_top_k=10.\\nReranking\\nAfter dense retrieval, we implement a reranking step. While our dense retrieval component is already highly capable of retrieving relevant sources, the Rerank endpoint provides an additional boost to the quality of the search results, especially for complex and domain-specific queries. It takes the search results and sorts them according to their relevance to the query.\\nWe call the Rerank endpoint with co.rerank() and pass the query and the list of document chunks to be reranked. We also define the number of top reranked document chunks to retrieve using the attribute self.rerank_top_k=3. The model we use is rerank-english-v3.0, which lets you rerank documents that contain multiple fields, in the form of JSON objects. In our case, we\\'ll use the title and text fields for reranking.\\nThis method returns the top retrieved document chunks as a Python list docs_retrieved, so that they can be passed to the chatbot, which we’ll implement next.\\nclass Vectorstore: ... ... def retrieve(self, query: str) -> List[Dict[str, str]]: \"\"\" Retrieves document chunks based on the given query. Parameters: query (str): The query to retrieve document chunks for. Returns: List[Dict[str, str]]: A list of dictionaries representing the retrieved document chunks, with \\'title\\', \\'text\\', and \\'url\\' keys. \"\"\" # Dense retrieval query_emb = co.embed( texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\" ).embeddings doc_ids = self.idx.knn_query(query_emb, k=self.retrieve_top_k)[0][0] # Reranking rank_fields = [\"title\", \"text\"] # We\\'ll use the title and text fields for reranking docs_to_rerank = [self.docs[doc_id] for doc_id in doc_ids] rerank_results = co.rerank( query=query, documents=docs_to_rerank, top_n=self.rerank_top_k, model=\"rerank-english-v3.0\", rank_fields=rank_fields ) docs_retrieved = [] for doc_id in doc_ids_reranked: docs_retrieved.append( { \"title\": self.docs[doc_id][\"title\"], \"text\": self.docs[doc_id][\"text\"], \"url\": self.docs[doc_id][\"url\"], } ) return docs_retrieved \\nProcess the Documents\\nWe can now process the raw documents. We do that by creating an instance of Vectorstore. In our case, we get a total of 136 documents, chunked from the four web URLs.\\nvectorstore = Vectorstore(raw_documents) \\nLoading documents... Embedding documents... Indexing documents... Indexing complete with 134 documents. \\nTest Retrieval\\nBefore going further, we first test the document retrieval part of the system. First, we create an instance of the Vectorstore with the raw documents that we have defined. Then, we use the retrieve method to retrieve the most relevant documents to the query \"Prompting by giving examples.\"\\nvectorstore.retrieve(\"Prompting by giving examples\") \\nAnd here’s the response. We can see that the document chunks returned are indeed highly relevant to the query we sent.\\n[{\\'title\\': \\'Advanced Prompt Engineering Techniques\\', \\'text\\': \\'Few-shot Prompting\\\\n\\\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.\\', \\'url\\': \\'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\\'}, {\\'title\\': \\'Crafting Effective Prompts\\', \\'text\\': \\'Incorporating Example Outputs\\\\n\\\\nLLMs respond well when they have specific examples to work from. For example, instead of asking for the salient points of the text and using bullet points “where appropriate”, give an example of what the output should look like.\\', \\'url\\': \\'https://docs.cohere.com/docs/crafting-effective-prompts\\'}, {\\'title\\': \\'Advanced Prompt Engineering Techniques\\', \\'text\\': \\'In addition to giving correct examples, including negative examples with a clear indication of why they are wrong can help the LLM learn to distinguish between correct and incorrect responses. Ordering the examples can also be important; if there are patterns that could be picked up on that are not relevant to the correctness of the question, the model may incorrectly pick up on those instead of the semantics of the question itself.\\', \\'url\\': \\'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\\'}] \\nRun the Chatbot\\nWe can now run the chatbot. For this, we create a generate_chat function which includes the RAG components:\\nSearch query generation: For each user message, we use the endpoint’s search query generation feature to turn the message into one or more queries that are optimized for retrieval. The endpoint can even return no query, which means that a user message can be responded to directly without retrieval. This is done by calling the Chat endpoint with the search_queries_only parameter and setting it as True.\\nDocument retrieval: If there is no search query generated, we call the Chat endpoint to generate a response directly. If there is at least one, we call the retrieve method from the Vectorstore instance to retrieve the most relevant documents to each query.\\nResponse and citation generation: Finally, all the results from all queries are appended to a list and passed to the Chat endpoint for response generation. We print the response, together with the citations and the list of document chunks cited, for easy reference.\\nThe generate_chat function handles the chatbot logic, from getting the user message to generating the response\\ndef run_chatbot(message, chat_history=[]): # Generate search queries, if any response = co.chat(message=message, model=\"command-r-plus\", search_queries_only=True, chat_history=chat_history) search_queries = [] for query in response.search_queries: search_queries.append(query.text) # If there are search queries, retrieve the documents if search_queries: print(\"Retrieving information...\", end=\"\") # Retrieve document chunks for each query documents = [] for query in search_queries: documents.extend(vectorstore.retrieve(query)) # Use document chunks to respond response = co.chat_stream( message=message, model=\"command-r-plus\", documents=documents, chat_history=chat_history, ) else: response = co.chat_stream( message=message, model=\"command-r-plus\", chat_history=chat_history, ) # Print the chatbot response and citations chatbot_response = \"\" print(\"\\\\nChatbot:\") for event in response: if event.event_type == \"text-generation\": print(event.text, end=\"\") chatbot_response += event.text if event.event_type == \"stream-end\": if event.response.citations: print(\"\\\\n\\\\nCITATIONS:\") for citation in event.response.citations: print(citation) if event.response.documents: print(\"\\\\nCITED DOCUMENTS:\") for document in event.response.documents: print(document) # Update the chat history for the next turn chat_history = event.response.chat_history return chat_history \\nSearch Query Generation\\nLet\\'s take a deeper look at the search query generation feature. Based on the user message, the chatbot needs to decide if it needs to consult external information before responding. If so, the chatbot determines an optimal set of search queries to use for retrieval. When we call co.chat() with search_queries_only=True, the Chat endpoint handles this for us automatically.\\nThe generated queries can be accessed from the search_queries field of the object that is returned. To understand how this works, let’s look at a few scenarios:\\nNo query needed: Suppose we have a user message of “Hello, I need help with a report I\\'m writing”. This type of message doesn’t require any additional context from external information, so retrieval is not required. A direct chatbot response will suffice (for example: “Sure, how can I help?”). When we send this to the Chat endpoint, we get an empty search_queries result, which is what we expect.\\nOne query generated: Take this user message: \"What did the report say about the company\\'s Q4 performance?” This does require additional context as it refers to a report, hence retrieval is required. Given this message, the Chat endpoint returns the search_queries result of Q4 company performance. Here it turns the user message into a query optimized for search. Another important scenario is generating queries in the context of the conversation. Suppose there’s an ongoing conversation where the user is learning from the chatbot about deep learning. If at some point, the user asks, “Why is it important”, then the generated search_queries will become why is deep learning important, providing the much-needed context for the retrieval process.\\nMore than one query generated: What if the user message is a bit more complex, such as \"What did the report say about the company\\'s Q4 performance and its range of products and services?” This requires multiple pieces of information to be retrieved. Given this message, the Chat endpoint returns two search_queries results: Q4 company performance and company\\'s range of products and services.\\nThese scenarios highlight the adaptability of the Chat endpoint to decide on the next course of action based on a user message.\\nDocument Retrieval\\nLet\\'s take a deeper look at the document retrieval step. What happens next depends on how many search queries are returned.\\nIf search queries are returned\\nIf the chatbot response contains at least one search query, we call the retrieve() method from the Vectorstore class instance to retrieve document chunks that are relevant to the queries.\\nThen, we call the Chat endpoint to generate a response, adding a documents parameter to the call to pass the relevant document chunks.\\nIf no search queries are returned\\nMeanwhile, if the chatbot response doesn’t contain any search queries, then it doesn’t require information retrieval. To generate the response, we call the Chat endpoint another time, passing the user message and without needing to add any sources to the call.\\nIn either case, we also pass the chat_history parameter, which retains the interactions between the user and the chatbot in the same conversation thread. We also use the chat_stream endpoint so we can stream the chatbot response to the application.\\nResponse and Citation Generation\\nLet\\'s take a deeper look at the response generation step. The chatbot response includes a stream of events, such as the generated text and citations followed by a final object which contains the sources used by the chatbot along with other details.\\nTo display the response, we use the text-generation events from the response stream.\\nThe citation-generation events indicate the spans of text from the retrieved document chunks on which the response is grounded. Here is one example:\\nstart=382 end=397 text=\\'similar vectors\\' document_ids=[\\'doc_0\\', \\'doc_2\\'] \\nThe format of each citation is:\\nstart: The starting point of a span where one or more documents are referenced\\nend: The ending point of a span where one or more documents are referenced\\ntext: The text representing this span\\ndocument_ids: The IDs of the document chunks being referenced (doc_0 being the ID of the first document chunk passed to the documents creating parameter in the endpoint call, and so on)\\nThe final response object includes a list of the document chunks, which we access from the documents attribute.\\nExample conversation\\nHere’s an example of a conversation that happens over a few turns:\\n# Turn # 1 chat_history = run_chatbot(\"Hello, I have a question\") \\nChatbot: Of course! I am here to help. Please go ahead with your question, and I will do my best to assist you. \\n# Turn # 2 chat_history = run_chatbot(\"What\\'s the difference between zero-shot and few-shot prompting\", chat_history) \\nRetrieving information... Chatbot: Zero-shot prompting involves asking the model to perform a task without providing any examples. On the other hand, few-shot prompting is a technique where the model is provided with a few relevant and diverse examples of the task being performed before asking the specific question to be answered. These examples help steer the model toward a high-quality solution and condition it to the expected response type and style. CITATIONS: start=0 end=19 text=\\'Zero-shot prompting\\' document_ids=[\\'doc_0\\'] start=29 end=95 text=\\'asking the model to perform a task without providing any examples.\\' document_ids=[\\'doc_0\\'] start=115 end=133 text=\\'few-shot prompting\\' document_ids=[\\'doc_0\\'] start=159 end=217 text=\\'model is provided with a few relevant and diverse examples\\' document_ids=[\\'doc_0\\'] start=246 end=297 text=\\'before asking the specific question to be answered.\\' document_ids=[\\'doc_0\\'] start=318 end=364 text=\\'steer the model toward a high-quality solution\\' document_ids=[\\'doc_0\\'] start=369 end=422 text=\\'condition it to the expected response type and style.\\' document_ids=[\\'doc_0\\'] CITED DOCUMENTS: {\\'id\\': \\'doc_0\\', \\'text\\': \\'Few-shot Prompting\\\\n\\\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.\\', \\'title\\': \\'Advanced Prompt Engineering Techniques\\', \\'url\\': \\'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\\'} \\n# Turn # 3 chat_history = run_chatbot(\"How would the latter help?\", chat_history) \\nRetrieving information... Chatbot: Few-shot prompting can vastly improve the quality of the model\\'s completions. Providing a few relevant and diverse examples helps steer the model toward a high-quality solution by conditioning it to the expected response type and style. CITATIONS: start=23 end=77 text=\"vastly improve the quality of the model\\'s completions.\" document_ids=[\\'doc_2\\'] start=90 end=123 text=\\'few relevant and diverse examples\\' document_ids=[\\'doc_0\\'] start=130 end=176 text=\\'steer the model toward a high-quality solution\\' document_ids=[\\'doc_0\\'] start=180 end=236 text=\\'conditioning it to the expected response type and style.\\' document_ids=[\\'doc_0\\'] CITED DOCUMENTS: {\\'id\\': \\'doc_2\\', \\'text\\': \\'Advanced Prompt Engineering Techniques\\\\n\\\\nSuggest Edits\\\\n\\\\nThe previous chapter discussed general rules and heuristics to follow for successfully prompting the Command family of models. Here, we will discuss specific advanced prompt engineering techniques that can in many cases vastly improve the quality of the model’s completions. These include how to give clear and unambiguous instructions, few-shot prompting, chain-of-thought (CoT) techniques, and prompt chaining.\\', \\'title\\': \\'Advanced Prompt Engineering Techniques\\', \\'url\\': \\'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\\'} {\\'id\\': \\'doc_0\\', \\'text\\': \\'Few-shot Prompting\\\\n\\\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.\\', \\'title\\': \\'Advanced Prompt Engineering Techniques\\', \\'url\\': \\'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\\'} \\n# Turn # 4 chat_history = run_chatbot(\"What do you know about 5G networks?\", chat_history) \\nRetrieving information... Chatbot: Sorry, I don\\'t have any information about 5G networks. Can I help you with anything else? There are a few observations worth pointing out: Direct response: For user messages that don’t require retrieval (“Hello, I have a question”), the chatbot responds directly without requiring retrieval. Citation generation: For responses that do require retrieval (\"What\\'s the difference between zero-shot and few-shot prompting\"), the endpoint returns the response together with the citations. These are fine-grained citations, which means they refer to specific spans of the generated text. State management: The endpoint maintains the state of the conversation via the chat_history parameter, for example, by correctly responding to a vague user message such as \"How would the latter help?\" Response synthesis: The model can decide if none of the retrieved documents provide the necessary information to answer a user message. For example, when asked the question, “What do you know about 5G networks”, the chatbot retrieves external information from the index. However, it doesn’t use any of the information in its response as none of it is relevant to the question. \\nIn the conversation above, notice a few observations that reflect the different components of what we built:\\nDirect response: For user messages that don’t require retrieval (“Hello, I have a question”), the chatbot responds directly without requiring retrieval.\\nCitation generation: For responses that do require retrieval (“What’s the difference between word and sentence embeddings”), the endpoint returns the response together with the citations.\\nState management: The endpoint maintains the state of the conversation via the conversation_id parameter, for example, by being able to correctly respond to a vague user message of “And what are their similarities”\\nResponse synthesis: The model can decide if none of the retrieved documents provide the necessary information required to answer a user message. For example, when asked the question, “What do you know about 5G networks”, the chatbot goes on and retrieves external information from the index. However, it doesn’t use any of the information in its response as none of them is relevant to the question.\\nHere are the contents of the chat history.\\nprint(\"Chat history:\") for c in chat_history: print(c, \"\\\\n\") \\nChat history: message=\\'Hello, I have a question\\' tool_calls=None role=\\'USER\\' message=\\'Of course! I am here to help. Please go ahead with your question, and I will do my best to assist you.\\' tool_calls=None role=\\'CHATBOT\\' message=\"What\\'s the difference between zero-shot and few-shot prompting\" tool_calls=None role=\\'USER\\' message=\\'Zero-shot prompting involves asking the model to perform a task without providing any examples. On the other hand, few-shot prompting is a technique where the model is provided with a few relevant and diverse examples of the task being performed before asking the specific question to be answered. These examples help steer the model toward a high-quality solution and condition it to the expected response type and style.\\' tool_calls=None role=\\'CHATBOT\\' message=\\'How would the latter help?\\' tool_calls=None role=\\'USER\\' message=\"Few-shot prompting can vastly improve the quality of the model\\'s completions. Providing a few relevant and diverse examples helps steer the model toward a high-quality solution by conditioning it to the expected response type and style.\" tool_calls=None role=\\'CHATBOT\\' message=\\'What do you know about 5G networks?\\' tool_calls=None role=\\'USER\\' message=\"Sorry, I don\\'t have any information about 5G networks. Can I help you with anything else?\" tool_calls=None role=\\'CHATBOT\\' \\nConclusion\\nIn this chapter, you learned how to build a RAG-powered chatbot with the Chat endpoint. With access to a collection of documents, the chatbot is able to provide contextually relevant responses to user requests, along with verifiable citations.\\nWe used the Chat endpoint in document mode. This mode highlights the modularity of the endpoint, giving developers the flexibility to customize each component of the system.\\nAn alternative to this is connectors mode. It abstracts away some of the steps we saw in the documents mode, which makes it simpler to build applications. It also makes it easy to connect to enterprise data sources and do that at scale.\\nContinue to the next chapter to learn about connectors and how to build RAG applications using the web search connector.\\nAbout Cohere’s LLM University\\nOur comprehensive curriculum aims to equip you with the skills to develop your own AI applications. We cater to learners from all backgrounds, covering everything from the basics to the most advanced topics in large language models (LLMs). Plus, you\\'ll have the opportunity to work on hands-on exercises, allowing you to build and deploy your very own solutions. Take a course today.\\nThis LLMU module consists of the following chapters:\\nIntroduction to RAG\\nRAG with Chat, Embed, and Rerank (this chapter)\\nRAG with Connectors\\nRAG with Quickstart Connectors\\nRAG over Large-Scale Data'),\n",
       " Document(metadata={'title': 'How to Build RAG Applications With Connectors', 'source': 'https://cohere.com/llmu/rag-connectors'}, page_content='How to Build RAG Applications With ConnectorsQualified\\nIn this chapter, you’ll learn about connectors and how to build RAG applications using the web search connector.\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nContents\\nWhat Are Connectors?\\nStep-by-Step Guide\\nSetup\\nCreate the Chatbot Component\\nRun the chatbot\\nConclusion\\nIn the previous chapter, we built the chatbot using the Chat endpoint’s document mode. Document mode provides developers with the flexibility to customize each component of a RAG stack.\\nThere is another way to build RAG systems with the Chat endpoint, which is through the connector mode. Connector mode simplifies the development of RAG systems by abstracting away some of the complexities.\\nWe’ll explore connectors over the next three chapters:\\nIn this chapter (Chapter 3), we’ll discuss how to get started quickly with connectors using the web search connector. It’s a connector managed by Cohere, and because of that, we can focus on connector usage instead of implementation.\\nIn Chapter 4, we’ll implement one of Cohere’s quickstart connectors, which are over 80 pre-built connectors that you can use to connect to popular enterprise datastores.\\nIn Chapter 5, we’ll see how to use connectors at scale, specifically on multiple datastores and long documents.\\nWhat Are Connectors?\\nConnectors are independent REST APIs that can be used in a RAG workflow to provide secure, real-time access to private data.\\nIn enterprises, data lives in many different places. The ability of enterprises to realize the full value of RAG rests on their ability to bring these data sources together. Cohere’s build-your-own connectors framework enables developers to develop a connector to any datastore that offers an accompanying search API.\\nCohere’s connectors framework simplifies connecting RAG systems to datastores\\nAt a high level, here’s what connectors do. When the Chat endpoint calls a connector, what happens is that the endpoint is sending a query to the search endpoint of that connector. The connector will then return the list of documents that it deems the most relevant to the query.\\nThe build-your-own connectors framework allows developers to build any logic behind a connector. For example, you can define the retrieval implementation—whether it’s running a semantic similarity search over a vector database, searching over an existing full-text search engine, or utilizing the existing search APIs of platforms like Google Drive or Notion.\\nAdditionally, in connector mode, most of the RAG building blocks are taken care of by the endpoint. This includes deciding whether to retrieve information, generating queries, retrieving documents, chunking and reranking documents (post-retrieval), and generating the response.\\nRecall that in the previous chapter (document mode), we implemented the following steps.\\nStep 1: Get the user message\\nStep 2: Call the Chat endpoint in query-generation mode\\nIf at least one query is generated:\\nStep 3: Retrieve and rerank relevant documents\\nStep 4: Call the Chat endpoint in document mode to generate a grounded response with citations\\nIf no query is generated:\\nStep 4: Call the Chat endpoint in normal mode to generate a direct response\\nIn connector mode, this is simplified to the following two steps.\\nStep 1: Get the user message\\nStep 2: Call the Chat endpoint in connector mode to generate a response (this can be either a grounded response with citations or a direct response)\\nStep-by-Step Guide\\nBelow is a diagram that provides an overview of what we’ll build. We’ll build a RAG chatbot that can search the web, retrieve relevant results to a user query, and generate grounded responses to the query.\\nAn overview of what we\\'ll build\\nSetup\\nFirst, let’s install and import the cohere library, and then create a Cohere client using an API key.\\nimport uuid import cohere from cohere import ChatConnector from typing import List co = cohere.Client(\"COHERE_API_KEY\") \\nCreate the Chatbot Component\\nThe change from document mode to connector mode requires just one change to the Chat endpoint, which is swapping the documents parameter with the connectors parameter.\\nHere’s how it looks with the web search connector. We supply the connector id, which is web-search as an argument to the connectors parameter.\\nresponse = co.chat_stream(message=\"What is LLM university\", connectors = [ChatConnector(id=\"web-search)]) \\nThe one line of code above is enough to get a full RAG-enabled response—the response text, the citations, and the source documents, which in this case are snippets from the most relevant information available on the web based on a given user message.\\nBut in order to run this in a multi-turn chatbot scenario, we need to build the chatbot component. The good news is that we can adapt the chatbot we built in the previous chapter.\\nThere are a few changes to make, including:\\nRemove the query generation logic (done by the endpoint)\\nRemove the retrieval logic (done by the endpoint)\\nChange the Chatbot initialization to use connectors instead\\nUse the connectors parameter instead of documents in the Chat endpoint call\\nclass Chatbot: def __init__(self, connectors: List[str]): \"\"\" Initializes an instance of the Chatbot class. \"\"\" self.conversation_id = str(uuid.uuid4()) self.connectors = [ChatConnector(id=connector) for connector in connectors] def run(self): \"\"\" Runs the chatbot application. \"\"\" while True: # Get the user message message = input(\"User: \") # Typing \"quit\" ends the conversation if message.lower() == \"quit\": print(\"Ending chat.\") break else: # If using Google Colab, remove this line to avoid printing the same thing twice print(f\"User: {message}\") # If using Google Colab, remove this line to avoid printing the same thing twice # Generate response response = co.chat_stream( message=message, model=\"command-r-plus\", conversation_id=self.conversation_id, connectors=self.connectors, ) # Print the chatbot response, citations, and documents print(\"\\\\nChatbot:\") citations = [] cited_documents = [] # Display response for event in response: if event.event_type == \"text-generation\": print(event.text, end=\"\") elif event.event_type == \"citation-generation\": citations.extend(event.citations) elif event.event_type == \"stream-end\": cited_documents = event.response.documents # Display citations and source documents if citations: print(\"\\\\n\\\\nCITATIONS:\") for citation in citations: print(citation) print(\"\\\\nDOCUMENTS:\") for document in cited_documents: print({\\'id\\': document[\\'id\\'], \\'snippet\\': document[\\'snippet\\'][:400] + \\'...\\', \\'title\\': document[\\'title\\'], \\'url\\': document[\\'url\\']}) print(f\"\\\\n{\\'-\\'*100}\\\\n\") \\nRun the Chatbot\\nAnd that’s about it. We are now ready to run the chatbot.\\nFirst we define the connector to use, which is web-search. Next, we create an instance of the Chatbot class using the connector, and then we run the chatbot.\\n# Define the connector connectors = [\"web-search\"] # Create an instance of the Chatbot class chatbot = Chatbot(connectors) # Run the chatbot chatbot.run() \\nAnd we get the same type of response as we’ve seen in the previous chapter – the text response followed by the citations and source documents used.\\nUser: What is Cohere\\'s LLM University Chatbot: Cohere\\'s LLM University (LLMU) is a set of comprehensive learning resources for anyone interested in natural language processing (NLP), from beginners to advanced learners. The curriculum covers everything from the basics of LLMs to the most advanced topics, including generative AI. The course is designed to give learners a solid foundation in NLP and help them develop their own applications. CITATIONS: start=24 end=30 text=\\'(LLMU)\\' document_ids=[\\'web-search_0\\', \\'web-search_1\\'] start=36 end=75 text=\\'set of comprehensive learning resources\\' document_ids=[\\'web-search_1\\'] start=101 end=134 text=\\'natural language processing (NLP)\\' document_ids=[\\'web-search_0\\', \\'web-search_1\\'] start=141 end=172 text=\\'beginners to advanced learners.\\' document_ids=[\\'web-search_0\\', \\'web-search_1\\'] start=177 end=187 text=\\'curriculum\\' document_ids=[\\'web-search_0\\', \\'web-search_1\\'] start=215 end=229 text=\\'basics of LLMs\\' document_ids=[\\'web-search_0\\', \\'web-search_1\\'] start=237 end=283 text=\\'most advanced topics, including generative AI.\\' document_ids=[\\'web-search_1\\'] start=326 end=349 text=\\'solid foundation in NLP\\' document_ids=[\\'web-search_0\\', \\'web-search_1\\'] start=364 end=395 text=\\'develop their own applications.\\' document_ids=[\\'web-search_0\\', \\'web-search_1\\'] DOCUMENTS: {\\'id\\': \\'web-search_0\\', \\'snippet\\': \\'Guides and ConceptsAPI ReferenceRelease NotesApplication ExamplesLLMU\\\\n\\\\nCoralDashboardDocumentationPlaygroundCommunityLog In\\\\n\\\\nCoralDashboardDocumentationPlaygroundCommunityLog In\\\\n\\\\nWelcome to LLM University!\\\\n\\\\nWelcome to LLM University by Cohere!\\\\n\\\\nWe’re so happy that you’ve chosen to learn Natural Language Processing and Large Language Models with us.\\\\n\\\\nOur comprehensive curriculum aims to give you a ...\\', \\'title\\': \\'LLM University (LLMU) | Cohere\\', \\'url\\': \\'https://docs.cohere.com/docs/llmu\\'} {\\'id\\': \\'web-search_1\\', \\'snippet\\': \"Introducing LLM University — Your Go-To Learning Resource for NLP🎓\\\\n\\\\nDiscover our comprehensive NLP curriculum at LLM University. From the fundamentals of LLMs all the way to the most advanced topics, including generative AI\\\\n\\\\nWe\\'re excited to announce the launch of LLM University (LLMU), a set of comprehensive learning resources for anyone interested in natural language processing (NLP), from begin...\", \\'title\\': \\'Introducing LLM University — Your Go-To Learning Resource for NLP🎓\\', \\'url\\': \\'https://txt.cohere.com/llm-university/\\'} ---------------------------------------------------------------------------------------------------- Ending chat. \\nConclusion\\nIn this chapter, you learned about the concept of connectors and how to build a RAG-powered chatbot using connectors. In particular, we used the web search connector, which is a Cohere-managed connector that you can use immediately.\\nContinue to the next chapter to learn how to connect RAG applications to datastores by leveraging Cohere’s pre-built quickstart connectors.\\nAbout Cohere’s LLM University\\nOur comprehensive curriculum aims to equip you with the skills to develop your own AI applications. We cater to learners from all backgrounds, covering everything from the basics to the most advanced topics in large language models (LLMs). Plus, you\\'ll have the opportunity to work on hands-on exercises, allowing you to build and deploy your very own solutions. Take a course today.\\nThis LLMU module consists of the following chapters:\\nIntroduction to RAG\\nRAG with Chat, Embed, and Rerank\\nRAG with Connectors (this chapter)\\nRAG with Quickstart Connectors\\nRAG over Large-Scale Data'),\n",
       " Document(metadata={'title': 'How to Build RAG Applications With Quickstart Connectors', 'source': 'https://cohere.com/llmu/rag-quickstart-connectors'}, page_content='How to Build RAG Applications With Quickstart ConnectorsQualified\\nIn this chapter, you’ll learn how to connect RAG applications to datastores by leveraging Cohere’s pre-built quickstart connectors.\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook. Note: To run the notebook, you must first deploy your own Google Drive connector as a web-based REST API (we’ll cover the steps in this chapter – here is the implementation code).\\nContents\\nWhat Are Quickstart Connectors?\\nStep-by-Step Guide\\nSetup\\nBuild and Deploy the Connector \\nEnable Google Drive Access\\nConfigure the Connector\\nAdd Authentication\\nDeploy the Connector\\nTest the Connector\\nRegister the Connector\\nUse the Connector\\nConclusion\\nIn the previous chapter, you learned about the concept of connectors and how to build a RAG-powered chatbot using connectors. In particular, we used the web search connector, a Cohere-managed connector.\\nIn this chapter, you’ll learn about how to build your own connector using one of 80+ pre-built quickstart connectors. We’ll use it to connect a chatbot to a Google Drive, enabling the chatbot to use the Google Drive API to find answers to a user’s question by searching documents in the Google Drive.\\nWhat Are Quickstart Connectors?\\nCohere’s build-your-own connectors framework enables developers to build a connector to any datastore that offers an accompanying search API. However, connecting to multiple data sources still requires effort to build the connectors. Developers will need to learn about the API of each platform, whether it is Google Drive, Slack, Notion, or GitHub, before they can build the connector. As the number of data sources increases, this becomes a bottleneck.\\nQuickstart connectors solve this problem. Developers can use pre-built implementations of over 80 connectors immediately without having to build them themselves. They can either use these quickstart connectors directly or adapt them to their organization’s needs.\\nCohere’s quickstart connectors are open-sourced and available in our connectors repository. Here are a few examples:\\nVector databases: Weaviate, Qdrant, Pinecone, Milvus\\nFull-text search engines: Elastic, Opensearch, Vespa, Solr\\nCollaboration: Slack, Linear, Asana, Jira, Trello, Miro\\nFile management: Google Drive, Dropbox, Box\\nData warehouse: Snowflake, Amazon Redshift\\nContent management: Readme, Wordpress, Medium\\nand many more\\nStep-by-Step Guide\\nTo illustrate how quickstart connectors work, let’s build an example RAG-powered chatbot and connect the Cohere Chat endpoint to Google Drive.\\nAn overview of what we\\'ll build\\nHere are some additional references:\\nChat endpoint API reference page, containing a detailed description of the Chat endpoint’s input parameters and response objects\\nConnectors documentation\\nFirst, let’s install and import the cohere library, and then create a Cohere client using an API key.\\nSetup\\nFirst, let’s install and import the cohere library, and then create a Cohere client using an API key.\\nimport cohere from cohere import ChatConnector import uuid from typing import List co = cohere.Client(\"COHERE_API_KEY\") \\nBuild and Deploy the Connector\\nWe’ll need to first build a Google Drive connector and deploy it as a web-based REST API.\\nEnable Google Drive Access\\nFirst, we need to enable access to Google Drive. The Google Drive connector supports two types of authentication: Service Account and OAuth. In this example, we’ll use the Service Account option.\\nThe steps are as follows:\\nCreate a project in Google Cloud Console.\\nCreate a service account and activate the Google Drive API in the Google Cloud Console.\\nCreate a service account key and download the credentials file as JSON. We’ll need to use this later. The credentials file should look like this:\\n{ \"type\": \"service_account\", \"project_id\": \"{project id}\", \"private_key_id\": \"{private_key_id}\", \"private_key\": \"{private_key}\", \"client_email\": \"{client_email}\", \"client_id\": \"{client_id}\", \"auth_uri\": \"{auth_uri}\", \"token_uri\": \"{token_uri}\", \"auth_provider_x509_cert_url\": \"{auth_provider_x509_cert_url}\", \"client_x509_cert_url\": \"{client_x509_cert_url}\", \"universe_domain\": \"{universe_domain}\" } \\nOnce you are done with these steps, go to any Google Drive account and share the folder(s) you want to search with the service account email address. You can find the email address in Google Cloud Console > Service Accounts > Your project. As an example, we\\'ll use the contents from LLM University: Prompt Engineering which explains the techniques of prompting LLMs. It consists of five web pages, which we’ll download and store as five documents in Google Drive.\\nConfigure the Connector\\nNext, we clone the quickstart connector repository and choose the Google Drive connector from the list of options. Here’s a quick look at what’s inside the Google Drive connector:\\napp: This module handles the API endpoints and authentication. It defines the search function, which is the endpoint for the search API. It also handles the extraction of the access token from the request headers.\\nasync_download: This module is responsible for downloading files from Google Drive asynchronously.\\nprovider: This module interacts with the Google Drive API. It defines the search function, which performs a search on Google Drive using the provided query and access token. It also processes the search results, extracting the necessary information and downloading the file contents using the async_download module.\\napi.yaml file: This OpenAPI specification describes the API endpoints, request bodies, responses, and security schemes.\\nWe then create an .env file to define the environment variables (use the .env-template file provided).\\nFinally, we define the GDRIVE_SERVICE_ACCOUNT_INFO. For this, we convert the service account key we created earlier to a string through json.dumps(credentials) and use this string as the environment variable.\\nAdd Authentication\\nOur next step involves adding another environment variable, GDRIVE_CONNECTOR_API_KEY. This is required for connector authentication purposes.\\nImplementing authentication and authorization methods to protect your web services is crucial. It ensures that only legitimate users can access your service, protects data from being accessed or modified by unauthorized individuals, and prevents service abuse.\\nCohere supports a few authentication/authorization options: service-to-service authentication, OAuth 2.0, and pass-through. Find more details about how to implement these in the documentation.\\nIn service-to-service authentication, Cohere sends requests to our connector with this connector API key. You can generate an API key using libraries like secrets.\\nimport secrets secrets.token_urlsafe(32) \\nFinally, when registering the API as a connector, we include this connector API key. As a result, Cohere will pass the key in every search request to the connector. We’ll see how to register a connector shortly.\\nDeploy the Connector\\nNext, we deploy the connector API as a web service that can be accessed via the Internet. There are many options available, including cloud service providers, platform-as-a-service (PaaS) providers, and more. We won’t cover deploying the API in this article, but if you are looking for an example, refer to this Render template for deploying a Poetry/Flask application or this guide on Docker deployment.\\nTo make the API compatible as a connector, ensure the following:\\nExpose an endpoint that will return the retrieval results, which in our case is the search endpoint\\nThis endpoint must return a list of dictionaries called results\\nEach dictionary item can contain any number of fields, with the minimum being the text field\\nWith the quickstart connectors, these have already been implemented.\\nTest the Connector\\nNow, we can test if the API is working by making a curl request to the search endpoint.\\ncurl --request POST \\\\ --url https://demo-conn-gdrive.myurl.com/search \\\\ --header \\'Authorization: Bearer YOUR_CONNECTOR_API_KEY\\' \\\\ --header \\'Content-Type: application/json\\' \\\\ --data \\'{\"query\": \"word embeddings\"}\\' \\nA successful response will return data containing some metadata followed by the fields that a connector is configured to provide (in this case, text, title, and url).\\n{ \"results\": [ { \"editedBy\": \"Meor Amer\", \"id\": \"10x9mJOnEr62hg1IFxgAtD1aIFS4NXJ2l5Lt-UhJXLVg\", \"mimeType\": \"application/vnd.google-apps.document\", \"modifiedTime\": \"2023-12-01T07:49:27.196Z\", \"text\": \"\\\\ufeffEvaluating Outputs\\\\r\\\\nIn this chapter, you\\'ll learn about the different techniques for evaluating LLM outputs.Introduction\\\\r\\\\nLarge language models (LLMs) offer exciting new ways to build applications that leverage natural language as the interface ...\", \"title\": \"Evaluating Outputs\", \"url\": \"https://docs.google.com/document/d/10x9mJOnEr62hg1IFxgAtD1aIFS4NXJ2l5Lt-UhJXLVg/edit?usp=drivesdk\" }, { \"editedBy\": \"Meor Amer\", \"id\": \"1wngAfCJY1IgD6H__4AkQXFfymKUpSeJL13TItbigdyA\", \"mimeType\": \"application/vnd.google-apps.document\", \"modifiedTime\": \"2023-12-01T07:50:02.204Z\", \"text\": \"\\\\ufeffValidating Outputs\\\\r\\\\nIn this chapter ...\", \"title\": \"Validating Outputs\", \"url\": \"https://docs.google.com/document/d/1wngAfCJY1IgD6H__4AkQXFfymKUpSeJL13TItbigdyA/edit?usp=drivesdk\" }, ... } ] } \\nRegister the Connector\\nNext, we register the Google Drive connector as a connector with Cohere. We do this by sending a POST request to the Cohere API. We’ll need to provide the following information:\\nThe Cohere API key\\nThe name we want to call this connector\\nThe URL of the connector API’s search endpoint\\nThe connector API key\\nHere’s an example request:\\ncurl --request POST \\\\\\\\\\\\\\\\ --url \\'<https://api.cohere.ai/v1/connectors>\\' \\\\\\\\\\\\\\\\ --header \\'Authorization: Bearer {Cohere API key}\\' \\\\\\\\\\\\\\\\ --header \\'Content-Type: application/json\\' \\\\\\\\\\\\\\\\ --data \\'{ \"name\":\"demo-conn-gdrive\", \"url\":\"<https://demo-conn-gdrive.example.com/search>\", \"service_auth\": { \"type\": \"bearer\", \"token\": \"{Connector API Key}\" } }\\' \\nAnd with that, we have successfully registered the API as a connector.\\nUse the Connector\\nOur new connector is now ready to use. To create a chatbot, we can reuse the same exact code from the previous chapter.\\nclass Chatbot: def __init__(self, connectors: List[str]): \"\"\" Initializes an instance of the Chatbot class. \"\"\" self.conversation_id = str(uuid.uuid4()) self.connectors = [ChatConnector(id=connector) for connector in connectors] def run(self): \"\"\" Runs the chatbot application. \"\"\" while True: # Get the user message message = input(\"User: \") # Typing \"quit\" ends the conversation if message.lower() == \"quit\": print(\"Ending chat.\") break else: # If using Google Colab, remove this line to avoid printing the same thing twice print(f\"User: {message}\") # If using Google Colab, remove this line to avoid printing the same thing twice # Generate response response = co.chat_stream( message=message, model=\"command-r-plus\", conversation_id=self.conversation_id, connectors=self.connectors, ) # Print the chatbot response, citations, and documents print(\"\\\\nChatbot:\") citations = [] cited_documents = [] # Display response for event in response: if event.event_type == \"text-generation\": print(event.text, end=\"\") elif event.event_type == \"citation-generation\": citations.extend(event.citations) elif event.event_type == \"stream-end\": cited_documents = event.response.documents # Display citations and source documents if citations: print(\"\\\\n\\\\nCITATIONS:\") for citation in citations: print(citation) print(\"\\\\nDOCUMENTS:\") for document in cited_documents: print({\\'id\\': document[\\'id\\'], \\'text\\': document[\\'text\\'][:50] + \\'...\\'}) print(f\"\\\\n{\\'-\\'*100}\\\\n\") \\nAnd when running the chatbot, we define the connector we have created.\\n# Define the connector connectors = [\"demo-conn-gdrive-6bfrp6\"] # Create an instance of the Chatbot class chatbot = Chatbot(connectors) # Run the chatbot chatbot.run() \\nHere’s an example conversation:\\nUser: What is prompt engineering Chatbot: Prompt engineering is a process of constructing prompts for a large language model (LLM) to get a desired response. It involves techniques such as adding specific details, context, output format requirements, and examples to a prompt to guide the LLM\\'s response. Prompt engineering also involves chaining multiple prompts together to accomplish a complex task and validating and evaluating the outputs of LLMs to ensure the quality and accuracy of the responses. CITATIONS: start=35 end=55 text=\\'constructing prompts\\' document_ids=[\\'demo-conn-gdrive-6bfrp6_0\\', \\'demo-conn-gdrive-6bfrp6_1\\', \\'demo-conn-gdrive-6bfrp6_2\\', \\'demo-conn-gdrive-6bfrp6_3\\', \\'demo-conn-gdrive-6bfrp6_4\\'] start=62 end=88 text=\\'large language model (LLM)\\' document_ids=[\\'demo-conn-gdrive-6bfrp6_0\\', \\'demo-conn-gdrive-6bfrp6_3\\'] start=98 end=115 text=\\'desired response.\\' document_ids=[\\'demo-conn-gdrive-6bfrp6_0\\', \\'demo-conn-gdrive-6bfrp6_3\\'] start=154 end=170 text=\\'specific details\\' document_ids=[\\'demo-conn-gdrive-6bfrp6_0\\'] start=172 end=179 text=\\'context\\' document_ids=[\\'demo-conn-gdrive-6bfrp6_0\\'] start=181 end=207 text=\\'output format requirements\\' document_ids=[\\'demo-conn-gdrive-6bfrp6_0\\'] start=213 end=221 text=\\'examples\\' document_ids=[\\'demo-conn-gdrive-6bfrp6_0\\'] start=237 end=262 text=\"guide the LLM\\'s response.\" document_ids=[\\'demo-conn-gdrive-6bfrp6_0\\'] start=296 end=330 text=\\'chaining multiple prompts together\\' document_ids=[\\'demo-conn-gdrive-6bfrp6_1\\'] start=364 end=374 text=\\'validating\\' document_ids=[\\'demo-conn-gdrive-6bfrp6_3\\'] start=379 end=389 text=\\'evaluating\\' document_ids=[\\'demo-conn-gdrive-6bfrp6_4\\'] start=424 end=431 text=\\'quality\\' document_ids=[\\'demo-conn-gdrive-6bfrp6_4\\'] start=436 end=444 text=\\'accuracy\\' document_ids=[\\'demo-conn-gdrive-6bfrp6_3\\'] DOCUMENTS: {\\'id\\': \\'demo-conn-gdrive-6bfrp6_0\\', \\'text\\': \"\\\\ufeffConstructing Prompts\\\\r\\\\nIn this chapter, you\\'ll lea...\"} {\\'id\\': \\'demo-conn-gdrive-6bfrp6_1\\', \\'text\\': \"\\\\ufeffChaining Prompts\\\\r\\\\nIn this chapter, you\\'ll learn a...\"} {\\'id\\': \\'demo-conn-gdrive-6bfrp6_2\\', \\'text\\': \"\\\\ufeffUse Case Patterns\\\\r\\\\nIn this chapter, you\\'ll learn ...\"} {\\'id\\': \\'demo-conn-gdrive-6bfrp6_3\\', \\'text\\': \"\\\\ufeffValidating Outputs\\\\r\\\\nIn this chapter, you\\'ll learn...\"} {\\'id\\': \\'demo-conn-gdrive-6bfrp6_4\\', \\'text\\': \"\\\\ufeffEvaluating Outputs\\\\r\\\\nIn this chapter, you\\'ll learn...\"} ---------------------------------------------------------------------------------------------------- Ending chat. \\nConclusion\\nIn this chapter, you learned how to build your own connector for Google Drive, one of 80+ pre-built quickstart connectors available.\\nContinue to the next chapter to learn how to build RAG applications over multiple datastores and long documents.\\nAbout Cohere’s LLM University\\nOur comprehensive curriculum aims to equip you with the skills to develop your own AI applications. We cater to learners from all backgrounds, covering everything from the basics to the most advanced topics in large language models (LLMs). Plus, you\\'ll have the opportunity to work on hands-on exercises, allowing you to build and deploy your very own solutions. Take a course today.\\nThis LLMU module consists of the following chapters:\\nIntroduction to RAG\\nRAG with Chat, Embed, and Rerank\\nRAG with Connectors\\nRAG with Quickstart Connectors (this chapter)\\nRAG over Large-Scale Data'),\n",
       " Document(metadata={'title': 'How to Build RAG Applications Over Large-Scale Data', 'source': 'https://cohere.com/llmu/rag-large-scale-data'}, page_content='How to Build RAG Applications Over Large-Scale DataQualified\\nIn this chapter, you’ll learn how to build RAG applications over multiple datastores and long documents.\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook. Note: To run the notebook, you must first deploy your own Google Drive connector as a web-based REST API (we covered the steps in the previous chapter).\\nContents\\nStep-by-Step Guide\\nSetup\\nUsing Multiple Connectors\\nHandling Long and Large Volume of Documents \\nChunking\\nReranking\\nInterleaving\\nPrompt Building\\nConclusion\\nIn the previous chapter, you learned how to build your own Google Drive connector, which is one of 80+ pre-built quickstart connectors available.\\nIn this chapter, you’ll learn how to use connectors at scale, such as connecting to multiple datastores, working with large volumes of documents, and handling long documents. Enterprises need a RAG system that can efficiently handle vast amounts of data from diverse sources, and in this chapter, you’ll learn about how this can be automated with the Chat endpoint.\\nIn an enterprise setting, data is distributed across multiple platforms and datastores. The real value of using connectors comes from being able to use multiple connectors at the same time. This way, we are maximizing the RAG system’s potential as an intelligent knowledge assistant, giving it access to various data sources, so it can synthesize the information from all these data sources.\\nStep-by-Step Guide\\nLet’s now look at an example of using the two connectors we used in the previous two chapters: Google Drive and web search.\\nAn overview of what we\\'ll build\\nSetup\\nFirst, let’s install and import the cohere and other necessary libraries, and then create a Cohere client using an API key.\\nimport cohere from cohere import ChatConnector import uuid from typing import List, Dict co = cohere.Client(\"COHERE_API_KEY\") \\nUsing Multiple Connectors\\nIn the previous two chapters, we only examined examples where one connector was defined at a time. However, the Chat endpoint can accept multiple connectors and retrieve information from all the defined connectors.\\nTo create a chatbot, we can reuse the same exact code we used in the previous chapter.\\nclass Chatbot: def __init__(self, connectors: List[str]): \"\"\" Initializes an instance of the Chatbot class. \"\"\" self.conversation_id = str(uuid.uuid4()) self.connectors = [ChatConnector(id=connector) for connector in connectors] def run(self): \"\"\" Runs the chatbot application. \"\"\" while True: # Get the user message message = input(\"User: \") # Typing \"quit\" ends the conversation if message.lower() == \"quit\": print(\"Ending chat.\") break else: # If using Google Colab, remove this line to avoid printing the same thing twice print(f\"User: {message}\") # If using Google Colab, remove this line to avoid printing the same thing twice # Generate response response = co.chat_stream( message=message, model=\"command-r-plus\", conversation_id=self.conversation_id, connectors=self.connectors, ) # Print the chatbot response, citations, and documents print(\"\\\\nChatbot:\") citations = [] cited_documents = [] # Display response for event in response: if event.event_type == \"text-generation\": print(event.text, end=\"\") elif event.event_type == \"citation-generation\": citations.extend(event.citations) elif event.event_type == \"stream-end\": cited_documents = event.response.documents # Display citations and source documents if citations: print(\"\\\\n\\\\nCITATIONS:\") for citation in citations: print(citation) print(\"\\\\nDOCUMENTS:\") for document in cited_documents: print({\\'id\\': document[\\'id\\'], \\'text\\': document.get(\\'text\\', document.get(\\'snippet\\', \\'\\'))[:50] + \\'...\\'}) # \"text\" for Gdrive, \"snippet\" for web search print(f\"\\\\n{\\'-\\'*100}\\\\n\") \\nAnd when running the chatbot, we define the connectors we want the endpoint to retrieve information from.\\nThe Chatbot class has already been prepared to accept multiple connectors.\\nclass Chatbot: def __init__(self, connectors: List[str]): ... self.connectors = [{\"id\": c} for c in connectors] ... \\nAnd what’s actually sent as the connectors parameter in the endpoint call will be the following. \\nresponse = co.chat( message=message, connectors=[ChatConnector(id=\"demo-conn-gdrive-6bfrp6\"), ChatConnector(id=\"web-search\")] ... ) \\nWhen creating the Chatbot instance, we define the connector IDs as a list of strings.\\n# Define connectors connectors = [\"demo-conn-gdrive-6bfrp6\", \"web-search\"] # Create an instance of the Chatbot class chatbot = Chatbot(connectors) # Run the chatbot chatbot.run() \\nHere’s an example conversation. The connector uses information retrieved from both sources, as can be seen in the list of source documents.\\nUser: What is chain of thought prompting Chatbot: Chain-of-Thought (CoT) prompting is a technique used to guide Large Language Models (LLMs) to follow a reasoning process when dealing with complex problems. This is done by providing the model with a few examples where the step-by-step reasoning is clearly laid out. The model is then expected to follow that \"chain of thought\" reasoning to get to the correct answer. CoT prompting is a prompt engineering technique that aims to improve language models\\' performance on tasks requiring logic, calculation and decision-making by structuring the input prompt in a way that mimics human reasoning. To construct a chain-of-thought prompt, a user typically appends an instruction such as \"Describe your reasoning in steps\" or \"Let\\'s think step by step\" to the end of their query to a large language model (LLM). This encourages the model to generate intermediate steps before providing a final answer. CITATIONS: start=17 end=22 text=\\'(CoT)\\' document_ids=[\\'web-search_0\\', \\'web-search_1\\', \\'web-search_2\\', \\'web-search_3\\', \\'web-search_5\\', \\'web-search_7\\', \\'web-search_8\\', \\'demo-conn-gdrive-6bfrp6_11\\', \\'demo-conn-gdrive-6bfrp6_12\\'] start=56 end=61 text=\\'guide\\' document_ids=[\\'web-search_3\\', \\'web-search_4\\', \\'web-search_7\\'] start=62 end=83 text=\\'Large Language Models\\' document_ids=[\\'web-search_0\\', \\'web-search_2\\', \\'web-search_3\\', \\'web-search_4\\', \\'web-search_5\\', \\'web-search_7\\', \\'demo-conn-gdrive-6bfrp6_11\\'] start=84 end=90 text=\\'(LLMs)\\' document_ids=[\\'web-search_0\\', \\'web-search_2\\', \\'web-search_3\\', \\'web-search_4\\', \\'web-search_5\\', \\'web-search_7\\', \\'demo-conn-gdrive-6bfrp6_11\\'] start=94 end=120 text=\\'follow a reasoning process\\' document_ids=[\\'web-search_1\\', \\'web-search_3\\', \\'web-search_4\\', \\'web-search_7\\'] start=139 end=156 text=\\'complex problems.\\' document_ids=[\\'web-search_3\\', \\'web-search_5\\', \\'web-search_7\\'] start=200 end=212 text=\\'few examples\\' document_ids=[\\'web-search_1\\', \\'web-search_3\\', \\'web-search_5\\', \\'web-search_7\\', \\'demo-conn-gdrive-6bfrp6_11\\'] start=223 end=266 text=\\'step-by-step reasoning is clearly laid out.\\' document_ids=[\\'web-search_1\\', \\'web-search_3\\', \\'web-search_4\\', \\'web-search_5\\', \\'web-search_7\\', \\'demo-conn-gdrive-6bfrp6_11\\'] start=297 end=337 text=\\'follow that \"chain of thought\" reasoning\\' document_ids=[\\'web-search_3\\', \\'web-search_5\\'] start=341 end=367 text=\\'get to the correct answer.\\' document_ids=[\\'web-search_3\\', \\'web-search_4\\', \\'web-search_5\\'] start=388 end=416 text=\\'prompt engineering technique\\' document_ids=[\\'web-search_4\\', \\'web-search_5\\'] start=430 end=466 text=\"improve language models\\' performance\" document_ids=[\\'web-search_4\\'] start=486 end=524 text=\\'logic, calculation and decision-making\\' document_ids=[\\'web-search_4\\'] start=528 end=556 text=\\'structuring the input prompt\\' document_ids=[\\'web-search_4\\'] start=571 end=594 text=\\'mimics human reasoning.\\' document_ids=[\\'web-search_4\\'] start=684 end=718 text=\\'\"Describe your reasoning in steps\"\\' document_ids=[\\'web-search_4\\', \\'demo-conn-gdrive-6bfrp6_11\\'] start=722 end=748 text=\\'\"Let\\\\\\'s think step by step\"\\' document_ids=[\\'web-search_1\\', \\'web-search_3\\', \\'web-search_5\\', \\'web-search_8\\', \\'demo-conn-gdrive-6bfrp6_11\\', \\'demo-conn-gdrive-6bfrp6_12\\'] start=846 end=864 text=\\'intermediate steps\\' document_ids=[\\'web-search_0\\', \\'web-search_1\\', \\'web-search_2\\', \\'web-search_3\\', \\'web-search_4\\', \\'web-search_5\\', \\'web-search_7\\', \\'demo-conn-gdrive-6bfrp6_11\\'] DOCUMENTS: {\\'id\\': \\'web-search_0\\', \\'text\\': \\'Skip to main content\\\\n\\\\nWe gratefully acknowledge su...\\'} {\\'id\\': \\'web-search_1\\', \\'text\\': \\'General Tips for Designing Prompts\\\\n\\\\nChain-of-Thoug...\\'} {\\'id\\': \\'web-search_2\\', \\'text\\': \\'BlogDocsCommunityHackAPrompt Playground\\\\n\\\\nLanguage ...\\'} {\\'id\\': \\'web-search_3\\', \\'text\\': \\'We now support using Microsoft Azure hosted OpenAI...\\'} {\\'id\\': \\'web-search_5\\', \\'text\\': \\'Comprehensive Guide to Chain-of-Thought Prompting\\\\n...\\'} {\\'id\\': \\'web-search_7\\', \\'text\\': \\'ResourcesArticleChain-of-Thought Prompting: Helpin...\\'} {\\'id\\': \\'web-search_8\\', \\'text\\': \\'Skip to main content\\\\n\\\\nScan this QR code to downloa...\\'} {\\'id\\': \\'demo-conn-gdrive-6bfrp6_11\\', \\'text\\': \"\\\\ufeffConstructing Prompts\\\\r\\\\nIn this chapter, you\\'ll lea...\"} {\\'id\\': \\'demo-conn-gdrive-6bfrp6_12\\', \\'text\\': \"\\\\ufeffUse Case Patterns\\\\r\\\\nIn this chapter, you\\'ll learn ...\"} {\\'id\\': \\'web-search_4\\', \\'text\\': \\'Tech Accelerator What is generative AI? Everything...\\'} ---------------------------------------------------------------------------------------------------- Ending chat. \\nHandling Long and Large Volume of Documents\\nWith all these documents coming from various connectors, you may be asking a couple of questions:\\nHow to handle long documents? Connecting to multiple connectors means having to deal with various APIs, each with its own way of providing documents. Some may return a complete document with tens or hundreds of pages. There are a couple of problems with this. First, stuffing a long document into an LLM prompt means its context limit will be reached, resulting in an error. Second, even if the context limit is not reached, the LLM response will likely not be very good because it is getting a lot of irrelevant information from a long document instead of specific chunks from the document that are the most relevant.\\nHow to handle multiple documents from multiple connectors and queries? For a specific connector, the retrieval and reranking implementation is within the developer’s control. But with multiple connectors, that is not possible because these documents are aggregated at the Chat endpoint. As the number of connectors increases, this becomes a bigger problem because we don’t have control over the relevancy of the documents sent to the LLM prompt. And then there is the same problem of possible context length limits being reached. Furthermore, if more than one query is generated, the number of documents retrieved will multiply with the same number.\\nThe Chat endpoint solves these problems with its automated chunking and reranking process. Let’s see how it’s done.\\nNote that for this to happen, the prompt_truncation parameter should be set as AUTO (default) and not OFF.\\nChunking\\nThe command-r family of models supports a large context length (128k tokens), offering ample room for retrieved documents. However, in the scenario where this context length is exceeded, the automated chunking feature will be activated. \\nThe first step is to split every document sent by the connectors into smaller chunks. Each chunk is between 100 and 400 words, and sentences are kept intact where possible.\\nChunking the retrieved documents\\nReranking\\nThe Chat endpoint then uses the Rerank endpoint to take all the chunked documents from all connectors and rerank them based on contextual relevance to the query.\\nReranking the chunked documents\\nThis will be independent for each query and connector. For example, let’s say that a user asks the question, “What is AI and how can enterprises use it?” resulting in two queries generated by the endpoint: “What is AI?” and “How can enterprises use AI?” Also, let’s assume that there are two connectors: “web search” and “notion.”\\nThis means that there will be four lists of chunked documents (two queries for two connectors), each to be reranked separately.\\nThe reranking step takes the top 20 chunks from each list and drops the rest.\\nInterleaving\\nThe reranked documents from the different lists are then interleaved into one list.\\nInterleaving the reranked chunks\\nWith our example above, let’s say that these are the four lists of reranked documents:\\nWeb search results (“What is AI”): web_ai_1, web_ai_2, web_ai_3\\nNotion search results (“What is AI”): notion_ai_1, notion_ai_2, notion_ai_3\\nWeb search results (“How can enterprises use AI”): web_enterprise_1, web_enterprise_2, web_enterprise_3\\nNotion search results (“How can enterprises use AI”): notion_enterprise_1, notion_enterprise_2, notion_enterprise_3\\nThe documents will be interleaved in a list in this order:\\nDocuments: web_ai_1, notion_ai_1, web_enterprise_1, notion_enterprise_1, web_ai_2, notion_ai_2, web_enterprise_2, notion_enterprise_2, web_ai_3, notion_ai_3, web_enterprise_3, notion_enterprise_3\\nThis list is what gets sent to the LLM prompt.\\nPrompt Building\\nBy setting the prompt_truncation parameter by setting it to AUTO, some elements from chat_history and documents will be dropped in an attempt to construct a prompt that fits within the model\\'s context length limit.\\nDocuments and chat history will be iteratively added until the prompt is too long. This prompt will be passed to the Command model for response generation.\\nConclusion\\nIn this chapter, you learned how to use connectors at scale. The Chat endpoint allows you to define multiple connectors in an endpoint call, and will aggregate the retrieved documents from these connectors. You can also leverage the automated handling of long documents and large volumes of documents, where the endpoint takes care of chunking, reranking, and interleaving of documents, as well as prompt building.\\nAbout Cohere’s LLM University\\nOur comprehensive curriculum aims to equip you with the skills to develop your own AI applications. We cater to learners from all backgrounds, covering everything from the basics to the most advanced topics in large language models (LLMs). Plus, you\\'ll have the opportunity to work on hands-on exercises, allowing you to build and deploy your very own solutions. Take a course today.\\nThis LLMU module consists of the following chapters:\\nIntroduction to RAG\\nRAG with Chat, Embed, and Rerank\\nRAG with Connectors\\nRAG with Quickstart Connectors\\nRAG over Large-Scale Data (this chapter)'),\n",
       " Document(metadata={'title': 'From RAG to Tool Use', 'source': 'https://cohere.com/llmu/from-rag-to-tool-use'}, page_content='From RAG to Tool UseQualified\\nContents\\nIntroduction\\nRAG\\nTool Use\\nDecision-making\\nAction\\nFunction Calling\\nCitations\\nConclusion\\nIntroduction\\nWhat does it take to turn an application that can answer questions like:\\n“How many employees does Microsoft have?”\\n…into one that can automate tasks like:\\n“Create a plot of the number of employees at the three tech companies with the highest market cap in the United States in 2024.”?\\nThe answer is to transition from a retrieval-augmented generation (RAG) to a tool use approach.\\nSuppose we are asking a human assistant to perform these two tasks. For the first task, simply entering the exact question into a web search engine would provide the answer.\\nAs for the second task, they would have to:\\nBreak down the task into smaller parts and perform them in a sequence (first, find the three companies, and only then find the employee number for each)\\nPerform an action multiple times (multiple web searches)\\nNot only perform information retrieval (web search) but also perform some kind of action (creating a plot)\\nThis planning, and then the execution of every task, can be performed with tool use.\\nTool use can automate the tasks required to fulfill a user\\'s request \\nIn this module, we’ll learn how to leverage tool use by building:\\nA RAG assistant that can query structured data about sales and product\\nA calendar assistant that can check and schedule appointments\\nA data analyst that’s capable of answering questions like the above\\nRAG\\nTo understand what tool use is, it’s helpful to start by understanding RAG. We’ll only do a brief recap here as we covered RAG in detail in Module 7.\\nRAG gives LLMs the capability to ground their responses in external text data. This means we can build applications that are more robust against hallucination, can access recent events, and are personalized to an enterprise’s data and context.\\nWith RAG, an LLM retrieves unstructured text documents from external data sources to provide useful and accurate responses to user queries.\\nRAG utilizes text documents from external data sources\\nA common setup is to store these documents in a vector database as text embeddings. During query time, we use search techniques, such as semantic search, to retrieve the documents most relevant to a user\\'s query.\\nThese documents are then augmented to the LLM prompt, acting as the context to inform and guide the LLM\\'s response to the user.\\nThe document retrieval system can be set up for any source that contains textual, unstructured data. Examples would be a company\\'s meeting notes stored on Google Drive, its internal wiki stored in Notion, its customer conversations on Intercom, PDF documents, and so on.\\nIn a RAG workflow, documents are used to guide an LLM\\'s response\\nTool use takes the RAG concept to the next level. It extends the principles of RAG, where external systems are used to guide the response of an LLM, but by leveraging a much bigger set of tools than what’s possible with RAG — as we’ll see in this module.\\nWith the RAG example earlier, we are already using a type of tool — the document retriever. But tools can come in many forms beyond just that.\\nIf there is a way to communicate and send information to and from a service via an API, that service can become a tool for an LLM to utilize. It can be a web search engine, a SQL database, a weather forecast service, or a Python interpreter, to name a few.\\nSo, instead of retrieving information from unstructured documents (typical RAG), we can retrieve information from any source (tool use). Plus, we\\'ll see later on that this tool use setup allows us to not only retrieve information from external tools, but also take actions through them.\\nIn a tool use workflow, results from tool calls are used to guide an LLM\\'s response\\nDecision-Making\\nThe concept of tool use expands on LLMs\\' capabilities of acting as a reasoning and decision-making engine.\\nWith a standard RAG setup, an LLM needs to make one type of decision. Given a user query, it has to decide whether it needs external data to respond or can respond directly. With Cohere’s Chat endpoint, this is done using the search query generation feature.\\nFor example, \"How do I build a great company?\" doesn\\'t usually require retrieving a specific piece of data, but \"What was Apple\\'s revenue in 2023?\" does.\\nDecision-making in RAG\\nWith tool use, the model needs to make two types of decisions.\\nThe first is similar to RAG, like in the example above, which is a yes/no decision on whether a user query can be answered directly or requires some external context\\nThe second is if tools are required, which tools would those be (and how to call them)\\nIn a tool use setting, a model is often given several tools to pick from. For example, let\\'s say that those are a web search engine and a Python interpreter.\\nSo, if a user asks, \"What were Apple\\'s and Google\\'s revenue in 2023?\" Then, the model would determine that only the web search engine is needed.\\nBut if the question is \"What were Apple\\'s and Google\\'s revenue in 2023, and plot them on a bar chart?\" then the model would determine that both the web search engine and the Python interpreter are needed.\\nDecision-making in tool use\\nThis concept of routing a request to only the relevant tools can enhance the quality of a model’s response. In a naive RAG setting, the application would aggregate retrieved documents from all the different sources it is connected to. This may contribute to noise from less relevant documents.\\nAdditionally, it doesn’t take into consideration that, given a data source\\'s nature, it might be less or more relevant to a query compared to the other data sources.\\nFor example, suppose an application is connected to two data sources, one to access public data (the web) and the other to private data (a company’s internal Notion knowledge base). Given the question of “Who were the largest car manufacturers in 2023?”, the information is more likely to be available publicly, and it makes more sense to search just the web rather than both.\\nA naive RAG system does not offer this routing capability. Moving to a tool use paradigm solves this.\\nTool use enables routing a request to only the relevant tools\\nAction\\nA standard RAG setup is read-only. The RAG application can only retrieve documents from the source (or a read action), but it doesn\\'t need to make any changes in the opposite direction (or a write action).\\nA standard RAG setup is read-only\\nTool use enables developers to build applications that can take actions. That is, doing both read and write operations on an external system.\\nFor example, if it is a calendar service, the tool use approach not only enables the application to look up a person’s availability (read) but also create new events on the calendar (write).\\nTool use enables developers to build applications that can take actions\\nThe ability to read, write, and call APIs means that an LLM is more connected to the external world, can take a broader range of actions, and is more flexible overall.\\nTool use enables LLMs to be more connected to the external world\\nFunction Calling\\nIf tool use is the what of LLM capability extension, function calling is the how.\\nWhile tool use is a broader concept that encompasses the use of various external systems and tools to enhance LLM capabilities, function calling, on the other hand, is a specific implementation of tool use, focusing on the technical details of picking the tools to call and generating the payload to call the tool.\\nFunction calling is the process of tool selection and payload generation to perform specific tasks within a program. It is a standardized way for an LLM to interface with tools to send and receive information.\\nEvery tool can be represented as a function with a schema that informs the LLM about what the tool does, what parameters the tool accepts, what these parameters do, and the types of the parameters. These are used by the LLM to make tool use decisions. We’ll go deeper into function calling in the coming chapters.\\nFunction calling is a specific implementation of tool use\\nCitations\\nTool use is such a natural extension of RAG that many concepts we see in RAG can be ported over to the tool use scenario.\\nOne such way is generating citations. A big part of building a successful RAG application is implementing a citation mechanism to trace LLM responses back to the source documents. This allows an LLM’s responses to be easily verifiable, building trust with the user.\\nIn the same way, you need the same citation mechanism when tool results are used to augment an LLM’s response. The range of tools is now much wider, but the need to trace the response to its source remains.\\nThrough Cohere’s Command R family of models, you get citation generation as a built-in feature in RAG without having to do prompt engineering or fine-tuning. And the same goes for tool use!\\nWhen using Command R/R+ to generate responses, it will generate fine-grained citations referencing the tool results being used to ground its response. Here, fine-grained means specific spans of the text generated based on results from one or more tools — more specific than just at the response or even sentence level.\\nCohere\\'s Command R/R+ models produce fine-grained citations to their responses\\nConclusion\\nThis was a quick introduction to the concept of tool use, starting with using RAG as the starting point. In the coming chapters, we’ll look at hands-on examples and concrete use cases of how tool use is applied in actual applications.\\nIn Chapter 2, we’ll look at the building blocks of a tool use system and how they all work together in a workflow. We’ll also look at our first code example by building a RAG assistant that can query structured sales data.'),\n",
       " Document(metadata={'title': 'Tool Use Anatomy', 'source': 'https://cohere.com/llmu/tool-use-anatomy'}, page_content='Tool Use AnatomyQualified\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nContents\\nIntroduction\\nSetting Up the Tools \\nStep 1: Create the Tool\\nStep 2: Define Tool Schema\\nStep 3: Create Custom Preamble (Optional)\\nRunning the tool workflow \\nStep 1: Get User Message\\nStep 2: Generate Tool Calls\\nStep 3: Get Tool Results\\nStep 4: Generate Response and Citations\\nStreaming the Response\\nTool Use Modes \\nSingle-Step\\nSingle-Step, Parallel\\nMulti-Step\\nMulti-Step, Parallel\\nIntroduction\\nIn this chapter, we’ll dissect the key components of a tool use system and what a tool use workflow looks like. And we’ll do that with a concrete code example.\\nTo build an intuition of how tool use works, let’s start with a simple example. We’ll look at a use case of a RAG assistant that can query the sales database of an e-commerce company.\\nIn summary, there are two parts to building a tool use application:\\nSetting up the tools\\nRunning the tool use workflow\\nFirst, let’s install the Cohere Python SDK and set up the Cohere client.\\n# pip install cohere import json import cohere co = cohere.Client(api_key=\"YOUR_COHERE_API_KEY\") # Get your API key: https://dashboard.cohere.com/api-keys \\nThe pre-requisite, or Step 0, before we can run a tool use workflow, is to set up the tools. We can break this further into two steps:\\nCreating the tool\\nDefining the tool schema\\nThe pre-requisite of running a tool use workflow is to set up the tools\\nLet’s create a function to query a sales database called daily_sales_report and represent it as a tool. For simplicity, it contains a mock database containing just three data entries and the logic to return the data given a user query.\\nIn this example, we are defining a Python function as the tool. But more broadly, the tool can be any function or service that can receive and send data. It could be an email service, an SQL database, a vector database, a weather data service, a sports data service, a web search engine, or even another LLM, just to give a few examples.\\ndef daily_sales_report(day: str) -> dict: \"\"\" Function to retrieve the sales report for the given day \"\"\" # Mock database containing daily sales reports sales_database = { \\'2023-09-28\\': {\\'total_sales_amount\\': 5000,\\'total_units_sold\\': 100}, \\'2023-09-29\\': {\\'total_sales_amount\\': 10000,\\'total_units_sold\\': 250}, \\'2023-09-30\\': {\\'total_sales_amount\\': 8000,\\'total_units_sold\\': 200} } report = sales_database.get(day, {}) if report: return { \\'date\\': day, \\'summary\\': f\"Total Sales Amount: {report[\\'total_sales_amount\\']}, Total Units Sold: {report[\\'total_units_sold\\']}\" } else: return {\\'date\\': day, \\'summary\\': \\'No sales data available for this day.\\'} functions_map = { \"daily_sales_report\": daily_sales_report } \\nNext, we define the tool schema for the sales database tool. This schema is what will be passed to the Cohere API when running a tool use workflow. The schema must contain the following fields:\\nname: the name of the tool.\\ndescription: a description of what the tool is and what it is used for.\\nparameter_definitions: a list of parameters that the tool accepts. For each parameter, we need to define the following fields:\\ndescription (details about the parameter)\\ntype (the parameter’s data type, e.g., str, int, and so on)\\nrequired (either True or False) fields\\nThis schema informs the LLM about what the tool does, and the LLM decides whether to use a particular tool based on it. Therefore, the more descriptive and specific the schema, the more likely the LLM will make the right tool call decisions.\\nLet’s name our tool daily_sales_report and populate the schema. The function has only one parameter, day, which requires the caller to pass a date in the YYYY-MM-DD string format.\\ntools = [ { \"name\": \"daily_sales_report\", \"description\": \"Connects to a database to retrieve overall sales volumes and sales information for a given day.\", \"parameter_definitions\": { \"day\": { \"description\": \"Retrieves sales data for this day, formatted as YYYY-MM-DD.\", \"type\": \"str\", \"required\": True } } } ] \\nStep 3: Create Custom Preamble (Optional)\\nAn optional step is to add a custom preamble, which is the LLM’s system message.\\nThe recommended approach is to use two H2 Markdown headers: \"Task & Context\" and \"Style Guide\" in the exact order (read the complete guide).\\nIt’s a completely optional step, though it’s likely needed if we want to create a robust and reliable application. Also note that the preamble is not related to the tool setup that we covered earlier, rather it’s part of the instruction to the LLM.\\nLet’s create a custom preamble as follows:\\npreamble = \"\"\"## Task & Context You help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user\\'s needs as best you can, which will be wide-ranging. ## Style Guide Unless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling. \"\"\" \\nThe tool is now ready to use. We can think of a tool use system as consisting of four components:\\nThe user\\nThe application\\nThe LLM\\nThe tools\\nAt its most basic, these four components interact in a workflow through four steps:\\nStep 1: Get user message. The LLM gets the user message (via the application).\\nStep 2: Generate tool calls. The LLM decides which tools to call (if any) and generates the tool calls.\\nStep 3: Get tool results. The application executes the tools, and the results are sent to the LLM.\\nStep 4: Generate response and citations. The LLM generates the response and citations back to the user.\\nAn overview of the four-step tool use workflow\\nLet’s walk through these four steps using a code example. In this first example, we’ll use the simplest possible scenario where:\\nThere is only one tool (the sales database)\\nTool calling happens only once (and only one tool is called)\\nThere is only one turn in the conversation (no conversation memory preserved)\\nStep 1: Get User Message\\nStep 1: Getting the user message\\nIn the first step, the application receives the user\\'s message. As an example, we’ll use “Can you provide a sales summary for 29th September 2023?”\\nmessage = \"Can you provide a sales summary for 29th September 2023?\" \\nStep 2: Generating the tool calls\\nDuring this step:\\nThe application calls the Chat endpoint to generate the list of tool calls. We add the following parameters:\\nmessage - the user message.\\nmodel - the model of choice. Here we use Command R+.\\npreamble - the custom preamble.\\ntools - the list of tools we defined earlier (only one in this example).\\nforce_single_step - we define it as True. This forces the endpoint to run tool calls only once and not over a sequence of runs (we’ll see how the opposite – multi-step – works in Chapter 4).\\nThe endpoint sends back a list of tool calls to be made (if it determines that tools are required).\\nHere’s the code for running this step.\\nmodel = \"command-r-plus\" # Initial response to the user message response = co.chat( message=message, model=model, preamble=preamble, tools=tools, force_single_step=True ) tool_calls = response.tool_calls print(\"Tool calls:\\\\n\") for i, t in enumerate(tool_calls): print(f\"#{i+1}\\\\nTool: {t.name}\\\\nParameters: {t.parameters}\\\\n\") \\nAnd here’s the list of tool calls (only one in our case). The model determines that to answer the user question, it needs to call the daily_sales_report tool with the parameter \\'day\\': \\'2023-09-29\\' – as we’d expect.\\nTool calls: #1 Tool: daily_sales_report Parameters: {\\'day\\': \\'2023-09-29\\'} \\nStep 3: Getting the tool results\\nDuring this step, the application performs the function calling. It calls the necessary tools based on the tool call payloads given by the endpoint. In our case, it calls the daily_sales_report function.\\nHere’s the code for running this step.\\ntool_results = [] for tc in tool_calls: tool_output = functions_map[tc.name](**tc.parameters) tool_call = {\"name\": tc.name, \"parameters\": tc.parameters} tool_results.append({\"call\": tool_call, \"outputs\": [tool_output]}) print(\"Tool results:\\\\n\") for i, t in enumerate(tool_results): print(f\"#{i+1}\\\\nTool call: {t[\\'call\\']}\\\\nOutputs: {t[\\'outputs\\']}\\\\n\") \\nAnd here are the tool results (again, only one in our case). The tool successfully returns the correct data on the sales amount and units sold.\\nTool results: #1 Tool call: {\\'name\\': \\'daily_sales_report\\', \\'parameters\\': {\\'day\\': \\'2023-09-29\\'}} Outputs: [{\\'date\\': \\'2023-09-29\\', \\'summary\\': \\'Total Sales Amount: 10000, Total Units Sold: 250\\'}] \\nStep 4: Generate Response and Citations\\nStep 4: Generating the response and citations\\nWe have now come to the fourth and final step: response generation. By this time, the tool call has already been executed, and the result has been returned to the LLM.\\nDuring this step, the application calls the Chat endpoint to generate the response to the user. Compared to the call in Step 2, here are the parameter changes required:\\nmessage - we put a blank value \"\" as this is a call to generate the chatbot’s response rather than to get a new user message (which doesn’t change)\\ntool_results - here we introduce this parameter as a way to pass the tool results generated in the previous step\\nchat_history - we use this parameter to pass the chat_history value from Step 2’s response. It contains the context required to generate the response – the user message and the tool call(s) made – as shown below.\\n# response.chat_history [Message_User(message=\\'Can you provide a sales summary for 29th September 2023?\\', tool_calls=None, role=\\'USER\\'), Message_Chatbot(tool_calls=[ToolCall(name=\\'daily_sales_report\\', parameters={\\'day\\': \\'2023-09-29\\'})], role=\\'CHATBOT\\')] \\nHere’s the code for running this step:\\n# Generate response response = co.chat( message=\"\", model=model, preamble=preamble, tools=tools, tool_results=tool_results, chat_history=response.chat_history ) print(\"Final response:\") print(response.text) print(\"=\"*50) if response.citations: print(\"\\\\nCitations:\") for citation in response.citations: print(citation) print(\"\\\\nCited Documents:\") for document in response.documents: print(document) \\nAnd here’s the final response from the LLM, which accurately answers the user’s question (total sales amount: 10,000, and total number of units sold: 250).\\nIt also generates fine-grained citations, which are included with Command R+ out of the box. Here, we see the model generating two citations, one for each specific span in its response, where it uses the tool result to answer the question.\\nFor example, the first citation shows that the total sales amount was 10,000 portion of the response comes from the daily_sales_report tool. It also provides the values of the start and end character numbers that can be used for front-end development purposes to render citations easily and elegantly.\\nFinal response: On 29 September 2023, we sold 250 units, totalling 10,000 in sales. ================================================== Citations: start=30 end=39 text=\\'250 units\\' document_ids=[\\'daily_sales_report:0:2:0\\'] start=51 end=66 text=\\'10,000 in sales\\' document_ids=[\\'daily_sales_report:0:2:0\\'] Cited Documents: {\\'date\\': \\'2023-09-29\\', \\'id\\': \\'daily_sales_report:0:2:0\\', \\'summary\\': \\'Total Sales Amount: 10000, Total Units Sold: 250\\', \\'tool_name\\': \\'daily_sales_report\\'} \\nThe screenshot below, taken from Cohere’s Chat UI, shows an example of displaying fine-grained citations on a user interface. In this example, a web search tool is used to find the capital of Ontario. The response includes highlighting Toronto as the span that cites the web search results.\\nAn example of displaying fine-grained citations on a user interface, taken from Cohere’s Chat UI\\nStreaming the Response\\nWe can stream the response from the Chat endpoint for each generated token instead of having to wait for the full response. To enable streaming, we need to change the endpoint call from co.chat to co.chat_stream.\\nThe code from Step 4 would then change to the following. The difference now is we are capturing the response events as the endpoint streams them.\\n# Generate response response = co.chat_stream( message=\"\", model=model, preamble=preamble, tools=tools, tool_results=tool_results, chat_history=response.chat_history ) print(\"Final response:\\\\n\") chatbot_response = \"\" for event in response: if event.event_type == \"text-generation\": print(event.text, end=\"\") chatbot_response += event.text if event.event_type == \"stream-end\": if event.response.citations: print(\"\\\\n\\\\nCitations:\") for citation in event.response.citations: print(citation) if event.response.documents: print(\"\\\\nCited Documents\") for document in event.response.documents: print(document) \\nA good tool use API must be able to handle different scenarios, from simple to complex LLM tool calls. This translates to being able to call tools in different modes: single-step, multi-step, and parallel.\\nThe good news is that Cohere’s API supports all these modes. We’ll walk through them in detail in the coming two chapters, but for now, let’s briefly look at what these different modes are.\\nCohere’s API supports different modes of tool use\\nSingle-Step\\nThis is the most straightforward mode, so let\\'s start here. An LLM calls a single tool, the tool returns the result, and the result gets appended to the prompt.\\nReferring to the four-step workflow above, this means that Steps 2 (generating the tool calls) and 3 (getting the tool results) happen exactly once.\\nThis is the scenario that we saw in the sales database example earlier.\\nTo illustrate this further, let\\'s assume we have two tools:\\nA web search tool: It takes a query and returns relevant search results from the web\\nA Python interpreter: It takes a block of Python code, runs it, and returns the output\\nSuppose a user asks, \"What was Apple\\'s revenue in 2023?\"\\nTo answer this, a single web search of \"Apple 2023 revenue\" will suffice.\\nSingle-step tool use\\nSingle-Step, Parallel\\nThe LLM can also decide that more than one tool call is required, which means calling tools in parallel. This can be calling the same tool multiple times or different tools for any number of calls.\\nWe’ll see an example of this in Chapter 3.\\nSuppose the user now asks, \"What were Apple\\'s and Google\\'s revenue in 2023?\"\\nTo answer this, two web searches of \"Apple 2023 revenue\" and \"Google 2023 revenue\" are needed.\\nSingle-step, parallel tool use\\nMulti-Step\\nThis is needed when more than one tool is required, and the output of one tool is needed as the input to the other tool.\\nIn other words, tool-calling needs to happen in a sequence.\\nReferring to the four-step workflow above, this means that Steps 2 (generating the tool calls) and 3 (getting the tool results) can happen multiple times before moving on to Step 4 (generating the response).\\nWe’ll see examples of this in Chapters 4 and 5.\\nSuppose the user now asks, \"Plot Apple\\'s 2023 revenue on a bar chart\".\\nThis requires first a web search for Apple\\'s revenue, and then using the search result to generate the Python code for plotting.\\nMulti-step tool use\\nMulti-Step, Parallel\\nTool calling can run in parallel in a multi-step scenario as it can in the single-step case.\\nWe’ll see examples of this in Chapters 4 and 5.\\nLet\\'s say the user now asks, \"Plot Apple\\'s and Google\\'s 2023 revenue on a bar chart\".\\nThis requires two parallel web search calls — one each for Apple and Google — and then running the Python plotting code in the second step.\\nMulti-step, parallel tool use\\nConclusion\\nIn this chapter, we looked at the components of a tool use system, the tool use workflow, state management in tool use, and different modes of tool use.\\nWe also looked at a code example using a single-step scenario. But it was a basic example that doesn’t address some questions like:\\nHow to set up a system with more than one tool?\\nDoes the assistant always need to call tools?\\nHow does the assistant work in a chat setting consisting of multiple turns?\\nIn Chapter 3, we’ll address these questions and improve on the code example in this chapter.'),\n",
       " Document(metadata={'title': 'Multi-Step Tool Use', 'source': 'https://cohere.com/llmu/multi-step-tool-use-2'}, page_content='Multi-Step Tool UseQualified\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nContents\\nIntroduction\\nReAct-Style Prompting\\nMulti-Step Tool Use Workflow \\nPlanning and Execution\\nSelf-Correction\\nSetting Up the Tools \\nStep 1: Create Tools\\nStep 2: Define Tool Schema\\nStep 3: Create Custom Preamble (Optional)\\nMulti-Step Tool Use\\nTool Planning\\nMulti-Step, Parallel Tool Use\\nState Management (Memory)\\nConclusion\\nIntroduction\\nIn Chapter 2, we briefly discussed that multi-step tool use happens when more than one tool is required and the output of one tool is needed as the input to another tool.\\nIn other words, tool-calling needs to happen in a sequence.\\nThis is especially true in complex workflows where tasks have to be broken down into a sequence of steps.\\nLet\\'s say a user asks, \"Plot Apple\\'s 2023 revenue on a bar chart\".\\nThis requires first a web search for Apple\\'s revenue and then generating the right Python code using the search results. In the same way as a single-step scenario, multi-step tool use can also happen in parallel.\\nMulti-step tool use serves tasks where tool-calling needs to happen in a sequence\\nIn this chapter, we’ll focus on multi-step tool use and formalize our understanding through code examples.\\nWe’ll build a calendar assistant that can check a user’s calendar and create new appointments based on available times. This use case requires tool calls to happen in sequence, i.e., multi-step. It is also an example of using tools for performing not just a read operation (retrieving information) but also a write operation (taking actions).\\nReAct-Style Prompting\\nA common approach for multi-step tool use implementation is using the ReAct-style of prompting.\\nThe ReAct framework (Yao et al., 2022) is a technique for improving the accuracy and reliability of responses from LLMs. It combines \"reasoning\" and \"acting,\" inspired by the natural human ability to learn and make decisions by blending these two elements. It allows an LLM to create a plan to solve a task and then execute the plan step by step in an iterative process, where it reflects and adjusts its plan after each action it takes. By integrating both reasoning and acting within LLMs, ReAct guides them to produce verbal reasoning paths and specific actions for a given task.\\nThe framework follows a series of Thought-Action-Observation reasoning sequences. Given a user message, the LLM first generates a thought, which includes the plan of what it needs to do. Then, it generates an action, which is a call to an external tool. The result of this call is then passed to its observation step. It will repeat this sequence until a task has been fulfilled.\\nCohere’s multi-step tool workflow uses a style similar to the ReAct framework. The prompt is comprised of:\\nThe preamble, consisting of the system and custom (if defined) preambles\\nThe list of available tools, taken from the tool schema defined by the developer\\nThe user message\\nAnd finally, the instructions to run the ReAct-style workflow (to be described next)\\nPlanning and Execution\\nYou can view the complete prompt to the model by enabling return_prompt to True when calling the Chat endpoint. But for our purpose, let’s just focus on the portion with the instructions to run the ReAct-style workflow.\\n... Carefully perform the following instructions, in order, starting each with a new line. Firstly, You may need to use complex and advanced reasoning to complete your task and answer the question. Think about how you can use the provided tools to answer the question and come up with a high level plan you will execute. Write \\'Plan:\\' followed by an initial high level plan of how you will solve the problem including the tools and steps required. Secondly, Carry out your plan by repeatedly using actions, reasoning over the results, and re-evaluating your plan. Perform Action, Observation, Reflection steps with the following format. Write \\'Action:\\' followed by a json formatted action containing the \"tool_name\" and \"parameters\" Next you will analyze the \\'Observation:\\', this is the result of the action. After that you should always think about what to do next. Write \\'Reflection:\\' followed by what you\\'ve figured out so far, any changes you need to make to your plan, and what you will do next including if you know the answer to the question. ... (this Action/Observation/Reflection can repeat N times) Thirdly, Decide which of the retrieved documents are relevant to the user\\'s last input by writing \\'Relevant Documents:\\' followed by comma-separated list of document numbers. If none are relevant, you should instead write \\'None\\'. Fourthly, Decide which of the retrieved documents contain facts that should be cited in a good answer to the user\\'s last input by writing \\'Cited Documents:\\' followed a comma-separated list of document numbers. If you dont want to cite any of them, you should instead write \\'None\\'. Fifthly, Write \\'Answer:\\' followed by a response to the user\\'s last input in high quality natural english. Use the retrieved documents to help you. Do not insert any citations or grounding markup. Finally, Write \\'Grounded answer:\\' followed by a response to the user\\'s last input in high quality natural english. ... \\nThe first instruction is to perform a “planning” step consisting of an initial high-level plan of how the model will solve the problem, including the tools and steps required.\\nThis is followed by actually carrying out the plan by repeatedly using actions (tool calls), with every step going through an Action-Observation-Reflection sequence.\\nWith the Cohere API, this sequence of reasoning is captured as an assistant’s tool plan before generating the actual tool calls, as we’ll see in the code example.\\nCohere’s multi-step tool use workflow uses a style similar to the ReAct framework\\nSelf-Correction\\nWhat’s not explicitly mentioned in the prompt but something the model can handle is self-correction.\\nSo far, we have assumed that every tool call will return a successful result. But that does not necessarily have to be the case. Calling external functions could instead return errors, such as missing information, unsuccessful attempts, and many others.\\nFor example, when calling a web search as a tool call, a timeout error might occur for some reason. That means that the tool result returned will not contain the required information to generate a response.\\nIn this kind of situation, the model will be able to reflect on this returned result and take a different course of action than what was originally planned. In this case, instead of immediately going to the next task it had planned, it might attempt to rerun the same step that returned the error.\\nAnd with that, let’s start building our calendar assistant.\\nLet’s first set up the Cohere client. We’ll also need to import the json module for use later.\\n# pip install cohere import json import cohere co = cohere.Client(api_key=\"YOUR_COHERE_API_KEY\") # Get your API key: https://dashboard.cohere.com/api-keys \\nLet’s now create two tools:\\nA function called list_calendar_events to list the existing calendar events based on a given date. For simplicity, we are not going to connect to an actual calendar. Instead, we’ll include a mock events record, which is a simple list of events in a day.\\nA function called create_calendar_event to create a new calendar event based on the provided date, time, and duration. Again, to keep things simple, we will not make actual changes to a database but instead will just return a simple success message for illustration purposes.\\nSetting up the tools\\ndef list_calendar_events(date: str): events = [{\"start\": \"8:00\", \"end\": \"8:59\"}, {\"start\": \"9:00\", \"end\": \"9:59\"}, {\"start\": \"11:00\", \"end\": \"11:59\"},{\"start\": \"12:00\", \"end\": \"12:59\"}] return { \"existing_events\": events } def create_calendar_event(date: str, time: str, duration: int): return { \"is_success\": True, \"message\": f\"Created a {duration} hour long event at {time} on {date}\" } functions_map = { \"list_calendar_events\": list_calendar_events, \"create_calendar_event\": create_calendar_event } \\nNext, we define the tool schema for the two tools.\\ntools = [ { \"name\": \"list_calendar_events\", \"description\": \"Returns a list of existing events for the specified date, including the start time and end time for each event.\", \"parameter_definitions\": { \"date\": { \"description\": \"the date to list events for, formatted as mm/dd/yy\", \"type\": \"str\", \"required\": True } } }, { \"name\": \"create_calendar_event\", \"description\": \"Creates a new calendar event of the specified duration at the specified time and date. A new event cannot be created on the same time as an existing event.\", \"parameter_definitions\": { \"date\": { \"description\": \"the date on which the event starts, formatted as mm/dd/yy\", \"type\": \"str\", \"required\": True }, \"time\": { \"description\": \"the time of the event, formatted using 24h military time formatting\", \"type\": \"str\", \"required\": True }, \"duration\": { \"description\": \"the number of hours the event lasts for\", \"type\": \"float\", \"required\": True } } } ] \\nStep 3: Create Custom Preamble (Optional)\\nLet’s also create a custom preamble. The important part here is specifying the date since we are not implementing any date-handling with the list_calendar_events tool. What we’re doing is we are nudging the model to specify this date in the create_calendar_event tool call, which requires a date field.\\npreamble=\"\"\"## Task & Context You are a calendar assistant who helps people schedule events on their calendar. You must make sure that a new event does not overlap with any existing event. Today is Thursday, May 23, 2024 \"\"\" \\nLet’s reuse the run_assistant function we created in the previous chapter. Here’s a recap of what it does.\\nGet the user message (Step 1)\\nCall the Chat endpoint for tool call generation (Step 2)\\nIf the response contains at least one tool call, execute the tool call(s) and get the tool results (Step 3)\\nRepeat Steps 2 and 3 until there are no more tool calls\\nGenerate the final response with citations (Step 4)\\nAn overview of the four-step tool use workflow\\nThe good news is that it is already compatible with multi-step tool use because we have already implemented Steps 2 and 3 to run in a loop to handle zero, one, or multiple steps of tool calls.\\nBut there is one change we need to make. Here, we remove the force_single_step argument as it is False by default. This tells the API to enable a multi-step workflow and use the right prompt template to run the workflow.\\nmodel = \"command-r-plus\" def run_assistant(message, chat_history=[]): # Step 1: Get user message print(f\"Question:\\\\n{message}\") print(\"=\"*50) # Step 2: Generate tool calls (if any) response = co.chat( message=message, model=model, preamble=preamble, tools=tools, chat_history=chat_history ) while response.tool_calls: tool_calls = response.tool_calls if response.text: print(\"Tool plan:\") print(response.text,\"\\\\n\") print(\"Tool calls:\") for call in tool_calls: print(f\"Tool name: {call.name} | Parameters: {call.parameters}\") print(\"=\"*50) # Step 3: Get tool results tool_results = [] for tc in tool_calls: tool_call = {\"name\": tc.name, \"parameters\": tc.parameters} tool_output = functions_map[tc.name](**tc.parameters) tool_results.append({\"call\": tool_call, \"outputs\": [tool_output]}) # Step 4: Generate response and citations response = co.chat( message=\"\", model=model, preamble=preamble, tools=tools, tool_results=tool_results, chat_history=response.chat_history ) # Update the chat history chat_history = response.chat_history # Print final response print(\"Final response:\") print(response.text) print(\"=\"*50) # Print citations (if any) if response.citations: print(\"Citations:\") for citation in response.citations: print(citation) print(\"\\\\nCited Documents:\") for document in response.documents: print(document) print(\"=\"*50) return chat_history \\nLet’s now ask the assistant the first question, starting with a simple one about the number of meetings for the day. This requires just a single step of tool calling to check existing calendar events.\\nchat_history = run_assistant(\"How many meetings do I have today\") \\nThat’s exactly what the assistant does, and it gives the correct answer.\\nQuestion: How many meetings do I have today ================================================== Tool plan: I will use the \\'list_calendar_events\\' tool to find out how many meetings the user has today. Tool calls: Tool name: list_calendar_events | Parameters: {\\'date\\': \\'05/23/2024\\'} ================================================== Final response: You have four meetings today. ================================================== Citations: start=9 end=22 text=\\'four meetings\\' document_ids=[\\'list_calendar_events:0:2:0\\'] Cited Documents: {\\'existing_events\\': \\'[{\"end\":\"8:59\",\"start\":\"8:00\"},{\"end\":\"9:59\",\"start\":\"9:00\"},{\"end\":\"11:59\",\"start\":\"11:00\"},{\"end\":\"12:59\",\"start\":\"12:00\"}]\\', \\'id\\': \\'list_calendar_events:0:2:0\\', \\'tool_name\\': \\'list_calendar_events\\'} ================================================== \\nLet’s now try to ask a question that requires multi-step tool calling — this one asking the assistant to book an appointment. To complete this task, the assistant will have to first query the existing events and then use the information to create a new event that wouldn’t cause a conflict.\\nchat_history = run_assistant(\"Create an hour-long appointment for the first available free slot after 9am\") \\nHere’s the assistant’s response. In the first step, it calls the list_calendar_events tool to get a list of existing events.\\nIn the second step, it calls the create_calendar_event tool to create a new event. It creates a new event at 10 a.m., which is indeed the first available slot after 9 a.m.\\nQuestion: Create an hour-long appointment for the first available free slot after 9am ================================================== Tool plan: I will first check the user\\'s calendar for 23/05/2024 to see if there are any free slots after 9am. I will then create an hour-long appointment for the first available free slot. Tool calls: Tool name: list_calendar_events | Parameters: {\\'date\\': \\'05/23/2024\\'} ================================================== Tool plan: The user\\'s calendar shows that they have a free slot from 10:00 to 10:59. I will now create an hour-long appointment for this time slot. Tool calls: Tool name: create_calendar_event | Parameters: {\\'date\\': \\'05/23/2024\\', \\'duration\\': 1, \\'time\\': \\'10:00\\'} ================================================== Final response: I\\'ve created a 1-hour long appointment for 10:00 on 23/05/2024. ================================================== Citations: start=15 end=62 text=\\'1-hour long appointment for 10:00 on 23/05/2024\\' document_ids=[\\'create_calendar_event:0:4:0\\'] Cited Documents: {\\'id\\': \\'create_calendar_event:0:4:0\\', \\'is_success\\': \\'true\\', \\'message\\': \\'Created a 1 hour long event at 10:00 on 05/23/2024\\', \\'tool_name\\': \\'create_calendar_event\\'} ================================================== \\nAnd here’s a look at the chat history. Compared to the single-step scenario, it contains multiple pairs of CHATBOT - TOOL messages, with each pair being one step in the sequence of tool calls. In this particular case, we have two of them.\\nmessage=\\'Create an hour-long appointment for the first available free slot after 9am\\' tool_calls=None role=\\'USER\\' message=\"I will first check the user\\'s calendar for 23/05/2024 to see when they are free after 9am. I will then create an hour-long appointment for the first available slot.\" tool_calls=[ToolCall(name=\\'list_calendar_events\\', parameters={\\'date\\': \\'05/23/2024\\'})] role=\\'CHATBOT\\' tool_results=[ToolResult(call=ToolCall(name=\\'list_calendar_events\\', parameters={\\'date\\': \\'05/23/2024\\'}), outputs=[{\\'existing_events\\': [{\\'end\\': \\'8:59\\', \\'start\\': \\'8:00\\'}, {\\'end\\': \\'9:59\\', \\'start\\': \\'9:00\\'}, {\\'end\\': \\'11:59\\', \\'start\\': \\'11:00\\'}, {\\'end\\': \\'12:59\\', \\'start\\': \\'12:00\\'}]}])] role=\\'TOOL\\' message=\\'The user has an appointment from 9am to 9:59am. I will now create an hour-long appointment for the first available slot after this.\\' tool_calls=[ToolCall(name=\\'create_calendar_event\\', parameters={\\'date\\': \\'05/23/2024\\', \\'duration\\': 1, \\'time\\': \\'10:00\\'})] role=\\'CHATBOT\\' tool_results=[ToolResult(call=ToolCall(name=\\'create_calendar_event\\', parameters={\\'date\\': \\'05/23/2024\\', \\'duration\\': 1, \\'time\\': \\'10:00\\'}), outputs=[{\\'is_success\\': True, \\'message\\': \\'Created a 1 hour long event at 10:00 on 05/23/2024\\'}])] role=\\'TOOL\\' message=\"I\\'ve created an hour-long appointment for 10am on 23/05/2024.\" tool_calls=None role=\\'CHATBOT\\' \\nComparing the events within a single chat turn for single-step and multi-step scenarios\\nThe chat history also highlights another difference between the single-step and multi-step scenarios.\\nNotice that in the multi-step example above, the CHATBOT turns generate a textual response at each tool-calling step (look for the message) before making the actual tool calls.\\nLet’s call this the model’s tool plan. These intermediate responses are the model’s internal reasoning logic, which guides its next course of action.\\nThis goes back to the ReAct framework discussed earlier. The first of these plans is the assistant’s initial planning step, which lists its set of actions to take. The subsequent responses consist of its reflection on the current state and decision on the next immediate actions.\\nHere’s how the tool plan in multi-step looks again in the chat history.\\n... message=\\'The user has an appointment from 9am to 9:59am. I will now create an hour-long appointment for the first available slot after this.\\' tool_calls=[ToolCall(name=\\'create_calendar_event\\', parameters={\\'date\\': \\'05/23/2024\\', \\'duration\\': 1, \\'time\\': \\'10:00\\'})] role=\\'CHATBOT\\' ... \\nMeanwhile, in the single-step case, the model doesn’t generate any tool plan (message=None) but instead generates the tool calls directly.\\n... message=None tool_calls=[ToolCall(name=\\'daily_sales_report\\', parameters={\\'day\\': \\'2023-09-29\\'})] role=\\'CHATBOT\\' ... \\nMulti-step tool use contains tool plans, which single-step doesn\\'t\\nLet’s now look at an example of multi-step, parallel tool use. Let’s ask the assistant to create two separate events on the same day.\\nchat_history = run_assistant(\"Book two hour-long appointments for any available time between 8am to 6pm\") \\nAnd here’s the assistant’s response. It follows the same two steps as in the earlier example: first, checking the existing events, and second, creating the new events.\\nThe difference this time is that since it needs to create two events, it calls create_calendar_event twice within the same step. Here, the assistant makes the correct judgment and tool calls are independent of each other, which means they can be done in parallel.\\nQuestion: Create two hour-long appointments for any available time between 8am to 6pm ================================================== Tool plan: I will first list the events for 23/05/2024. Then, I will create two hour-long appointments for any available time between 8am and 6pm. Tool calls: Tool name: list_calendar_events | Parameters: {\\'date\\': \\'05/23/2024\\'} ================================================== Tool plan: The available times between 8am and 6pm are: 10am-11am, 1pm-6pm. I will now create two hour-long appointments for the user during these times. Tool calls: Tool name: create_calendar_event | Parameters: {\\'date\\': \\'05/23/2024\\', \\'duration\\': 1, \\'time\\': \\'10:00\\'} Tool name: create_calendar_event | Parameters: {\\'date\\': \\'05/23/2024\\', \\'duration\\': 1, \\'time\\': \\'13:00\\'} ================================================== Final response: I have created two hour-long appointments for you today: one at 10am and another at 1pm. ================================================== Citations: start=64 end=68 text=\\'10am\\' document_ids=[\\'create_calendar_event:0:4:0\\'] start=84 end=87 text=\\'1pm\\' document_ids=[\\'create_calendar_event:1:4:0\\'] Cited Documents: {\\'id\\': \\'create_calendar_event:0:4:0\\', \\'is_success\\': \\'true\\', \\'message\\': \\'Created a 1 hour long event at 10:00 on 05/23/2024\\', \\'tool_name\\': \\'create_calendar_event\\'} {\\'id\\': \\'create_calendar_event:1:4:0\\', \\'is_success\\': \\'true\\', \\'message\\': \\'Created a 1 hour long event at 13:00 on 05/23/2024\\', \\'tool_name\\': \\'create_calendar_event\\'} ================================================== \\nState Management (Memory)\\nIn the previous chapter, we saw how state management works with the Chat endpoint from the context of single-step tool use.\\nMulti-step tool use works the same way. The chat history for each turn consists of the following messages, and will accumulate with every new turn.\\nThe USER message\\nFollowed by the CHATBOT message with the list of tool calls\\nFollowed the TOOL message with the list of tool results\\nFinally, followed by the CHATBOT message with the final response to the user\\nThe difference here is, as we’ve seen, that each turn can have multiple pairs of CHATBOT - TOOL messages, representing its multi-step nature. The other difference is the presence of the tool plans, which are not present in the single-step case.\\nMaintaining the state over multiple chat turns (multi-step tool use)\\nConclusion\\nIn this chapter, we learned about multi-step tool use by creating a calendar assistant that can check and create events on behalf of a user. We saw how the multi-step approach enables agents to perform a complex task via repeated sequences of planning, action, observation, and reflection. We also saw how all this can be done using the Cohere native API.\\nThere is another way you can run multi-step tool use with Cohere, and that is via open-source frameworks. We have worked with the Langchain team to make it simple for developers to build Cohere-backed agents on Langchain.\\nIn Chapter 5, we’ll learn how to use Langchain to create a multi-step agent.'),\n",
       " Document(metadata={'title': 'Single-Step Tool Use', 'source': 'https://cohere.com/llmu/single-step-tool-use'}, page_content='Single-Step Tool UseQualified\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nContents\\nIntroduction\\nSetting Up the Tools \\nStep 1: Create Tools\\nStep 2: Define Tool Schema\\nStep 3: Define the Preamble (Optional)\\nSingle-Step Tool Use\\nSingle-Step, Parallel Tool Use\\nDirectly Answering\\nState Management (Memory)\\nConclusion\\nIntroduction\\nIn the previous chapter, we saw that tool use can run in two different modes: single-step and multi-step.\\nIn a single-step tool use scenario, an LLM calls a single tool or many tools in parallel. The tools then return the results, which the LLM uses to generate its response.\\nIn single-step tool use, an LLM calls a single tool or many tools in parallel\\nWe also went through a code example of single-step tool use. However, that example was a basic one. We haven’t yet seen these cases:\\nMultiple tools: The example only had one tool, but what if the model had multiple tools at its disposal? How does the model reason over which tools to use at a particular time?\\nParallel tool calls: We had only seen a single tool call in a single step, but what does it look like if more than one tool call is required?\\nWhen not to use tools: What if a question can, and should, be answered directly by a model without needing a tool?\\nTool use in a chat setting: What if the user had follow-up questions, i.e., how does the assistant maintain the context of a conversation in a multi-turn chat setting?\\nIn this chapter, let’s go deeper into single-step tool use and address the questions above.\\nLet’s first set up the Cohere client. We’ll also need to import the json module for use later.\\n# pip install cohere import json import cohere co = cohere.Client(api_key=\"YOUR_COHERE_API_KEY\") # Get your API key: https://dashboard.cohere.com/api-keys \\nLet’s now create two tools:\\nA function to query the sales database called query_daily_sales_report, which includes a mock sales database (same as the previous chapter)\\nA function to query the product catalog called query_product_catalog, which includes a mock product catalog (a new one)\\nSetting up the tools\\ndef daily_sales_report(day: str) -> dict: \"\"\" Function to retrieve the sales report for the given day \"\"\" # Mock database containing daily sales reports sales_database = { \\'2023-09-28\\': {\\'total_sales_amount\\': 5000,\\'total_units_sold\\': 100}, \\'2023-09-29\\': {\\'total_sales_amount\\': 10000,\\'total_units_sold\\': 250}, \\'2023-09-30\\': {\\'total_sales_amount\\': 8000,\\'total_units_sold\\': 200} } report = sales_database.get(day, {}) if report: return { \\'date\\': day, \\'summary\\': f\"Total Sales Amount: {report[\\'total_sales_amount\\']}, Total Units Sold: {report[\\'total_units_sold\\']}\" } else: return {\\'date\\': day, \\'summary\\': \\'No sales data available for this day.\\'} def product_database(category: str) -> dict: \"\"\" Function to retrieve products for the given category \"\"\" # Mock product catalog product_catalog = { \\'Electronics\\': [ {\\'product_id\\': \\'E1001\\', \\'name\\': \\'Smartphone\\', \\'price\\': 500, \\'stock_level\\': 20}, {\\'product_id\\': \\'E1002\\', \\'name\\': \\'Laptop\\', \\'price\\': 1000, \\'stock_level\\': 15}, {\\'product_id\\': \\'E1003\\', \\'name\\': \\'Tablet\\', \\'price\\': 300, \\'stock_level\\': 25}, ], \\'Clothing\\': [ {\\'product_id\\': \\'C1001\\', \\'name\\': \\'T-Shirt\\', \\'price\\': 20, \\'stock_level\\': 100}, {\\'product_id\\': \\'C1002\\', \\'name\\': \\'Jeans\\', \\'price\\': 50, \\'stock_level\\': 80}, {\\'product_id\\': \\'C1003\\', \\'name\\': \\'Jacket\\', \\'price\\': 100, \\'stock_level\\': 40}, ] } products = product_catalog.get(category, []) return { \\'category\\': category, \\'products\\': products } functions_map = { \"daily_sales_report\": daily_sales_report, \"product_database\": product_database } \\nNext, we define the tool schema for the two tools. Both functions accept one parameter, day and category respectively.\\ntools = [ { \"name\": \"daily_sales_report\", \"description\": \"Connects to a database to retrieve overall sales volumes and sales information for a given day.\", \"parameter_definitions\": { \"day\": { \"description\": \"Retrieves sales data for this day, formatted as YYYY-MM-DD.\", \"type\": \"str\", \"required\": True } } }, { \"name\": \"product_database\", \"description\": \"A database that contains information about all the products of this company, including categories, prices, and stock levels.\", \"parameter_definitions\": { \"category\": { \"description\": \"Retrieves product information data for all products in this category.\", \"type\": \"str\", \"required\": True } } } ] \\nStep 3: Create Custom Preamble (Optional)\\nWe keep the same preamble as in the previous chapter.\\npreamble = \"\"\"## Task & Context You help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user\\'s needs as best you can, which will be wide-ranging. ## Style Guide Unless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling. \"\"\" \\nLet’s now take the code used in the previous chapter and automate the four-step tool use workflow. Let’s put the code in a function called run_assistant. Here’s what it does:\\nGets the user message (Step 1)\\nCalls the Chat endpoint for tool call generation (Step 2)\\nIf the response contains at least one tool call, executes the tool call(s) and gets the tool results (Step 3)\\nGenerates the final response with citations (Step 4)\\nAn overview of the four-step tool use workflow\\nThe main difference from the previous chapter is that this time, we put Steps 2 and 3 in a loop, checking if the model’s response contains tool calls. This allows the assistant to handle any number of tool call steps (zero, one, or multiple) for a given user message.\\nThe other key difference is we are also enabling the assistant to retain the state (or memory) of a conversation, which allows it to handle multi-turn chat scenarios. We do this by having the run_assistant function take the chat history from the previous turns as an argument and return the updated chat history.\\nmodel = \"command-r-plus\" def run_assistant(message, chat_history=[]): # Step 1: Get user message print(f\"Question:\\\\n{message}\") print(\"=\"*50) # Step 2: Generate tool calls (if any) response = co.chat( message=message, model=model, preamble=preamble, tools=tools, chat_history=chat_history, force_single_step=True ) while response.tool_calls: tool_calls = response.tool_calls if response.text: print(\"Tool plan:\") print(response.text,\"\\\\n\") print(\"Tool calls:\") for call in tool_calls: print(f\"Tool name: {call.name} | Parameters: {call.parameters}\") print(\"=\"*50) # Step 3: Get tool results tool_results = [] for tc in tool_calls: tool_call = {\"name\": tc.name, \"parameters\": tc.parameters} tool_output = functions_map[tc.name](**tc.parameters) tool_results.append({\"call\": tool_call, \"outputs\": [tool_output]}) # Step 4: Generate response and citations response = co.chat( message=\"\", model=model, preamble=preamble, tools=tools, tool_results=tool_results, chat_history=response.chat_history ) # Update the chat history chat_history = response.chat_history # Print final response print(\"Final response:\") print(response.text) print(\"=\"*50) # Print citations (if any) if response.citations: print(\"Citations:\") for citation in response.citations: print(citation) print(\"\\\\nCited Documents:\") for document in response.documents: print(document) print(\"=\"*50) return chat_history \\nLet’s now ask the assistant the first question, which is the same as in the previous chapter.\\nchat_history = run_assistant(\"Can you provide a sales summary for 29th September 2023?\") \\nThe assistant correctly identifies that, out of the two tools available, query_daily_sales_report is sufficient to answer the question. And it answers the question correctly.\\nQuestion: Can you provide a sales summary for 29th September 2023? ================================================== Tool calls: Tool name: daily_sales_report | Parameters: {\\'day\\': \\'2023-09-29\\'} ================================================== Final response: On 29 September 2023, we made a total of 10,000 in sales from 250 units. ================================================== Citations: start=32 end=56 text=\\'total of 10,000 in sales\\' document_ids=[\\'daily_sales_report:0:2:0\\'] start=62 end=72 text=\\'250 units.\\' document_ids=[\\'daily_sales_report:0:2:0\\'] Cited Documents: {\\'date\\': \\'2023-09-29\\', \\'id\\': \\'daily_sales_report:0:2:0\\', \\'summary\\': \\'Total Sales Amount: 10000, Total Units Sold: 250\\', \\'tool_name\\': \\'daily_sales_report\\'} ================================================== \\nIn this example, the model requires a single step to answer the question, and within that single step, it requires just one tool call.\\nAs mentioned, the model can decide that more than one tool is required to provide a response. This means calling multiple tools in parallel within the same step. This can either be:\\nCalling different tools in parallel\\nCalling the same tool multiple times in parallel\\nOr a combination of both\\nDifferent types of parallel tool calls\\nTo illustrate this, let’s try to ask the model another question. Let’s ask about the sales summary of two different dates and also about the stock level information of a product category.\\nchat_history = run_assistant(\"Can you provide a sales summary for 28th and 29th September 2023 as well as the stock level of the products in the \\'Electronics\\' category?\") \\nThis time, the model generates three tool calls in parallel:\\nTwo queries to the sales database, one for each date\\nAnd one query to the product catalog\\nThese are exactly what are needed to answer the question. And it correctly answers the question.\\nQuestion: Can you provide a sales summary for 28th and 29th September 2023 as well as the stock level of the products in the \\'Electronics\\' category? ================================================== Tool calls: Tool name: daily_sales_report | Parameters: {\\'day\\': \\'2023-09-28\\'} Tool name: daily_sales_report | Parameters: {\\'day\\': \\'2023-09-29\\'} Tool name: product_database | Parameters: {\\'category\\': \\'Electronics\\'} ================================================== Final response: On 28 September 2023, the total sales amount was 5000, with 100 units sold. The following day, 29 September 2023, the total sales amount was 10000, with 250 units sold. Here is the stock level of the products in the \\'Electronics\\' category: - Smartphone (E1001) - 20 - Laptop (E1002) - 15 - Tablet (E1003) - 25 ================================================== Citations: start=3 end=20 text=\\'28 September 2023\\' document_ids=[\\'daily_sales_report:0:2:0\\'] start=26 end=53 text=\\'total sales amount was 5000\\' document_ids=[\\'daily_sales_report:0:2:0\\'] start=60 end=74 text=\\'100 units sold\\' document_ids=[\\'daily_sales_report:0:2:0\\'] start=95 end=112 text=\\'29 September 2023\\' document_ids=[\\'daily_sales_report:1:2:0\\'] start=118 end=146 text=\\'total sales amount was 10000\\' document_ids=[\\'daily_sales_report:1:2:0\\'] start=153 end=167 text=\\'250 units sold\\' document_ids=[\\'daily_sales_report:1:2:0\\'] start=244 end=267 text=\\'Smartphone (E1001) - 20\\' document_ids=[\\'product_database:2:2:0\\'] start=270 end=289 text=\\'Laptop (E1002) - 15\\' document_ids=[\\'product_database:2:2:0\\'] start=292 end=311 text=\\'Tablet (E1003) - 25\\' document_ids=[\\'product_database:2:2:0\\'] Cited Documents: {\\'date\\': \\'2023-09-28\\', \\'id\\': \\'daily_sales_report:0:2:0\\', \\'summary\\': \\'Total Sales Amount: 5000, Total Units Sold: 100\\', \\'tool_name\\': \\'daily_sales_report\\'} {\\'date\\': \\'2023-09-29\\', \\'id\\': \\'daily_sales_report:1:2:0\\', \\'summary\\': \\'Total Sales Amount: 10000, Total Units Sold: 250\\', \\'tool_name\\': \\'daily_sales_report\\'} {\\'category\\': \\'Electronics\\', \\'id\\': \\'product_database:2:2:0\\', \\'products\\': \\'[{\"name\":\"Smartphone\",\"price\":500,\"product_id\":\"E1001\",\"stock_level\":20},{\"name\":\"Laptop\",\"price\":1000,\"product_id\":\"E1002\",\"stock_level\":15},{\"name\":\"Tablet\",\"price\":300,\"product_id\":\"E1003\",\"stock_level\":25}]\\', \\'tool_name\\': \\'product_database\\'} ================================================== \\nDirectly Answering\\nA key attribute of tool use systems is the model’s ability to choose the right tools for a task. This includes the ability to decide to not use any tool, and instead, respond to a user message directly.\\nThis could happen if a user’s question inherently can be answered directly or none of the available tools are helpful to answer the question.\\nComparing the sequence of events of using tools and directly answering\\nLet’s look at the first scenario. The question below asks about building a great company. It is a rather general question that a good LLM would be able to answer directly without needing any additional help.\\nchat_history = run_assistant(\"Give me 3 concise tips on how to build a great company\") \\nAnd here’s the response. Notice that the model doesn’t trigger any tool calls, but instead goes directly to answering the question.\\nQuestion: Give me 3 concise tips on how to build a great company ================================================== Final response: 1. Start with a strong foundation: Build a solid business plan, secure adequate funding, and establish a clear mission and vision. 2. Focus on people: Hire and retain talented employees by creating a positive company culture and offering competitive benefits. 3. Stay agile: Adapt to market changes and be open to innovation. Stay connected to your customers\\' needs and wants and be willing to pivot when necessary. ================================================== \\nLet’s look at the other scenario. The question below asks about the company\\'s employee count, which is a specific piece of information that an LLM would not possess and requires context from an external tool.\\nchat_history = run_assistant(\"How many employees does this company have?\") \\nBut because none of the available tools can provide this information, the model doesn’t attempt any tool call. Instead, it responds directly to the user question, mentioning that it doesn’t have the information needed to answer the question.\\nQuestion: How many employees does this company have? ================================================== Final response: I don\\'t have any information about the company\\'s number of employees. Is there anything else I can help you with? ================================================== \\nState Management (Memory)\\nWhen we created the run_assistant function, we already enabled it to handle multi-turn chat scenarios. The function represents one turn of a conversation. For each turn, it accepts the latest chat history of a conversation and returns the updated one once the turn has been completed.\\nThis can continue for any number of turns, the limit being the model’s maximum context length (with Command R/R+, that’s 128k tokens).\\nThe chat history for each turn consists of:\\nThe USER message\\nFollowed by the CHATBOT message with the list of tool calls\\nFollowed by the TOOL message with the list of tool results\\nFinally, followed by the CHATBOT message with the final response to the user\\nMaintaining the state over multiple chat turns (single-step tool use)\\nThese four entries in the chat history directly map to the four steps in the tool use workflow. Here’s an example chat history taken from the first user message example earlier.\\n# Print chat history for turn in chat_history: print(turn,\"\\\\n\") \\nmessage=\\'Can you provide a sales summary for 29th September 2023?\\' tool_calls=None role=\\'USER\\' message=None tool_calls=[ToolCall(name=\\'daily_sales_report\\', parameters={\\'day\\': \\'2023-09-29\\'})] role=\\'CHATBOT\\' tool_results=[ToolResult(call=ToolCall(name=\\'daily_sales_report\\', parameters={\\'day\\': \\'2023-09-29\\'}), outputs=[{\\'date\\': \\'2023-09-29\\', \\'summary\\': \\'Total Sales Amount: 10000, Total Units Sold: 250\\'}])] role=\\'TOOL\\' message=\\'On 29 September 2023, we sold 250 units, totalling a sales amount of 10,000.\\' tool_calls=None role=\\'CHATBOT\\' \\nAll this becomes part of the prompt to a chatbot at a particular turn, giving it the full context of a conversation.\\nLet’s try it out, starting with the same question about the sales summary.\\nchat_history = run_assistant(\"Can you provide a sales summary for 29th September 2023?\") \\nThis gives the same answer as before.\\nQuestion: Can you provide a sales summary for 29th September 2023? ================================================== Tool calls: Tool name: daily_sales_report | Parameters: {\\'day\\': \\'2023-09-29\\'} ================================================== Final response: On 29 September 2023, the total sales amount was 10,000 and the total number of units sold was 250. ================================================== Citations: start=26 end=55 text=\\'total sales amount was 10,000\\' document_ids=[\\'daily_sales_report:0:2:0\\'] start=64 end=98 text=\\'total number of units sold was 250\\' document_ids=[\\'daily_sales_report:0:2:0\\'] Cited Documents: {\\'date\\': \\'2023-09-29\\', \\'id\\': \\'daily_sales_report:0:2:0\\', \\'summary\\': \\'Total Sales Amount: 10000, Total Units Sold: 250\\', \\'tool_name\\': \\'daily_sales_report\\'} ================================================== \\nLet’s now ask a follow-up question, a rather vague question that a model would not be able to answer without the context of the previous turn. Here, we pass the first turn’s chat history to the run_assistant function.\\nchat_history = run_assistant(\"What about the 28th?\", chat_history) \\nThe model is able to infer that “the 28th” is likely referring to September 28th 2023, because of what was asked in the previous turn.\\nQuestion: What about the 28th? ================================================== Tool calls: Tool name: daily_sales_report | Parameters: {\\'day\\': \\'2023-09-28\\'} ================================================== Final response: On 28 September 2023, the total sales amount was 5,000 and the total number of units sold was 100. ================================================== Citations: start=26 end=54 text=\\'total sales amount was 5,000\\' document_ids=[\\'daily_sales_report:0:6:0\\'] start=63 end=97 text=\\'total number of units sold was 100\\' document_ids=[\\'daily_sales_report:0:6:0\\'] Cited Documents: {\\'date\\': \\'2023-09-28\\', \\'id\\': \\'daily_sales_report:0:6:0\\', \\'summary\\': \\'Total Sales Amount: 5000, Total Units Sold: 100\\', \\'tool_name\\': \\'daily_sales_report\\'} ================================================== \\nLet’s continue the conversation.\\nchat_history = run_assistant(\"How many units were sold over both days\", chat_history) \\nAgain, the model is able to infer what “both days” are from the chat context.\\nQuestion: How many units were sold over both days ================================================== Tool calls: Tool name: daily_sales_report | Parameters: {\\'day\\': \\'2023-09-29\\'} Tool name: daily_sales_report | Parameters: {\\'day\\': \\'2023-09-28\\'} ================================================== Final response: Combined, 28 and 29 September 2023 saw 350 units sold. ================================================== Citations: start=39 end=53 text=\\'350 units sold\\' document_ids=[\\'daily_sales_report:0:10:0\\', \\'daily_sales_report:1:10:0\\'] Cited Documents: {\\'date\\': \\'2023-09-29\\', \\'id\\': \\'daily_sales_report:0:10:0\\', \\'summary\\': \\'Total Sales Amount: 10000, Total Units Sold: 250\\', \\'tool_name\\': \\'daily_sales_report\\'} {\\'date\\': \\'2023-09-28\\', \\'id\\': \\'daily_sales_report:1:10:0\\', \\'summary\\': \\'Total Sales Amount: 5000, Total Units Sold: 100\\', \\'tool_name\\': \\'daily_sales_report\\'} ================================================== \\nHere’s a look at the chat history. It consists of a set of USER, CHATBOT, and TOOL messages appended in the right sequence of turns, providing the model with the right context each time it generates a new response.\\nmessage=\\'Can you provide a sales summary for 29th September 2023?\\' tool_calls=None role=\\'USER\\' message=None tool_calls=[ToolCall(name=\\'daily_sales_report\\', parameters={\\'day\\': \\'2023-09-29\\'})] role=\\'CHATBOT\\' tool_results=[ToolResult(call=ToolCall(name=\\'daily_sales_report\\', parameters={\\'day\\': \\'2023-09-29\\'}), outputs=[{\\'date\\': \\'2023-09-29\\', \\'summary\\': \\'Total Sales Amount: 10000, Total Units Sold: 250\\'}])] role=\\'TOOL\\' message=\\'On 29 September 2023, we sold 250 units, totalling a revenue of 10,000.\\' tool_calls=None role=\\'CHATBOT\\' message=\\'What about the 28th?\\' tool_calls=None role=\\'USER\\' message=None tool_calls=[ToolCall(name=\\'daily_sales_report\\', parameters={\\'day\\': \\'2023-09-28\\'})] role=\\'CHATBOT\\' tool_results=[ToolResult(call=ToolCall(name=\\'daily_sales_report\\', parameters={\\'day\\': \\'2023-09-28\\'}), outputs=[{\\'date\\': \\'2023-09-28\\', \\'summary\\': \\'Total Sales Amount: 5000, Total Units Sold: 100\\'}])] role=\\'TOOL\\' message=\\'On 28 September 2023, we sold 100 units, totalling a revenue of 5,000.\\' tool_calls=None role=\\'CHATBOT\\' message=\\'How many units were sold over both days\\' tool_calls=None role=\\'USER\\' message=None tool_calls=[ToolCall(name=\\'daily_sales_report\\', parameters={\\'day\\': \\'2023-09-29\\'}), ToolCall(name=\\'daily_sales_report\\', parameters={\\'day\\': \\'2023-09-28\\'})] role=\\'CHATBOT\\' tool_results=[ToolResult(call=ToolCall(name=\\'daily_sales_report\\', parameters={\\'day\\': \\'2023-09-29\\'}), outputs=[{\\'date\\': \\'2023-09-29\\', \\'summary\\': \\'Total Sales Amount: 10000, Total Units Sold: 250\\'}]), ToolResult(call=ToolCall(name=\\'daily_sales_report\\', parameters={\\'day\\': \\'2023-09-28\\'}), outputs=[{\\'date\\': \\'2023-09-28\\', \\'summary\\': \\'Total Sales Amount: 5000, Total Units Sold: 100\\'}])] role=\\'TOOL\\' message=\\'Over the two days, we sold 350 units.\\' tool_calls=None role=\\'CHATBOT\\' \\nHere, the four entries in the chat history directly map to the four-step workflow. It starts with the USER message, the CHATBOT message with the list of tool calls, the TOOL message with the list of tool results, and the CHATBOT message with the final response to the user.\\nConclusion\\nIn this chapter, we added another tool to the structured data RAG assistant from the previous chapter. We also looked at the different dynamics this introduces, such as calling tools in parallel and being able to decide on the right tool(s) for a given user message.\\nHowever, all the examples so far have been confined to a single-step scenario. For example, “Create a plot of the number of employees at the three tech companies with the highest market cap in the United States in 2024.” requires a number of tool calls that happen in sequence. These types of tasks can’t be handled with single-step tool use.\\nAn assistant\\'s potential can only be truly unlocked when it can reason over complex tasks that require multiple steps of tool-calling. We’ll learn about exactly that in the next two chapters. \\nLet’s now turn our focus to multi-step tool use in Chapter 4.'),\n",
       " Document(metadata={'title': 'Tool Use on LangChain', 'source': 'https://cohere.com/llmu/tool-use-on-langchain'}, page_content='Tool Use on LangChainQualified\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nContents\\nIntroduction\\nUsing Cohere with LangChain\\nDefining Tools \\nTavily Search\\nPython REPL\\nCreating a ReAct Agent\\nMulti-Step Tool Use\\nConclusion\\nIntroduction\\nThe previous chapter showed how to implement multi-step tool use using the native Cohere API. A similar implementation is also available on LangChain. \\nIn this chapter, we’ll look at how to use LangChain to create an assistant powered by multi-step tool use.\\nLangChain is an open-source framework for building applications based on large language models (LLMs). It provides tools and APIs that simplify the process of building LLM-driven applications like chatbots and virtual agents.\\nIt provides abstractions for the various components needed to build LLM-powered applications. This includes the ReAct-style implementation of LLM agents and a collection of ready-to-use tools to simplify the development process.\\nIn this chapter, we’ll build a data analyst that can research the web, get up-to-date data, and perform data visualization using Python.\\nUsing Cohere with LangChain\\nWith LangChain, the four-step tool-use workflow remains the same. The only difference is that interfacing with the LLM and tools is done via the LangChain API, as illustrated in the diagram below.\\nInterfacing with the LLM and tools is done via the LangChain API\\nLangChain makes it easy to create tools, with different options available:\\nBuilt-in tools: Ready implementations of tools that can be used immediately (see full list). We’ll use two of them in this chapter.\\nToolkits: Collections of tools that are designed to be used together for specific tasks (see full list). Some examples are toolkits for working with SQL databases, Gmail, or GitHub.\\nCustom tools: There’s the option to build our own tools, such as those we created in the previous chapters.\\nLet’s get started. First, we need to install a few LangChain libraries.\\npip install langchain langchain-cohere langchain-experimental \\nTo build our data analysis agent, we’ll be using two tools, both of which are LangChain built-in tools:\\nTavily Search - for performing web search\\nPython REPL - for executing Python code\\nTavily Search\\nThe Tavily Search is used for performing web search queries. Given a query, it returns a list of relevant document snippets from the internet. It requires an API key which you can get here under the free plan.\\nWe access Tavily Search via the langchain_community library. The steps to define the tool are shown below.\\nfrom langchain_community.tools.tavily_search import TavilySearchResults os.environ[\\'TAVILY_API_KEY\\'] = \"<INSERT TAVILY API KEY HERE>\" internet_search = TavilySearchResults() internet_search.name = \"internet_search\" internet_search.description = \"Returns a list of relevant document snippets for a textual query retrieved from the internet.\" from langchain_core.pydantic_v1 import BaseModel, Field class TavilySearchInput(BaseModel): query: str = Field(description=\"Query to search the internet with\") internet_search.args_schema = TavilySearchInput \\nPython REPL\\nNext, we’ll create the Python REPL to execute the Python code. For our example, we’ll need it to plot data on a chart.\\nWe access the Python REPL via the langchain_experimental library. The steps to define the tool are shown below.\\nfrom langchain.agents import Tool from langchain_experimental.utilities import PythonREPL python_repl = PythonREPL() repl_tool = Tool( name=\"python_repl\", description=\"Executes python code and returns the result. The code runs in a static sandbox without interactive mode, so print output or save output to a file.\", func=python_repl.run, ) repl_tool.name = \"python_interpreter\" # from langchain_core.pydantic_v1 import BaseModel, Field class ToolInput(BaseModel): code: str = Field(description=\"Python code to execute.\") repl_tool.args_schema = ToolInput \\nCreating a ReAct Agent\\nIn the previous chapter, we saw that the native Cohere API uses a ReAct-style approach for implementing multi-step tool use. As a quick recap, the agent workflow starts with a planning step and is followed by a series of Action-Observation-Reflection sequences until a task is deemed complete.\\nThe same approach is used when using Cohere with LangChain, which can be utilized via the create_cohere_react_agent module.\\nfrom langchain.agents import AgentExecutor from langchain_cohere.react_multi_hop.agent import create_cohere_react_agent from langchain_core.prompts import ChatPromptTemplate \\nThe overall implementation to create an agent in LangChain is pretty simple:\\nCreate the prompt using Cohere’s template\\nCreate an agent by passing the model type, list of tools, and the prompt\\nCreate an agent executor using AgentExecutor\\n# Create the prompt prompt = ChatPromptTemplate.from_template(\"{input}\") # Create the ReAct agent agent = create_cohere_react_agent( llm=chat, tools=[internet_search, repl_tool], prompt=prompt, ) agent_executor = AgentExecutor(agent=agent, tools=[internet_search, repl_tool], verbose=True) \\nThe agent is now ready to use. We can run it using agent_executor.invoke.\\nLet’s ask the agent a question that requires it to run a multi-step sequence: “Create a plot of the number of full-time employees at the three tech companies with the highest market cap in the United States in 2024.”\\nIf a human assistant were given this query, they would need to do a few steps in the right order:\\nFirst, find the three tech companies with the highest market cap in the United States in 2024\\nNext, find the number of full-time employees at each of the companies\\nFinally, create a plot based on these numbers\\nLet’s see how the agent performs.\\nagent_executor.invoke({ \"input\": \"Create a plot of the number of full time employees at the 3 tech companies with the highest market cap in the United States in 2024.\", }) \\nThe whole response is pretty long, so let’s break down the responses into the different steps the agent takes.\\nFirst, it generates a plan of what it needs to perform: “First, I will search for the three tech companies with the highest market cap in the US in 2024. Then, I will search for the number of full-time employees at each of these companies, and plot the data using Python.”\\nThen, it executes its first action: using the internet search tool to find the three companies and display the results.\\n> Entering new AgentExecutor chain... First, I will search for the three tech companies with the highest market cap in the US in 2024. Then, I will search for the number of full-time employees at each of these companies, and plot the data using Python. {\\'tool_name\\': \\'internet_search\\', \\'parameters\\': {\\'query\\': \\'top 3 tech companies highest market cap US 2024\\'}} [{\\'url\\': \\'<https://www.fool.com/research/largest-companies-by-market-cap/>\\', \\'content\\': \"It\\'s the most valuable automaker in the world and has the world\\'s best-selling car in the Model Y.\\\\\\\\nTesla is most famous for its vehicles, and it\\'s second only to China\\'s BYD Company (OTC:BYDDY) among the largest EV companies in terms of manufacturing. While it\\'s most famous for Windows, Microsoft also has a diverse selection of products and services that has helped to build on its success, including:\\\\\\\\nMicrosoft has been the world\\'s largest company before, and it briefly surpassed Apple for the biggest market cap in 2021. Walmart\\\\\\\\nWalmart (NYSE:WMT) may not have the largest market cap, but it is No. 1 in terms of revenue, and it’s the largest retailer in the world. Microsoft\\\\\\\\nConsidering the popularity of the Windows operating system, it’s no surprise that Microsoft (NASDAQ:MSFT) has consistently ranked as one of the largest companies in the world. Although the top spot has changed hands on multiple occasions, Apple has spent the most time there and is currently the most valuable company in the world.\\\\\\\\n\"}, {\\'url\\': ...(truncated) \\nUsing the results, the agent generates a reflection step that identifies the list of companies from its results. It then executes its next action: using the internet search tool to find the number of employees of the first company in the list, Microsoft.\\nI have found that the three tech companies with the highest market cap in the US in 2024 are Microsoft, Apple and NVIDIA. Now, I will search for the number of full-time employees at each of these companies and plot the data. {\\'tool_name\\': \\'internet_search\\', \\'parameters\\': {\\'query\\': \\'Microsoft full time employees 2024\\'}} [{\\'url\\': \\'<https://www.statista.com/statistics/273475/number-of-employees-at-the-microsoft-corporation-since-2005/>\\', \\'content\\': \\'Digital & Trend reports\\\\\\\\nOverview and forecasts on trending topics\\\\\\\\nIndustry & Market reports\\\\\\\\nIndustry and market insights and forecasts\\\\\\\\nCompanies & Products reports\\\\\\\\nKey figures and rankings about companies and products\\\\\\\\nConsumer & ...(truncated) \\nHaving received the results, it moves to the next company in the list.\\n{\\'tool_name\\': \\'internet_search\\', \\'parameters\\': {\\'query\\': \\'Apple full time employees 2024\\'}} [{\\'url\\': \\'<https://www.macrotrends.net/stocks/charts/AAPL/apple/number-of-employees>\\', \\'content\\': \\'Employee Count. Interactive chart of Apple ...(truncated) \\nAnd then, the third and final company on the list.\\n{\\'tool_name\\': \\'internet_search\\', \\'parameters\\': {\\'query\\': \\'NVIDIA full time employees 2024\\'}} [{\\'url\\': \\'<https://www.macrotrends.net/stocks/charts/NVDA/nvidia/number-of-employees>\\', \\'content\\': \\'Employee Count. Interactive chart of NVIDIA (NVDA) annual worldwide employee count from 2010 to 2024. NVIDIA total number of employees in 2024 was 29,600, a 12.99% increase \\nNow that it has retrieved all the data it needs, it can proceed with the next action: writing and executing the Python code to plot this data.\\nIt generates a code block as the parameter for calling the Python REPL tool. Here, it opts to use matplotlib as the library to plot the data and finally save it as a PNG file.\\nI have found the following data on the number of full-time employees at each company: Microsoft: 221,000 Apple: 161,000 NVIDIA: 29,600 Now, I will write and execute Python code to plot this data. {\\'tool_name\\': \\'python_interpreter\\', \\'parameters\\': {\\'code\\': \\'import matplotlib.pyplot as plt\\\\\\\\n\\\\\\\\n# Data on the number of full-time employees at each company\\\\\\\\ncompanies = [\\\\\\\\n \"Microsoft\", \"Apple\", \"NVIDIA\"\\\\\\\\n]\\\\\\\\nfull_time_employees = [221000, 161000, 29600]\\\\\\\\n\\\\\\\\n# Plot the data\\\\\\\\nplt.bar(companies, full_time_employees)\\\\\\\\nplt.xlabel(\"Company\")\\\\\\\\nplt.ylabel(\"Number of Full Time Employees\")\\\\\\\\nplt.xticks(rotation=45)\\\\\\\\nplt.tight_layout()\\\\\\\\nplt.savefig(\"tech_companies_market_cap_employees.png\")\\'}} \\nNow that it has completed the task, it generates the final response to the user. It contains a text commentary on the number of employees and a rendering of the plot it had created.\\nAs with the previous chapters, it also generates fine-grained citations that refer to the tools it used to generate the response.\\nRelevant Documents: 0,1,3,4,5,7,8,9,10,11,12,15,18,20 Cited Documents: 1,3,4,7,8,10,11,15,18,20 Answer: Here is a plot showing the number of full-time employees at the three US tech companies with the highest market cap in 2024: ![Number of Full Time Employees](\\'tech_companies_market_cap_employees.png\\') The companies with the highest number of full-time employees are Microsoft with 221,000, Apple with 161,000 and NVIDIA with 29,600. Grounded answer: Here is a plot showing the number of full-time employees at the three US tech companies with the highest market cap in 2024: <co: 20>! [Number of Full Time Employees](\\'tech_companies_market_cap_employees.png\\')</co: 20> The companies with the highest number of full-time employees are <co: 1,4>Microsoft</co: 1,4> with <co: 7,8>221,000</co: 7,8>, <co: 1,3,4>Apple</co: 1,3,4> with <co: 10,11>161,000</co: 10,11> and <co: 1,4>NVIDIA</co: 1,4> with <co: 15,18>29,600</co: 15,18>. \\nThe matplotlib plot generated through the Python REPL \\nConclusion\\nIn this chapter, we looked at how to build a multi-step agent using LangChain. This lets us build ReAct-style agents that can reason over multiple steps, similar to the one we built with the native Cohere API in the previous chapter.\\nThis agent also had access to the two LangChain built-in tools: Tavily Search and Python REPL. The built-in tools and toolkit simplify the process of creating tool use applications, so the developer can focus on building the application’s core logic.\\nThat concludes the LLM University module on tool use, for now. There is still so much to discuss on the subject of agents and tool use, so be sure to keep an eye on LLM University for new modules on this topic in future.'),\n",
       " Document(metadata={'title': 'Introduction to Cohere on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-bedrock'}, page_content=\"Introduction to Cohere on Amazon BedrockQualified\\nContents\\nIntroduction\\nCohere's Model Deployment Options\\nWhat Is Amazon Bedrock?\\nFeatures of Amazon Bedrock\\nCohere Models on Amazon Bedrock \\nList of Models\\nPricing Mechanism\\nExample Use Cases\\nGetting Started with Amazon Bedrock \\nStep 1: Set Up Amazon Bedrock\\nStep 2: Choose Your Models\\nStep 3: Test on the Playground\\nConclusion\\nIntroduction\\nThe most common way to access Cohere’s large language models (LLMs) is through the Cohere platform, which is fully managed by Cohere and accessible through an API.\\nBut that’s not the only way to access Cohere’s models. In an enterprise setting, companies might require more control over where and how the models are hosted. The good news is that Cohere provides enterprises with flexibility and choice around how they want to deploy the models.\\nIn this module, Cohere on AWS, you’ll learn how to deploy Cohere’s LLMs through Amazon Web Services (AWS). In this first chapter of the module, you’ll get an overview of Amazon Bedrock, AWS’s fully managed service for deploying foundational models (FMs).\\nCohere's Model Deployment Options\\nSpecifically, Cohere offers four types of deployment options:\\nCohere’s SaaS Platform and API\\nThis is the fastest and easiest way to start using Cohere’s models. The models are hosted on Cohere infrastructure and available on our public SaaS platform (which provides an API data opt-out), which is fully managed by Cohere.\\nCloud AI Services (Managed)\\nThese managed services enable enterprises to access Cohere’s models while easily integrating the service and dealing with sensitive data. In this scenario, Cohere’s models are hosted on the cloud provider’s infrastructure. Cohere is cloud-agnostic, meaning you can deploy our models through any cloud provider. Examples include Amazon Bedrock, Amazon SageMaker, Google Vertex AI, OCI Generative AI, and Azure AI.\\nPrivate Deployment - Cloud\\nPrivate deployments (cloud) allow enterprises to deploy the Cohere stack privately on cloud platforms. With AWS, Cohere’s models can be deployed in an enterprise’s AWS Cloud environment via their own VPC (EC2, EKS).\\nPrivate Deployment - On-Premises\\nPrivate deployments (on-premises) allow enterprises to deploy the Cohere stack privately on their own compute.\\nAmazon Bedrock (introduced in this chapter) and Amazon SageMaker (introduced in the next chapter) fall under the second category (Cloud AI Services).\\nAn overview of Cohere’s four deployment options\\nWhat Is Amazon Bedrock?\\nAmazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs).\\nUsing Amazon Bedrock, you can customize the models with your data using techniques such as fine-tuning and retrieval-augmented generation (RAG), and build agents that execute tasks (tool use) using your enterprise systems and data sources.\\nAmazon Bedrock is serverless, so you don't have to manage any infrastructure. You can securely integrate generative AI capabilities into your applications using the AWS services that you are already familiar with.\\nFeatures of Amazon Bedrock\\nThe following are some key features that come with Bedrock.\\nModel fine-tuning\\nBedrock allows companies to adapt Cohere’s models to meet their individual needs. It enables developers to fine-tune a model’s performance and optimize it for specific use cases, thus empowering you to build and scale generative AI applications.\\nData security and privacy\\nAmazon Bedrock helps to ensure that your data stays under your control. When you fine-tune a foundation model, we base it on a private copy of that model. This means that your data is not shared with model providers, and it is not used to improve the base models. You can use AWS PrivateLink to establish private connectivity from your Amazon Virtual Private Cloud (VPC) to Amazon Bedrock without exposing your VPC to internet traffic. Finally, Bedrock is in scope for common compliance standards, including ISO, SOC, and CSA STAR Level 2, it is HIPAA eligible, and customers can use Bedrock in compliance with the GDPR.\\nProvisioned throughput\\nBedrock makes it easy to get started with Cohere’s models without worrying about the underlying infrastructure as that is handled by AWS. That said, if you want greater control over a model’s performance, you can opt for the provisioned throughput option. When you configure Provisioned Throughput for a model, you receive a level of throughput at a fixed cost.\\nKnowledge bases\\nThis fully managed capability helps you implement the entire retrieval-augmented generation (RAG) workflow, from ingestion to retrieval and prompt augmentation, without having to build custom integrations to data sources and manage data flows. Session context management is built in, so your app can readily support multi-turn conversations.\\nGuardrails\\nGuardrails for Amazon Bedrock evaluates user inputs and FM responses based on use case-specific policies, and it provides an additional layer of safeguards regardless of the underlying FM. You can create multiple guardrails, each configured with a different combination of controls, and use them across different applications and use cases.\\nModel evaluation\\nAmazon Bedrock supports model evaluation jobs. The results of a model evaluation job allow you to compare model outputs and then choose the model best suited for your downstream generative AI applications.\\nNote: the list of features will continue to evolve and expand. To keep up with the latest, visit the Bedrock website.\\nCohere Models on Amazon Bedrock\\nList of Models\\nThe most up-to-date list of available Cohere’s models is on the Bedrock website, but here is the list at the time of writing:\\nGenerative models:\\nCommand R+\\nCommand R\\nCommand\\nCommand Light\\nEmbeddings models:\\nEmbed (English)\\nEmbed (Multilingual)\\nPricing Mechanism\\nThe pricing for Cohere models on Bedrock at the time of writing falls under two types:\\nOn-demand and batch\\nThis applies to default Cohere models on Bedrock. Pricing is based on tokens and broken down into:\\nPrice per 1,000 input tokens\\nPrice per 1,000 output tokens\\nCustomization (fine-tuning)\\nThis applies to fine-tuned Cohere models on Bedrock. Pricing is broken down into:\\nPrice to train 1,000 tokens\\nPrice to store each custom model per month\\nPrice to infer from a custom model per model unit per hour (with no-commit Provisioned Throughput pricing)\\nTo get the most updated pricing mechanism with further details, visit the Bedrock pricing page.\\nExample Use Cases\\nThe following are some example use cases that you can build on top of Cohere’s models on Bedrock.\\nGenerative AI: Build applications that can write product descriptions, help draft emails, suggest example press releases, and much more.\\nChat: Combine the power of text generation with a conversational interface. Build powerful chatbots and interactive knowledge assistants that give users accurate answers from connected enterprise data, citing sources if required.\\nSemantic Search and Retrieval: Easily build powerful search solutions in English and over 100 other languages using Cohere’s industry-leading Embed model. Cohere provides the Embed model, connectors to common enterprise data sources, and customization tools to maximize search relevance by domain.\\nRAG: Enhance the performance of generative AI applications by enabling a model to access and utilize supplementary information from external documents, thereby improving the accuracy of its responses. The RAG approach significantly reduces the hallucination issue common in LLMs.\\nGetting Started with Amazon Bedrock\\nWe’ll start implementing actual use cases in later chapters, but for now, let’s first get set up with Amazon Bedrock.\\nStep 1: Set Up Amazon Bedrock\\nTo get started, follow the steps outlined on the Bedrock page, and in particular, be sure to do the following:\\nSign up for an AWS account\\nCreate an administrative user\\nGrant programmatic access\\nGet console and playground access\\nAdd model access\\nStep 2: Choose Your Models\\nAfter completing the step above, you’ll reach the page where you can select models to access. To access Cohere’s models, locate the list of Cohere’s models and select the ones you want.\\nA screenshot of Bedrock’s model selection page\\nStep 3: Test on the Playground\\nIn Step 1, you established console and playground access. The Bedrock playground provides a console environment that allows you to experiment with running inference on different models and with different configurations before deciding to use them in your application. You can test out the models, such as Command, and get a feel for their capabilities. Enter a prompt, adjust the configurations (if needed), and you’ll get the output generated.\\nA screenshot of the Bedrock playground\\nConclusion\\nThis chapter introduced Cohere’s deployment options: its SaaS platform, Cloud AI platforms, virtual private cloud, and private deployments. We also introduced Amazon Bedrock, a fully managed service by AWS that hosts foundational models, including Cohere’s LLMs. We also went through the steps to get set up with Bedrock.\\nIn Chapter 2, you’ll learn about another way to access Cohere’s models on AWS: Amazon SageMaker.\"),\n",
       " Document(metadata={'title': 'Introduction to Cohere on Amazon SageMaker', 'source': 'https://cohere.com/llmu/co-aws-sagemaker'}, page_content=\"Introduction to Cohere on Amazon SageMakerQualified\\nContents\\nIntroduction\\nFeatures of Amazon SageMaker\\nCohere Models on Amazon SageMaker \\nList of Models\\nPricing Mechanism\\nGetting Started with Amazon SageMaker\\nConclusion\\nIntroduction\\nIn the previous chapter, we discussed using Amazon Bedrock to access Cohere’s models. Amazon SageMaker provides another option.\\nAmazon SageMaker is a fully managed service where you can build, train, and deploy ML models at scale using tools like notebooks, debuggers, profilers, pipelines, MLOps, and more — all in one integrated development environment (IDE).\\nFor each of the available models, Cohere ships to Amazon Elastic Container Registry (AWS ECR) the elements required to run a model replica. This includes a container image and the model weights.\\nYou can subscribe to Cohere’s models via AWS Marketplace. Once subscribed, you can create a new inference endpoint deployment from SageMaker. This will create a deployment of a model replica and an associate compute instance that runs the replica within your VPC. The model will be available for communication through your inference endpoint.\\nWhen you use a VPC interface endpoint, communication between your VPC and the SageMaker API or Runtime is conducted entirely and securely within an AWS network.\\nAn overview of the Amazon SageMaker solution\\nDifferences Between Amazon Bedrock and SageMaker\\nWhat is the difference between SageMaker and Bedrock? While Bedrock is a platform that’s focused on foundational models (FMs), SageMaker is a platform that caters to a much broader range of machine learning (ML) models.\\nAnother key difference is that Bedrock offers a simpler way to get started using Cohere’s models, while SageMaker offers greater control over the underlying infrastructure hosting the models.\\nThe table below summarizes the key differences between Bedrock and SageMaker.\\nModuleTitleColab\\nScope\\tFocused on foundation models and generative AI applications\\tCaters to a much broader range of machine learning models and applications\\t\\nEase of Use\\tSimple API for foundation models, easy to integrate and experiment\\tRelatively more involved setup requiring more steps to get started\\t\\nControl\\tLimited control over infrastructure and model updates, managed entirely by AWS\\tGreater control over model versions, endpoints, and security configurations\\t\\nPricing\\tPer-token pricing based on tokens processed, as well as separate compute/storage time used for fine-tuning\\tPer-hour pricing based on software and infrastructure licensing\\t\\nFeatures of Amazon SageMaker\\nSageMaker offers a wide range of services for building, training, and deploying machine learning models. It provides fully managed infrastructure, tools, and workflows to help engineers and data scientists.\\nHere are some of the relevant features in the context of working with foundational models:\\nSageMaker Studio: A single, web-based interface for end-to-end ML development, SageMaker Studio provides a fully integrated development environment (IDE) for machine learning, including a visual interface for all ML development activities.\\nSageMaker Jumpstart: An ML hub that forms a part of the SageMaker Studio experience, SageMaker Jumpstart accelerates ML with built-in algorithms, pre-trained models, and pre-built solutions for common use cases, enabling one-click deployment. Jumpstart also allows users to share ML models and notebooks within their organizations.\\nAutomatic scaling: SageMaker supports automatic scaling for hosted models, dynamically adjusting the number of instances provisioned based on workload changes to optimize resource usage.\\nData labeling: SageMaker Ground Truth is a service that helps build high-quality training datasets for machine learning models by combining human and automatic labeling.\\nCohere Models on Amazon SageMaker\\nList of Models\\nAt the time of writing, the following Cohere models are available on SageMaker.\\nGenerative models:\\nCommand R+\\nCommand R\\nCommand\\nCommand Light\\nEmbeddings models:\\nEmbed v3 (English & Multilingual)\\nEmbed Light v3 (English & Multilingual)\\nRerank 2 and 3 (English & Multilingual)\\nClassification (English & Multilingual)\\nTo get the most updated list of available models on SageMaker, visit Cohere’s listing on the AWS Marketplace.\\nPricing Mechanism\\nThe pricing for Cohere models on SageMaker is broken down into two portions:\\nSoftware: Hourly pricing that can vary by instance type\\nInfrastructure: Hourly pricing that can vary by region and instance type\\nAdditionally, you can choose from two inference options — real-time inference and batch transform — each with different pricing.\\nGetting Started with Amazon SageMaker\\nStep 1: Set Up Amazon SageMaker\\nTo get set up with SageMaker, follow the steps outlined in the Cohere documentation, which take you through the prerequisites and getting access to Cohere models.\\nIn order to successfully subscribe to Cohere’s offerings on SageMaker, you’ll need the right identity and access management (IAM) permissions, which is described in the documentation linked above.\\nStep 2: Get Model Access\\nThe guide above also includes the steps for subscribing to Cohere’s models.\\nStep 3: Test on SageMaker Studio\\nYou can test out the models on SageMaker Studio, which offers a hosted environment in which to access the models on Jupyter notebooks. To do this, follow these steps:\\nIn the AWS Console, go to Amazon SageMaker and click Studio.\\nThen, click Open Studio. If you don't see this option, you must first set up a SageMaker domain.\\nA new JupyterLab tab will open. Look for Prebuilt and automated solutions and click JumpStart.\\nA list of models will appear. In the Foundation Models: Text Generation category, look for Cohere Command and then click View notebook.\\nThis will open up a sample notebook to get started with the model.\\nConclusion\\nThis chapter explored Amazon SageMaker, another platform that hosts Cohere's models. Unlike Amazon Bedrock, which focuses on foundational models, SageMaker caters to a wide range of machine learning models, providing greater control over infrastructure. We also looked at SageMaker's features, including SageMaker Studio, SageMaker Jumpstart, automatic scaling, and data labeling. The chapter concluded with a guide on getting started with SageMaker, including setting up, subscribing to Cohere's models, and testing on SageMaker Studio.\\nIn Chapter 3, you’ll learn how to use Cohere Command on Amazon Bedrock for text generation use cases.\"),\n",
       " Document(metadata={'title': 'Text Generation Using Cohere Command on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-generation'}, page_content='Text Generation Using Cohere Command on Amazon BedrockQualified\\nContents\\nIntroduction\\nCode Walkthrough \\nSetup\\nCreate Contextual Information\\nCreate the Function to Generate Text\\nText Generation\\nText Summarization\\nText Rewriting\\nText Extraction\\nBuilding a Chatbot\\nConclusion\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nIntroduction\\nLarge language models (LLMs) enable a long list of text generation use cases. For instance, in business writing, they can assist with creating compelling narratives or drafting precise reports. LLMs are also useful in answering questions accurately and efficiently, drawing from a vast knowledge base. Additionally, they can summarize vast amounts of information, making it easier to glean key insights. Another noteworthy application is rewriting text to, for example, enhance readability or alter the tone to suit different audiences.\\nCohere\\'s Command family of models, available on Amazon Bedrock, are powerful LLMs that offer these capabilities. This includes cutting-edge models like Command R and Command R+, which we’ll be using in our code examples in this chapter.\\nFor example, a major luxury goods retailer wanted to help its client advisors, reducing the time advisors needed to find and research products. It engaged Cohere to create a tool that could help advisors suggest products to customers based on demographics, purchase history, and personal preferences.\\nUsing retrieval-augmented generation (RAG), Cohere Command and Rerank were integrated into the customer’s website, product catalog, inventory, and customer service apps to build a conversational virtual shopping assistant.\\nImplementing a virtual shopping assistant at a major luxury goods retailer\\nIn this chapter, we\\'ll explore how to use Cohere\\'s Command R+ model on Amazon Bedrock. For the most updated list of Cohere models available on Amazon Bedrock, visit the Amazon Bedrock website.\\nCode Walkthrough\\nLet’s walk through how to use Command R+ to power a range of tasks in a customer support agent scenario, including text generation, summarization, rewriting, and extraction. We’ll also use the Chat endpoint to build a simple chatbot that can maintain the memory of a conversation that runs over multiple turns.\\nAn overview of what we\\'ll cover in the code walkthrough\\nSetup\\nFirst, let\\'s install and import the necessary libraries and set up our Cohere client using the cohere SDK. To use Bedrock, we create a BedrockClient by passing the necessary AWS credentials.\\nimport cohere co = cohere.BedrockClient( aws_region=\"YOUR_AWS_REGION\", aws_access_key=\"YOUR_AWS_ACCESS_KEY_ID\", aws_secret_key=\"YOUR_AWS_SECRET_ACCESS_KEY\", aws_session_token=\"YOUR_AWS_SESSION_TOKEN\", ) \\nCreate Contextual Information\\nBefore we begin, let\\'s create some context to use in our text generation tasks. In this example, we\\'ll use a set of technical support frequently asked questions (FAQs) as our context.\\n# Technical support FAQ faq_tech_support = \"\"\"- Question: How do I set up my new smartphone with my mobile plan? - Answer: - Insert your SIM card into the device. - Turn on your phone and follow the on-screen setup instructions. - Connect to your mobile network and enter your account details when prompted. - Download and install any necessary apps or updates. - Contact customer support if you need further assistance. - Question: My internet connection is slow. How can I improve my mobile data speed? - Answer: - Check your signal strength and move to an area with better coverage. - Restart your device and try connecting again. - Ensure your data plan is active and has sufficient data. - Consider upgrading your plan for faster speeds. - Question: I can\\'t connect to my mobile network. What should I do? - Answer: - Check your SIM card is inserted correctly and not damaged. - Restart your device and try connecting again. - Ensure your account is active and not suspended. - Check for any network outages in your area. - Contact customer support for further assistance. - Question: How do I set up my voicemail? - Answer: - Dial your voicemail access number (usually provided by your carrier). - Follow the prompts to set up your voicemail greeting and password. - Record your voicemail greeting and save it. - Test your voicemail by calling your number and leaving a message. - Question: I\\'m having trouble sending text messages. What could be the issue? - Answer: - Check your signal strength and move to an area with better coverage. - Ensure your account has sufficient credit or an active plan. - Restart your device and try sending a message again. - Check your message settings and ensure they are correct. - Contact customer support if the issue persists.\"\"\" \\nCreate the Function to Generate Text\\nNow, let\\'s define a function to generate text using the Command R+ model on Bedrock. We’ll use this function a few times throughout.\\nThis function takes a user message and generates the response via the chat endpoint. We also define the model as cohere.command-r-plus-v1:0. Note that this is how the model is named on Bedrock, compared to command-r-plus for the same model via the native Cohere platform.\\ndef generate_text(message): response = co.chat(message=message, model=\"cohere.command-r-plus-v1:0\") return response \\nText Generation\\nLet’s look at the first use case, which is in the broad category of text generation. Given a prompt, the model will generate a corresponding response.\\nSuppose we have a customer support agent who responds to customer inquiries using the help of an LLM to craft accurate responses. For this, we pass the technical support FAQs as context to the prompt for the LLM. We then structure the prompt to include the instruction, context (FAQs), and the customer inquiry.\\nWe then run the generate_text function on the prompt and get the response. The textual content of the response is stored in the response.text field.\\ninquiry = \"I\\'ve noticed some fluctuations in my mobile network\\'s performance recently.The connection seems stable most of the time, but every now and then, I experience brief periods of slow data speeds. It happens a few times a day and is quite inconvenient.\" prompt = f\"\"\"Use the FAQs below to provide a concise response to this customer inquiry. # Customer inquiry {inquiry} # FAQs {faq_tech_support}\"\"\" response = generate_text(prompt) print(response.text)\\nAnd here’s the response. It uses the information available from the FAQs and adds its own style for responding.\\nResponse to customer inquiry: Brief slowdowns in data speed can be frustrating. Here are some quick steps to improve your mobile data performance: 1. Check your signal strength: Move to an area with better coverage if needed. 2. Restart your device: A simple restart can often improve connectivity. 3. Data plan status: Ensure your data plan is active and you have sufficient data allocated for faster speeds. 4. Consider a plan upgrade: If the issue persists, consider contacting your provider to discuss faster speed plans. Additionally, checking for any network outages in your area and ensuring your SIM card is inserted correctly can also help maintain a stable connection. If the issue continues, reach out to your mobile provider\\'s customer support for further guidance and troubleshooting. \\nText Summarization\\nAnother type of use case is text summarization. Now, let\\'s summarize the customer inquiry into a single sentence. We add an instruction to the prompt and then pass the inquiry to the prompt.\\nprompt=f\"\"\"Summarize this customer inquiry into one short sentence. Inquiry: {inquiry}\"\"\" response = generate_text(prompt) print(response.text) \\nAnd here’s the response. As specified in the instruction, it generates a one-sentence summary about the inquiry.\\nI experience intermittent periods of slow data speeds a few times a day on my mobile network. \\nText Rewriting\\nAnother useful use case is rewriting or transforming a piece of text to suit a different need. Typically, we want to modify some aspects of the text while maintaining its overall meaning.\\nIn our example, we want to turn the response from the customer support agent, which is done via chat, into an email format that can be sent to the customer as a follow-up. It follows a similar format for building the prompt: specifying the instruction followed by adding the necessary context.\\nprompt=f\"\"\"Rewrite this customer support agent response into an email format, ready to send to the customer. If you\\'re experiencing brief periods of slow data speeds or difficulty sending text messages and connecting to your mobile network, here are some troubleshooting steps you can follow: 1. Check your signal strength - Move to an area with better coverage. 2. Restart your device and try connecting again. 3. Ensure your account is active and not suspended. 4. Contact customer support for further assistance. (This can include updating your plan for better network performance.) Did these steps help resolve the issue? Let me know if you need further assistance.\"\"\" response = generate_text(prompt) print(response.text) \\nAnd here’s the response, which is an email version of the customer support agent’s response.\\nSubject: Troubleshooting Guide for Slow Data and Network Issues Dear [Customer Name], I hope this email finds you well. I understand that experiencing slow data speeds and network connectivity issues can be frustrating. Here are some detailed troubleshooting steps to help resolve these problems: - Signal Strength: - Please check your device\\'s signal strength by looking at the signal bars displayed on your screen. - If you\\'re in an area with weak coverage, try moving to a different location with better reception. You can also try going outdoors or near a window to improve signal strength. - Device Restart: - Simply restart your device. This step can often resolve minor glitches and connectivity issues. Turn your device off, wait for a few seconds, and ... (truncated for brevity) \\nWith text extraction, we can pull out certain pieces of information from a document. In our example, we want to generate a support ticket and populate it with information about the interaction, such as the category of inquiry, product type, and the ticket status. Additionally, we can ask the model to generate the output in a specific format, such as markdown, JSON, HTML, and others. In our example, we ask the model to generate a JSON string.\\nprompt=f\"\"\"Extract the following as a JSON from the text provided below: - category (Options: technical support, promotions, billing) - product (Options: broadband, mobile, TV) - status (Options: open, closed) # Customer Inquiry {inquiry} # Customer Support Agent\\'s Response If you\\'re experiencing brief periods of slow data speeds or difficulty sending text messages and connecting to your mobile network, here are some troubleshooting steps you can follow: 1. Check your signal strength - Move to an area with better coverage. 2. Restart your device and try connecting again. 3. Ensure your account is active and not suspended. 4. Contact customer support for further assistance. (This can include updating your plan for better network performance.) Did these steps help resolve the issue? Let me know if you need further assistance.\"\"\" response = generate_text(prompt) print(response.text) \\nAnd here’s the response, formatted in JSON as instructed.\\n{ \"category\": \"technical support\", \"product\": \"mobile\", \"status\": \"open\" } \\nBuilding a Chatbot\\nAll our examples so far run on a single-turn interaction. But with the Chat endpoint, we can build a chatbot that keeps a memory of all previous interactions. This allows developers to build chatbot-style applications that maintain the state of a conversation.\\nThe following is an implementation of a simple chatbot in a customer support setting, where the chatbot acts as a helpful customer support agent. We create a function called run_chatbot to implement these steps and print out the key events and messages along the way. This function also optionally accepts the chat history as an argument to keep the state in a multi-turn conversation.\\nFor this, we introduce a couple of additional parameters to the Chat endpoint:\\npreamble: A preamble contains instructions to help steer a chatbot’s response toward specific characteristics, such as a persona, style, or format. Here, we are using a simple preamble: “You are a helpful customer support agent that assists customers of a mobile network service.”\\nchat_history: We store the history of a conversation between a user and the chatbot as a list, append every new conversation turn, and pass this information to the next endpoint call.\\n# Define a preamble preamble = \"\"\"## Task and Context You are a helpful customer support agent that assists customers of a mobile network service.\"\"\" # Run the chatbot def run_chatbot(message, chat_history=[]): response = co.chat(message=message, model=\"cohere.command-r-plus-v1:0\", preamble=preamble, chat_history=chat_history) print(response.text) chat_history = response.chat_history return chat_history\\nAnd here’s an example conversation that runs over a few turns.\\nTurn #1:\\nchat_history = run_chatbot(\"Hi. I\\'ve noticed some fluctuations in my mobile network\\'s performance recently.\") \\nI\\'m sorry to hear that you\\'ve been experiencing issues with your mobile network\\'s performance. Can you tell me a little more about the specific problems you\\'ve been facing? For example, are you experiencing dropped calls, slow data speeds, or something else entirely? The more information you can provide, the better I\\'ll be able to assist you in troubleshooting the issue.\\nTurn #2:\\nchat_history = run_chatbot(\"At times, the data speed is very poor. What should I do?\", chat_history)\\nI\\'m sorry to hear that you\\'re experiencing issues with your data speed. Here are a few troubleshooting steps you can try to improve your mobile data performance: - Check your device\\'s signal: Weak or fluctuating signals can lead to poor data speeds. Try moving to a different location or adjusting your device\\'s position to see if that improves your signal strength. - Restart your device: Sometimes, ... (truncated for brevity)\\nTurn #3:\\nchat_history = run_chatbot(\"Thanks, I\\'ll try these.\", chat_history)\\nYou\\'re welcome! If you have any other questions or concerns, feel free to reach out. I\\'m here to help. \\nHere\\'s what is contained in the chat history after a few turns.\\nprint(\"Chat history:\") for turn in chat_history: print(turn, \"\\\\n\")\\nChat history: message=\"Hi. I\\'ve noticed some fluctuations in my mobile network\\'s performance recently.\" tool_calls=None role=\\'USER\\' message=\"I\\'m sorry to hear that you\\'ve been experiencing issues with your mobile network\\'s performance. Can you tell me a little more about the specific problems you\\'ve been facing? For example, are you experiencing dropped calls, slow data speeds, or something else entirely? The more information you can provide, the better I\\'ll be able to assist you in troubleshooting the issue.\" tool_calls=None role=\\'CHATBOT\\' message=\\'At times, the data speed is very poor. What should I do?\\' tool_calls=None role=\\'USER\\' message=\"I\\'m sorry to hear that you\\'re experiencing issues with your data speed. Here are a few troubleshooting steps you can try to improve your mobile data performance: \\\\n\\\\n- Check your device\\'s signal: Weak or fluctuating signals can lead to poor data ... (truncated for brevity) \" tool_calls=None role=\\'CHATBOT\\' message=\"Thanks, I\\'ll try these.\" tool_calls=None role=\\'USER\\' message=\"You\\'re welcome! If you have any other questions or concerns, feel free to reach out. I\\'m here to help.\" tool_calls=None role=\\'CHATBOT\\' \\nConclusion\\nIn this chapter, we explored how to use Command R+ on Amazon Bedrock for various text generation and manipulation tasks. We covered text generation, summarization, rewriting, and extraction, demonstrating how these capabilities can be applied to real-world scenarios.\\nIf you want to learn more LLM use cases, visit our LLMU chapter on use case patterns.\\nIn Chapter 4, you’ll learn how to perform semantic search using the Cohere Embed model on Amazon Bedrock.'),\n",
       " Document(metadata={'title': 'Semantic Search Using Cohere Embed on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-search'}, page_content='Semantic Search Using Cohere Embed on Amazon BedrockQualified\\nContents\\nIntroduction\\nCode Walkthrough \\nSetup\\nDownload Dataset\\nPre-Process Dataset\\nEmbed and Index Documents\\nSend Query and Retrieve Documents\\nConclusion\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nIntroduction\\nEnterprises have a lot of data at their disposal, but much of it is hard to tap into. This is because a good chunk of this data is unstructured, and digging into it to find insights isn\\'t straightforward. Traditional methods, like keyword matching, can only do so much and they often fail to capture the full context and depth of a document.\\nThis is where we introduce the concept of text embeddings. Essentially, text embeddings are numerical representations created by language models that convert text into vectors. They capture and encode the context of a document. These vectors store a wealth of context about the documents they represent, opening up the possibility of various applications, from semantic search and retrieval-augmented generation (RAG) to topic modeling and text classification.\\nCohere\\'s Embed model, available on Amazon Bedrock, is a powerful text embeddings model that offers these capabilities. This model supports over 100 languages and is unique among text embedding models due to its emphasis on document quality for applications like semantic search.\\nFor example, a global network security company wanted to build an AI assistant to help its IT professionals respond to queries on network security setup and security best practices. Its security policy documentation repository includes tens of thousands of articles consisting of unstructured data, and its legacy search tools often produce inaccurate results.\\nThey built the AI assistant with retrieval-augmented generation (RAG). They used Cohere Embed for semantic search to generate embeddings of the security policy documentation and Cohere Rerank to further improve retrieval accuracy by selecting the most relevant documents for answer generation. The team then fine-tuned Cohere Command to optimize how it answered user questions.\\nImplementing an AI assistant at a global network security company\\nIn this chapter, we\\'ll explore how to use Cohere\\'s Embed model on Amazon Bedrock. At the time of writing, users can choose between two models on Amazon Bedrock: Embed English and Embed Multilingual which covers 100+ languages. For the most updated list of models, visit the Amazon Bedrock website.\\nCode Walkthrough\\nUsing Cohere Embed in a financial services scenario, let’s build an application that can perform semantic search across financial news in different languages.\\nFinancial analysts need to digest a lot of content, such as financial publications and news media, to stay informed. A semantic search system helps analysts quickly search across numerous article titles in multiple languages for the most relevant articles, saving an enormous amount of time and effort. Unlike traditional keyword-based search, semantic search goes beyond matching exact terms and instead focuses on understanding the underlying concepts and relationships between words and phrases.\\nAn overview of what we\\'ll cover in the code walkthrough\\nSetup\\nFirst, let\\'s install and import the necessary libraries and set up our Cohere client using the cohere SDK. To use Bedrock, we create a BedrockClient by passing the necessary AWS credentials.\\n# ! pip install cohere pandas hnswlib -q \\nimport pandas as pd import hnswlib import re import cohere co = cohere.BedrockClient( aws_region=\"YOUR_AWS_REGION\", aws_access_key=\"YOUR_AWS_ACCESS_KEY_ID\", aws_secret_key=\"YOUR_AWS_SECRET_ACCESS_KEY\", aws_session_token=\"YOUR_AWS_SESSION_TOKEN\", ) \\nDownload Dataset\\nWe use a dataset (MultiFIN) containing a list of real-world article headlines covering 15 languages (English, Turkish, Danish, Spanish, Polish, Greek, Finnish, Hebrew, Japanese, Hungarian, Norwegian, Russian, Italian, Icelandic, and Swedish). This is an open-source dataset curated for financial natural language processing (NLP) and is available on a GitHub repository.\\nIn our case, we’ve created a CSV file with MultiFIN’s data, as well as a column with translations. We don’t use this column to feed the model; we use it to help us follow along when we print the results for those who don’t speak Danish or Spanish. We point to that CSV to create our dataframe.\\nurl = \"https://raw.githubusercontent.com/cohere-ai/cohere-aws/main/notebooks/bedrock/multiFIN_train.csv\" df = pd.read_csv(url) # Inspect dataset df.head(5) \\nPre-Process Dataset\\nMultiFIN has over 6,000 records in 15 different languages. For our example use case, we focus on three languages: English, Spanish, and Danish.\\nFor this, we’ll need to do some pre-processing steps. First, we remove the duplicates, remove the languages other than the three we need, and pick the top 80 articles for demonstration purposes.\\n# Ensure there is no duplicated text in the headers def remove_duplicates(text): return re.sub(r\\'((\\\\b\\\\w+\\\\b.{1,2}\\\\w+\\\\b)+).+\\\\1\\', r\\'\\\\1\\', text, flags=re.I) df [\\'text\\'] = df[\\'text\\'].apply(remove_duplicates) # Keep only selected languages languages = [\\'English\\', \\'Spanish\\', \\'Danish\\'] df = df.loc[df[\\'lang\\'].isin(languages)] # Pick the top 80 longest articles df[\\'text_length\\'] = df[\\'text\\'].str.len() df.sort_values(by=[\\'text_length\\'], ascending=False, inplace=True) top_80_df = df[:80] # Language distribution top_80_df[\\'lang\\'].value_counts() \\nEmbed and Index Documents\\nNow, we want to embed our documents and store the embeddings. The embeddings are very large vectors that encapsulate the semantic meaning of our document. In particular, we use Cohere’s embed-multilingual-v3.0 model, which creates embeddings with 1,024 dimensions.\\nWith the v3.0 embeddings models, we need to specify the input_type parameter to indicate the nature of the document. In semantic search applications, this is either search_document, which is for the documents to search, or search_query, which is for the search query that we’ll define later.\\nWe also keep track of the language and translation of the document to enrich the display of the results.\\nNext, we create a search index using the hnsw vector library. This stores the embeddings in an index, which makes searching the documents more efficient.\\n# Embed documents docs = top_80_df[\\'text\\'].to_list() docs_lang = top_80_df[\\'lang\\'].to_list() translated_docs = top_80_df[\\'translation\\'].to_list() #for reference when returning non-English results doc_embs = co.embed(texts=docs, model=\"cohere.embed-multilingual-v3\", input_type=\\'search_document\\').embeddings # Create a search index index = hnswlib.Index(space=\\'ip\\', dim=1024) index.init_index(max_elements=len(doc_embs), ef_construction=512, M=64) index.add_items(doc_embs, list(range(len(doc_embs)))) \\nSend Query and Retrieve Documents\\nNext, we build a function that takes a query as input, embeds it, and finds the four headers more closely related to it.\\n# Retrieval of 4 closest docs to query def retrieval(query): # Embed query and retrieve results query_emb = co.embed(texts=[query], model=\"cohere.embed-multilingual-v3\", input_type=\"search_query\").embeddings doc_ids = index.knn_query(query_emb, k=3)[0][0] # we will retrieve 4 closest neighbors # Print and append results print(f\"QUERY: {query.upper()} \\\\n\") retrieved_docs, translated_retrieved_docs = [], [] for doc_id in doc_ids: # Append results retrieved_docs.append(docs[doc_id]) translated_retrieved_docs.append(translated_docs[doc_id]) # Print results print(f\"ORIGINAL ({docs_lang[doc_id]}): {docs[doc_id]}\") if docs_lang[doc_id] != \"English\": print(f\"TRANSLATION: {translated_docs[doc_id]} \\\\n----\") else: print(\"----\") print(\"END OF RESULTS \\\\n\\\\n\") return retrieved_docs, translated_retrieved_docs \\nLet’s now try to query the index with a couple of examples, one each in English and Danish.\\nqueries = [ \"Can data science help meet sustainability goals?\", # English example \"Hvor kan jeg finde den seneste danske boligplan?\" # Danish example - \"Where can I find the latest Danish property plan?\" ] for query in queries: retrieval(query) \\nHere’s the response for the English query, showing semantic search in action. Notice how the retrieval system was able to surface documents similar in meaning, i.e., data science vs. AI. Keyword-based search systems would not be able to capture this.\\nQUERY: CAN DATA SCIENCE HELP MEET SUSTAINABILITY GOALS? ORIGINAL (English): Using AI to better manage the environment could reduce greenhouse gas emissions, boost global GDP by up to 38m jobs by 2030 ---- ORIGINAL (English): Quality of business reporting on the Sustainable Development Goals improves, but has a long way to go to meet and drive targets. ---- ORIGINAL (English): Only 10 years to achieve Sustainable Development Goals but businesses remain on starting blocks for integration and progress ---- END OF RESULTS \\nHere’s the response for the Danish query. This example highlights the ability to perform cross-lingual searches with the Embed Multilingual model. You can enter a query in one language and get relevant search results in other languages.\\nAnother observation here is that the English acronym “PP&E” stands for “property, plant, and equipment,” and the model was able to connect it to the query.\\nQUERY: HVOR KAN JEG FINDE DEN SENESTE DANSKE BOLIGPLAN? ORIGINAL (Danish): Nyt fra CFOdirect: Ny PP&E-guide, FAQs om den nye leasingstandard, podcast om udfordringerne ved implementering af leasingstandarden og meget mere TRANSLATION: New from CFOdirect: New PP&E guide, FAQs on the new leasing standard, podcast on the challenges of implementing the leasing standard and much more ---- ORIGINAL (Danish): Lovforslag fremlagt om rentefri lån, udskudt frist for lønsumsafgift, førtidig udbetaling af skattekredit og loft på indestående på skattekontoen TRANSLATION: Bills presented on interest -free loans, deferred deadline for payroll tax, early payment of tax credit and ceiling on the balance in the tax account ---- ORIGINAL (Danish): Nyt fra CFOdirect: Shareholder-spørgsmål til ledelsen, SEC cybersikkerhedsguide, den amerikanske skattereform og meget mere TRANSLATION: New from CFOdirect: Shareholder questions for management, the SEC cybersecurity guide, US tax reform and more ---- END OF RESULTS \\nConclusion\\nSemantic search applications, enabled by text embeddings, offer a significantly more effective approach to retrieving and analyzing information. Cohere\\'s Embed model can do this across over 100 languages. Its application in fields like financial analysis, as demonstrated in this chapter, shows how it can transform data retrieval and processing tasks, saving time and improving accuracy.\\nIn Chapter 5, we’ll switch to Amazon SageMaker and look at an example using the Cohere Rerank model on SageMaker.'),\n",
       " Document(metadata={'title': 'What Are Word and Sentence Embeddings?', 'source': 'https://cohere.com/llmu/sentence-word-embeddings'}, page_content=\"What Are Word and Sentence Embeddings?What Are Word and Sentence Embeddings?\\nWord and sentence embeddings are the bread and butter of language models. This chapter shows a very simple introduction to what they are.\\nWhat Are Word and Sentence Embeddings? - YouTube\\nIn old futuristic movies, such as the 2001 Space Odyssey, the main computer (HAL) was able to talk to humans and understand what they would say with great ease. At the time, getting computers to understand and produce language seemed like an impossible task, but the latest large language models (LLM) are able to do this in a way that makes it almost impossible for a human to tell if they are talking to another human, or to a computer.\\nThe quintessential task of natural language processing (NLP) is to understand human language. However, there is a big disconnection there. Humans speak in words and sentences, but computers only understand and process numbers. How can we turn words and sentences into numbers in a coherent way? An assignment of words to numbers is called a word embedding. We can think of a word embedding as an assignment of scores to the words, with some nice properties (that we’ll learn soon).\\nWhat is a Word Embedding?\\nBefore we get into what is a word embedding, let me test your intuition. In the figure underneath (Quiz 1), I have located 12 words in the plane. The words are the following:\\nBanana\\nBasketball\\nBicycle\\nBuilding\\nCar\\nCastle\\nCherry\\nHouse\\nSoccer\\nStrawberry\\nTennis\\nTruck\\nNow, the question is, where would you locate the word “Apple” in this plane? There are many places it could go, but I’m allowing 3 possibilities labeled A, B, and C.\\nWhat I would do, is locate it in point C, because it would make sense to have the word “Apple” close to the words “Banana”, “Strawberry”, and “Cherry”, and far from the other words such as “House”, “Car”, or “Tennis”. This is precisely a word embedding. And what are the numbers we are assigning to each word? Simply the horizontal and vertical coordinates of the location of the word. In this way, the word “Apple” is assigned to the numbers [5,5], and the word “Bicycle” to the coordinates [5,1].\\nFor the sake of redundancy, let’s enumerate some properties that a nice word embedding should have:\\nWords that are similar should correspond to points that are close by (or equivalently, to scores that are similar).\\nWords that are different should correspond to points that are far away (or equivalently, to scores that are significantly different).\\nWord Embeddings Capture Features of the Word\\nThe word embedding above satisfies properties 1 and 2. Is that it? Not yet. There is something more to these word embeddings, and it is that they don’t only capture word similarity, but they also capture other properties of the language. In language, words can be combined to get more complicated concepts. In mathematics, numbers can be added or subtracted to get other numbers. Could we build a word embedding that captures relations between words, as relations between numbers?\\nLet’s look at four words, “Puppy”, “Dog”, “Calf”, and “Cow”. These words are clearly correlated. Now to test your intuition again, I’m going to locate the words “Puppy”, “Dog”, and “Calf” in the plane, and I’ll ask you to add the word “Cow”. Where would you add it, in the spot labeled A, B, or C?\\nWhile it would make sense to locate it in A, closer to “Calf” since they are both bovine, or in B, since it’s an adult animal, like “Dog”, the place where I would put this is in spot C, with coordinates [3,4]. Why? Because the rectangle formed by the four words captures some very important relationships between them. For instance, two analogies are captured here. The analogy “A puppy is to a dog like a calf is to a cow” can be translated into “The path from the word puppy to the word dog is the same as the path from the word calf to the word cow”. The analogy “A dog is to a cow like a puppy is to a calf” is also captured in this rectangle, as it’s shown in the figure below.\\nHowever, this is not even the tip of the iceberg. The main property of word embeddings that is in effect here is that the two axes (vertical and horizontal) represent different things. If you look carefully, moving towards the right turns the puppy into a dog, and the calf into a cow, which is an increase in age. Likewise, moving upwards turns a puppy into a calf and a dog into a cow, which is an increase in the size of the animal. It seems that this embedding is understanding that the words in it have two main properties, or features: age and size. Furthermore, it seems that the embedding is locating age in the horizontal axis and size in the vertical axis. In that case, where would you imagine that the word “whale” goes? Probably somewhere above the word “cow”. And if there was a word for “really old dog”? That word would go somewhere to the right of the word “dog”.\\nA good word embedding would be able to capture not only age and size, but also many more features of the words. Since each feature is one new axis, or coordinate, then a good embedding must have many more than two coordinates assigned to every word. One of the Cohere embeddings, for example, has 1024 coordinates associated with each word. These rows of 1024 (or however many) coordinates are called vectors, so we often talk about the vector corresponding to a word, and to each of the numbers inside a vector as a coordinate. Some of these coordinates may represent important properties of the word, such as age, gender, size. Some may represent combinations of properties. But some others may represent obscure properties that a human may not be able to understand. But all in all, a word embedding can be seen as a good way to translate human language (words) into computer language (numbers), so that we can start training machine learning models with these numbers.\\nSentence embeddings\\nSo word embeddings seem to be pretty useful, but in reality, human language is much more complicated than simply a bunch of words put together. Human language has structure, sentences, etc. How would one be able to represent, for instance, a sentence? Well, here’s an idea. How about the sums of scores of all the words? For example, say we have a word embedding that assigns the following scores to these words:\\nNo: [1,0,0,0]\\nI: [0,2,0,0]\\nAm: [-1,0,1,0]\\nGood: [0,0,1,3]\\nThen the sentence “No, I am good!” corresponds to the vector [0,2,2,3]. However, the sentence “I am no good” will also correspond to the vector [0,2,2,3]. This is not a good thing, since the computer understand these two sentences in the exact same way, yet they are quite different, almost opposite sentences! Therefore, we need better embeddings that take into account the order of the words, the semantics of the language, and the actual meaning of the sentence.\\nThis is where sentence embeddings come into play. A sentence embedding is just like a word embedding, except it associates every sentence with a vector full of numbers, in a coherent way. By coherent, I mean that it satisfies similar properties as a word embedding. For instance, similar sentences are assigned to similar vectors, different sentences are assigned to different vectors, and most importantly, each of the coordinates of the vector identifies some (whether clear or obscure) property of the sentence.\\nThe Cohere embedding does just this. Using transformers, attention mechanisms, and other cutting edge algorithms, this embedding sends every sentence to a vector formed by 4096 numbers, and this embedding works really well. As a small example, here is a heatmap of an embedding containing 10 entries for each sentence, for several sentences (writing the entire 4096 entries will take too much space, so we compressed it using a dimensionality reduction algorithm called Principal Component Analysis.\\nNotice that these sentences are all very similar. In particular, the three highlighted sentences pretty much have the same meaning. If you look at their corresponding vectors, these are also really similar. That is exactly what an embedding should do.\\nHow to Use These Embeddings?\\nNow that you’ve learned how useful these embeddings are, it’s time to start playing with them and finding good practical uses for them! The Cohere dashboard provides a very friendly interface to use them. Here is a small example, with the following phrases:\\nI like my dog\\nI love my dog\\nI adore my dog\\nHello, how are you?\\nHey, how's it going?\\nHi, what's up?\\nI love watching soccer\\nI enjoyed watching the world cup\\nI like watching soccer matches\\nTo see the results of the sentence embedding, go to the “Embed” tab in the Cohere dashboard, and type the sentences (click here for an embed demo you can play with).\\nThe results come out as vectors with 4096 entries for each sentence. These are obviously hard to visualize, but there is a way to bring them down to 2 entries per sentence in order to be easily visualized. This visualization is shown in the plot below.\\nNotice that the embedding seemed to capture the essence of the sentences, and there are 3 clear clusters of sentences. In the top left corner you find the sentences that greet a person, in the middle, those that talk about a person’s dog, and in the bottom right corner, those that talk about soccer. Notice that sentences such as “Hey what’s up” and “Hello, how are you?” have no words in common, yet the model can tell that they have the same meaning.\\nMultilingual Sentence Embeddings\\nMost word and sentence embeddings are dependent on the language that the model is trained on. If you were to try to fit the French sentence “Bonjour, comment ça va?” (meaning: hello, how are you?) in the embedding from the previous section, it will struggle to understand that it should be close to the sentence “Hello, how are you?” in English. For the purpose of unifying many languages into one, and being able to understand text in all these languages, Cohere has trained a large multilingual model, that has showed wonderful results with more than 100 languages. Here is a small example, with the following sentences in English, French, and Spanish.\\nThe bear lives in the woods\\nEl oso vive en el bosque\\nL’ours vit dans la foret\\nThe world cup is in Qatar\\nEl mundial es en Qatar\\nLa coupe du monde est au Qatar\\nAn apple is a fruit\\nUna manzana es una fruta\\nUne pomme est un fruit\\nEl cielo es azul\\nThe sky is blue\\nLe ciel est bleu\\nThe model returned the following embedding.\\nNotice that the model managed to identify the sentences about the bear, soccer, an apple, and the sky, even if they are in different languages.\\nConclusion\\nWord and sentence embeddings are the bread and butter of LLMs. They are the basic building block of most language models, since they translate human speak (words) into computer speak (numbers) in a way that captures many relations between words, semantics, and nuances of the language, into equations regarding the corresponding numbers.\\nSentence embeddings can be extended to language embeddings, in which the numbers attached to each sentence are language-agnostic. These models are very useful for translation and for searching and understanding text in different languages.\"),\n",
       " Document(metadata={'title': 'Reranking Using Cohere Rerank on Amazon SageMaker', 'source': 'https://cohere.com/llmu/co-aws-rerank'}, page_content='Reranking Using Cohere Rerank on Amazon SageMakerQualified\\nContents\\nHow Cohere Rerank Works\\nCode Walkthrough \\nSetup\\nCreate Endpoint\\nRetrieve Documents\\nRerank Documents\\nView Results\\nConclusion\\nWe’ll use Cohere’s Python SDKs (cohere and cohere-aws) for the code examples. Follow along in this notebook.\\nIntroduction\\nReranking is an essential technique in information retrieval systems, especially in large-scale search applications. It is a process of reordering a set of initially retrieved documents based on their relevance to a user\\'s query.\\nWith the vast amount of information available, reranking helps refine and improve the quality of search results, ensuring that the most relevant and useful documents are presented to the user. Cohere’s reranking models offer a powerful solution to enhance the search experience.\\nWhat’s great about reranking is that while it gives a huge boost to search results, implementing Cohere’s Rerank models requires adding only one line of code to any existing search system, whether it’s a semantic search system or a traditional search system that uses keyword-based approaches.\\nFor example, a major provider of digital workforce collaboration and productivity tools found that their existing search tools did not perform well across customer-generated knowledge bases, and the large volume of new customer-generated content meant embedding was not viable.\\nCohere Rerank was integrated with the customer’s search systems. Rerank used output from existing search tools and reordered the results for use by the company’s Q&A app to provide relevant and more accurate answers.\\nImplementing a reranking system at a provider of digital workforce collaboration and productivity tools\\nHow Cohere Rerank Works\\nThe Cohere Rerank endpoint takes a query and a response and outputs a relevance score between them. This enables the retrieval of the most relevant documents to a particular query. For example, when searching for \"Regulatory approval\", the initial results may include documents that only tangentially mention the topic. However, by applying Rerank, the top results are those with the highest relevance scores, directly answering the query.\\nRerank improves the ordering of documents based on relevance to a query\\nRerank 3 is Cohere’s latest reranking model, which provides state-of-the-art capabilities for enterprise search, including:\\n4k context length to significantly improve search quality for longer documents\\nAbility to search over multi-aspect and semi-structured data, such as emails, invoices, JSON documents, code, and tables\\nMultilingual coverage of 100+ languages\\nImproved latency and lower total cost of ownership (TCO)\\nIn this chapter, we\\'ll explore how to use the Cohere Rerank endpoint on Amazon SageMaker. To see the most updated list of available models on SageMaker, visit Cohere’s listing on the AWS Marketplace.\\nCode Walkthrough\\nLet’s look at an example of a multi-aspect search on semi-structured data, and walk through how to perform reranking on email data that contains multiple fields: “title” and “content.”\\nAn overview of what we\\'ll cover in the code walkthrough\\nSetup\\nTo get set up with SageMaker, follow the steps outlined in the Cohere documentation, which takes you through the prerequisites and getting access to Cohere models.\\nLet’s now install and import the necessary libraries and set up our Cohere client.\\nWe’ll need to create a SageMaker endpoint that exposes access to a Cohere model (Rerank v3 in our case). For this, we’ll use the cohere_aws SDK which makes it easy to set up the endpoint, together with AWS’s boto3 library.\\n# ! pip install cohere cohere-aws boto3\\nimport os import boto3 import cohere import cohere_aws from cohere_aws import Client \\nOnce the endpoint is created (as we’ll walk through later), we can access it using the Cohere SDK. To do this, let’s create a SagemakerClient by passing the necessary AWS credentials.\\nimport cohere # Create SageMaker client via the native Cohere SDK # Contact your AWS administrator for the credentials co = cohere.SagemakerClient( aws_region=\"YOUR_AWS_REGION\", aws_access_key=\"YOUR_AWS_ACCESS_KEY_ID\", aws_secret_key=\"YOUR_AWS_SECRET_ACCESS_KEY\", aws_session_token=\"YOUR_AWS_SESSION_TOKEN\", ) # For creating an endpoint, you need to use the cohere_aws client: Set environment variables with the AWS credentials os.environ[\\'AWS_ACCESS_KEY_ID\\'] = \"YOUR_AWS_ACCESS_KEY_ID\" os.environ[\\'AWS_SECRET_ACCESS_KEY\\'] = \"YOUR_AWS_SECRET_ACCESS_KEY\" os.environ[\\'AWS_SESSION_TOKEN\\'] = \"YOUR_AWS_SESSION_TOKEN\" \\nThe next step is to define the model package Amazon Resource Names (ARN) for the Rerank model. The ARN is an identifying string for a SageMaker resource, and it varies between the regions where a resource is available.\\nHere, we define the Cohere package for the Rerank model and map the model package against each region, which gives the complete ARN for each region.\\n# Create SageMaker endpoint via the cohere_aws SDK cohere_package = \"cohere-rerank-english-v3-01-d3687e0d2e3a366bb904275616424807\" model_package_map = { \"us-east-1\": f\"arn:aws:sagemaker:us-east-1:865070037744:model-package/{cohere_package}\", \"us-east-2\": f\"arn:aws:sagemaker:us-east-2:057799348421:model-package/{cohere_package}\", \"us-west-1\": f\"arn:aws:sagemaker:us-west-1:382657785993:model-package/{cohere_package}\", \"us-west-2\": f\"arn:aws:sagemaker:us-west-2:594846645681:model-package/{cohere_package}\", \"ca-central-1\": f\"arn:aws:sagemaker:ca-central-1:470592106596:model-package/{cohere_package}\", \"eu-central-1\": f\"arn:aws:sagemaker:eu-central-1:446921602837:model-package/{cohere_package}\", \"eu-west-1\": f\"arn:aws:sagemaker:eu-west-1:985815980388:model-package/{cohere_package}\", \"eu-west-2\": f\"arn:aws:sagemaker:eu-west-2:856760150666:model-package/{cohere_package}\", \"eu-west-3\": f\"arn:aws:sagemaker:eu-west-3:843114510376:model-package/{cohere_package}\", \"eu-north-1\": f\"arn:aws:sagemaker:eu-north-1:136758871317:model-package/{cohere_package}\", \"ap-southeast-1\": f\"arn:aws:sagemaker:ap-southeast-1:192199979996:model-package/{cohere_package}\", \"ap-southeast-2\": f\"arn:aws:sagemaker:ap-southeast-2:666831318237:model-package/{cohere_package}\", \"ap-northeast-2\": f\"arn:aws:sagemaker:ap-northeast-2:745090734665:model-package/{cohere_package}\", \"ap-northeast-1\": f\"arn:aws:sagemaker:ap-northeast-1:977537786026:model-package/{cohere_package}\", \"ap-south-1\": f\"arn:aws:sagemaker:ap-south-1:077584701553:model-package/{cohere_package}\", \"sa-east-1\": f\"arn:aws:sagemaker:sa-east-1:270155090741:model-package/{cohere_package}\", } region = boto3.Session().region_name if region not in model_package_map.keys(): raise Exception(\"UNSUPPORTED REGION\") model_package_arn = model_package_map[region]\\nThe ARN for each model is available in the Marketplace once you subscribe to the model and proceed to configure it. The screenshot below shows the Rerank 3 English listing as an example.\\nRerank 3 English model listing on the AWS Marketplace\\nCreate Endpoint\\nWith SageMaker, we’ll need to create an endpoint via an AWS instance. The marketplace listing provides more details, including pricing, on the recommended instance type for a particular model.\\nTo create the endpoint, we define:\\narn: The model package ARN we defined in the previous step\\nendpoint_name: A name we can give as an identifier\\ninstance_type: The instance type to be used\\nn_instances: The number of instances\\nWe pass the arguments to the create_endpoint method from the cohere_aws library.\\nco_aws = Client(region_name=region) co_aws.create_endpoint(arn=model_package_arn, endpoint_name=\"my-rerank-v3\", instance_type=\"ml.g5.xlarge\", n_instances=1) \\nThe following output indicates that the process of creating an endpoint is complete.\\nRetrieve Documents\\nLet’s assume that the first stage of retrieval has already been performed, whether it’s through a semantic, keyword, or any other type of search system.\\nHere we have a list of nine documents that represent the search results of that first stage. Each document has two fields, Title and Content, corresponding to the contents of an email. Each email is a dictionary containing these fields that preserve its semi-structured format, which the Rerank endpoint can use.\\ndocuments = [ {\"Title\":\"Incorrect Password\",\"Content\":\"Hello, I have been trying to access my account for the past hour and it keeps saying my password is incorrect. Can you please help me?\"}, {\"Title\":\"Confirmation Email Missed\",\"Content\":\"Hi, I recently purchased a product from your website but I never received a confirmation email. Can you please look into this for me?\"}, {\"Title\":\"Questions about Return Policy\",\"Content\":\"Hello, I have a question about the return policy for this product. I purchased it a few weeks ago and it is defective.\"}, {\"Title\":\"Customer Support is Busy\",\"Content\":\"Good morning, I have been trying to reach your customer support team for the past week but I keep getting a busy signal. Can you please help me?\"}, {\"Title\":\"Received Wrong Item\",\"Content\":\"Hi, I have a question about my recent order. I received the wrong item and I need to return it.\"}, {\"Title\":\"Customer Service is Unavailable\",\"Content\":\"Hello, I have been trying to reach your customer support team for the past hour but I keep getting a busy signal. Can you please help me?\"}, {\"Title\":\"Return Policy for Defective Product\",\"Content\":\"Hi, I have a question about the return policy for this product. I purchased it a few weeks ago and it is defective.\"}, {\"Title\":\"Wrong Item Received\",\"Content\":\"Good morning, I have a question about my recent order. I received the wrong item and I need to return it.\"}, {\"Title\":\"Return Defective Product\",\"Content\":\"Hello, I have a question about the return policy for this product. I purchased it a few weeks ago and it is defective.\"} ]\\nRerank Documents\\nTo use the endpoint, we now use the cohere SDK. Adding a reranking component is simple with Cohere Rerank. It takes just one line of code to implement.\\nCalling the Rerank endpoint requires the following arguments:\\ndocuments: The list of documents, which we defined in the previous section\\nquery: The user query; we’ll use \\'What emails have been about refunds?\\' as an example\\nrank_fields: The list of fields that the data contains, which in our case are Title and Content\\ntop_n: The number of documents we want to be returned, sorted from the most to the least relevant document\\nmodel: The endpoint_name we defined earlier, which in our case is my-rerank-v3\\nquery = \\'What emails have been about refunds?\\' response = co.rerank(documents=documents, query=query, rank_fields=[\"Title\",\"Content\"], top_n=3, model=\"my-rerank-v3\")\\nView Results\\nSince we defined top_n=3, we’ll get the top three most relevant documents to the query. For each document, the response contains the index of its position in the original list and its relevance score against the query.\\nLet’s print the results and see the outcome.\\nprint(\"Documents\",\"\\\\n\") for idx,doc in enumerate(response.results): print(f\"#{idx+1}:\\\\n{documents[doc.index]}\\\\n\")\\nThe search query was looking for emails about refunds. But none of the documents mention the word “refunds” specifically.\\nHowever, the Rerank model was able to retrieve the right documents. Some of the documents mentioned the word “return.” The Rerank model can capture semantically similar meanings between two pieces of text, so it can return documents that mention return instead, which has a very similar meaning to return.\\nDocuments #1: {\\'Title\\': \\'Questions about Return Policy\\', \\'Content\\': \\'Hello, I have a question about the return policy for this product. I purchased it a few weeks ago and it is defective.\\'} #2: {\\'Title\\': \\'Return Policy for Defective Product\\', \\'Content\\': \\'Hi, I have a question about the return policy for this product. I purchased it a few weeks ago and it is defective.\\'} #3: {\\'Title\\': \\'Return Defective Product\\', \\'Content\\': \\'Hello, I have a question about the return policy for this product. I purchased it a few weeks ago and it is defective.\\'} \\nImportant Note: You will continue to incur charges as long as an endpoint is running, so remember to delete it when your usage ends.\\nco_aws.delete_endpoint() co_aws.close() \\nConclusion\\nReranking is a valuable technique used in information retrieval systems to enhance the relevance of search results. Cohere\\'s Rerank endpoint, including its latest model, Rerank 3, offers improved capabilities for enterprise search.\\nBy incorporating reranking with a single line of code, as shown in our example in this chapter, the model successfully identified semantically similar documents, even when specific keywords were absent from the query. This example highlights the potential benefits of integrating reranking into existing search systems to enhance search accuracy and user satisfaction.\\nIn Chapter 6, we’ll explore how to build a retrieval-augmented generation (RAG) application using Chat and Embed on Amazon Bedrock and Rerank on Amazon SageMaker.'),\n",
       " Document(metadata={'title': 'What is Similarity Between Sentences?', 'source': 'https://cohere.com/llmu/what-is-similarity-between-sentences'}, page_content='What is Similarity Between Sentences?What is Similarity Between Sentences?\\nWhat is Similarity Between Sentences? - YouTube\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nFor large language models, it is crucial to know when two words, or two sentences, are similar or different. This can be a hard problem, but luckily, word and sentence embeddings are very helpful for this task. In this chapter, we go over some different notions of similarity.\\nSimilarity between text\\nKnowing if two words are similar or different is a very important task for every large language model. An even harder problem is knowing if two different sentences are similar or different. Luckily, word and sentence embeddings are very useful for this task.\\nIn the previous chapter, I explained the concept of word embeddings. In a nutshell, a word embedding is an assignment of a list of numbers (vector) to every word, in a way that semantic properties of the word translate into mathematical properties of the numbers. What do we mean by this? For example, two similar words will have similar vectors, and two different words will have different vectors. But most importantly, each entry in the vector corresponding to a word keeps track of some property of the word. Some of these properties can be understandable to humans, such as age, size, gender, etc., but some others could potentially only be understood by the computer. Either way, we can benefit from these embeddings for many useful tasks.\\nSentence embeddings are even more powerful, as they assign a vector of numbers to each sentence, in a way that these numbers also carry important properties of the sentence. One of the Cohere embeddings assigns a vector of length 4096 (i.e., a list of 4096 numbers) to each sentence. Furthermore, multilingual embedding does this for sentences in more than 100 languages. In this way, the sentence “Hello, how are you?” and its corresponding French translation, “Bonjour, comment ça va?” will be assigned very similar numbers, as they have the same semantic meaning.\\nNow that we know embeddings quite well, let’s move on to using them to find similarities. There are two types of similarities we’ll define in this post: dot product similarity and cosine similarity. Both are very similar and very useful to determine if two words (or sentences) are similar.\\nDot Product Similarity\\nLet’s start with a small example of sentence embedding. For simplicity, let’s consider a dataset of 4 sentences, all movie titles, and an embedding of dimension 2, meaning that each sentence is assigned to two numbers. Let’s say that the embedding is the following:\\nYou’ve Got Mail: [0, 5]\\nRush Hour: [6, 5]\\nRush Hour 2: [7, 4]\\nTaken: [7, 0]\\nLet’s take a closer look at these scores. Would they mean anything? As mentioned before, these scores sometimes mean something that humans can understand, and other times they don’t. In this case, notice that the first score is 0 for You’ve Got Mail, but high for all the other movies. Is there a feature that these three movies have, and that You’ve Got Mail doesn’t? I can think of one: being an action movie. Similarly, the second score is high for You’ve Got Mail, Rush Hour, and Rush Hour 2, but low for Taken. What could this property be? Comedy seems to be one. Therefore, in our embedding, it could well be that the first score is the amount of action in the movie, and the second score is the amount of comedy. The following table represents the embedding.\\nMovieScore 1 (Action)Score 2 (Comedy)\\nYou’ve got mail\\t0\\t5\\t\\nRush Hour\\t6\\t5\\t\\nRush Hour 2\\t7\\t4\\t\\nTaken\\t7\\t0\\t\\nNow, imagine that we want to find the similarities between these movies. In particular, how similar would you say Taken is from You’ve Got Mail? How similar is Rush Hour to Rush Hour 2? In my opinion, Taken and You’ve Got Mail are very different, and Rush Hour and Rush Hour 2 are very similar. We now need to create a similarity score that is low for the pair [You’ve Got Mail, Taken], and high for the pair [Rush Hour, Rush Hour 2].\\nHere is one way to create this similarity score. Notice that if two movies are similar, then they must have similar action scores and similar comedy scores. So if we multiply the two action scores, then multiply the two comedy scores, and add them, this number would be high if the scores match. On the other hand, if the scores don’t match very well, the similarity score would be lower. This operation is called the dot product. Let’s see how it works for the two pairs of movies.\\nDot product for the pair [You’ve got mail, Taken] = 0*7 + 5*0 = 0\\nDot product for the pair [Rush Hour, Rush Hour 2] = 6*7 + 5*4 = 62\\nThis matches our intuition since we were expecting a low similarity for the first pair, and a high similarity for the second pair.\\nCosine Similarity\\nAnother measure of similarity between sentences (and words) is to look at the angle between them. For example, let’s plot the movie embedding in the plane, where the horizontal axis represents the action score, and the vertical axis represents the comedy score. The embedding looks like this.\\nNotice that You’ve Got Mail is quite far from Taken, which makes sense since they are very different movies. Furthermore, Rush Hour and Rush Hour 2 are very close, as they are similar movies. So Euclidean distance (the length of the line between the points) is a good measure for similarity. We need to tweak it a little bit, since we want a measure of similarity that is high for sentences that are close to each other, and low for sentences that are far away from each other. Distance does the exact opposite. So in order to tweak this metric, let’s look at the angle between the rays from the origin (the point with coordinates [0,0]), and each sentence. Notice that this angle is small if the points are close to each other, and large if the points are far away from each other. Now we need the help of another function, the cosine. The cosine of angles close to zero is close to 1, and as the angle grows, the cosine decreases. This is exactly what we need. Therefore, we define the cosine distance as the cosine of the angle formed by the two rays going from the origin, to the two sentences.\\nNotice that in the ongoing example, the angle between the movies You’ve Got Mail, and Taken, is 90 degrees, with a cosine of 0. Therefore, the similarity between them is 0. On the other hand, the angle between the movies Rush Hour and Rush Hour 2 is 11.31 degrees. Its cosine is 0.98, which is quite high. In fact, the similarity between a sentence and itself is always 1, as the angle is 0, with a cosine of 1.\\nReal-Life Example\\nOf course, this was a very small example. Let’s do a real-life example with the Cohere embedding.\\nTo set up, we first import several tools we\\'ll need.\\nimport numpy as np import seaborn as sns import altair as alt from sklearn.metrics.pairwise import cosine_similarity \\nWe also import the Cohere module and create a client.\\nimport cohere co = cohere.Client(\"COHERE_API_KEY\") # Your Cohere API key \\nConsider the following 3 sentences, stored in the Python list texts.\\ntexts = [\"I like to be in my house\", \"I enjoy staying home\", \"the isotope 238u decays to 206pb\"] \\nOne would expect the two first sentences to have a high similarity score when compared to each other, and the third one to have a very low similarity score when compared to the other two.\\nTo get the corresponding sentence embeddings, we call the Embed endpoint with co.embed(). We supply three parameters:\\ntexts - our list of sentences\\nmodel - we use embed-english-v3.0, Cohere\\'s latest (at the time of writing) English-only embeddings model to generate the embeddings\\ninput_type - we use search_document to indicate that we intend to use the embeddings for search use-cases\\nYou\\'ll learn about these parameters in more detail in the LLMU Module on Text Representation.\\nresponse = co.embed( texts=texts, model=\\'embed-english-v3.0\\', input_type=\\'search_document\\' ) \\nThe embeddings are stored in the embeddings value of the response. After getting the embeddings, we separate them by sentence and print the values.\\nembeddings = response.embeddings [sentence1, sentence2, sentence3] = embeddings print(\"Embedding for sentence 1\", np.array(sentence1)) print(\"Embedding for sentence 2\", np.array(sentence2)) print(\"Embedding for sentence 3\", np.array(sentence3)) \\nThe results are as follows:\\nEmbedding for sentence 1 [ 0.04968262 0.03799438 -0.02963257 ... -0.0737915 -0.0079422 -0.01863098] Embedding for sentence 2 [ 0.043396 0.05401611 -0.02461243 ... -0.06216431 -0.0196228 -0.00948334] Embedding for sentence 3 [ 0.0243988 0.00712967 -0.04669189 ... -0.03903198 -0.02403259 0.01942444] \\nNote that the embeddings are vectors (lists) of 1024 numbers, so they are truncated here (thus the dots in between). One would expect that the vectors corresponding to sentences 1 and 2 are similar to each other and that both are different from the vector corresponding to sentence 3. However, from inspection, this is not very clear. We need to calculate some similarities to see if this is the case.\\nDot Product Similarity\\nLet’s calculate the dot products between the three sentences. The following line of code will do it.\\nprint(\"Similarity between sentences 1 and 2:\", np.dot(sentence1, sentence2)) print(\"Similarity between sentences 1 and 3:\", np.dot(sentence1, sentence3)) print(\"Similarity between sentences 2 and 3:\", np.dot(sentence2, sentence3)) \\nAnd the results are:\\nSimilarity between sentences 1 and 2: 0.818827121924668 Similarity between sentences 1 and 3: 0.19770800712384107 Similarity between sentences 2 and 3: 0.19897217756830138 \\nThe similarity between sentences 1 and 2 (0.8188) is much larger than the similarities between the other pairs. This confirms our predictions.\\nJust for consistency, we also calculate the similarities between each sentence and itself, to confirm that a sentence and itself has the highest similarity score.\\nSimilarity between sentences 1 and 1: 0.9994656785851899 Similarity between sentences 2 and 2: 1.0006820582016114 Similarity between sentences 3 and 3: 1.0005095878377965 \\nThis checks out—the similarity between a sentence and itself is around 1, which is higher than all the other similarities.\\nCosine Similarity\\nNow let’s calculate the cosine similarities between them.\\nprint(\"Cosine similarity between sentences 1 and 2:\", cosine_similarity([sentence1], [sentence2])[0][0]) print(\"Cosine similarity between sentences 1 and 3:\", cosine_similarity([sentence1], [sentence3])[0][0]) print(\"Cosine similarity between sentences 2 and 3:\", cosine_similarity([sentence2], [sentence3])[0][0]) \\nThe results are the following:\\nCosine similarity between sentences 1 and 2: 0.818766792354783 Cosine similarity between sentences 1 and 3: 0.1977104790996451 Cosine similarity between sentences 2 and 3: 0.19885369669720415 \\nNext, we check the cosine similarity between each sentence and itself.\\nCosine similarity between sentences 1 and 1: 0.9999999999999998 Cosine similarity between sentences 2 and 2: 1.0000000000000004 Cosine similarity between sentences 3 and 3: 1.0000000000000004 \\nWe also plot the results in a grid.\\nThe similarity between each sentence and itself is 1 (the diagonal in the plot), which is consistent with our expectations. Furthermore, a sentence and itself represent the same point in space, which gives an angle of 0 with the origin, so it makes sense that the similarity is the cosine of 0, which is 1!\\nNotice that the dot product and cosine distance give nearly identical values. The reason for this is that the embedding is normalized (meaning each vector has norm equal to 1). When the embedding is not normalized, the dot product and cosine distance would give different values.\\nConclusion\\nIn the previous chapter, we learned that sentence embeddings are the bread and butter of language models, as they associate each sentence with a particular list of numbers (a vector), in a way that similar sentences give similar vectors. We can think of embeddings as a way to locate each sentence in space (a high dimensional space, but a space nonetheless), in a way that similar sentences are located close by. Once we have each sentence somewhere in space, it’s natural to think of distances between them. And an even more intuitive way to think of distances is to think of similarities, i.e., a score assigned to each pair of sentences, which is high when these sentences are similar, and low when they are different. The similarity is a very useful concept in large language models, as it can be used for search, for translation, for summarization, and in many other applications. To learn more about these applications, stay tuned for the next article!'),\n",
       " Document(metadata={'title': 'What Is Attention in Language Models?', 'source': 'https://cohere.com/llmu/what-is-attention-in-language-models'}, page_content=\"What Is Attention in Language Models?What is Attention in Language Models? - YouTubeThe Attention Mechanism in Large Language Models - YouTubeThe math behind Attention: Keys, Queries, and Values matrices\\nWhat is Attention in Language Models? - YouTube\\nIn the previous chapters, you learned about word and sentence embeddings and similarity between words and sentences. In short, a word embedding is a way to associate words with lists of numbers (vectors) in such a way that similar words are associated with numbers that are close by, and dissimilar words with numbers that are far away from each other. A sentence embedding does the same thing, but associating a vector to every sentence. Similarity is a way to measure how similar two words (or sentences) are, by assigning large numbers to words (sentences) that are similar, and small numbers to those that are different.\\nHowever, word embeddings have a huge Achilles heel: words that have more than one definition. If a word embedding assigns a vector to, say, the word ‘bank’, it assigns the same vector to all the definitions of ‘bank’. What if you want to use this word in different contexts? Here is where attention comes into play. Self-attention was introduced in the seminal paper Attention is All you Need, written by several co-authors, including Cohere’s cofounder Aidan Gomez. Attention is a very clever way to tell words apart when they are used in different contexts (which turns word embeddings into contextualized word embeddings).\\nOne Word, Multiple Meanings\\nIn order to understand attention, let’s look at two sentences:\\nSentence 1: The bank of the river.\\nSentence 2: Money in the bank.\\nHow would a computer know that the word “bank” in the first sentence refers to a setting in nature, and in the second sentence to a financial setting? Well, let’s ask a simpler question: How would a human know this? How did you figure out these two settings? The way you and I did it was probably to look at the neighbouring words. In the first sentence, the word “river” was the one that hinted at the nature setting, and in the second sentence, the word “money” was key to the financial setting. So in short, we need a way to use the other words in the sentence to understand what context of the word “bank” we want to use.\\nHere is where word embeddings come into play. As you learned in a previous chapter, word embeddings are a way to assign vectors (lists of numbers) to each word. I like to imagine them geometrically. Imagine that the words “bank”, “river”, and “money” are all attached on a cork board. Furthermore, this cork board contains all the existing words, and in such a way that two words that are similar (such as “apple” and “pear”) are close by. Now, in this cork board, “bank”, “river”, and “money” are not exactly close by. However, what you do is take the word “bank”, and move it slightly towards the word “river”. Call this word “bank1”. Now, take another copy of the word “bank”, and move it slightly towards the word “money”. Call this one “bank2”. Now, consider the following two modified sentences.\\nModified sentence 1: The bank1 of the river.\\nModified sentence 2: Money in the bank2.\\nIn these two sentences, the computer now knows a little more about the context of the word “bank”, as the word has been split into two distinct ones. One whose definition is closer to “river”, and another one whose definition is closer to “money”. That, in short, is how attention mechanisms work. However, there may be many questions lingering in your head. For example:\\nWhat do you mean by “moving a word closer to another one”?\\nWhy did you ignore the other words in the sentence? How did you know the words “river” and “money” were the ones dictating the context, instead of the words “the”, “in”, or “of”? As humans, we know which words provide context, but a computer wouldn’t have a clue.\\nAs computers only handle numbers, how do we attach numbers to all these methods?\\nAll these (and hopefully more!) questions will be answered next.\\nMoving Words on the Cork Board\\nFirst, let me tell you what I mean by “moving a word closer to another one”. The way I like to imagine this, is to average two words. For example, let’s say that I want to move the word “bank” 10% closer to the word river. I now think of the word “0.9_Bank + 0.1_River”. That is, “bank1” is 90% “bank”, and 10% “river”. Also, let’s say that “bank2” is 80% “bank” and 20% “money”. So let’s say these are the modified words:\\nBank1 = 0.9*Bank + 0.1*River\\nBank2 = 0.8*Bank + 0.2*Money\\nHow did I come up with the numbers 0.9, 0.1, 0.8, and 0.2? That comes later (ok I have to spoil it, the answer is similarity, but I’ll elaborate later in this chapter). For now, you may be wondering what do I mean by 0.9*Bank + 0.1*River. Well, in the embeddings chapter, we learned that a word embedding consists of assigning a vector (list) of numbers to each word. The Cohere embedding associates each word with a vector of length 4096 (that is, a list of 4096 numbers per word). For simplicity, let’s imagine an embedding that associates a vector of two numbers to each word, and that the following are the numbers:\\nRiver: [0,5]\\nMoney: [8,0]\\nBank: [6,6]\\nThese two numbers can be interpreted as coordinates in the plane, where the first number is the horizontal coordinate, and the second one, the vertical coordinate. This gives a graphic like the one below.\\nSo in order to calculate the embeddings of Bank1 and Bank2, we simply do the math componentwise (that means, for each of the two components of the vector, separately). We get this:\\nBank1: 0.9*Bank + 0.1*River = 0.9*[6, 6] + 0.1*[0, 5]\\n= [5.4, 5.4] + [0, 0.5]\\n= [5.4, 5.9]\\nBank2: 0.8_Bank + 0.2_Money = 0.8*[6,6] + 0.2*[8,0]\\n= [4.8, 4.8] + [1.6, 0]\\n= [6.4, 4.8]\\nAs you can see, “bank1” is closer to “river”, and “bank2” is closer to “money”. As a matter of fact, “bank1” is on the line between “bank” and “river”, 10% along the way. Similarly, “bank2” is on the line between “bank” and “money”, 20% along the way.\\nThus, the attention mechanism managed to split the word “bank” into two words, and use each one in the corresponding sentence. You may still have some questions, however, such as the following one.\\nHow to Decide Which Words Determine Context?\\nIn other words, why did I pick the words “river” and “money” instead of “the”, “of”, and “in”, in order to determine the context of “bank”. Obviously the answer is “because I’m human and I know the language”. But what can the computer do? It can rely on two mechanisms, one of them is a metric such as similarity, which you learned in the previous chapter. The second one is multi-head attention, which we'll talk about at the end of the chapter.\\nLet’s first discuss the similarity mechanism. What the computer is going to do is to consider all the words in the sentence as context, including irrelevant words such as “the”, “of”, and “in”. However, it’s going to consider them a certain amount, and that amount is precisely the similarity between the word, and “bank”. We trust that in a good embedding, the similarity between “bank” and a word such as “the” is almost zero, as they are unrelated. Therefore, the model will know to ignore these words, and focus on those that may have a higher similarity with the word “bank”.\\nBut let me add some numbers to this reasoning to make it more clear. Imagine that we calculate similarities for the words in each sentence, and we get the following:\\nThis similarity makes sense in the following ways:\\nThe similarity between each word and itself is 1.\\nThe similarity between any irrelevant word (“the”, “of”, etc.) and any other word is 0.\\nThe similarity between “bank” and “river” is 0.11.\\nThe similarity between “bank” and “money” is 0.25.\\nWhy is the similarity between “bank” and “money” higher than the similarity between “bank” and “river”. We can imagine that “bank” gets used more often in the same context as “money”, than as “river”, and that explains the difference. We are simplifying this model quite a bit, it could be that the similarity between “the” and “of” is not zero, but 0.001. However, to simplify our calculations, we’ll use these numbers.\\nNow, on to the next step. We are going to use the similarities to transform each of the words of this sentence. We’ll call these new words “the1”, “bank1”, “of1”, “river1” for the first sentence, and “money2”, “in2”, “the2”, “bank2” for the second sentence. Beware, we’ll be doing some math with words, but it won’t be very different from the one we did before. Let’s look at the first sentence, “The bank of the river”, and the word “the”. The similarities with the other words are as follows:\\nSimilarities with “the”:\\nthe: 1\\nbank: 0\\nof: 0\\nriver: 0\\nTherefore, we turn the word “the” into the new word “the1”, which corresponds to the sum 1*“the” + 0*”bank” + 0*”of” + 0*”river”. This is equal to the word “the”. That means, the word “the” doesn’t change, and it’s equal to “the1”.\\nNow, let’s follow the same procedure with the word “bank”, and see what we get. The similarities of the word “bank” with the other words of the sentence “The bank of the river” are the following:\\nthe: 0\\nbank: 1\\nof: 0\\nriver: 0.11\\nTherefore, the word “bank” turns into the word “bank1”, given by the equation 1_”bank” + 0.11_”river”. We are almost done. We want the coefficients of the word to add to 1, so we can divide everything by their sum, which is 1.11. When we do that, we get that the word “bank” gets transformed into the word 0.9*”bank” + 0.1”river”.\\nLet’s do one more for consistency. The word “money” in the second sentence turns into “money2”, given by the equation 1*”money” + 0.25*”bank”. When we normalize, we divide everything by 1.25, to get the equation 0.8*”money” + 0.2*”bank”. All these equations are summarized in the table below.\\nThis is the way we obtained the modified words that we use in the attention mechanism above. The modified sentences then become the following:\\nIs There More to Attention?\\nWhat you learned in this chapter is simple self-attention. However, we can do much better than that. There is a method called multi-head attention, in which one doesn't only consider one embedding, but several different ones. These are all obtained from the original by transforming it in different ways. Multi-head attention has been very successful at the task of adding context to text. If you'd like to learn more about the self and multi-head attention, you can check out the following two videos:\\nThe Attention Mechanism in Large Language Models - YouTube\\nThe math behind Attention: Keys, Queries, and Values matrices - YouTube\\nConclusion\\nIn this post, you learned what attention mechanisms are. They are a very useful method that helps give words the context coming from the sentence where they belong. In this way, the model has less chance of getting confused by words taken out of context. LLMs make great use of attention mechanisms in order to understand text.\"),\n",
       " Document(metadata={'title': 'What Are Transformer Models and How Do They Work?', 'source': 'https://cohere.com/llmu/what-are-transformer-models'}, page_content=\"What Are Transformer Models and How Do They Work?What are Transformer Models and How do they Work? - YouTubeThe Narrated Transformer Language Model - YouTubeWhat are Transformer Models and how do they work?\\nWhat are Transformer Models and How do they Work? - YouTube\\nTransformer models are one of the most exciting new developments in machine learning. They were introduced in the paper Attention is All You Need. Transformers can be used to write stories, essays, poems, answer questions, translate between languages, chat with humans, and they can even pass exams that are hard for humans! But what are they? You’ll be happy to know that the architecture of transformer models is not that complex, it simply is a concatenation of some very useful components, each of which has its own function. In this chapter, you will learn all of these components.\\nIn a nutshell, what does a transformer do? Imagine that you’re writing a text message on your phone. After each word, you may get three words suggested to you. For example, if you type “Hello, how are”, the phone may suggest words such as “you”, or “your” as the next word. Of course, if you continue selecting the suggested word in your phone, you’ll quickly find that the message formed by these words makes no sense. If you look at each set of 3 or 4 consecutive words, it may make sense, but these words don’t concatenate to anything with a meaning. This is because the model used in the phone doesn’t carry the overall context of the message, it simply predicts which word is more likely to come up after the last few. Transformers, on the other hand, keep track of the context of what is being written, and this is why the text that they write makes sense.\\nThe phone can suggest the next word to use in a text message, but does not have the power to generate coherent text.\\nThe phone can suggest the next word to use in a text message, but does not have the power to generate coherent text.\\nI have to be honest with you, the first time I found out that transformers build text one word at a time, I couldn’t believe it. First of all, this is not how humans form sentences and thoughts. We first form a basic thought, and then start refining it and adding words to it. This is also not how ML models do other things. For example, images are not built this way. Most neural network based graphical models form a rough version of the image, and slowly refine it or add detail until it is perfect. So why would a transformer model build text word by word? One answer is, because that works really well. A more satisfying one is that because transformers are so incredibly good at keeping track of the context, that the next word they pick is exactly what it needs to keep going with an idea.\\nAnd how are transformers trained? With a lot of data, all the data on the internet, in fact. So when you input the sentence “Hello, how are” into the transformer, it simply knows that, based on all the text in the internet, the best next word is “you”. If you were to give it a more complicated command, say, “Write a story.”, it may figure out that a good next word to use is “Once”. Then it adds this word to the command, and figures out that a good next word is “upon”, and so on. And word by word, it will continue until it writes a story.\\nCommand: Write a story.\\nResponse: Once\\nNext command: Write a story. Once\\nResponse: upon\\nNext command: Write a story. Once upon\\nResponse: a\\n**Next command: Write a story. Once upon a\\n**Response:** time\\n**Next command: Write a story. Once upon a time\\n**Response:** there\\netc.\\nNow that we know what transformers do, let’s get to their architecture. If you’ve seen the architecture of a transformer model, you may have jumped in awe like I did the first time I saw it, it looks quite complicated! However, when you break it down into its most important parts, it’s not so bad. The transformer has 4 main parts:\\nTokenization\\nEmbedding\\nPositional encoding\\nTransformer block (several of these)\\nSoftmax\\nThe fourth one, the transformer block, is the most complex of all. Many of these can be concatenated, and each one contains two main parts: The attention and the feedforward components.\\nThe architecture of a transformer model\\nLet’s study these parts one by one.\\nTokenization\\nTokenization is the most basic step. It consists of a large dataset of tokens, including all the words, punctuation signs, etc. The tokenization step takes every word, prefix, suffix, and punctuation signs, and sends them to a known token from the library.\\nTokenization: Turning words into tokens\\nFor example, if the sentence is “Write a story”, then the 4 corresponding tokens will be <Write>, <a>, <story>, and \\\\<.>.\\nEmbedding\\nOnce the input has been tokenized, it’s time to turn words into numbers. For this, we use an embedding. In a previous chapter, you learned about how text embeddings send every piece of text to a vector (a list) of numbers. If two pieces of text are similar, then the numbers in their corresponding vectors are similar to each other (componentwise, meaning each pair of numbers in the same position is similar). Otherwise, if two pieces of text are different, then the numbers in their corresponding vectors are different.\\nFor example, if the sentence we are considering is “Write a story.” and the tokens are <Write>, <a>, <story>, and \\\\<.>, then each one of these will be sent to a long vector, and we’ll have four vectors.\\nIn general embeddings send every word (token) to a long list of numbers.\\nPositional encoding\\nOnce we have the vectors corresponding to each of the tokens in the sentence, the next step is to turn all these into one vector to process. The most common way to turn a bunch of vectors into one vector is to add them, componentwise. That means, we add each coordinate separately. For example, if the vectors (of length 2) are [1,2], and [3,4], their corresponding sum is [1+3, 2+4], which equals [4, 6]. This can work, but there’s a small caveat. Addition is commutative, meaning that if you add the same numbers in a different order, you get the same result. In that case, the sentence “I’m not sad, I’m happy” and the sentence “I’m not happy, I’m sad”, will result in the same vector, given that they have the same words, except in different order. This is not good. Therefore, we must come up with some method that will give us a different vector for the two sentences. Several methods work, and we’ll go with one of them: positional encoding. Positional encoding consists of adding a sequence of predefined vectors to the embedding vectors of the words. This ensures we get a unique vector for every sentence, and sentences with the same words in different order will be assigned different vectors. In the example below, the vectors corresponding to the words “Write”, “a”, “story”, and “.” become the modified vectors that carry information about their position, labeled “Write (1)”, “a (2)”, “story (3)”, and “. (4)”.\\nPositional encoding adds a positional vector to each word, in order to keep track of the positions of the words.\\nNow that we know we have a unique vector corresponding to the sentence, and that this vector carries the information on all the words in the sentence and their order, we can move to the next step.\\nTransformer block\\nLet’s recap what we have so far. The words come in and get turned into tokens (tokenization), tokenized words are turned into numbers (embeddings), then order gets taken into account (positional encoding). This gives us a vector for every token that we input to the model. Now, the next step is to predict the next word in this sentence. This is done with a really really large neural network, which is trained precisely with that goal, to predict the next word in a sentence.\\nWe can train such a large network, but we can vastly improve it by adding a key step: the attention component. Introduced in the seminal paper Attention is All you Need, it is one of the key ingredients in transformer models, and one of the reasons they work so well. Attention is explained in the previous section, but for now, imagine it as a way to add context to each word in the text.\\nThe attention component is added at every block of the feedforward network. Therefore, if you imagine a large feedforward neural network whose goal is to predict the next word, formed by several blocks of smaller neural networks, an attention component is added to each one of these blocks. Each component of the transformer, called a transformer block, is then formed by two main components:\\nThe attention component.\\nThe feedforward component.\\nThe transformer is a concatenation of many transformer blocks.\\nThe transformer is a concatenation of many transformer blocks. Each one of these is composed by an attention component followed by a feedforward component (a neural network).\\nAttention\\nThe next step is attention. As you learned in the previous chapter, the attention mechanism deals with a very important problem: the problem of context. Sometimes, as you know, the same word can be used with different meanings. This tends to confuse language models, since an embedding simply sends words to vectors, without knowing which definition of the word they’re using.\\nAttention is a very useful technique that helps language models understand the context. In order to understand how attention works, consider the following two sentences:\\nSentence 1: The bank of the river.\\nSentence 2: Money in the bank.\\nAs you can see, the word ‘bank’ appears in both, but with different definitions. In sentence 1, we are referring to the land at the side of the river, and in the second one to the institution that holds money. The computer has no idea of this, so we need to somehow inject that knowledge into it. What can help us? Well, it seems that the other words in the sentence can come to our rescue. For the first sentence, the words ‘the’, and ‘of’ do us no good. But the word ‘river’ is the one that is letting us know that we’re talking about the land at the side of the river. Similarly, in sentence 2, the word ‘money’ is the one that is helping us understand that the word ‘bank’ is now referring to the institution that holds money.\\nAttention helps give context to each word, based on the other words in the sentence (or text).\\nIn short, what attention does is it moves the words in a sentence (or piece of text) closer in the word embedding. In that way, the word “bank” in the sentence “Money in the bank” will be moved closer to the word “money”. Equivalently, in the sentence “The bank of the river”, the word “bank” will be moved closer to the word “river”. That way, the modified word “bank” in each of the two sentences will carry some of the information of the neighboring words, adding context to it.\\nThe attention step used in transformer models is actually much more powerful, and it’s called multi-head attention. In multi-head attention, several different embeddings are used to modify the vectors and add context to them. Multi-head attention has helped language models reach much higher levels of efficacy when processing and generating text.\\nThe Softmax Layer\\nNow that you know that a transformer is formed by many layers of transformer blocks, each containing attention and a feedforward layer, you can think of it as a large neural network that predicts the next word in a sentence. The transformer outputs scores for all the words, where the highest scores are given to the words that are most likely to be next in the sentence.\\nThe last step of a transformer is a softmax layer, which turns these scores into probabilities (that add to 1), where the highest scores correspond to the highest probabilities. Then, we can sample out of these probabilities for the next word. In the example below, the transformer gives the highest probability of 0.5 to “Once”, and probabilities of 0.3 and 0.2 to “Somewhere” and “There”. Once we sample, the word “once” is selected, and that’s the output of the transformer.\\nThe softmax layer turns the scores into probabilities, and these are used to pick the next word in the text.\\nNow what? Well, we repeat the step. We now input the text “Write a story. Once” into the model, and most likely, the output will be “upon”. Repeating this step again and again, the transformer will end up writing a story, such as “Once upon a time, there was a …”.\\nPost Training\\nNow that you know how transformers work, we still have a bit of work to do. Imagine the following: You ask the transformer “What is the capital of Algeria?”. We would love for it to answer “Algiers”, and move on. However, the transformer is trained on the entire internet. The internet is a big place, and it’s not necessarily the best question/answer repository. Many pages, for example, would have long lists of questions without answers. In this case, the next sentence after “What is the capital of Algeria?” could be another question, such as “What is the population of Algeria?”, or “What is the capital of Burkina Faso?”. The transformer is not a human who thinks about their responses, it simply mimics what it sees on the internet (or any dataset that has been provided). So how do we get the transformer to answer questions?\\nThe answer is post-training. In the same way that you would teach a person to do certain tasks, you can get a transformer to perform tasks. Once a transformer is trained on the entire internet, then it is trained again on a large dataset which corresponds to lots of questions and their respective answers. Transformers (like humans), have a bias towards the last things they’ve learned, so post-training has proven a very useful step to help transformers succeed at the tasks they are asked to.\\nPost-training also helps with many other tasks. For example, one can post-train a transformer with large datasets of conversations, in order to help it perform well as a chatbot, or to help us write stories, poems, or even code.\\nMore\\nAs mentioned above, this is a conceptual introduction to give you an idea of how transformers generate text. If you'd like to open the hood and get a more detailed intuition of the mathematics behind a transformer, we invite you to check out the following articles and video by our course instructors, Jay Alammar, and Luis Serrano.\\nThe Illustrated Transformer\\nHow GPT3 Works\\nThe Narrated Transformer Language Model - YouTube\\nWhat are Transformer Models and how do they work? - YouTube\\nConclusion\\nIn this chapter you’ve learned how transformers work. They are formed by several blocks, each one with its own function, working together to understand the text and generate the next word. These blocks are the following:\\nTokenizer: Turns words into tokens.\\nEmbedding: Turns tokens into numbers (vectors)\\nPositional encoding: Adds order to the words in the text.\\nTransformer block: Guesses the next word. It is formed by an attention block and a feedforward block.\\nAttention: Adds context to the text.\\nFeedforward: Is a block in the transformer neural network, which guesses the next word.\\nSoftmax: Turns the scores into probabilities in order to sample the next word.\\nThe repetition of these steps is what writes the amazing text you’ve seen transformers create. The main reason they work so well is because they have a huge amount of parameters that can capture many aspects of the context. We’re excited to see what you can build using transformer models!\"),\n",
       " Document(metadata={'title': 'Retrieval-Augmented Generation (RAG) Using Cohere on AWS', 'source': 'https://cohere.com/llmu/co-aws-rag'}, page_content='Retrieval-Augmented Generation (RAG) Using Cohere on AWSQualified\\nContents\\nIntroduction\\nSet Up Amazon Bedrock and SageMaker \\nSetup Bedrock\\nSetup SageMaker\\nCreate SageMaker Endpoint\\nQuick Example\\nCode Walkthrough \\nSetup\\nDefine Documents\\nCreate Vectorstore\\nProcess Documents\\nRun Chatbot\\nConclusion\\nWe’ll use Cohere’s Python SDKs (cohere and cohere-aws) for the code examples. Follow along in this notebook.\\nIntroduction\\nLarge Language Models (LLMs) have proven effective at performing text generation tasks and maintaining the context of a conversation in a chat setting. However, we can sometimes encounter a scenario where an LLM hallucinates and provides factually inaccurate responses to a given question. This is especially true in business settings, where companies have proprietary data that an LLM would not have seen during its training phase.\\nRetrieval-augmented generation (RAG) bridges the gap by allowing an LLM to integrate external data sources and use them in its response generation. This significantly minimizes the hallucination issue, making the model\\'s responses more accurate and reliable.\\nWith RAG, an LLM can use external data sources in its response generation\\nFor example, a major provider of CRM and ERP software wanted to provide a better support Q&A experience that could report on transactional data.\\nThe customer used Cohere Command and Rerank with retrieval-augmented generation (RAG) to build a conversational support app. Users could ask detailed technical questions and get relevant responses, along with citations.\\nImplementing a RAG system at a major provider of CRM and ERP software\\nIn previous chapters, we have seen how to use Cohere’s Chat, Embed, and Rerank endpoints individually. In this chapter, we’ll explore how to build a RAG application using all three endpoints: Chat and Embed on Amazon Bedrock and Rerank on Amazon SageMaker (at the time of writing, Rerank is not available on Bedrock).\\nAn overview of what we\\'ll cover in the code walkthrough\\nWith Cohere Chat, in particular, you get the complete suite of tools needed to build a high-quality RAG application in the shortest time possible. Some of the highlights (see a more comprehensive list) of Cohere’s RAG capabilities include:\\nQuery generation: With Cohere’s RAG solution, you get an LLM that’s trained for query generation. It takes a user message and transforms it into queries that are more relevant and optimized for the retrieval process.\\nRetrieval models: Cohere Embed helps you build a high-quality semantic search system that retrieves the most relevant documents using embeddings. Cohere Rerank, on the other hand, helps you boost the results further by reranking the search results based on relevance.\\nFine-grained citation: Each grounded response includes fine-grained citations linking back to the source documents. This makes the response easily verifiable and builds trust with the user.\\nCohere Chat provides a complete set of tools needed to build a high-quality RAG application\\nSet Up Amazon Bedrock and SageMaker\\nFirst, we set up the clients for Bedrock (to be used for Chat and Embed) and SageMaker (to be used for Rerank) using the same steps as in the previous chapters. Here we name the clients co_br for Bedrock and co_sm for SageMaker.\\n# ! pip install cohere cohere-aws boto3 hnswlib unstructured -q\\nimport os import cohere import boto3 import cohere_aws from cohere_aws import Client\\nSetup Bedrock\\nTo use Bedrock, we create a BedrockClient by passing the necessary AWS credentials.\\n# Create Bedrock client via the native Cohere SDK # Contact your AWS administrator for the credentials co_br = cohere.BedrockClient( aws_region=\"YOUR_AWS_REGION\", aws_access_key=\"YOUR_AWS_ACCESS_KEY_ID\", aws_secret_key=\"YOUR_AWS_SECRET_ACCESS_KEY\", aws_session_token=\"YOUR_AWS_SESSION_TOKEN\", ) \\nSetup SageMaker\\nLater, we’ll need to create a SageMaker endpoint that exposes access to a Cohere model (Rerank v3 in our case). For this, we’ll use the cohere_aws SDK which makes it easy to set up the endpoint, together with AWS’s boto3 library.\\nOnce the endpoint is created (as we’ll walk through later), we can access it using the Cohere SDK. To do this, let’s create a SagemakerClient by passing the necessary AWS credentials.\\n# Create SageMaker client via the native Cohere SDK # Contact your AWS administrator for the credentials co_sm = cohere.SagemakerClient( aws_region=\"YOUR_AWS_REGION\", aws_access_key=\"YOUR_AWS_ACCESS_KEY_ID\", aws_secret_key=\"YOUR_AWS_SECRET_ACCESS_KEY\", aws_session_token=\"YOUR_AWS_SESSION_TOKEN\", ) # For creating an endpoint, you need to use the cohere_aws client: Set environment variables with the AWS credentials os.environ[\\'AWS_ACCESS_KEY_ID\\'] = \"YOUR_AWS_ACCESS_KEY_ID\" os.environ[\\'AWS_SECRET_ACCESS_KEY\\'] = \"YOUR_AWS_SECRET_ACCESS_KEY\" os.environ[\\'AWS_SESSION_TOKEN\\'] = \"YOUR_AWS_SESSION_TOKEN\"\\nCreate SageMaker Endpoint\\nThe next step is to create a Rerank SageMaker endpoint by defining the model package Amazon Resource Names (ARN) for the Rerank model. The ARN is an identifying string for a SageMaker resource, and it varies between the regions where a resource is available.\\nHere, we define the Cohere package for the Rerank model and map the model package against each region, which gives the complete ARN for each region.\\n# Create SageMaker endpoint via the cohere_aws SDK cohere_package = \"cohere-rerank-english-v3-01-d3687e0d2e3a366bb904275616424807\" model_package_map = { \"us-east-1\": f\"arn:aws:sagemaker:us-east-1:865070037744:model-package/{cohere_package}\", \"us-east-2\": f\"arn:aws:sagemaker:us-east-2:057799348421:model-package/{cohere_package}\", \"us-west-1\": f\"arn:aws:sagemaker:us-west-1:382657785993:model-package/{cohere_package}\", \"us-west-2\": f\"arn:aws:sagemaker:us-west-2:594846645681:model-package/{cohere_package}\", \"ca-central-1\": f\"arn:aws:sagemaker:ca-central-1:470592106596:model-package/{cohere_package}\", \"eu-central-1\": f\"arn:aws:sagemaker:eu-central-1:446921602837:model-package/{cohere_package}\", \"eu-west-1\": f\"arn:aws:sagemaker:eu-west-1:985815980388:model-package/{cohere_package}\", \"eu-west-2\": f\"arn:aws:sagemaker:eu-west-2:856760150666:model-package/{cohere_package}\", \"eu-west-3\": f\"arn:aws:sagemaker:eu-west-3:843114510376:model-package/{cohere_package}\", \"eu-north-1\": f\"arn:aws:sagemaker:eu-north-1:136758871317:model-package/{cohere_package}\", \"ap-southeast-1\": f\"arn:aws:sagemaker:ap-southeast-1:192199979996:model-package/{cohere_package}\", \"ap-southeast-2\": f\"arn:aws:sagemaker:ap-southeast-2:666831318237:model-package/{cohere_package}\", \"ap-northeast-2\": f\"arn:aws:sagemaker:ap-northeast-2:745090734665:model-package/{cohere_package}\", \"ap-northeast-1\": f\"arn:aws:sagemaker:ap-northeast-1:977537786026:model-package/{cohere_package}\", \"ap-south-1\": f\"arn:aws:sagemaker:ap-south-1:077584701553:model-package/{cohere_package}\", \"sa-east-1\": f\"arn:aws:sagemaker:sa-east-1:270155090741:model-package/{cohere_package}\", } region = boto3.Session().region_name if region not in model_package_map.keys(): raise Exception(\"UNSUPPORTED REGION\") model_package_arn = model_package_map[region] co_aws = Client(region_name=region) co_aws.create_endpoint(arn=model_package_arn, endpoint_name=\"my-rerank-v3\", instance_type=\"ml.g5.xlarge\", n_instances=1)\\nThe following output indicates that the process of creating an endpoint is complete.\\nQuick Example\\nWe’ll start with a quick example to understand the key aspects of RAG.\\nWith RAG, the first step is to define the documents that an LLM will have access to. Here, we have a short list of simple documents. Typically, there is a retrieval process to retrieve the most relevant documents based on a user query, which we’ll cover in the longer walkthrough next. But at this point, let’s assume that these are the only documents and we’ll pass all of them to the LLM.\\ndocuments = [ { \"title\": \"Tall penguins\", \"text\": \"Emperor penguins are the tallest.\"}, { \"title\": \"Penguin habitats\", \"text\": \"Emperor penguins only live in Antarctica.\"}, { \"title\": \"What are animals?\", \"text\": \"Animals are different from plants.\"} ] \\nWe have seen how to use the Chat endpoint in the text generation chapter. To use the RAG feature, we simply need to add one additional parameter, documents, to the endpoint call. These are the documents we defined earlier, which are now available for the model to consider utilizing in its response.\\nLet’s now see how the model responds when given the user message, What are the tallest living penguins?\\nmessage = \"What are the tallest living penguins?\" response = co_br.chat(message=message, documents=documents, model=\"cohere.command-r-plus-v1:0\") print(\"\\\\nRESPONSE:\\\\n\") print(response.text) if response.citations: print(\"\\\\nCITATIONS:\\\\n\") for citation in response.citations: print(citation)\\nAnd here’s the response. The model used the documents to inform its answer to the question. For example, the tallest living penguins are the Emperor penguins part of its response was cited from doc_0, which is the first document in the list containing the text Emperor penguins are the tallest.\\nRESPONSE: The tallest living penguins are the Emperor penguins. These penguins only live in Antarctica. CITATIONS: start=4 end=53 text=\\'tallest living penguins are the Emperor penguins.\\' document_ids=[\\'doc_0\\'] start=69 end=93 text=\\'only live in Antarctica.\\' document_ids=[\\'doc_1\\']\\nCode Walkthrough\\nNow that we’ve covered the basics, let’s look at a more comprehensive example of RAG that includes:\\nBuilding a retrieval system that includes turning documents into text embeddings and storing them in an index\\nBuilding a query generation system that turns user messages into optimized queries for retrieval\\nWrapping a user interaction with an LLM in a chat interface\\nBuilding a response generation system that’s able to answer different types of queries, such as those that require and don’t require RAG\\nSetup\\nFirst, let’s import the necessary libraries for this project. This includes hnswlib for the vector library and unstructured for chunking the documents (more details on these later).\\nimport uuid import hnswlib from typing import List, Dict from unstructured.partition.html import partition_html from unstructured.chunking.title import chunk_by_title \\nDefine Documents\\nNext, we’ll define the documents we’ll use for RAG. We’ll use a few pages from the Cohere documentation that discuss prompt engineering, each in the Python list raw_documents below. Each entry is identified by its title and URL.\\nraw_documents = [ { \"title\": \"Crafting Effective Prompts\", \"url\": \"https://docs.cohere.com/docs/crafting-effective-prompts\"}, { \"title\": \"Advanced Prompt Engineering Techniques\", \"url\": \"https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\"}, { \"title\": \"Prompt Truncation\", \"url\": \"https://docs.cohere.com/docs/prompt-truncation\"}, { \"title\": \"Preambles\", \"url\": \"https://docs.cohere.com/docs/preambles\"} ] \\nCreate Vectorstore\\nThe Vectorstore class handles the ingestion of documents into embeddings (or vectors) and the retrieval of relevant documents given a query.\\nThe Vectorstore component for handling document ingestion and retrieval\\nIt includes a few methods:\\nload_and_chunk: Loads the raw documents from the URL and breaks them into smaller chunks. We’ll utilize the partition_html method from the unstructured library to perform the chunking.\\nembed: Generates embeddings of the chunked documents. We use the Embed endpoint available on Bedrock, which uses the cohere.embed-english-v3 model.\\nindex: Indexes the document chunk embeddings to ensure efficient similarity search during retrieval. For this, we’ll use the hnswlib vector library.\\nretrieve: Uses semantic search to retrieve relevant document chunks from the index, given a query. It involves two steps: first, dense retrieval from the index via the Embed endpoint, and second, a reranking via the Rerank endpoint to boost the search results further.\\nclass Vectorstore: def __init__(self, raw_documents: List[Dict[str, str]]): self.raw_documents = raw_documents self.docs = [] self.docs_embs = [] self.retrieve_top_k = 10 self.rerank_top_k = 3 self.load_and_chunk() self.embed() self.index() def load_and_chunk(self) -> None: \"\"\" Loads the text from the sources and chunks the HTML content. \"\"\" print(\"Loading documents...\") for raw_document in self.raw_documents: elements = partition_html(url=raw_document[\"url\"]) chunks = chunk_by_title(elements) for chunk in chunks: self.docs.append( { \"title\": raw_document[\"title\"], \"text\": str(chunk), \"url\": raw_document[\"url\"], } ) def embed(self) -> None: \"\"\" Embeds the document chunks using the Cohere API. \"\"\" print(\"Embedding document chunks...\") batch_size = 90 self.docs_len = len(self.docs) for i in range(0, self.docs_len, batch_size): batch = self.docs[i : min(i + batch_size, self.docs_len)] texts = [item[\"text\"] for item in batch] docs_embs_batch = co_br.embed( texts=texts, model=\"cohere.embed-english-v3\", input_type=\"search_document\" ).embeddings self.docs_embs.extend(docs_embs_batch) def index(self) -> None: \"\"\" Indexes the document chunks for efficient retrieval. \"\"\" print(\"Indexing document chunks...\") self.idx = hnswlib.Index(space=\"ip\", dim=1024) self.idx.init_index(max_elements=self.docs_len, ef_construction=512, M=64) self.idx.add_items(self.docs_embs, list(range(len(self.docs_embs)))) print(f\"Indexing complete with {self.idx.get_current_count()} document chunks.\") def retrieve(self, query: str) -> List[Dict[str, str]]: \"\"\" Retrieves document chunks based on the given query. Parameters: query (str): The query to retrieve document chunks for. Returns: List[Dict[str, str]]: A list of dictionaries representing the retrieved document chunks, with \\'title\\', \\'text\\', and \\'url\\' keys. \"\"\" # Dense retrieval query_emb = co_br.embed( texts=[query], model=\"cohere.embed-english-v3\", input_type=\"search_query\" ).embeddings doc_ids = self.idx.knn_query(query_emb, k=self.retrieve_top_k)[0][0] # Reranking rank_fields = [\"title\", \"text\"] # We\\'ll use the title and text fields for reranking docs_to_rerank = [self.docs[doc_id] for doc_id in doc_ids] rerank_results = co_sm.rerank( query=query, documents=docs_to_rerank, top_n=self.rerank_top_k, rank_fields=rank_fields, model=\"my-rerank-v3\") doc_ids_reranked = [doc_ids[result.index] for result in rerank_results.results] docs_retrieved = [] for doc_id in doc_ids_reranked: docs_retrieved.append( { \"title\": self.docs[doc_id][\"title\"], \"text\": self.docs[doc_id][\"text\"], \"url\": self.docs[doc_id][\"url\"], } ) return docs_retrieved\\nProcess Documents\\nNow that the Vectorstore component is set up, we can process the documents, which will involve chunking, embedding, and indexing. We do this by creating an instance of the Vectorstore and passing the raw documents we defined earlier.\\n# Create an instance of the Vectorstore class with the given sources vectorstore = Vectorstore(raw_documents) \\nThe result is 44 chunks from the original four web pages.\\nLoading documents... Embedding document chunks... Indexing document chunks... Indexing complete with 44 document chunks. \\nWe can test if the retrieval is working by entering a search query.\\nvectorstore.retrieve(\"Prompting by giving examples\") \\nThis returns the following results, which indeed are returning relevant chunks to the query.\\n[{\\'title\\': \\'Advanced Prompt Engineering Techniques\\', \\'text\\': \\'Few-shot Prompting\\\\n\\\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.\\', \\'url\\': \\'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\\'}, {\\'title\\': \\'Crafting Effective Prompts\\', \\'text\\': \\'Incorporating Example Outputs\\\\n\\\\nLLMs respond well when they have specific examples to work from. For example, instead of asking for the salient points of the text and using bullet points “where appropriate”, give an example of what the output should look like.\\', \\'url\\': \\'https://docs.cohere.com/docs/crafting-effective-prompts\\'}, {\\'title\\': \\'Advanced Prompt Engineering Techniques\\', \\'text\\': \\'In addition to giving correct examples, including negative examples with a clear indication of why they are wrong can help the LLM learn to distinguish between correct and incorrect responses. Ordering the examples can also be important; if there are patterns that could be picked up on that are not relevant to the correctness of the question, the model may incorrectly pick up on those instead of the semantics of the question itself.\\', \\'url\\': \\'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\\'}] \\nRun Chatbot\\nWe can now run the chatbot. For this, we create a run_chatbot function, which is an expanded version of the same function we created in Chapter 3.\\nThis time, it includes the RAG components:\\nFor each user message, we use the endpoint’s search query generation feature to turn the message into one or more queries that are optimized for retrieval. The endpoint can even return no query, meaning a user message can be responded to directly without retrieval. This is done by calling the Chat endpoint with the search_queries_only parameter and setting it as True.\\nIf no search query is generated, we call the Chat endpoint to generate a response directly. If there is at least one, we call the retrieve method from the Vectorstore instance to retrieve the most relevant documents to each query.\\nFinally, all the results from all queries are appended to a list and passed to the Chat endpoint for response generation.\\nWe print the response, together with the citations and the list of document chunks cited, for easy reference.\\ndef run_chatbot(message, chat_history=None): if chat_history is None: chat_history = [] # Generate search queries, if any response = co_br.chat(message=message, search_queries_only=True, model=\"cohere.command-r-plus-v1:0\", chat_history=chat_history) search_queries = [] for query in response.search_queries: search_queries.append(query.text) # If there are search queries, retrieve the documents if search_queries: print(\"Retrieving information...\", end=\"\") # Retrieve document chunks for each query documents = [] for query in search_queries: documents.extend(vectorstore.retrieve(query)) # Use document chunks to respond response = co_br.chat( message=message, model=\"cohere.command-r-plus-v1:0\", documents=documents, chat_history=chat_history) else: response = co_br.chat( message=message, model=\"cohere.command-r-plus-v1:0\", chat_history=chat_history) # Print the chatbot response, citations, and documents print(\"\\\\nRESPONSE:\\\\n\") print(response.text) if response.citations: print(\"\\\\nCITATIONS:\\\\n\") for citation in response.citations: print(citation) print(\"\\\\nDOCUMENTS:\\\\n\") for document in response.documents: print(document) chat_history = response.chat_history return chat_history \\nHere is a sample conversation consisting of a few turns.\\nTurn #1:\\nchat_history = run_chatbot(\"Hello, I have a question\") \\nRESPONSE: Of course! I am here to help. Please go ahead and ask your question, and I will do my best to provide a helpful response.\\nTurn #2:\\nchat_history = run_chatbot(\"What\\'s the difference between zero-shot and few-shot prompting\", chat_history) \\nRetrieving information... RESPONSE: Zero-shot prompting is when no examples of the task are provided to the model. On the other hand, few-shot prompting is a technique where a model is given a few examples of the task being performed before asking the specific question to be answered. CITATIONS: start=0 end=19 text=\\'Zero-shot prompting\\' document_ids=[\\'doc_0\\'] start=28 end=78 text=\\'no examples of the task are provided to the model.\\' document_ids=[\\'doc_0\\'] start=98 end=116 text=\\'few-shot prompting\\' document_ids=[\\'doc_0\\'] start=140 end=197 text=\\'model is given a few examples of the task being performed\\' document_ids=[\\'doc_0\\'] start=205 end=249 text=\\'asking the specific question to be answered.\\' document_ids=[\\'doc_0\\'] DOCUMENTS: {\\'id\\': \\'doc_0\\', \\'text\\': \\'Few-shot Prompting\\\\n\\\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.\\', \\'title\\': \\'Advanced Prompt Engineering Techniques\\', \\'url\\': \\'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\\'}\\nTurn #3:\\nchat_history = run_chatbot(\"How would the latter help?\", chat_history) \\nRetrieving information... RESPONSE: Few-shot prompting can vastly improve the quality of the model\\'s completions. By providing a few relevant and diverse examples, the model can be steered toward a high-quality solution. These examples condition the model to the expected response type and style. CITATIONS: start=23 end=77 text=\"vastly improve the quality of the model\\'s completions.\" document_ids=[\\'doc_2\\'] start=97 end=126 text=\\'relevant and diverse examples\\' document_ids=[\\'doc_0\\'] start=145 end=184 text=\\'steered toward a high-quality solution.\\' document_ids=[\\'doc_0\\'] start=200 end=260 text=\\'condition the model to the expected response type and style.\\' document_ids=[\\'doc_0\\'] DOCUMENTS: {\\'id\\': \\'doc_2\\', \\'text\\': \\'Advanced Prompt Engineering Techniques\\\\n\\\\nSuggest Edits\\\\n\\\\nThe previous chapter discussed general rules and heuristics to follow for successfully prompting the Command family of models. Here, we will discuss specific advanced prompt engineering techniques that can in many cases vastly improve the quality of the model’s completions. These include how to give clear and unambiguous instructions, few-shot prompting, chain-of-thought (CoT) techniques, and prompt chaining.\\', \\'title\\': \\'Advanced Prompt Engineering Techniques\\', \\'url\\': \\'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\\'} {\\'id\\': \\'doc_0\\', \\'text\\': \\'Few-shot Prompting\\\\n\\\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.\\', \\'title\\': \\'Advanced Prompt Engineering Techniques\\', \\'url\\': \\'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\\'}\\nTurn #4:\\nchat_history = run_chatbot(\"What do you know about 5G networks?\", chat_history) \\nRetrieving information... RESPONSE: Sorry, I don\\'t have any information about 5G networks. Is there anything else you would like to ask?\\nThere are a few observations worth pointing out:\\nDirect response: For user messages that don’t require retrieval (“Hello, I have a question”), the chatbot responds directly without requiring retrieval.\\nCitation generation: For responses that do require retrieval (\"What\\'s the difference between zero-shot and few-shot prompting\"), the endpoint returns the response together with the citations. These are fine-grained citations, which means they refer to specific spans of the generated text.\\nState management: The endpoint maintains the state of the conversation via the chat_history parameter, for example, by correctly responding to a vague user message, such as \"How would the latter help?\"\\nResponse synthesis: The model can decide if none of the retrieved documents provide the necessary information to answer a user message. For example, when asked the question, “What do you know about 5G networks”, the chatbot retrieves external information from the index. However, it doesn’t use any of the information in its response as none of it is relevant to the question.\\nConclusion\\nThis chapter demonstrated how to create a RAG application using Cohere Chat and Embed on Amazon Bedrock and Cohere Rerank on Amazon SageMaker. RAG enhances LLMs by enabling them to integrate external data sources and reduce hallucination, resulting in more accurate and reliable responses.\\nBy following the steps outlined in this chapter, you can leverage Cohere\\'s RAG capabilities to build high-quality applications that utilize LLMs and external data effectively, making it a powerful tool for creating robust and trustworthy AI solutions.\\nBut we can extend the concept of RAG and make it more powerful using a tool use approach. This makes it possible to build applications that can not only answer questions, but also automate tasks.\\nIn Chapter 7, we’ll learn how to use Command R+ on Amazon Bedrock to implement tool use.'),\n",
       " Document(metadata={'title': 'Introduction to Text Embeddings', 'source': 'https://cohere.com/llmu/text-embeddings'}, page_content='Introduction to Text EmbeddingsQualified\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nWhen you hear about large language models (LLMs), probably the first thing that comes to mind is the text generation capability, such as writing an essay or creating marketing copy.\\nAnother thing you can get is text representation: a set of numbers that represent what the text means and capture the semantics of the text. These numbers are called text embeddings.\\nText generation outputs text, while text representation outputs embeddings\\nText embeddings give you the ability to turn unstructured text data into a structured form. With embeddings, you can compare two or more pieces of text, be it single words, sentences, paragraphs, or even longer documents. And since these are sets of numbers, the ways you can process and extract insights from them are limited only by your imagination.\\nWhat does this bring? It opens up many possible use cases that apply in the real world today. Embeddings power applications we interact with on a daily basis, such as modern search engines, eCommerce product recommendations, social media content moderation, email spam filtering, customer support conversational agents, and many more.\\nIn this chapter, we take a visual approach to understand the intuition behind text embeddings.\\nStep-by-Step Guide\\nTo set up, we first import several tools. We\\'ll use the same notebook for the next several chapters, and we\\'ll import everything we need here.\\nimport pandas as pd import numpy as np import altair as alt from sklearn.decomposition import PCA from sklearn.metrics.pairwise import cosine_similarity from sklearn.cluster import KMeans \\nWe also import the Cohere module and create a client.\\nimport cohere co = cohere.Client(\"COHERE_API_KEY\") # Your Cohere API key \\nStep 1: Prepare the Dataset\\nWe\\'ll work a subset of the Airline Travel Information System (ATIS) intent classification dataset [Source]. The following code loads the dataset into a pandas Dataframe df with a single column \"queries\" containing 91 inquiries coming to airline travel inquiry systems.\\n# Load the dataset to a dataframe df_orig = pd.read_csv(\\'https://raw.githubusercontent.com/cohere-ai/notebooks/main/notebooks/data/atis_intents_train.csv\\', names=[\\'intent\\',\\'query\\']) # Take a small sample for illustration purposes sample_classes = [\\'atis_airfare\\', \\'atis_airline\\', \\'atis_ground_service\\'] df = df_orig.sample(frac=0.1, random_state=30) df = df[df.intent.isin(sample_classes)] df_orig = df_orig.drop(df.index) df.reset_index(drop=True,inplace=True) # Remove unnecessary column intents = df[\\'intent\\'] #save for a later need df.drop(columns=[\\'intent\\'], inplace=True) \\nHere are a few example data points:\\n- which airlines fly from boston to washington dc via other cities - show me the airlines that fly between toronto and denver - show me round trip first class tickets from new york to miami - i\\'d like the lowest fare from denver to pittsburgh - show me a list of ground transportation at boston airport - show me boston ground transportation - of all airlines which airline has the most arrivals in atlanta - what ground transportation is available in boston - i would like your rates between atlanta and boston on september third - which airlines fly between boston and pittsburgh \\nStep 2: Turn Text into Embeddings\\nNext, we embed each inquiry by calling Cohere’s Embed endpoint with co.embed(). It takes in texts as input and returns embeddings as output. We supply three parameters:\\ntexts: The list of texts you want to embed\\nmodel: The model to use to generate the embedding. At the time of writing, there are four models available:\\nembed-english-v3.0 (English)\\nembed-english-light-v3.0 (English)\\nembed-multilingual-v3.0 (Multilingual: 100+ languages)\\nembed-multilingual-light-v3.0 (Multilingual: 100+ languages)\\ninput_type — Specifies the type of document to be embedded. At the time of writing, there are four options:\\nsearch_document: For documents against which search is performed\\nsearch_query: For query documents\\nclassification: For when the embeddings will be used as an input to a text classifier\\nclustering: For when you want to cluster the embeddings\\ndef get_embeddings(texts, model=\\'embed-english-v3.0\\', input_type=\"search_document\"): output = co.embed( model=model, input_type=input_type, texts=texts) return output.embeddings df[\\'query_embeds\\'] = get_embeddings(df[\\'query\\'].tolist()) \\nFor every piece of text passed to the Embed endpoint, a sequence of 1024 numbers will be generated. Each number represents a piece of information about the meaning contained in that piece of text. Here are the first few dimensions given by the embed-english-v3.0 model for \"show me a list of ground transportation at boston airport\":\\n[0.03793335, -0.008010864, -0.002319336, -0.0110321045, -0.019882202, -0.023864746, 0.011428833, -0.030349731, -0.044830322, 0.028289795, -0.02810669, -0.0032749176, -0.04208374, -0.0077705383, -0.0033798218, -0.06335449, ... ] \\nStep 3: Visualize Embeddings with a Heatmap\\nLet’s get some visual intuition about this by plotting these numbers in a heatmap. What we can do is compress the dimension to a much lower number, say 10.\\nThe get_pc() function below does this via a technique called Principal Component Analysis (PCA), which reduces the number of dimensions in an embedding while retaining as much information as possible. We set embeds_pc to the ten-dimensional version of the document embeddings.\\n# Function to return the principal components def get_pc(arr, n): pca = PCA(n_components=n) embeds_transform = pca.fit_transform(arr) return embeds_transform # Reduce embeddings to 10 principal components to aid visualization embeds = np.array(df[\\'query_embeds\\'].tolist()) embeds_pc = get_pc(embeds, 10) \\nWe’ll use the 9 data point above as examples and display their compressed embeddings on a heatmap. We have each data point on the y-axis and its corresponding set of 10 embedding values on the x-axis, which looks like this:\\nA heatmap showing 10-dimensional embeddings of 9 data points\\nThere are some patterns emerging. To see this, let’s look at a smaller number of examples.\\nTake these three for example. They are all inquiries about ground transportation in Boston. And by visual inspection, we can see that their embedding patterns are very similar.\\nThe 10-dimensional embeddings of 3 inquiries, all about ground transportation in Boston\\nNow, compare them to the other kinds of inquiries, such as those related to airline information (see two examples below). Notice that while the embeddings about ground transportation inquiries look very similar to each other, they are distinctive from the rest.\\nThe 10-dimensional embeddings of 2 inquiries about other matters\\nHere, the model was able to capture the context and meaning of each piece of text, and it then represents them as embeddings. Each dimension of an embedding, called a feature, represents a certain universal characteristic of text according to how the model understands it.\\nHow is this possible? A large language model has been pre-trained with a vast amount of text data, where the training objective is set up in such a way as to encourage the model to extract contextual information about a piece of text and store it as embeddings.\\nStep 4: Visualize Embeddings on a 2D Plot\\nWe can investigate this further by compressing the embeddings to two dimensions and plotting them on a scatter plot. What we would expect is that texts of similar meaning would be closer to each other, and vice versa.\\nDo note that as we compress the embeddings to lower dimensions, the information retained becomes lesser. However, humans can only visualize in 2D or 3D, and it turns out this is still a good enough approximation to help us gain intuition about the data.\\nA plot showing 2D embeddings of 9 data points\\nBy visual inspection, we can see that texts of similar meaning are indeed located close together. We see inquiries about tickets on the left, inquiries about airlines somewhere around the middle, and inquiries about ground transportation on the top right.\\nThese kinds of insights enable various downstream analyses and applications, such as topic modeling, by clustering documents into groups. In other words, text embeddings allow us to take a huge corpus of unstructured text and turn it into a structured form, making it possible to objectively compare, dissect, and derive insights from all that text.\\nIn the coming chapters, we\\'ll dive deeper into these topics.\\nConclusion\\nIn this chapter you learned about the Embed endpoint. Text embeddings make possible a wide array of downstream applications such as semantic search, clustering, and classification. You\\'ll learn more about those in the subsequent chapters.'),\n",
       " Document(metadata={'title': 'Tool Use and Agents on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-tooluse'}, page_content='Tool Use and Agents on Amazon BedrockQualified\\nContents\\nIntroduction\\nCode Walkthrough\\nSetup \\nSet Up the Tools\\nRun the Tool Use Workflow\\nSingle-Step Scenario\\nMulti-Step Scenario\\nConclusion\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nIntroduction\\nTool use extends the capabilities of retrieval-augmented generation (RAG) by making it possible to build applications that can not only answer questions but also automate tasks.\\nWith a tool use approach, external systems are used to guide an LLM’s response, but they do so by leveraging a much bigger set of tools than what’s possible with RAG. The concept of tool use expands on LLMs\\' useful feature of being able to act as a reasoning and decision-making engine. Tool use also enables developers to build agentic applications that can take actions; that is, they are capable of doing both read and write operations on an external system.\\nFor example, a global financial consultancy wanted to build an executive AI assistant for a national telecom company that could support executive decision-making. To meet the client’s requirements, the solution needed to extract information from internal document stores and real-time data sources.\\nUsing Cohere’s tool use solution, along with RAG, the firm was able to leverage Command’s multi-step tool use capabilities and external agents (e.g., calculators and stock price sources) to retrieve and manipulate structured data.\\nImplementing a tool use workflow at a global financial consultancy\\nIn this chapter, we\\'ll explore how to build an agentic application that leverages tool using Cohere\\'s Command R+ model on Amazon Bedrock. For the most updated list of Cohere models available on Amazon Bedrock, visit the Amazon Bedrock website.\\nCode Walkthrough\\nLet’s learn about tool use by building an onboarding assistant that helps new hires of a fictitious company called Co1t get set up in their first week. The assistant can help answer user questions about the company, search for information from emails, and create meeting appointments.\\nAn overview of what we\\'ll cover in the code walkthrough\\nSetup\\nFirst, let\\'s install and import the necessary libraries and set up our Cohere client using the cohere SDK. To use Bedrock, we create a BedrockClient by passing the necessary AWS credentials.\\nimport cohere # Create Bedrock client via the native Cohere SDK # Contact your AWS administrator for the credentials co = cohere.BedrockClient( aws_region=\"YOUR_AWS_REGION\", aws_access_key=\"YOUR_AWS_ACCESS_KEY_ID\", aws_secret_key=\"YOUR_AWS_SECRET_ACCESS_KEY\", aws_session_token=\"YOUR_AWS_SESSION_TOKEN\", )\\nThe pre-requisite, before we can run a tool use workflow, is to set up the tools. Let\\'s create three tools:\\nsearch_faqs: A tool for searching the FAQs. For simplicity, we\\'ll not implement any retrieval logic, but we\\'ll simply pass a list of three predefined documents. In practice, we would set up a retrieval system as we did in Chapters 4, 5, and 6.\\nsearch_emails: A tool for searching the emails. Same as above, we\\'ll simply pass a list of predefined emails.\\ncreate_calendar_event: A tool for creating new calendar events. Again, for simplicity, we\\'ll only return mock successful event creations without actual implementation. In practice, we can connect to a calendar service API and implement all the necessary logic here.\\nHere, we are defining a Python function for each tool, but more broadly, the tool can be any function or service that can receive and send objects.\\ndef search_faqs(query): faqs = [ {\"text\" : \"Submitting Travel Expenses:\\\\nSubmit your expenses through our user-friendly finance tool.\"}, {\"text\" : \"Side Projects Policy:\\\\nWe encourage you to explore your passions! Just ensure there\\'s no conflict of interest with our business.\"}, {\"text\" : \"Wellness Benefits:\\\\nTo promote a healthy lifestyle, we provide gym memberships, on-site yoga classes, and health insurance.\"} ] return {\"faqs\" : faqs} def search_emails(query): emails = [ {\"from\": \"hr@co1t.com\", \"to\": \"david@co1t.com\", \"date\": \"2024-06-24\", \"subject\": \"A Warm Welcome to Co1t, David!\", \"text\": \"We are delighted to have you on board. Please find attached your first week\\'s agenda.\"}, {\"from\": \"it@co1t.com\", \"to\": \"david@co1t.com\", \"date\": \"2024-06-24\", \"subject\": \"Instructions for IT Setup\", \"text\": \"Welcome, David! To get you started, please follow the attached guide to set up your work accounts.\"}, {\"from\": \"john@co1t.com\", \"to\": \"david@co1t.com\", \"date\": \"2024-06-24\", \"subject\": \"First Week Check-In\", \"text\": \"Hi David, let\\'s chat briefly tomorrow to discuss your first week. Also, come join us for lunch this Thursday at noon to meet everyone!\"}, ] return {\"emails\" : emails} def create_calendar_event(date: str, time: str, duration: int): # You can implement any logic here return {\"is_success\": True, \"message\": f\"Created a {duration} hour long event at {time} on {date}\"} functions_map = { \"search_faqs\": search_faqs, \"search_emails\": search_emails, \"create_calendar_event\": create_calendar_event }\\nThe next step is to define the tool schemas in a format that can be passed to the Chat endpoint. The schema must contain the following fields: name, description, and parameter_definitions in the format shown below.\\nThis schema informs the LLM about what the tool does, and the LLM decides whether to use a particular tool based on it. Therefore, the more descriptive and specific the schema, the more likely the LLM will make the right tool call decisions.\\ntools = [ { \"name\": \"search_faqs\", \"description\": \"Given a user query, searches a company\\'s frequently asked questions (FAQs) list and returns the most relevant matches to the query.\", \"parameter_definitions\": { \"query\": { \"description\": \"The query from the user\", \"type\": \"str\", \"required\": True } } }, { \"name\": \"search_emails\", \"description\": \"Given a user query, searches a person\\'s emails and returns the most relevant matches to the query.\", \"parameter_definitions\": { \"query\": { \"description\": \"The query from the user\", \"type\": \"str\", \"required\": True } } }, { \"name\": \"create_calendar_event\", \"description\": \"Creates a new calendar event of the specified duration at the specified time and date. A new event cannot be created on the same time as an existing event.\", \"parameter_definitions\": { \"date\": { \"description\": \"the date on which the event starts, formatted as mm/dd/yy\", \"type\": \"str\", \"required\": True }, \"time\": { \"description\": \"the time of the event, formatted using 24h military time formatting\", \"type\": \"str\", \"required\": True }, \"duration\": { \"description\": \"the number of hours the event lasts for\", \"type\": \"float\", \"required\": True } } } ]\\nThe tool is now ready to use. We can think of a tool use system as consisting of four components:\\nThe user\\nThe application\\nThe LLM\\nThe tools\\nAt its most basic, these four components interact in a workflow through four steps:\\nStep 1: Get user message. The LLM gets the user message (via the application).\\nStep 2: Generate tool calls. The LLM makes a decision on the tools to call (if any) and generates the tool calls.\\nStep 3: Get tool results. The application executes the tools and sends the tool results to the LLM.\\nStep 4: Generate response and citations. The LLM generates the response and citations and sends them back to the user.\\nLet\\'s create a function called run_assistant to implement these steps and print out the key events and messages along the way. This function also optionally accepts the chat history as an argument to keep the state in a multi-turn conversation.\\nmodel = \"cohere.command-r-plus-v1:0\" preamble=\"\"\"## Task and Context You are an assistant who assist new employees of Co1t with their first week. You respond to their questions and assist them with their needs. Today is Monday, June 24, 2024\"\"\" def run_assistant(message, chat_history=None): if chat_history is None: chat_history = [] # Step 1: get user message print(f\"Question:\\\\n{message}\") print(\"=\"*50) # Step 2: Generate tool calls (if any) response = co.chat( message=message, model=model, preamble=preamble, tools=tools, chat_history=chat_history ) while response.tool_calls: tool_calls = response.tool_calls if response.text: print(\"Tool plan:\") print(response.text,\"\\\\n\") print(\"Tool calls:\") for call in tool_calls: print(f\"Tool name: {call.name} | Parameters: {call.parameters}\") print(\"=\"*50) # Step 3: Get tool results tool_results = [] for tc in tool_calls: tool_call = {\"name\": tc.name, \"parameters\": tc.parameters} tool_output = functions_map[tc.name](**tc.parameters) tool_results.append({\"call\": tool_call, \"outputs\": [tool_output]}) # Step 4: Generate response and citations response = co.chat( message=\"\", model=model, preamble=preamble, tools=tools, tool_results=tool_results, chat_history=response.chat_history ) chat_history = response.chat_history # Print final response print(\"RESPONSE:\\\\n\") print(response.text) print(\"=\"*50) # Print citations (if any) if response.citations: print(\"\\\\nCITATIONS:\\\\n\") for citation in response.citations: print(citation) print(\"\\\\nCITED REFERENCES:\\\\n\") for document in response.documents: print(document) return chat_history\\nSingle-Step Scenario\\nCohere supports running tool use in single-step and multi-step modes. In a single-step scenario, the model will make only one round of tool calling. Having said that, within this one round, the model can decide to call multiple tools in parallel. This can be calling the same tool multiple times, calling different tools, or a combination of both.\\nLet’s look at an example of a new hire asking about IT access and the travel expense process.\\nchat_history = run_assistant(\"Any emails about setting up IT access? Also, how do I submit travel expenses?\") \\nGiven three tools to choose from, the model is able to pick the right tools (in this case, search_faqs and search_emails) based on what the user is asking for.\\nAlso, notice that the model first generates a plan about what it should do (\"I will do ...\") before actually generating the tool call(s).\\nAdditionally, the model also generates fine-grained citations in tool use mode based on the tool results it receives, the same way we saw with RAG.\\nQuestion: Any emails about setting up IT access? Also, how do I submit travel expenses? ================================================== Tool plan: I will search for emails about setting up IT access and travel expenses. Tool calls: Tool name: search_emails | Parameters: {\\'query\\': \\'IT access\\'} Tool name: search_emails | Parameters: {\\'query\\': \\'travel expenses\\'} ================================================== RESPONSE: You have an email from *it@co1t.com* with the subject \"Instructions for IT Setup\". It contains an attached guide to set up your work accounts. To submit travel expenses, you can send an email to *expenses@co1t.com*. ================================================== CITATIONS: start=23 end=35 text=\\'*it@co1t.com\\' document_ids=[\\'search_emails:0:2:0\\', \\'search_emails:1:2:0\\'] start=54 end=80 text=\\'\"Instructions for IT Setup\\' document_ids=[\\'search_emails:0:2:0\\', \\'search_emails:1:2:0\\'] start=98 end=141 text=\\'attached guide to set up your work accounts\\' document_ids=[\\'search_emails:0:2:0\\', \\'search_emails:1:2:0\\'] CITED REFERENCES: {\\'emails\\': \\'[{\"date\":\"2024-06-24\",\"from\":\"hr@co1t.com\",\"subject\":\"A Warm Welcome to Co1t, David!\",\"text\":\"We are delighted to have you on board. Please find attached your first week\\\\\\'s agenda.\",\"to\":\"david@co1t.com\"},{\"date\":\"2024-06-24\",\"from\":\"it@co1t.com\",\"subject\":\"Instructions for IT Setup\",\"text\":\"Welcome, David! To get you started, please follow the attached guide to set up your work accounts.\",\"to\":\"david@co1t.com\"},{\"date\":\"2024-06-24\",\"from\":\"john@co1t.com\",\"subject\":\"First Week Check-In\",\"text\":\"Hi David, let\\\\\\'s chat briefly tomorrow to discuss your first week. Also, come join us for lunch this Thursday at noon to meet everyone!\",\"to\":\"david@co1t.com\"}]\\', \\'id\\': \\'search_emails:0:2:0\\', \\'tool_name\\': \\'search_emails\\'} {\\'emails\\': \\'[{\"date\":\"2024-06-24\",\"from\":\"hr@co1t.com\",\"subject\":\"A Warm Welcome to Co1t, David!\",\"text\":\"We are delighted to have you on board. Please find attached your first week\\\\\\'s agenda.\",\"to\":\"david@co1t.com\"},{\"date\":\"2024-06-24\",\"from\":\"it@co1t.com\",\"subject\":\"Instructions for IT Setup\",\"text\":\"Welcome, David! To get you started, please follow the attached guide to set up your work accounts.\",\"to\":\"david@co1t.com\"},{\"date\":\"2024-06-24\",\"from\":\"john@co1t.com\",\"subject\":\"First Week Check-In\",\"text\":\"Hi David, let\\\\\\'s chat briefly tomorrow to discuss your first week. Also, come join us for lunch this Thursday at noon to meet everyone!\",\"to\":\"david@co1t.com\"}]\\', \\'id\\': \\'search_emails:1:2:0\\', \\'tool_name\\': \\'search_emails\\'}\\nMulti-Step Scenario\\nCohere supports running more complex tasks in tool use — tasks that require tool calls to happen in a sequence. This is referred to as \"multi-step\" tool use.\\nTo illustrate this, let\\'s ask the assistant to block time for any lunch invites received in the email.\\nchat_history = run_assistant(\"Can you check if there are any lunch invites, and for those days, block an hour on my calendar from 12-1PM.\")\\nHere, we see the assistant running these steps:\\nFirst, it calls the search_emails tool to find any lunch invites, and it finds one.\\nNext, it calls the create_calendar_event tool to create an event to block the person\\'s calendar on the day mentioned in the email.\\nThis is also an example of tool use enabling a write operation instead of just a read operation that we saw with RAG.\\nQuestion: Can you check if there are any lunch invites, and for those days, block an hour on my calendar from 12-1PM. ================================================== Tool plan: I will search the user\\'s emails for lunch invites, and then create calendar events for the dates of the invites. Tool calls: Tool name: search_emails | Parameters: {\\'query\\': \\'lunch invite\\'} ================================================== Tool plan: I have found an email from John inviting David for lunch on Thursday at noon. I will now create a calendar event for this lunch. Tool calls: Tool name: create_calendar_event | Parameters: {\\'date\\': \\'06/27/24\\', \\'duration\\': 1, \\'time\\': \\'12:00\\'} ================================================== RESPONSE: I have found an email from John inviting you for lunch on Thursday at noon. I have created a calendar event for this lunch. ================================================== CITATIONS: start=16 end=31 text=\\'email from John\\' document_ids=[\\'search_emails:0:2:0\\'] start=49 end=74 text=\\'lunch on Thursday at noon\\' document_ids=[\\'search_emails:0:2:0\\'] start=93 end=107 text=\\'calendar event\\' document_ids=[\\'create_calendar_event:0:4:0\\'] CITED REFERENCES: {\\'emails\\': \\'[{\"date\":\"2024-06-24\",\"from\":\"hr@co1t.com\",\"subject\":\"A Warm Welcome to Co1t, David!\",\"text\":\"We are delighted to have you on board. Please find attached your first week\\\\\\'s agenda.\",\"to\":\"david@co1t.com\"},{\"date\":\"2024-06-24\",\"from\":\"it@co1t.com\",\"subject\":\"Instructions for IT Setup\",\"text\":\"Welcome, David! To get you started, please follow the attached guide to set up your work accounts.\",\"to\":\"david@co1t.com\"},{\"date\":\"2024-06-24\",\"from\":\"john@co1t.com\",\"subject\":\"First Week Check-In\",\"text\":\"Hi David, let\\\\\\'s chat briefly tomorrow to discuss your first week. Also, come join us for lunch this Thursday at noon to meet everyone!\",\"to\":\"david@co1t.com\"}]\\', \\'id\\': \\'search_emails:0:2:0\\', \\'tool_name\\': \\'search_emails\\'} {\\'id\\': \\'create_calendar_event:0:4:0\\', \\'is_success\\': \\'true\\', \\'message\\': \\'Created a 1 hour long event at 12:00 on 06/27/24\\', \\'tool_name\\': \\'create_calendar_event\\'}\\nConclusion\\nIn this chapter, we explored how to use Command R+ on Amazon Bedrock in an application that implements tool use. We covered the steps to create and define the tools and looked at how tool use works in single-step and multi-step scenarios.\\nIf you want to learn more LLM use cases, visit our LLMU chapter on use case patterns.\\nIn Chapter 8, we’ll look at an example of fine-tuning the Command R model using SageMaker.'),\n",
       " Document(metadata={'title': 'Fine-Tuning Cohere Command R on Amazon SageMaker', 'source': 'https://cohere.com/llmu/co-aws-finetuning'}, page_content='Fine-Tuning Cohere Command R on Amazon SageMakerQualified\\nContents\\nIntroduction\\nCode Walkthrough \\nFine-Tuning\\nInference\\nConclusion\\nWe’ll use Cohere’s Python SDK (cohere-aws) for the code examples. Follow along in this notebook.\\nIntroduction\\nFine-tuning an LLM involves taking a default model and tailoring it for a specific purpose or domain. This process enhances the model\\'s performance and adaptability by training it on a smaller, more focused dataset relevant to the desired task. This additional training refines the model\\'s capabilities and makes it more attuned to specific needs.\\nFine-tuning is recommended when you want to teach the model a new task or leverage your company\\'s unique knowledge base. Fine-tuning models is also helpful for generating a specific writing style or format.\\nFor example, a global provider of video conferencing services needed to add call summarization and action item extraction features to its video conferencing offering in response to strong customer demand.\\nThey built a solution by fine-tuning Cohere’s Command model on the nuances of transcript summarization. It accurately summarized live meetings in bullet points, including action items for call participants, and delivered it on the company’s proprietary conferencing platform.\\nImplementing an automated summarization system at a global provider of video conferencing services\\nFine-tuning of Cohere models is available in both Amazon Bedrock and Amazon SageMaker. Sample notebooks on how to fine-tune a model on both platforms are available on the cohere-aws repository.\\nIn this chapter, we’ll review one example using SageMaker to fine-tune the Command R model. At the time of writing, Command R fine-tuning is not yet available on Bedrock.\\nCode Walkthrough\\nWe’ll go through the fine-tuning steps, followed by using a model once fine-tuning is complete.\\nAn overview of what we\\'ll cover in the code walkthrough\\nFine-Tuning\\nLet\\'s install and import the necessary libraries and set up our Cohere client. Here we are using the cohere_aws SDK, which makes it easy to use Cohere\\'s models on AWS, together with AWS’s boto3 and sagemaker libraries.\\n# ! pip install --upgrade cohere-aws \\nimport os from cohere_aws import Client import boto3 import sagemaker as sage from sagemaker.s3 import S3Uploader ## Set environment variables with the AWS credentials os.environ[\\'AWS_ACCESS_KEY_ID\\'] = \"YOUR_AWS_ACCESS_KEY_ID\" os.environ[\\'AWS_SECRET_ACCESS_KEY\\'] = \"YOUR_AWS_SECRET_ACCESS_KEY\" os.environ[\\'AWS_SESSION_TOKEN\\'] = \"YOUR_AWS_SESSION_TOKEN\" \\nThe next step is to define the model package Amazon Resource Names (ARN) for the Command R model. The ARN is an identifying string for a SageMaker resource, and it varies between the regions where a resource is available.\\nHere, we define the Cohere package for the Command R model and map the model package against each region, which gives the complete ARN for each region.\\nregion = boto3.Session().region_name cohere_package = \"cohere-command-r-ft-v-0-1-2-bae2282f0f4a30bca8bc6fea9efeb7ca\" # Mapping for algorithms algorithm_map = { \"us-east-1\": f\"arn:aws:sagemaker:us-east-1:865070037744:algorithm/{cohere_package}\", \"us-east-2\": f\"arn:aws:sagemaker:us-east-2:057799348421:algorithm/{cohere_package}\", \"us-west-2\": f\"arn:aws:sagemaker:us-west-2:594846645681:algorithm/{cohere_package}\", \"eu-central-1\": f\"arn:aws:sagemaker:eu-central-1:446921602837:algorithm/{cohere_package}\", \"ap-southeast-1\": f\"arn:aws:sagemaker:ap-southeast-1:192199979996:algorithm/{cohere_package}\", \"ap-southeast-2\": f\"arn:aws:sagemaker:ap-southeast-2:666831318237:algorithm/{cohere_package}\", \"ap-northeast-1\": f\"arn:aws:sagemaker:ap-northeast-1:977537786026:algorithm/{cohere_package}\", \"ap-south-1\": f\"arn:aws:sagemaker:ap-south-1:077584701553:algorithm/{cohere_package}\", } if region not in algorithm_map.keys(): raise Exception(f\"Current boto3 session region {region} is not supported.\") arn = algorithm_map[region]\\nNext, we need to upload the dataset required for fine-tuning. The expected format for fine-tuning the Command R model via the Chat endpoint is a jsonl file, where each JSON object is a conversation containing a series of messages.\\nA System message in the beginning that acts as the preamble that guides the whole conversation\\nMultiple pairs of User and Chatbot messages representing the conversation that takes place between a human user and a chatbot\\nHere’s an example of what a data point looks like:\\n{\\'messages\\': [{\\'role\\': \\'System\\', \\'content\\': \\'You are a chatbot trained to answer to my every question.\\' }, {\\'role\\': \\'User\\', \\'content\\': \\'Hello\\' }, {\\'role\\': \\'Chatbot\\', \\'content\\': \\'Greetings! How can I help you?\\' }, {\\'role\\': \\'User\\', \\'content\\': \\'What makes a good running route?\\' }, {\\'role\\': \\'Chatbot\\', \\'content\\': \\'A sidewalk-lined road is ideal so that you’re up and off the road away from vehicular traffic.\\' } ] } \\nWe’ll use the ScienceQA dataset for this demonstration, with the pre-formatted versions of the training and evaluation sets available. Here are two example data points from the dataset:\\n{\"messages\": [{\"role\": \"User\", \"content\": \"Context: Fish live underwater. They have fins, not limbs. A tiger shark is an example of a fish.\\\\nQuestion: Select the fish below.\\\\nOptions:(A) black howler (B) cardinalfish\\\\nAnswer:\"}, {\"role\": \"Chatbot\", \"content\": \"B\"}]} \\n{\"messages\": [{\"role\": \"User\", \"content\": \"Context: Select the best estimate.\\\\nQuestion: How long is a guitar?\\\\nOptions:(A) 1 centimeter (B) 1 meter (C) 1 kilometer (D) 1 millimeter\\\\nAnswer:\"}, {\"role\": \"Chatbot\", \"content\": \"B\"}]} \\nA few points to note:\\nThe evaluation dataset is optional. If no evaluation dataset is provided, the training dataset will be auto-split into training and evaluation datasets at an 80:20 ratio.\\nThe dataset must contain at least one example. If an evaluation dataset is absent, the training dataset must contain at least two examples.\\nHowever, we recommend using a dataset that contains at least 100 examples. Larger datasets are more likely to yield high-quality fine-tunes.\\nWe can now upload the training and evaluation datasets to an S3 bucket using the S3Uploader.upload method.\\ns3_data_dir = f\"s3://YOUR_S3_DATA_PATH\" # DO NOT add a trailing slash at the end \\nsess = sage.Session() train_dataset = S3Uploader.upload(\"./sample_finetune_scienceQA_train.jsonl\", s3_data_dir, sagemaker_session=sess) eval_dataset = S3Uploader.upload(\"./sample_finetune_scienceQA_eval.jsonl\", s3_data_dir, sagemaker_session=sess)\\nWe also need to specify a directory on S3 where fine-tuned models should be stored.\\ns3_models_dir = f\"s3://YOUR_S3_MODEL_PATH\" # DO NOT add a trailing slash at the end \\nNext, we create a Cohere client and define the set of hyperparameters to be used during fine-tuning. Hyperparameters are optional, but they provide more flexibility to developers who want to further optimize the fine-tuning process.\\nDevelopers can define the following hyperparameters:\\ntrain_epochs: The maximum number of training steps\\nlearning_rate: The initial learning rate to be used during training\\ntrain_batch_size: The batch size during training\\nearly_stopping_enabled: Enables early stopping\\nearly_stopping_patience: Stops training if the loss metric does not improve beyond the early_stopping_threshold for this many rounds of evaluation\\nearly_stopping_threshold: How much the loss must improve to prevent early stopping\\nco = Client(region_name=region) # Example of how to pass hyperparameters to the fine-tuning job train_parameters = { \"train_epochs\": 1, \"early_stopping_patience\": 2, \"early_stopping_threshold\": 0.001, \"learning_rate\": 0.01, \"train_batch_size\": 16, } \\nWe are now ready to start the fine-tuning process. To do this, we use the create_finetune method and specify the required arguments, as follows.\\nfinetune_name = \"test-scienceQA\" co.create_finetune(arn=arn, name=finetune_name, train_data=train_dataset, eval_data=eval_dataset, s3_models_dir=s3_models_dir, instance_type=\"ml.p4de.24xlarge\", training_parameters=train_parameters )\\nA successful fine-tuning process will return a response like this:\\nINFO:sagemaker:Creating training-job with name: cohere-command-R-ft-v-0-1-1-2024-05-07-04-45-52-979 2024-05-07 04:45:53 Starting - Starting the training job... 2024-05-07 04:46:01 Pending - Training job waiting for capacity... 2024-05-07 04:46:39 Pending - Preparing the instances for training........................ 2024-05-07 04:50:49 Downloading - Downloading input data........................................................................................................................................................................ 2024-05-07 05:18:40 Training - Training image download completed. Training in progress.........INFO:root:Loading weights from /opt/ml/additonals3data INFO:root:[Rank 7]Finished loading all variables in 89.23 s INFO:root:[Rank 6]Finished loading all variables in 89.34 s INFO:root:[Rank 2]Finished loading all variables in 90.56 s INFO:root:[Rank 5]Finished loading all variables in 89.63 s INFO:root:[Rank 1]Finished loading all variables in 91.93 s INFO:root:[Rank 0]Finished loading all variables in 89.76 s INFO:root:[Rank 4]Finished loading all variables in 90.22 s INFO:root:[Rank 3]Finished loading all variables in 94.28 s INFO:root:Finished assigning 646 variables from checkpoint INFO:root:Trainable params: 194560, Non trainable params: 4636421376 INFO:root:Weights loaded from /opt/ml/additonals3data INFO:root:After removing empty, NA and duplicate documents: 5262 INFO:root:After removing empty, NA and duplicate documents: 1901 INFO:root:Finished step 1, train loss: 0.714844, step time: 10.336927s INFO:root:Finished step 2, train loss: 0.204102, step time: 6.781313s INFO:root:Finished step 3, train loss: 0.208984, step time: 6.751096s ... (truncated for brevity) INFO:root:Finished step 159, train loss: 0.072754, step time: 6.774719s INFO:root:Finished step 160, train loss: 0.032715, step time: 6.775016s INFO:root:Exporting finished. INFO:root:Model weights exported at /opt/ml/model/finetune/export_test-scienceQA in 135.482641s 2024-05-07 05:57:48 Uploading - Uploading generated training model 2024-05-07 05:59:24 Completed - Training job completed Training seconds: 4114 Billable seconds: 4114 \\nInference\\nWe can now use the fine-tuned model for inference. With SageMaker, we’ll need to create an endpoint via an AWS instance. The marketplace listing provides more details on the recommended instance type for a particular model, including pricing.\\nThe steps are the same as the SageMaker example in an earlier chapter, with a few additional arguments. In particular, s3_models_dir defines the location where the fine-tuned model is stored.\\nendpoint_name=\"command-finetune-test-scienceQA\" co.create_endpoint(arn=arn, endpoint_name=endpoint_name, s3_models_dir=s3_models_dir, recreate=True, instance_type=\"ml.p4de.24xlarge\") \\nThis returns the following response:\\nFound one fine-tuned model: s3://YOUR_S3_MODEL_PATH/test-scienceQA.tar.gz ......... --------------! \\nWe can then perform inference on the endpoint. For this, we call the Chat endpoint with the user message and get a response generated by the model.\\nmessage = \"Select the best estimate.\\\\nQuestion: How long is a guitar?\\\\nOptions:(A) 32 feet (B) 32 yards (C) 32 inches (D) 32 miles\\\\nAnswer:\" result = co.chat(message=message, return_prompt=True) print(result)\\ncohere.Chat { response_id: ba6be7d5-0509-4038-adfb-7de695c264b0 generation_id: 3b509b4f-b6b3-4eee-ae5d-ccb5e46f8822 text: C chat_history: [{\\'role\\': \\'USER\\', \\'message\\': \\'Select the best estimate.\\\\nQuestion: How long is a guitar?\\\\nOptions:(A) 32 feet (B) 32 yards (C) 32 inches (D) 32 miles\\\\nAnswer:\\'}, {\\'role\\': \\'CHATBOT\\', \\'message\\': \\'C\\'}] preamble: None finish_reason: COMPLETE token_count: None tool_calls: None citations: None documents: None search_results: None search_queries: None is_search_required: None } \\nWe can test the fine-tuning performance on the evaluation dataset, as follows:\\nimport json from tqdm import tqdm total = 0 correct = 0 for line in tqdm(open(\\'./sample_finetune_scienceQA_eval.jsonl\\').readlines()): total += 1 question_answer_json = json.loads(line) question = question_answer_json[\"messages\"][0][\"content\"] answer = question_answer_json[\"messages\"][1][\"content\"] model_ans = co.chat(message=question, temperature=0, k=1).text if model_ans == answer: correct +=1 \\nprint(f\"Accuracy of finetuned model is %.4f\" % (correct / total)) \\nIn our case, we get 83.10% accuracy.\\n100%|██████████| 2095/2095 [01:35<00:00, 21.83it/s] Accuracy of finetuned model is 0.8310 \\nImportant Note: You will continue to incur charges as long as an endpoint is running, so remember to delete it when your usage ends.\\nco.delete_endpoint() co.close() \\nConclusion\\nFine-tuning an LLM is a powerful technique for customizing and enhancing its performance for specific tasks and domains. Businesses can use fine-tuning to improve accuracy and address unique scenarios specific to their domain. In this chapter, we explored how to fine-tune Cohere Command on Amazon SageMaker.\\nThis chapter concludes our module, Cohere on AWS, and we hope you’ve found it useful and informative. We encourage you to continue your learning by exploring more modules at Cohere’s LLM University.'),\n",
       " Document(metadata={'title': 'Semantic Search', 'source': 'https://cohere.com/llmu/introduction-semantic-search'}, page_content='Semantic SearchQualified\\nThis chapter uses the same notebook as the previous chapter.\\nNote: This chapter covers the basics of semantic search. If you want to explore this topic further, we have a dedicated LLMU module on semantic search.\\nWe deal with unstructured text data on a regular basis, and one of the common needs is to search for information from a vast repository. A common approach is keyword-matching, but the problem with this is that the results are limited to the exact query entered.\\nThis is where we can utilize text embeddings. As you learned in the previous chapter, embeddings can capture the meaning of a piece of text beyond keyword-matching.\\nIn this chapter, you\\'ll learn how to use embeddings to build a search capability that surfaces relevant information based on the semantic meaning of a query.\\nStep-by-Step Guide\\nLet’s use the same 9 data points from the previous chapter and pretend that those make up a list of Frequently Asked Questions (FAQ). Whenever a new query comes in, we want to match that query to the closest FAQ so we can provide the most relevant answer. Here is the list again:\\n- which airlines fly from boston to washington dc via other cities - show me the airlines that fly between toronto and denver - show me round trip first class tickets from new york to miami - i\\'d like the lowest fare from denver to pittsburgh - show me a list of ground transportation at boston airport - show me boston ground transportation - of all airlines which airline has the most arrivals in atlanta - what ground transportation is available in boston - i would like your rates between atlanta and boston on september third - which airlines fly between boston and pittsburgh \\nLet’s say a person enters the query “How can I find a taxi or a bus when the plane lands?”. Note that the “taxi” and \"bus\" keywords don\\'t exist anywhere in our FAQ, so let’s see what results we get with semantic search.\\nImplementation-wise, there are many ways we can approach this. And in our case, we use cosine similarity to compare the embeddings of the search query with those from the FAQ and find the most similar ones.\\nStep 1: Embed the Documents\\nThe first step is to turn the documents into embeddings. We embed each inquiry by calling Cohere’s Embed endpoint with co.embed(). It takes in texts as input and returns embeddings as output. We supply three parameters:\\ntexts: The list of texts you want to embed\\nmodel: The model to use to generate the embedding. At the time of writing, there are four models available\\ninput_type — Specifies the type of document to be embedded. At the time of writing, there are four options:\\nsearch_document: For documents against which search is performed\\nsearch_query: For query documents\\nclassification: For when the embeddings will be used as an input to a text classifier\\nclustering: For when you want to cluster the embeddings\\ndef get_embeddings(texts, model=\\'embed-english-v3.0\\', input_type=\"search_document\"): output = co.embed( model=model, input_type=input_type, texts=texts) return output.embeddings df[\\'query_embeds\\'] = get_embeddings(df[\\'query\\'].tolist()) \\nStep 2: Embed the Search Query\\nNext, we embed the query using the same get_embeddings() function. But now we set search_query as the input_type because we\\'re now embedding the search query.\\n# Define new query new_query = \"How can I find a taxi or a bus when the plane lands?\" # Get embeddings of the new query new_query_embeds = get_embeddings([new_query], input_type=\"search_query\")[0] \\nStep 3: Perform Search\\nNext, we create a function get_similarity() that uses cosine similarity to determine how similar each of the documents is to the query.\\n# Calculate cosine similarity between the search query and existing queries def get_similarity(target, candidates): # Turn list into array candidates = np.array(candidates) target = np.expand_dims(np.array(target),axis=0) # Calculate cosine similarity sim = cosine_similarity(target, candidates) sim = np.squeeze(sim).tolist() sort_index = np.argsort(sim)[::-1] sort_score = [sim[i] for i in sort_index] similarity_scores = zip(sort_index,sort_score) # Return similarity scores return similarity_scores # Get the similarity between the search query and existing queries similarity = get_similarity(new_query_embeds, embeds[:sample]) \\nWe\\'ll then view the documents in decreasing order of similarity.\\n# View the top 5 articles print(\\'Query:\\') print(new_query,\\'\\\\n\\') print(\\'Most Similar Documents:\\') for idx, sim in similarity: print(f\\'Similarity: {sim:.2f};\\', df.iloc[idx][\\'query\\']) \\nBelow are the results, showing the FAQs with their similarity score (ranging from 0 to 1; higher scores are better). The top-3 ranked FAQs we get are inquiries about ground transportation, which are very relevant considering the other options. Notice that they don\\'t contain the keywords “taxi” or \"bus\", and the search query doesn\\'t contain the keywords “ground transportation”. But they are the most similar in meaning compared to the rest, and their embeddings capture this.\\nQuery: How can I find a taxi or a bus when the plane lands? Most Similar Documents: Similarity: 0.37; show me a list of ground transportation at boston airport Similarity: 0.36; what ground transportation is available in boston Similarity: 0.33; show me boston ground transportation Similarity: 0.27; show me the airlines that fly between toronto and denver Similarity: 0.25; which airlines fly from boston to washington dc via other cities Similarity: 0.24; of all airlines which airline has the most arrivals in atlanta Similarity: 0.18; i\\'d like the lowest fare from denver to pittsburgh Similarity: 0.17; show me round trip first class tickets from new york to miami Similarity: 0.17; i would like your rates between atlanta and boston on september third \\nStep 4: Visualize the Results in a 2D Plot\\nWe can also plot this new query on a 2D plot as we did earlier. And we see that the query is located closest to the FAQs about ground transportation.\\nThe query about \"a taxi or a bus\" is located closest to documents about ground transportation\\nConclusion\\nIn this chapter you learned how to use embedding and similarity to build a semantic search model. There are many more applications of embeddings, which you\\'ll learn in the following chapters!'),\n",
       " Document(metadata={'title': 'Text Clustering', 'source': 'https://cohere.com/llmu/clustering-with-embeddings'}, page_content='Text ClusteringQualified\\nThis chapter uses the same notebook as the previous chapter.\\nAs the amount of unstructured text data increases, organizations will want to be able to derive an understanding of its contents. One example would be to discover underlying topics in a collection of documents so we can explore trends and insights. Another could be for businesses to segment customers based on preferences and activity.\\nThese kinds of tasks fall under a category called clustering. In machine learning, clustering is a process of grouping similar documents into clusters. It is used to organize a large number of documents into a smaller number of groups. And it lets us discover emerging patterns in a collection of documents without us having to specify much information beyond supplying the data.\\nIn this chapter, you will learn how to use embeddings to partition a text dataset into distinct clusters of semantically related sentences.\\nStep-by-Step Guide\\nLet’s look at an example using the same 9 data points.\\n- which airlines fly from boston to washington dc via other cities - show me the airlines that fly between toronto and denver - show me round trip first class tickets from new york to miami - i\\'d like the lowest fare from denver to pittsburgh - show me a list of ground transportation at boston airport - show me boston ground transportation - of all airlines which airline has the most arrivals in atlanta - what ground transportation is available in boston - i would like your rates between atlanta and boston on september third - which airlines fly between boston and pittsburgh \\nStep 1: Embed the Text for Clustesring\\nWe embed the documents using the same get_embeddings() function as before, but now we set input_type=\"clustering\" because we\\'ll use the embeddings for clustering.\\n# Embed the text for clustering df[\\'clustering_embeds\\'] = get_embeddings(df[\\'query\\'].tolist(), input_type=\"clustering\") embeds = np.array(df[\\'clustering_embeds\\'].tolist()) \\nStep 2: Cluster the Embeddings\\nImplementation-wise, we use the K-means algorithms to cluster these data points (if you\\'d like to learn more about it, please check out this video about the K-means algorithm).\\nOther than providing the embeddings, the only other key information we need to provide for the algorithm is the number of clusters we want to find. This is normally larger in actual applications, but since our dataset is small, we’ll set the number of clusters to 2.\\n# Pick the number of clusters n_clusters = 2 # Cluster the embeddings kmeans_model = KMeans(n_clusters=n_clusters, random_state=0) classes = kmeans_model.fit_predict(embeds).tolist() # Store the cluster assignments df_clust = df_pc2.copy() df_clust[\\'cluster\\'] = (list(map(str,classes))) \\nStep 3: Visualize the Results in a 2D Plot\\nThe plot below shows the clusters that the algorithm returned. It looks to be spot on, where we have one cluster related to airline information and one cluster related to ground service information.\\nClustering results with 2 clusters\\nConclusion\\nIn this chapter, you learned how to cluster a dataset of sentences, and you observed that each cluster corresponds to a particular topic.'),\n",
       " Document(metadata={'title': 'Text Classification', 'source': 'https://cohere.com/llmu/text-classification'}, page_content='Text ClassificationQualified\\nIn previous chapters, we saw how to use embeddings to enable use cases such as semantic search, clustering, and topic modeling. All of these are \"unsupervised learning algorithms\", where we don’t know the number of groups and what they are.\\nBut what if we already know the groups (or classes) that we want to group our documents into? In this chapter, you\\'ll learn about classification, which is a \"supervised learning algorithm\" where we know the specific classes we want to use.\\nLike all machine learning models, classification models need to be evaluated in order to improve and measure their performance. In this chapter, you\\'ll also learn about four different ways to evaluate classification models.\\nTypes of Text Classification\\nA classification task falls under one of these two categories:\\nBinary classification, where the number of classes is two. Here are some examples:\\nA spam classifier could assign emails one one of two classes: \"Spam\" or \"Not spam\".\\nAn online forum could use a toxicity classifier to assist with content moderation by classifying posts as \"Neutral\" or \"Toxic\".\\nMulti-class classification, where the number of classes is more than two. Here are some examples:\\nA restaurant could use a sentiment classifier to categorize customer reviews as \"Positive\", \"Neutral\", or \"Negative\".\\nFor businesses that deal with a large volume of inbound communication, a topic classifier could assign the message to the relevant department (one of \"Customer Service\", \"Returns\", or \"Shipping\") for resolution. We could assign just one label to each message, or permit multiple labels. A classifier that allows multiple labels is a multi-label classifier.\\nA company could build a chatbot intent classifier to detect the intent behind a user\\'s message (one of \"Greeting\", \"Shipping and handling policy\", \"Start return or exchange\", \"Track order\", or \"Bye\"). Intent classifiers are broader than topic classifiers, as they include intents like “Greeting” or \"Bye\" that enable them to respond appropriately when a user says “hi” or “bye”, for example.\\nExamples of binary and multi-class classification\\nEvaluation Metrics\\nTo understand how to evaluate classification models, we\\'ll work with an email classification example. Our task is to classify a list of emails into one of two classes: Spam or Not Spam. We’ll represent Spam with the integer 1 (or Positive) and Not Spam with 0 (or Negative).\\nThe dataset contains 20 email titles. We put each data point through a binary classifier to get the predicted class and then compare it with its actual class.\\nThe classifier returns the following outcome:\\nThe email binary classification dataset with the actual and predicted classes\\nWe\\'ll cover 4 of the most important ways to evaluate classification models: Accuracy, Precision, Recall, and F1. In each case, we\\'ll calculate the performance of this classifier as an example.\\nAccuracy\\nThe most straightforward way to measure a classifier’s performance is using the Accuracy metric. Here, we compare the actual and predicted class of each data point, and each match counts for one correct prediction.\\nAccuracy is then given as the number of correct predictions divided by the total number of predictions. From the spam classifier output above, we have 15 correct predictions and 5 incorrect predictions, which gives us an Accuracy of 75%.\\nAccuracy calculation\\nAccuracy is often used as the measure of classification performance because it is simple to compute and easy to interpret. However, it can turn out to be misleading in some cases.\\nThis is especially true when dealing with imbalanced data, a scenario when certain classes contain way more data points than the others.\\nLet\\'s go back to our dataset to understand this. Notice that if the classifier had not been learning anything and was simply classifying all the outputs to be 0 (Not Spam), we would get 17 out of 20 correct classifications, which translates to a very high Accuracy of 85%! Clearly, something isn’t right.\\nIf you haven’t noticed yet, our dataset is indeed imbalanced. We have way more emails that are not spam than emails that are spam.\\nThe issue of imbalanced datasets is common in the real wo\\xad\\xadrld.\\xad For this, there must be a better way to measure a classifier’s performance than using Accuracy alone.\\nConfusion Matrix\\nThe other three metrics can provide a more balanced view of a classifier’s true performance. But before we can see them in action, we need to first understand the Confusion Matrix.\\nThe Confusion Matrix takes the classification results and groups them into four categories:\\nTrue Positive (TP): when both the actual and predicted values are 1.\\nTrue Negative (TN): when both the actual and predicted values are 0.\\nFalse Positive (FP): when the actual value is 0 but the predicted value is 1.\\nFalse Negative (FN): when the actual value is 1 but the predicted value is 0.\\nRecall that in our case, we refer to the event we want to capture (1 - Spam) as Positive and non-event (0 - Not Spam) as Negative.\\nThe Confusion Matrix for binary classification is a 2-by-2 matrix, where each column represents one class, as follows:\\nThe Confusion Matrix\\nApplied to our dataset, we get the following values:\\nTrue Positive (TP): 1\\nTrue Negative (TN): 14\\nFalse Positive (FP): 3\\nFalse Negative (FN): 2\\nWe can populate these values in the Confusion Matrix, as follows:\\nPopulating the classification outcome in the Confusion Matrix\\nWe can also map the Confusion Matrix to the Accuracy formula that we saw earlier, as follows:\\nAccuracy calculation via the Confusion Matrix\\nWe can now see via this matrix why Accuracy can sometimes hide the nuance of imbalanced datasets. The reason is in these kinds of datasets, the True Negative category dominates, diluting the effect of the rest.\\nSo even if the classifier were to perform poorly in the other three categories, its Accuracy will still look good, masking its deficiencies.\\nThe True Negatives dominate the Accuracy calculation\\nPrecision\\nLet’s now see how the other three metrics can provide a more balanced view of a classifier’s performance. Let’s start with Precision.\\nPrecision is calculated as follows:\\nPrecision calculation\\nNotice what just happened? Now, the True Negatives are not even part of the calculation. Precision focuses on the True Positives and False Positives, therefore providing a representation that may be missed via Accuracy. Whereas Accuracy looked impressive at 75% earlier, we now see that Precision is quite far off at 25%.\\nRecall\\nRecall uses the same principle as Precision, except the focus is now on the False Negatives instead of the False Positives. Again, the True Negatives are not part of the consideration.\\nRecall is calculated as follows:\\nRecall calculation\\nBetween Precision and Recall though, there is a tradeoff. It is hard to optimize for both simultaneously as optimizing for the False Positives (thereby improving Precision) comes at the expense of the False Negatives (thereby deteriorating Recall), and vice versa.\\nWhich then brings the question: which metric should you prioritize—Precision or Recall?\\nThe answer is that it depends on the nature of your task. Let’s see why.\\nSuppose the spam classifier achieved high Precision and low Recall (Scenario A). This would result in fewer non-spam emails flagged as spam (False Positive). But this would also mean more of the actual spam emails went undetected (False Negative).\\nConversely, if the classifier achieved high Recall and low Precision (Scenario B), there would be fewer undetected spam emails (False Negative), but it comes at the expense of more non-spam emails being flagged as spam (False Positive).\\nFor a spam classification task, it’s probably more desirable to avoid important emails being moved into the spam folder than to have the occasional spam emails going into the inbox. So for this task, we will want to prioritize Precision over Recall.\\nTwo example scenarios showing the Precision-Recall tradeoff\\nF1 Score\\nWhat if both Precision and Recall are important to you and you need the classifier to do well in both? The answer is, to use the final metric of the four—F1.\\nF1 takes into consideration both Precision and Recall. It is calculated as follows:\\nF1 score calculation\\nF1 provides the balance between Precision and Recall. Now, there are different versions of the ‘F-score’ family if you want to go for it, for example assigning bigger weight to either Precision or Recall, but F1 is a good enough option to get started.\\nF1 provides a balance between Precision and Recall\\nMulti-Class Classification\\nSo we have seen how the four metrics work in a binary classification case. But how do we compute them when the number of classes is more than two (i.e., multi-class classification)? They follow the same principle with some slight tweaks.\\nLet’s say we have the task of classifying a list of eCommerce customer emails into one of three classes: Shipping, Returns, and Tracking. We’ll represent each class with integer values of 0, 1, and 2 respectively.\\nThe dataset contains 15 email titles. We put each data point through a multi-class classifier to get the predicted class and then compare it with its actual class.\\nThe classifier returns the following outcome:\\nThe email multi-class classification dataset with the actual and predicted classes\\nFirst, Accuracy. The calculation is the same as we did with binary classification— the number of correct predictions divided by the total number of predictions. For our dataset, there are 10 correct predictions and 5 incorrect predictions, which give us an Accuracy of 67%.\\nNext, to compute Precision, Recall, and F1, we’ll build the Confusion Matrix.\\nSince we have three classes, the matrix now becomes a 3-by-3 matrix, each column representing one class. Applied to our dataset, we get the following matrix:\\nPopulating the classification outcome in the Confusion Matrix\\nPrecision and Recall are calculated in the same way as we did with binary classification. The only difference now is each class will have its own set of Precision and Recall values.\\nLet’s take class 0 (Shipping) as an example. Here, we use Positive to refer to the Shipping class and Negative to refer to all the other classes (Not Shipping).\\nPrecision for class 0 (Shipping) is then calculated as follows:\\nPrecision calculation for class 0 - Shipping\\nAnd Recall for class 0 (Shipping) is calculated as follows:\\nRecall calculation for class 0 - Shipping\\nEach class will have its own F1 too. F1 for class 0 (Shipping) is calculated as follows:\\nF1 calculation for class 0 - Shipping\\nGoing through all classes in our dataset, this is what Recall, Precision, and F1 look like:\\nPrecision, Recall, and F1 for all three classes\\nNow, what if the number of classes started to get really big? This table would become huge and it will be harder to get a snapshot of the performance. So we may want to still have a single value for each of these metrics.\\nThere are a few approaches we can take, and one common option is to take the average of all the classes. This is also called the Macro-Average, and we apply it to each of Precision, Recall, and F1 as follows:\\nMacro-averaged calculation of Precision, Recall, and F1\\nConclusion\\nIn this chapter, you learned what classification models are, and how they can be used for numerous applications. You also took a deep dive into the metrics used for evaluating classification models, which are accuracy, precision, recall, and F1-score. These will be necessary to successfully build classification models in the future.'),\n",
       " Document(metadata={'title': 'Few-Shot Classification', 'source': 'https://cohere.com/llmu/few-shot-classification'}, page_content='Few-Shot ClassificationQualifiedIntercom Live Chat\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nIn the text classification space, a trend is emerging where developers and teams are leveraging large language models (LLMs) when building an AI-based classifier system. This is opposed to building a system from scratch on their own, which first, requires the team to have the know-how in machine learning and engineering, and second, requires a huge amount of labeled training data to build a working solution.\\nWith LLMs, instead of having to prepare thousands of training data points, you can get up and running with just a handful of examples per class, called few-shot classification. Think about the sudden drop in the associated costs, time, and effort to collect and build a training dataset. This means that more teams can now think about deploying their own text classification systems, whereas they would not have considered it before.\\nLet\\'s see how to do this with an example. We\\'ll classify the sentiment of text into a number of classes, say, positive, negative, or neutral. This is useful for applications like analyzing social media content or categorizing product feedback.\\nFor example, a human can easily tell you that “Hello, World! What a beautiful day” conveys a positive sentiment, but let’s see if our models can do that too. And while we’re at it, let’s try classifying other phrases that you might find on social media.\\nIn this chapter, you\\'ll learn to classify text based on sentiment using Cohere\\'s Classify endpoint.\\nStep-by-Step Guide\\nTo set up, we first import several tools.\\nimport cohere from cohere import ClassifyExample \\nWe also create a Cohere client.\\nco = cohere.Client(\"COHERE_API_KEY\") # Your Cohere API key \\nStep 1: Prepare Examples and Input\\nA typical machine learning model requires many training examples to perform text classification, but with the Classify endpoint, you can get started with as few as two examples per class. You need to prepare the following:\\nExamples\\nThese are the training examples we give the model to show the output we want it to generate.\\nEach example contains the text itself and the corresponding label, or class.\\nThe minimum number of examples required is two per class.\\nYou can have as many classes as possible. If you are classifying text into two classes, that means you need a minimum of four examples, and if you have three, that means you need six examples, and so on.\\nInputs\\nThese are the list of text pieces you’d like to classify.\\nThe examples and inputs to a classifier\\nOur sentiment analysis classifier has three classes with five examples each: “Positive” for a positive sentiment, “Negative” for a negative sentiment, and “Neutral” for a neutral sentiment. The code looks as follows.\\nThe examples:\\nexamples = [ClassifyExample(text=\"I’m so proud of you\", label=\"positive\"), ClassifyExample(text=\"What a great time to be alive\", label=\"positive\"), ClassifyExample(text=\"That’s awesome work\", label=\"positive\"), ClassifyExample(text=\"The service was amazing\", label=\"positive\"), ClassifyExample(text=\"I love my family\", label=\"positive\"), ClassifyExample(text=\"They don\\'t care about me\", label=\"negative\"), ClassifyExample(text=\"I hate this place\", label=\"negative\"), ClassifyExample(text=\"The most ridiculous thing I\\'ve ever heard\", label=\"negative\"), ClassifyExample(text=\"I am really frustrated\", label=\"negative\"), ClassifyExample(text=\"This is so unfair\", label=\"negative\"), ClassifyExample(text=\"This made me think\", label=\"neutral\"), ClassifyExample(text=\"The good old days\", label=\"neutral\"), ClassifyExample(text=\"What\\'s the difference\", label=\"neutral\"), ClassifyExample(text=\"You can\\'t ignore this\", label=\"neutral\"), ClassifyExample(text=\"That\\'s how I see it\", label=\"neutral\")] \\nThe inputs (we have twelve in this example):\\ninputs = [\"Hello, world! What a beautiful day\", \"It was a great time with great people\", \"Great place to work\", \"That was a wonderful evening\", \"Maybe this is why\", \"Let\\'s start again\", \"That\\'s how I see it\", \"These are all facts\", \"This is the worst thing\", \"I cannot stand this any longer\", \"This is really annoying\", \"I am just plain fed up\"] \\nStep 2: Generate Predictions\\nWith the Classify endpoint, setting up the model is quite straightforward. The main thing to do is to define the model type. For the Classify endpoint, we need to use an embedding model, and we\\'ll useembed-english-v3.0.\\nPutting everything together with the Classify endpoint looks like the following:\\ndef classify_text(inputs, examples): \"\"\" Classifies a list of input texts given the examples Arguments: model (str): identifier of the model inputs (list[str]): a list of input texts to be classified examples (list[Example]): a list of example texts and class labels Returns: classifications (list): each result contains the text, labels, and conf values \"\"\" # Classify text by calling the Classify endpoint response = co.classify( model=\\'embed-english-v3.0\\', inputs=inputs, examples=examples) classifications = response.classifications return classifications # Classify the inputs predictions = classify_text(inputs, examples) \\nTogether with the predicted class, the endpoint also returns the confidence value of the prediction (between 0 and 1). These confidence values are split among the classes, in this case three, in which the values add up to a total of 1. The classifier then selects the class with the highest confidence value as the “predicted class.” A high confidence value for the predicted class therefore indicates that the model is very confident of its prediction, and vice versa.\\nHere’s a sample output returned:\\nInput: Hello, world! What a beautiful day Prediction: positive Confidence: 0.84 ---------- Input: It was a great time with great people Prediction: positive Confidence: 0.99 ---------- Input: Great place to work Prediction: positive Confidence: 0.91 ---------- Input: That was a wonderful evening Prediction: positive Confidence: 0.96 ---------- Input: Maybe this is why Prediction: neutral Confidence: 0.70 ---------- Input: Let\\'s start again Prediction: neutral Confidence: 0.83 ---------- Input: That\\'s how I see it Prediction: neutral Confidence: 1.00 ---------- Input: These are all facts Prediction: neutral Confidence: 0.78 ---------- Input: This is the worst thing Prediction: negative Confidence: 0.93 ---------- Input: I cannot stand this any longer Prediction: negative Confidence: 0.93 ---------- Input: This is really annoying Prediction: negative Confidence: 0.99 ---------- Input: I am just plain fed up Prediction: negative Confidence: 1.00 ---------- \\nThe model returned a Positive sentiment for “Hello, world! What a beautiful day,” which is what we would expect! And the predictions for all the rest look spot on too.\\nConclusion\\nIn this chapter, you used Cohere\\'s Classify endpoint for sentiment analysis with a simple dataset. This is just one example, but you can classify any kind of text into any number of possible classes according to your needs.'),\n",
       " Document(metadata={'title': 'Fine-Tuning for Classification', 'source': 'https://cohere.com/llmu/fine-tuning-for-classification'}, page_content='Fine-Tuning for ClassificationQualified\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nIn the previous chapter, you used few-shot classification for a sentiment analysis task and got decent results with a small dataset.\\nLet’s say you actually have a decent amount of training dataset at your disposal. Then, you can leverage these additional data points to build a potentially better-performing classifier through a process known as fine-tuning.\\nFine-tuning is a powerful concept, and here’s the reason. With fine-tuning, you can actually alter the model itself and customize it according to your task. This means that what you get is a custom model that’s fine-tuned to excel at a specific task, and potentially can outperform the few-shot approach from the previous chapter.\\nIn this chapter, you\\'ll learn how to fine-tune a model for classification.\\nStep-by-Step Guide\\nTo set up, we first import several tools.\\nimport os import json import numpy as np import pandas as pd from cohere import ClassifyExample from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.metrics import f1_score \\nWe also import the Cohere module and create a client.\\nimport cohere co = cohere.Client(\"COHERE_API_KEY\") # Your Cohere API key \\nStep 1: Prepare the Dataset\\nWe\\'ll use the Airline Travel Information System (ATIS) intent classification dataset [source]. For demonstration purposes, we’ll take just a small portion of the dataset: 1,000 data points in total.\\n# Load the dataset to a dataframe df = pd.read_csv(\\'https://raw.githubusercontent.com/cohere-ai/notebooks/main/notebooks/data/atis_subset.csv\\', names=[\\'query\\',\\'intent\\']) \\nThe first thing we need is to create a training dataset, to be used for building the classifier, and a test dataset, to be used for testing the classifier performance. We will use 800 and 200 data points for these datasets, respectively.\\n# Split the dataset into training and test portions df_train, df_test = train_test_split(df, test_size=200, random_state=21) \\nOur goal is to train the classifier so it can predict the class of a new customer inquiry out of eight classes, as follows:\\n[\\'atis_flight\\', \\'atis_airfare\\', \\'atis_ground_service\\', \\'atis_flight_time\\', \\'atis_airline\\', \\'atis_quantity\\', \\'atis_abbreviation\\', \\'atis_aircraft\\'] \\nWe transform the data to JSONL format to match the style expected by the Classification endpoint (documentation).\\ndef create_classification_data(text, label): formatted_data = { \"text\": text, \"label\": label } return formatted_data if not os.path.isfile(\"data.jsonl\"): print(\"Creating jsonl file ...\") with open(\"data.jsonl\", \\'w+\\') as file: for row in df_train.itertuples(): formatted_data = create_classification_data(row.query, row.intent) file.write(json.dumps(formatted_data) + \\'\\\\n\\') file.close() print(\"Done\") else: print(\"data.jsonl file already exists\") \\nThe JSONL file has a row for each example. Each example has \"text\" and \"label\" fields.\\n{\"text\": \" what flights go from newark to boston after 5 pm\", \"label\": \"atis_flight\"} {\"text\": \" can you show me economy class one way fares for flights from oakland to dallas on december sixteenth\", \"label\": \"atis_airfare\"} {\"text\": \" show me the latest flight from salt lake city to phoenix\", \"label\": \"atis_flight\"} {\"text\": \" pittsburgh to denver\", \"label\": \"atis_flight\"} {\"text\": \" show me all flights from san francisco to atlanta\", \"label\": \"atis_flight\"} ... \\nStep 2: Fine-Tune the Model\\nWe kick off a fine-tuning job by navigating to the fine-tuning tab of the Dashboard. Under \"Classify\", click on \"Create a Classify model\".\\nNext, upload the .jsonl file you just created as the training set by clicking on the \"TRAINING SET\" button. When ready, click on \"Review data\" to proceed to the next step.\\nThen, you\\'ll see a preview of how the model will ingest your data. If anything is wrong with the data, the page will also provide suggested changes to fix the training file. Otherwise, if everything looks good, you can proceed to the next step.\\nFinally, you\\'ll see a page where you\\'ll provide a nickname to your model. We used atis-classify-ft as the nickname for our model. Under \"BASE MODEL\", ensure \"english\" is selected.\\nThen click on \"Start training\" to kick off the fine-tuning process. This will navigate you to a page where you can monitor the status of the model. A model that has finished fine-tuning will show the status as READY.\\nStep 3: Use/Evaluate the Fine-Tuned Model\\nOnce the model has finished fine-tuning, it’s time to evaluate its performance. Navigate to the API tab of the fine-tuned model. There, you\\'ll see the model ID that you should use when calling co.classify().\\nWe fill in the model ID to generate test predictions.\\n# Generate classification predictions on the test dataset using the finetuned model # Classification function def classify_text_finetune(texts): classifications = co.classify( model=\\'b2c94ac3-7a74-4de7-a11a-9808a3b8ef59-ft\\', inputs=texts, examples=None ).classifications return [c.predictions[0] for c in classifications] # Create batches of texts and classify them BATCH_SIZE = 90 # The API accepts a maximum of 96 inputs y_pred = [] for i in range(0, len(df_test), BATCH_SIZE): batch_texts = df_test[\"query\"][i:i+BATCH_SIZE].tolist() y_pred.extend(classify_text_finetune(batch_texts)) \\nNext, we calculate the model\\'s test accuracy and F1 score.\\n# Compute metrics on the test dataset accuracy = accuracy_score(df_test[\"intent\"], y_pred) f1 = f1_score(df_test[\"intent\"], y_pred, average=\\'weighted\\') print(f\\'Accuracy: {100*accuracy:.2f}\\') print(f\\'F1-score: {100*f1:.2f}\\') \\nAccuracy: 98.00 F1-score: 98.05 \\nNot bad!\\nConclusion\\nIn this chapter, you saw that fine-tuning is an alternative to few-shot classification that allows you to have more control over the model\\'s training. It is generally preferred if you have a larger dataset and need superior performance.'),\n",
       " Document(metadata={'title': 'Multilingual Sentiment Analysis', 'source': 'https://cohere.com/llmu/multilingual-sentiment-analysis'}, page_content='Multilingual Sentiment AnalysisQualified\\nEmotional expression is subjective. It’s shaped by cultural background, emotional intelligence, individual personality traits and characteristics, and more. While we can communicate emotions clearly in verbal communication due to nuances of tone and inflection, it’s harder to determine how a person feels and what emotions they’re expressing through words alone.\\nFortunately we can use sentiment analysis to determine the tone—and by extension, the emotion—that text conveys. In this demonstration, we’re creating an app that uses sentiment analysis to determine what emotions a person is experiencing based on the text input.\\nThe steps for building the Sentiment Analysis application are:\\nStep 1: Gather Emotion Data\\nStep 2: Train the Emotion Classifier\\nStep 3: Get User Input\\nStep 4: Embed the Input\\nStep 5: Classify Sentiment\\nStep 6: Display Results\\nStep 7: Put It All together\\nThe repository for this project is here, and we encourage you to follow the code along with this tutorial.\\nStep-by-Step Guide\\nWe’ll use the embed-multilingual-v3.0 model via the Cohere API—specifically the Embed endpoint—to generate embeddings for user-provided text in real time. The embed-multilingual-v3.0 model generates embeddings for text data in over 100 languages. Then, we’ll use these embeddings to train a classifier to predict the emotions the user expressed.\\nStep 1: Gather Emotion Data\\nWe’ll use the XED dataset, consisting of 8 emotion categories, to train the emotion classifier. For this app, the embeddings for this dataset have already been created and stored as xed_with_embeddings.json, along with the labels for each data point.\\nStep 2: Train the Emotion Classifier\\nThe bulk of the code that drives this application is stored in the sentiment.py file. Let’s open it and examine the train_and_save function line by line.\\nFirst, the function reads xed_with_embeddings.json into a data frame. In this data frame, column df.embeddings contains the embeddings for each sample and column df.labels_text contains the emotion label for each sample. The function transforms the embeddings for each sample into a list, forming data matrix X.\\nThe function uses MultiLabelBinarizer from scikit-learn to one-hot encode df.labels_text and form y, which is a one-hot encoded label matrix. Next, the function executes the train-test split with the test set size configured at 1 percent of the total set. Afterwards, it instantiates a chain of classifiers with logistic regression as the base classifier.\\nA classifier chain is a technique for multi-label classification that involves training a chain of binary classifiers, one for each label. The output of each classifier serves as the input to the next classifier in the chain. Consequently, the final output is a vector of binary labels indicating the presence or absence of each emotion. This demo uses a chain of classifiers instantiated with logistic regression as the base classifier via ClassifierChain and LogisticRegression from scikit-learn. This allows us to predict the probability of each emotion class for each sample with chain_model.predict_proba in Step 5.\\nSubsequently, the train_and_save function fits the classifier on the train set and evaluates it with chain.score. This calculates the mean accuracy relative to the test set. In the last line, the function saves the trained classifier for determining emotion in a pickle file emotion_chain.pkl. A pickle file is a way of serializing and saving Python objects, such as trained machine learning models. By saving a trained model as a pickle file, we can reuse the model later without retraining it from scratch.\\nThen, we save the trained classifier for determining emotion into a pickle file named emotion_chain.pkl. To load the saved model later, we can simply use the pickle.load function in Python.\\ndef train_and_save(): full_df = pd.read_json(\"./data/xed_with_embeddings.json\", orient=\\'index\\') df = full_df mlb = MultiLabelBinarizer() X = np.array(df.embeddings.tolist()) y = mlb.fit_transform(df.labels_text) classes = mlb.classes_ classes_mapping = {index: emotion for index, emotion in enumerate(mlb.classes_)} X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=42) base_lr = LogisticRegression(solver=\\'lbfgs\\', random_state=0) chain = ClassifierChain(base_lr, order=\\'random\\', random_state=0) chain.fit(X_train, y_train) print(chain.score(X_test, y_test)) pickle.dump(chain, open(\"./data/models/emotion_chain.pkl\", \\'wb\\')) \\nStep 3: Get User Input\\nStreamlit functions facilitate user input. Calling st.text_input creates a page object that asks the user “How are you feeling?” and captures the user’s text response. It then presents the user a slider via st.slider, which they use to select the number of top emotions they’d like to be presented (k).\\nfeeling_text = st.text_input(\"How are you feeling?\", \"\") top_k = st.slider(\"Top Emotions\", min_value=1, max_value=len(classes_mapping), value=1, step=1) \\nStep 4: Embed the Input\\nThe beginning of function score_sentence embeds the user input text into embeddings and converts it into a torch-based tensor.\\nembeddings = torch.as_tensor(get_embeddings(co=co, model_name=model_name, texts=[text]), dtype=torch.float32) \\nThe function imports get_embeddings from the utils.py file. Then, the get_embeddings function calculates the embeddings by calling the embed-multilingual-v3.0model.\\ndef get_embeddings(co: cohere.Client, texts: List[str], model_name: str = \\'embed-multilingual-v3.0\\', input_type = \"search_document\", truncate: str = \\'RIGHT\\', batch_size: int = 2048) -> List[float]: @limiter.ratelimit(\"blobheart\", delay=True) def get_embeddings_api(texts_batch: List[str]): for i in range(N_MAX_RETRIES): try: output = co.embed(model=model_name, input_type=input_type, texts=texts_batch, truncate=truncate) break except Exception as e: if i == (N_MAX_RETRIES - 1): print(f\"Exceeded max retries with error {e}\") raise f\"Error {e}\" return output.embeddings st_pbar = tqdm(range(0, len(texts), batch_size)) for index in st_pbar: texts_batch = texts[index:index + batch_size] embeddings_batch = get_embeddings_api(texts_batch) #list(pool.imap(get_embeddings_api, [texts_batch])) embeddings.append(embeddings_batch) return np.concatenate(embeddings, axis=0).tolist() \\nStep 5: Classify Sentiment\\nIn the function setup within the sentiment.py tile, you can access the trained classifier model from emotion_chain.pkl where we saved it in Step 1.\\ndef setup(): … chain_model = pickle.load(open(model_path, \\'rb\\')) \\nWith the model stored as chain_model, we access it in score_sentence after acquiring the embeddings. The function then executes the model on the float tensor of embeddings to predict probabilities of emotion class(es), determined for each sample with chain_model.predict_proba. Then, torch.sort sorts the probability outputs for the emotion(s) associated with the user text input in ascending order.\\nIn the last step, we convert the tensor of probabilities and tensor of associated indices from GPU-based to CPU-based, allowing for each tensor to be configured into a NumPy array. This then reverses both NumPy arrays such that the emotion with the highest probability is first. This sets up the loop we’ll run to display the images associated with the determined top k emotions experienced by the user.\\noutputs = torch.as_tensor(chain_model.predict_proba(embeddings), dtype=torch.float32) probas, indices = torch.sort(outputs) probas = probas.cpu().numpy()[0][::-1] indices = indices.cpu().numpy()[0][::-1] \\nSection 6: Display Results\\nLet’s first head back to the setup function we discussed in step 5. The first part of this function initiates emotions2image_mapping as a dictionary and each of the eight emotion labels are mapped with file paths corresponding to respective emotion GIFs. Then, emotions2image_mapping iteratively filled with the emotion gifs themselves utilizing the file paths.\\nemotions2image_mapping = { \\'Anger\\': \\'./data/emotions/anger.gif\\', \\'Anticipation\\': \\'./data/emotions/anticipation.gif\\', \\'Disgust\\': \\'./data/emotions/disgust.gif\\', \\'Fear\\': \\'./data/emotions/fear.gif\\', \\'Joy\\': \\'./data/emotions/joy.gif\\', \\'Sadness\\': \\'./data/emotions/sadness.gif\\', \\'Surprise\\': \\'./data/emotions/surprise.gif\\', \\'Trust\\': \\'./data/emotions/trust.gif\\', } for key, value in emotions2image_mapping.items(): with open(value, \"rb\") as f: emotions2image_mapping[key] = f.read() \\nNow circling back to the score_sentence function, the call returns emotions2image_mapping to setup and is afterwards accessed in the loop inside score_sentence to display the top k emotions to the user as GIFs.\\nThe loop iterates through each of the top k emotions, with the index and probability associated with each emotion is extracted as tuple (index, p) at each iteration of the loop. Before the start of the loop, we use Streamlit to create k columns that will display the top k emotion(s) to the user.\\nIn the first iteration, a row of three columns is created via st.columns. Once three columns are filled with GIFs corresponding to ranked emotions, a new row of three columns is created in the following iteration. Structuring the GIFs to be displayed in rows of up to three emotions facilitates user readability.\\ncols = st.columns(top_k, gap=\"large\") for i, (index, p) in enumerate(zip(indices[:top_k], probas[:top_k])): if i % 3 == 0: cols = st.columns(3, gap=\"large\") emotion = classes_mapping[index] i = i % 3 image_file = emotions2image_mapping.get(emotion, None) if image_file: image_gif = base64.b64encode(image_file).decode(\"utf-8\") cols[i].markdown( f\\'<img src=\"data:image/gif;base64,{image_gif}\" style=\"width:250px;height:250px;border-radius: 25%;\">\\', unsafe_allow_html=True, ) cols[i].markdown(\"---\") cols[i].markdown(f\"**{emotion}**: {p * 100:.2f}%\") print(f\"Predicted emotion: {emotion}, with probability: {p}\") \\nConclusion\\nIn this chapter, we learned how to use embeddings to build a text classifier that can determine emotions. It is important to note the layers of this process. We first generated the embeddings by transforming the text via the embed-multilingual-v3.0 embedding model, and then we built the multi-label classifier. When we later predicted and displayed emotions to the user, we followed the same layered process of calculating embeddings and then applied the classifier to produce probabilities of detected emotions.'),\n",
       " Document(metadata={'title': 'Building a Chatbot', 'source': 'https://cohere.com/llmu/building-a-chatbot'}, page_content='Building a ChatbotQualified\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nUnderstanding text generation is the first step to creating your own chatbot, but the real learning takes place when you start building one from scratch. In this chapter, you’ll learn how to use Cohere’s Chat endpoint to build a simple chatbot that can respond to user messages and maintain the context of the conversation.\\nAdditionally, the API reference page contains a detailed description of the Chat endpoint’s input parameters and response objects.\\nStep-by-Step Guide\\nStep 1: Quickstart\\nTo set up, we first import the Cohere module and create a client.\\nimport cohere co = cohere.Client(\"COHERE_API_KEY\") # Your Cohere API key \\nAt its most basic, we only need to pass to the Chat endpoint the user message using the message parameter – the only required parameter for the endpoint.\\nAt its most basic, the endpoint takes a user message and generates a response\\nHere’s an example. We call the endpoint with \"Hello\" as the user message. In our case, we also use the model parameter to specify the underlying chat model we want to use. For this, we pick Command R+, Cohere\\'s newest large language model.\\nRight now, we’re interested in the main content of the response, which is stored in the text value of the response.\\nresponse = co.chat(message=\"Hello\", model=\"command-r-plus\") print(response.text) \\n# RESPONSE Hello! How can I help you today? \\nStep 2: Defining a Preamble\\nA conversation starts with a system message, or a preamble, to help steer a chatbot’s response toward certain characteristics. For example, if we want the chatbot to adopt a formal style, the preamble can be used to encourage the generation of more business-like and professional responses.\\nThe preamble and the current user message form the prompt to the chatbot\\nIn the quickstart example, we didn’t have to define a preamble because a default one was used. We can, however, define our own preamble using the preamble parameter.\\nHere’s an example. We added a preamble telling the chatbot to assume the persona of an expert public speaking coach. As a result, we get a response that adopts that persona.\\nresponse = co.chat(message=\"Hello\", model=\"command-r-plus\", preamble=\"You are an expert public speaking coach. Don\\'t use any greetings.\") print(response.text) \\n# RESPONSE How can I help you with your public speaking today? \\nStep 3: Streaming the Chatbot Response\\nOur examples so far generate responses in a non-streamed manner. This means that the endpoint would return a response object only after the model has generated the text in full. The longer the text is, the longer it takes to get back the response. If you are building an application, this directly impacts the user’s perception of the application’s latency.\\nThe Chat endpoint solves this problem by supporting streamed responses. In a streamed response, the endpoint would return a response object for each token as it is being generated. This means you can display the text incrementally without having to wait for the full completion.\\nTo activate it, use co.chat_stream() instead of co.chat().\\nIn streaming mode, the endpoint will generate a series of objects. To get the actual text contents, we take objects whose event_type is text-generation.\\nIf you have not already, make your own copy of the Google Colaboratory notebook and run the code in this section to see the same example with streamed responses activated.\\nstream = co.chat_stream(message=\"Hello. I\\'d like to learn about techniques for effective audience engagement\", model=\"command-r-plus\", preamble=\"You are an expert public speaking coach\") for event in stream: if event.event_type == \"text-generation\": print(event.text, end=\\'\\') \\n# RESPONSE (Streamed) Sure! Here are some techniques for effective audience engagement: - **Make eye contact:** Look at different individuals in the audience and try to maintain eye contact for a few seconds at a time. This helps create a sense of connection and makes your speech more intimate. - **Use gestures:** Incorporate hand gestures and body language to emphasize your points and add variety to your delivery. This helps keep your audience engaged and can also help you appear more confident and passionate about your topic. - **Vary your tone and volume:** Avoid monotone delivery by varying your pitch, pace, and volume. Emphasize important points by raising your volume or slowing down your pace. This helps to create emphasis and keeps your audience interested. - **Ask questions:** Pose rhetorical questions to your audience to get them thinking about your topic. You can also directly ask for their opinions or input, either by a show of hands or by inviting them to share their thoughts. This creates a dialogue and makes your speech more interactive. - **Tell stories:** Use anecdotes and personal stories to illustrate your points and create a connection with your audience. Stories are memorable and help your audience relate to you and your message. - **Use humor:** Appropriate humor can help to lighten the mood, engage your audience, and make your speech more enjoyable. However, be careful to avoid offensive or inappropriate jokes, and ensure your humor is relevant to your topic. - **Incorporate multimedia:** Use slides, videos, or props to enhance your presentation. Visual aids can help to break up your speech, provide additional information, and keep your audience focused. - **Encourage interaction:** If appropriate, include activities that involve your audience. This could be a group exercise, a poll, or a Q&A session. This helps to create a more engaging and interactive experience for your audience. - **Be passionate:** Show your enthusiasm for your topic. Audiences are more likely to engage if they sense your passion and authenticity. Let your personality shine through and speak from the heart. - **Practice and adapt:** Rehearse your speech and pay attention to your delivery. Practice in front of a mirror, record yourself, or seek feedback from others. This will help you refine your technique and improve your audience engagement skills. Remember, effective audience engagement is about creating a connection and making your speech memorable. By using these techniques and adapting them to your own style and personality, you can deliver a compelling presentation that resonates with your audience. \\nStep 4: Building the Chat History\\nAt the core of a conversation is a multi-turn dialog between the user and the chatbot. This requires the chatbot to have a “memory” of all the previous turns to maintain the state of the conversation.\\nOption 1: Using the Conversation History Persistence Feature\\nThe Chat endpoint supports state management by persisting the conversation history. As a conversation progresses, the endpoint continuously updates the conversation history. This means developers don’t have to deal with the complexity and inconvenience of managing conversation history in their application.\\nState management is handled by the Chat endpoint\\nTo use this feature, use the conversation_id parameter, which is a unique string you assign to a conversation. We\\'ll use the uuid library to do this.\\nPutting everything together, let’s now build a simple chat interface that takes in a user message, generates the chatbot response, automatically updates the conversation history, and repeats these steps until the user quits the conversation.\\nAs described before, in streaming mode, the Chat endpoint generates a series of objects. To get the conversation history, we take the object with event_type of \"stream-end\" and save it as a new variable chat_history.\\n# Create a conversation ID conversation_id = str(uuid.uuid4()) # Define the preamble preamble = \"You are an expert public speaking coach\" print(\\'Starting the chat. Type \"quit\" to end.\\\\n\\') while True: # User message message = input(\"User: \") # Typing \"quit\" ends the conversation if message.lower() == \\'quit\\': print(\"Ending chat.\") break # Chatbot response stream = co.chat_stream(message=message, model=\"command-r-plus\", preamble=preamble, conversation_id=conversation_id) print(\"Chatbot: \", end=\\'\\') for event in stream: if event.event_type == \"text-generation\": print(event.text, end=\\'\\') if event.event_type == \"stream-end\": chat_history = event.response.chat_history print(f\"\\\\n{\\'-\\'*100}\\\\n\") \\n# RESPONSE (Streamed) Starting the chat. Type \"quit\" to end. User: Hello Chatbot: Hello! How can I help you today? ---------------------------------------------------------------------------------------------------- User: I\\'d like to learn about techniques for effective audience engagement Chatbot: Sure! Here are some techniques for effective audience engagement: - Make eye contact with your audience and use hand gestures to emphasize your points. - Use a clear and concise language that your audience can easily understand. - Vary your tone and volume to keep your audience interested. - Use visual aids such as slides, props, or demonstrations to help illustrate your points. - Tell stories or anecdotes to help your audience connect with your message on a personal level. - Ask questions to involve your audience and encourage them to think about your topic. - Use humor appropriately to keep your audience entertained and engaged. - Be enthusiastic and passionate about your topic to show your audience that you care about what you\\'re talking about. - Connect with your audience by showing that you understand their interests and concerns. - End your speech with a call to action that encourages your audience to take action on your topic. Remember, effective audience engagement is about creating a connection with your audience and making your speech memorable and enjoyable for them. ---------------------------------------------------------------------------------------------------- User: Could you elaborate on the third point? Chatbot: Sure! Varying your tone and volume is an important technique for keeping your audience engaged. Here are some tips: - Use a varied tone of voice to express different emotions and emphasize certain words or phrases. For example, you might use a serious tone to convey the importance of a particular issue, or an excited tone to show your enthusiasm for a new idea. - Change your volume to highlight important points or to create a sense of drama. Speaking loudly can help to emphasize a particular word or phrase, while speaking softly can create a sense of intimacy or seriousness. - Avoid monotony by varying your pitch and pace. A monotone delivery can be boring and make it difficult for your audience to stay focused. Try to vary your pitch and pace to keep your audience engaged. - Practice your speech or presentation beforehand to get a feel for how your tone and volume can impact your delivery. Experiment with different tones and volumes to see what works best for your style and your message. By varying your tone and volume, you can add interest and emphasis to your speech, and keep your audience engaged and responsive. ---------------------------------------------------------------------------------------------------- User: quit Ending chat. \\nNext, we print the full conversation history.\\nfor chat in chat_history: print(chat) \\nrole=\\'USER\\' message=\\'Hello\\' generation_id=\\'b4146b59-0884-4aa3-a573-bb6bd4948511\\' response_id=\\'b72e9f29-ac13-41e2-8892-b6529f9547f2\\' role=\\'CHATBOT\\' message=\\'Hello! How can I help you today?\\' generation_id=\\'3a002415-0213-49b9-8d4b-b7c24eba95f8\\' response_id=\\'b72e9f29-ac13-41e2-8892-b6529f9547f2\\' role=\\'USER\\' message=\"I\\'d like to learn about techniques for effective audience engagement\" generation_id=\\'fee7fbf4-fd93-4477-b987-bd2765694962\\' response_id=\\'520e2329-979b-4e4b-9cdc-b2e4b49c3612\\' role=\\'CHATBOT\\' message=\"Sure! Here are some techniques for effective audience engagement:\\\\n\\\\n- Make eye contact with your audience and use hand gestures to emphasize your points.\\\\n- Use a clear and concise language that your audience can easily understand.\\\\n- Vary your tone and volume to keep your audience interested.\\\\n- Use visual aids such as slides, props, or demonstrations to help illustrate your points.\\\\n- Tell stories or anecdotes to help your audience connect with your message on a personal level.\\\\n- Ask questions to involve your audience and encourage them to think about your topic.\\\\n- Use humor appropriately to keep your audience entertained and engaged.\\\\n- Be enthusiastic and passionate about your topic to show your audience that you care about what you\\'re talking about.\\\\n- Connect with your audience by showing that you understand their interests and concerns.\\\\n- End your speech with a call to action that encourages your audience to take action on your topic.\\\\n\\\\nRemember, effective audience engagement is about creating a connection with your audience and making your speech memorable and enjoyable for them.\" generation_id=\\'2cc3d664-587f-4f1c-8c45-4ac658bb1668\\' response_id=\\'520e2329-979b-4e4b-9cdc-b2e4b49c3612\\' role=\\'USER\\' message=\\'Could you elaborate on the third point?\\' role=\\'CHATBOT\\' message=\\'Sure! Varying your tone and volume is an important technique for keeping your audience engaged. Here are some tips:\\\\n\\\\n- Use a varied tone of voice to express different emotions and emphasize certain words or phrases. For example, you might use a serious tone to convey the importance of a particular issue, or an excited tone to show your enthusiasm for a new idea.\\\\n- Change your volume to highlight important points or to create a sense of drama. Speaking loudly can help to emphasize a particular word or phrase, while speaking softly can create a sense of intimacy or seriousness.\\\\n- Avoid monotony by varying your pitch and pace. A monotone delivery can be boring and make it difficult for your audience to stay focused. Try to vary your pitch and pace to keep your audience engaged.\\\\n- Practice your speech or presentation beforehand to get a feel for how your tone and volume can impact your delivery. Experiment with different tones and volumes to see what works best for your style and your message.\\\\n\\\\nBy varying your tone and volume, you can add interest and emphasis to your speech, and keep your audience engaged and responsive.\\' \\nOption 2: Managing the Conversation History Yourself\\nIf you opt not to use the endpoint’s conversation history persistence feature, you can use the chat_history parameter to manage the conversation history yourself.\\nThe chat history is a list of multiple turns of messages from the user and the chatbot. Each item is a cohere.ChatMessage object containing the role, which can be either ”USER” or ”CHATBOT”, and the message containing the message string. The following is an example of a chat history.\\nfrom cohere import ChatMessage chat_history = [ ChatMessage(role=\"USER\", message=\"What is 2 + 2\"), ChatMessage(role=\"CHATBOT\", message=\"The answer is 4\"), ChatMessage(role=\"USER\", message=\"Add 5 to that number\"), ChatMessage(role=\"CHATBOT\", message=\"Sure. The answer is 9\"), ... ] \\nThe following modifies the previous implementation by using chat_history instead of conversation_id for managing the conversation history.\\n# Initialize the chat history chat_history = [] # Define the preamble preamble = \"You are an expert public speaking coach\" print(\\'Starting the chat. Type \"quit\" to end.\\\\n\\') while True: # User message message = input(\"User: \") # Typing \"quit\" ends the conversation if message.lower() == \\'quit\\': print(\"Ending chat.\") break # Chatbot response stream = co.chat_stream(message=message, model=\"command-r-plus\", preamble=preamble, chat_history=chat_history) chatbot_response = \"\" print(\"Chatbot: \", end=\\'\\') for event in stream: if event.event_type == \"text-generation\": print(event.text, end=\\'\\') chatbot_response += event.text print(\"\\\\n\") # Add to chat history chat_history.extend( [ChatMessage(role=\"USER\", message=message), ChatMessage(role=\"CHATBOT\", message=chatbot_response)] ) \\n# RESPONSE (Streamed) Starting the chat. Type \"quit\" to end. User: Hello Chatbot: Hello! How can I help you today? User: I\\'d like to learn about techniques for effective audience engagement Chatbot: Sure! Here are some techniques for effective audience engagement: - Make eye contact with your audience and use hand gestures to emphasize your points. - Use a clear and concise language that your audience can easily understand. - Vary your tone and volume to keep your audience interested. - Use stories, examples, and analogies to illustrate your points and make your message more memorable. - Ask questions to involve your audience and encourage them to think about your topic. - Use humor appropriately to lighten the mood and engage your audience. - Use visuals, such as slides, photos, or props, to break up your speech and add interest. - Move around the stage or room to change your position and keep your audience focused. - Use pauses to emphasize important points and give your audience time to absorb your message. - Finally, be passionate and enthusiastic about your topic. Your enthusiasm will be contagious and will help to engage your audience. Remember, effective audience engagement is about connecting with your audience and making your message memorable. By using these techniques, you can deliver a speech that is both informative and engaging. User: Could you elaborate on the fourth point? Chatbot: Certainly! Using stories, examples, and analogies is a powerful technique to engage your audience and help them understand and remember your message. Here\\'s how: - Stories: Sharing a relevant story or anecdote can make your speech more personal and engaging. Stories have the power to evoke emotions and create a deeper connection with your audience. For example, if you\\'re speaking about the importance of perseverance, you could share a story about a time when you faced a challenge and how you overcame it. - Examples: Providing concrete examples helps to illustrate your points and makes abstract concepts more tangible. For instance, if you\\'re explaining a complex technical process, you could use an example to simplify and break down the information for your audience. - Analogies: Using analogies is a way to explain something unfamiliar by comparing it to something familiar. Analogies can help your audience understand and remember your message by creating a mental link between two concepts. For example, if you\\'re describing a new technology, you could compare it to something your audience is already familiar with. When using stories, examples, and analogies, make sure they are relevant to your topic and concise. You don\\'t want to lose your audience\\'s attention by rambling or veering off-topic. Also, try to use vivid details and sensory information to create a more engaging and memorable narrative. By incorporating these techniques, you can make your speech more interesting, relatable, and easier for your audience to understand and retain. User: quit Ending chat. \\nAnd with that, we have built a simple chatbot that can respond to user messages and maintain the context of the conversation.\\nConclusion\\nThis chapter showed how to build a simple chatbot using the Chat endpoint and how to configure the chatbot, such as overriding the preamble, building the chat history, streaming the response, and modifying the parameters.'),\n",
       " Document(metadata={'title': 'Introduction to Text Generation', 'source': 'https://cohere.com/llmu/introduction-to-text-generation'}, page_content='Introduction to Text GenerationQualified\\nChatbots brought large language models (LLMs) into the mainstream. LLMs have been around for a few years, but their adoption was largely limited to the AI community. The launch of AI-powered consumer chatbots has made LLMs accessible to the everyday user, and now they\\'re a hot topic in tech and enterprise circles alike.\\nThis text generation module teaches you how to build LLM chatbots using Cohere’s Chat endpoint.\\nCommand Model\\nCommand is Cohere’s flagship LLM. It generates a response given a prompt or message from a user. It is trained to follow user commands and to be instantly useful in practical business applications, like summarization, copywriting, extraction, and question answering.\\nCommand has been trained with a large volume of multi-turn conversations to ensure that it excels at the various nuances associated with conversational language. It ranks at the top of the Holistic Evaluation of Language Models (HELM) benchmark, an evaluation leaderboard comparing large language models on a wide number of tasks (March ‘23 results).\\nCommand R and Command R+ Models\\nCommand R and Command R+ are designed to be the market leading family of models in the ‘scalable’ category that balance high efficiency with strong accuracy to enable enterprises to move from proof of concept into production-grade AI.\\nHere are some key features of Command R:\\nHigh-performance RAG: Retrieval-augmented generation (RAG) enables enterprises to give the model access to private knowledge that it otherwise would not have.\\nAccess to tools: Tool use enables enterprise developers to turn Command R into an engine for powering the automation of tasks and workflows that require using internal infrastructure like databases and software tools, as well as external tools like CRMs, search engines, and more. Command R+ supports Multi-Step Tool Use which allows the model to combine multiple tools over multiple steps to accomplish difficult tasks.\\nLow latency and high throughput: Command R targets the “scalable” category of models that balance high performance with strong accuracy, enabling companies to move beyond proof of concept and into production.\\n128k context length and lower pricing: Command R features a longer context length, supporting up to 128k tokens in its initial release.\\nStrong capabilities across 10 key languages: The model excels at 10 major languages for global business: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, and Chinese.\\nModel weights available for research and evaluation: Cohere For AI is releasing the weights for this version of Command R publicly, so that it can be used for research purposes.\\nHow an LLM Chatbot Works\\nTo understand how LLM chatbots work, it’s important to develop an understanding of their building blocks. This section focuses on how to build the generative part of a chatbot by looking at how to use a foundational model and added layers of context to generate answers in a conversation style.\\nThe Foundation of an LLM Chatbot\\nThe foundation of an LLM chatbot is an LLM that has been fine-tuned to follow instructions. It can generate a response given a prompt or message from a user. This type of model is tuned to follow instructions and questions, such as “Write a headline for my homemade jewelry product” or “What is the capital of Canada?”.\\nA message or prompt returning a model response\\nHowever, the LLM’s context is limited to only the last message it receives, and it does not consider any previous messages and responses.\\nYet, chatbots are characterized by their ability to maintain a conversation with a user, which takes place over multiple interactions.\\nA baseline LLM’s context is limited to only the last message it receives\\nThe goal of a chatbot is to solve this problem by linking a sequence of interactions into a single instance, allowing the chatbot to hold an ongoing conversation. In doing so, the model’s response can keep a memory of all the previous interactions instead of having to start from scratch every time.\\nHow to Build a Chatbot\\'s Memory\\nWorking off of the baseline generation model above, we can layer together multiple interactions into a single prompt and create a memory of the entire conversation.\\nFirst, we add a system-level prompt called a preamble. A preamble contains instructions to help steer a chatbot’s response toward specific characteristics, such as a persona, style, or format. For example, if we want the chatbot to adopt a formal style, the preamble can be used to encourage the generation of more business-like and professional responses. The preamble could be something like \"You are a helpful chatbot, trained to assist human users by providing responses in a formal and professional tone.\"\\nThen, we append the current user message to the preamble, which becomes the prompt for the chatbot’s response. Next, we append the chatbot response and the following user message to the prompt.\\nWe can repeat this step for any number of interactions until we reach the model’s maximum context length. Context length is the total number of tokens taken up by the prompt and response, and each model has a maximum context length that it can support.\\nBuilding a conversation by stitching multiple prompt-response pairs together\\nThis multi-turn framework is what gives chatbots the ability to hold the full context of conversation from start to finish.\\nMulti-turn conversations can happen when the full context is available\\nHowever, building on top of a baseline LLM alone is not sufficient.\\nChatbots need to perform well in a wide range of scenarios. To create a robust chatbot that consistently generates high-quality and reliable output, the baseline LLM needs to be adapted specifically to conversations. This means taking the baseline model and fine-tuning it further with a large volume of conversational data.\\nThis is what forms the foundation of Cohere’s Chat endpoint — let’s take a closer look.\\nCohere\\'s Chat Endpoint\\nImproving LLM chatbot performance starts with how the baseline LLM is trained. The model powering the Chat endpoint is Cohere’s Command model, trained with a large volume of multi-turn conversations. This ensures that the model will excel at the various nuances associated with conversational language and perform well across different use cases.\\nBeyond training, fine-tuning a baseline LLM for conversations requires adding a standardized interface on top of the prompt formatting system. The Chat endpoint provides a consistent, simplified, and structured way of handling the prompt formatting that defines how the prompt inputs should be organized, making it easier for developers to build chatbot applications. This added layer includes a fixed abstraction and schema, providing more stability to scale and build applications on top of the foundation model.\\nThe Chat endpoint includes all the elements required for an LLM chatbot (as discussed in the previous sections), exposing a simple interface for developers. It consists of the following key components:\\nPreamble management: Developers can opt to use the endpoint’s default preamble or override it with their own preambles.\\nMulti-turn conversations: The Chat endpoint builds upon the Command model by enabling multi-turn conversations.\\nState management: State management preserves the conversation memory. Developers can either leverage the endpoint’s conversation history persistence feature or manage the conversation history themselves.\\nFully-managed conversation: The abstraction layer of the Chat endpoint means there’s only one item to send to the API: the user message. Everything else is managed automatically. At the same time, developers who want greater control over a chatbot’s configuration can still do so.\\nThe Chat endpoint takes care of the underlying logic, exposing a simple interface for developers\\nConclusion\\nCohere\\'s Command model and Chat endpoint offer powerful tools for developers looking to harness the potential of LLMs in their applications. Command\\'s capabilities, coupled with the Chat endpoint\\'s developer-friendly interface, allow developers to build text generation applications across diverse business applications.'),\n",
       " Document(metadata={'title': 'Parameters for Controlling Outputs', 'source': 'https://cohere.com/llmu/parameters-for-controlling-outputs'}, page_content='Parameters for Controlling OutputsQualified\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nThe Chat endpoint is a versatile tool that empowers developers with an extensive array of options and parameters.\\nAs you’ll learn, the Command model has many variations to select from, where each has been carefully crafted to suit different needs. Additionally, you will see how to use parameters to control the creativity of model responses.\\nSetup\\nTo set up, we first import the Cohere module and create a client.\\nimport cohere co = cohere.Client(\"COHERE_API_KEY\") # Your Cohere API key \\nModel Type\\nWith the Chat endpoint , you can choose from several variations of the Command model . Different models may produce different output profiles, so you may want to experiment with different models to get the best output for your use case.\\nSee the documentation for the most updated list of available Cohere models. At the time of writing, the models are as follows:\\ncommand: The default model used in a Chat endpoint call. If you don’t define the model parameter, this model will be used.\\ncommand-light: A smaller, faster version of command. Almost as capable, but a lot faster.\\ncommand-r: Performs language tasks at a higher quality, more reliably, and with a longer context than previous models.\\ncommand-nightly and command-light-nightly: The latest, most experimental, and (possibly) unstable versions of their default counterparts. Not recommended for production use.\\nUse the model parameter to select a variation that suits your requirements. In the code cell, we select command-r.\\nresponse = co.chat(message=\"Hello\", model=\"command-r-plus\") print(response.text) \\n# RESPONSE Hello! How can I help you today? \\nRandomness\\nOften, you’ll need to control the level of randomness of the model. There are a number of factors to keep in mind when tuning model randomness, including:\\nTask type: You’ll likely want to decrease the randomness of the model when performing structured tasks that have a correct answer, like for question answering or summarization, or for generating technical text. In these cases, we want the model to yield a safe and predictable response. On the other hand, if you’re generating poetry or brainstorming ideas, you might want to increase the randomness to produce more diverse and creative responses.\\nModel behavior: We likely need to increase the randomness of the model if it gets stuck in a loop and starts repeating itself, or if it is producing overly generic phrases. Increasing the randomness will expand the set of words that the model can use when generating responses.\\nControlling style and tone: If generating text that needs to have a specific tone or style, like for a company blog post or customer support responses, a low level of randomness might be desirable, to keep the model from generating unusual words.\\nModifying the temperature parameter changes the extent to which the model considers incorporating unlikely tokens (can be words, parts of words, or punctuation) in its response, which can make the output more random and creative.\\nTo understand this, we’ll look at an example. The model would likely predict that the token cookies has a much higher likelihood than chair for appearing after the phrase I like to bake.\\nThe model assigns a likelihood number to each of all possible next tokens\\nBefore these likelihoods can be used to select the next token, they first need to be converted to probabilities. The temperature parameter controls how this conversion is done.\\nAt low temperature, low likelihood tokens are assigned very low probabilities, and high likelihood tokens are assigned very high probabilities.\\nAt high temperature, the probabilities will look roughly similar for each token, with high likelihood tokens assigned only slightly higher probability.\\nAdjusting the temperature setting\\nBuilding off the example above,\\nAt low temperature, there’s a probability that chair is selected, but the probability is significantly lower than cookies.\\nAt high temperature, the probability that chair is selected is only slightly lower than cookies.\\nThe temperature parameter is a value between 0 and 1. As you increase the temperature, the model gets more creative and random. Temperature can be tuned for different problems, and most people will find that the default temperature of 0.3 is a good starting point.\\nLet’s look at a code example, where we suggest that the model generate alternative names for a blog post. Prompting the endpoint five times when the temperature is set to 0 yields the same output each time.\\nmessage = \"\"\"Suggest a more exciting title for a blog post titled: Intro to Retrieval-Augmented Generation. \\\\ Respond in a single line.\"\"\" for _ in range(5): response = co.chat(message=message, temperature=0, model=\"command-r-plus\") print(response.text) \\n# RESPONSE The Future of AI: Unlocking the Power of Retrieval-Augmented Generation The Future of AI: Unlocking the Power of Retrieval-Augmented Generation The Future of AI: Unlocking the Power of Retrieval-Augmented Generation The Future of AI: Unlocking the Power of Retrieval-Augmented Generation The Future of AI: Unlocking the Power of Retrieval-Augmented Generation \\nHowever, if we increase the temperature to the maximum value of 1, the model gives different proposals.\\nmessage = \"\"\"Suggest a more exciting title for a blog post titled: Intro to Retrieval-Augmented Generation. \\\\ Respond in a single line.\"\"\" for _ in range(5): response = co.chat(message=message, temperature=1, model=\"command-r-plus\") print(response.text) \\n# RESPONSE Unleashing the Power of Retrieval-Augmented Generation: A Comprehensive Guide The Exciting Future of AI: How Retrieval-Augmented Generation Will Transform the Way We Interact With Machines The Magic of AI: Unlocking the Power of Retrieval-Augmented Generation The Future of AI: Unlocking the Power of Retrieval-Augmented Generation \"Unleashing the Power of AI: The Rise of Retrieval-Augmented Generation.\" \\nConclusion\\nIn this chapter, you learned how to call different variations of the Command model when using the Chat endpoint. You worked with a code example using the temperature parameter to control the Command model’s level of randomness.'),\n",
       " Document(metadata={'title': 'Prompt Engineering Basics', 'source': 'https://cohere.com/llmu/prompt-engineering-basics'}, page_content='Prompt Engineering BasicsQualified\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nNote: This chapter covers the basics of prompt engineering. If you want to explore this topic further, we have a dedicated LLMU module on prompt engineering as well as further documentation on prompt engineering.\\nPrompting is at the heart of working with LLMs. The prompt provides context for the text that we want the model to generate. The prompts we create can be anything from simple instructions to more complex pieces of text, and they are used to encourage the model to produce a specific type of output.\\nComing up with a good prompt is a bit of both science and art. On the one hand, we know the broad patterns that enable us to construct a prompt that will generate the output that we want. But on the other hand, there is so much room for creativity and imagination, as you’ll see in the examples in this section.\\nSetup\\nTo set up, we first import the Cohere module and create a client.\\nimport cohere co = cohere.Client(\"COHERE_API_KEY\") # Your Cohere API key \\nLet\\'s also define a function generate_text() to take a user message, call the Chat endpoint, and stream the response.\\ndef generate_text(message): stream = co.chat_stream(message=message, model=\"command-r-plus\") for event in stream: if event.event_type == \"text-generation\": print(event.text, end=\\'\\') \\nWriting a Basic Prompt\\nThe best way to design prompts for a model like Command is to give a command or an instruction. One way to do this is by using imperative verbs, for example: generate, write, list, provide, and other variations.\\nFor instance, let’s say that we are creating the product description copy for a wireless earbuds product. We can write the prompt as follows.\\ngenerate_text(\"Generate a concise product description for the product: wireless earbuds.\") \\n# RESPONSE Sure! Here is a concise product description for wireless earbuds: \"Experience unparalleled sound with our wireless earbuds. Immerse yourself in rich, high-quality audio and enjoy the freedom of movement without tangles or hassles. Our earbuds offer a secure and comfortable fit, ensuring they stay put during any activity. With a sleek and portable design, these earbuds are the perfect companion for your active lifestyle.\" \\nThat’s not bad. With a simple, one-line prompt, we already have a piece of product description that can make a digital marketer proud!\\nLayering Additional Instructions\\nBut perhaps we want to be more specific regarding what we want the output to look like. For this, we can layer additional instructions onto the model in the prompt.\\nLet’s say we want the model to write the product description in a particular format with specific information. In this case, we can append this specific instruction in the prompt.\\ngenerate_text(\"\"\" Generate a concise product description for the product: wireless earbuds. Use the following format: Hook, Solution, Features and Benefits, Call to Action. \"\"\") \\n# RESPONSE Here is a concise product description for wireless earbuds: **Hook:** Tired of tangled cords and restricted movement? **Solution:** Go wireless with our latest earbuds! **Features and Benefits:** - Sleek and ergonomic design ensures a secure and comfortable fit. - Advanced Bluetooth technology for seamless connectivity and uninterrupted audio. - Immersive sound quality with powerful bass and crystal-clear highs. - Built-in microphone for hands-free calls and voice assistant access. - Convenient charging case provides on-the-go power. **Call to Action:** Experience true wireless freedom and elevate your audio experience with our cutting-edge earbuds. Order now and enjoy uninterrupted music, calls, and more! \\nThe model returns an output following the format that we wanted.\\nAdding Context to a Prompt\\nThe prompt can also be constructed as a combination of an instruction and some context. Let’s see this in action with another example: emails. We can create a prompt to summarize an email, which is included in the prompt for context.\\ngenerate_text(\"\"\" Summarize this email in one sentence. Dear [Team Members], I am writing to thank you for your hard work and dedication in organizing our recent community meetup. The event was a great success and it would not have been possible without your efforts. I am especially grateful for the time and energy you have invested in making this event a reality. Your commitment to ensuring that everything ran smoothly and that our guests had a great time is greatly appreciated. I am also thankful for the support and guidance you have provided to me throughout the planning process. Your insights and ideas have been invaluable in ensuring that the event was a success. I am confident that our community will benefit greatly from this event and I am excited to see the positive impact it will have. Thank you again for your hard work and dedication. I am looking forward to working with you on future events. Sincerely, [Your Name] \"\"\") \\n# RESPONSE The email expresses gratitude to the team members for their hard work, dedication, and guidance in organizing a successful community meetup, and looks forward to future collaborations. \\nThis instruction–context prompt format is extremely useful as it means that we can supply additional information as context to help ground the model\\'s output. One such example is a question-answering system for, let\\'s say, a company\\'s knowledge base. Given a question (the instruction), the model will only be able to provide accurate answers if provided with the knowledge base (the context).\\nLet\\'s move to another example — an extraction task, which a generative model can do very well. Given context, which in this case is a description of a movie, we want the model to extract the movie title.\\ngenerate_text(\"\"\" Extract the movie title from the text below. Deadpool 2 | Official HD Deadpool\\'s \"Wet on Wet\" Teaser | 2018 \"\"\") \\nRewriting Text\\nThe model is also effective at tasks that involve taking a piece of text and rewriting it into another format that we need.\\nHere\\'s an example. We have a one-line instruction followed by the context, which in this case is a blog excerpt. The instruction is to generate a list of frequently asked questions (FAQ) based on the passage, which involves a mixture of several tasks, such as extraction and rewriting.\\ngenerate_text(\"\"\" Given the following text, write down a list of potential frequently asked questions (FAQ), together with the answers. The Cohere Platform provides an API for developers and organizations to access cutting-edge LLMs without needing machine learning know-how. The platform handles all the complexities of curating massive amounts of text data, model development, distributed training, model serving, and more. This means that developers can focus on creating value on the applied side rather than spending time and effort on the capability-building side. There are two key types of language processing capabilities that the Cohere Platform provides — text generation and text embedding — and each is served by a different type of model. With text generation, we enter a piece of text, or prompt, and get back a stream of text as a completion to the prompt. One example is asking the model to write a haiku (the prompt) and getting an originally written haiku in return (the completion). With text embedding, we enter a piece of text and get back a list of numbers that represents its semantic meaning (we’ll see what “semantic” means in a section below). This is useful for use cases that involve “measuring” what a passage of text represents, for example, in analyzing its sentiment. \"\"\") \\n# RESPONSE Here is a list of potential FAQs based on the provided text: - **Q: What does the Cohere Platform offer to developers and organizations?** A: The Cohere Platform offers an API that provides access to advanced Large Language Models (LLMs) without requiring machine learning expertise. It simplifies the process by handling data curation, model development, training, and serving. - **Q: What are the key language processing capabilities of the Cohere Platform?** A: The platform offers two main capabilities: text generation and text embedding. Text generation involves completing a prompt with a stream of generated text, like writing a haiku. Text embedding returns a numerical representation of the semantic meaning of a text input, useful for sentiment analysis and other measurements. - **Q: How does the Cohere Platform benefit developers?** A: Developers can focus on creating valuable applications without getting bogged down by the complexities of building language processing capabilities from scratch. The platform handles the heavy lifting, allowing developers to save time and effort. - **Q: What is the difference between text generation and text embedding?** A: Text generation is about creating new text based on a prompt, like generating a haiku or continuing a story. Text embedding, on the other hand, translates text into a series of numbers that represent its semantic meaning, enabling quantitative analysis and understanding of the text\\'s context. - **Q: Can I use the Cohere Platform for sentiment analysis?** A: Yes, the text embedding capability of the Cohere Platform is particularly useful for sentiment analysis. By converting text into numerical representations, you can quantitatively analyze and understand the sentiment or emotional tone expressed in a given piece of text. - **Q: Does the Cohere Platform require machine learning expertise to use?** A: No, the platform is designed to abstract away the complexities of machine learning. Developers can utilize the API without needing to understand the intricate details of machine learning or natural language processing. These FAQs aim to address common inquiries that users might have about the Cohere Platform and its language processing capabilities. \\nBy now, we can see how versatile our model is at performing various forms of tasks — not just freeform text generation, but also following instructions, working with contextual information, summarizing long passages, extracting information, rewriting text into different formats, and more.\\nThis is just a taste of what kinds of prompts you can design. You can keep layering your instructions to be as specific as you want, and see the output generated by the model. And there is really no right or wrong way to design a prompt. It’s really about applying an idea and continuing to iterate the prompt until you get the outcome you are looking for.\\nAfter completing this module, we encourage you to take a look at LLMU’s Prompt Engineering module to go deeper into prompt engineering techniques and apply them to Cohere’s Command model.\\nConclusion\\nIn this chapter, you learned how to prompt a model — probably the most important and definitely the most fun part of working with large language models.'),\n",
       " Document(metadata={'title': 'Fine-Tuning for Chat', 'source': 'https://cohere.com/llmu/fine-tuning-for-chat'}, page_content='Fine-Tuning for ChatQualified\\nWe’ll use Cohere’s Dashboard for the code examples. Follow along in this notebook.\\nCohere’s large language models (LLMs) have been trained to be useful in practical business applications. Using the Chat endpoint, you can leverage a pre-trained LLM to build a chatbot that performs tasks ranging from summarization to copywriting to question answering.\\nDepending on the use case you have in mind, you might want to amend how the chatbot generates its output. For instance, if your task uses highly technical data or you want to change the chatbot’s output format, you often need to perform another round of training on additional data to ensure the best performance. This extra training is referred to as fine-tuning.\\nFine-tuning is also recommended when you want to incorporate your company\\'s unique knowledge base. For example, if you are aiming to use a model to draft responses to customer-support inquiries, fine-tuning on old conversations with customers can improve the quality of the output.\\nStep-by-Step Guide\\nStep 1: Prepare and Validate the Dataset\\nWe will work with the CoEdIT dataset of text editing examples (Raheja, et al). In each example, the user asks a writing assistant to rewrite text to suit a specific task (editing fluency, coherence, clarity, or style) and receives a response. Below, you can see some examples from the raw dataset.\\n{ \"_id\": \"57241\", \"task\": \"coherence\", \"src\": \"Make the text more coherent: It lasted for 60 minutes. It featured the three men taking questions from a studio audience.\", \"tgt\": \"Lasting for 60 minutes, it featured the three men taking questions from a studio audience.\" } { \"_id\": \"69028\", \"task\": \"clarity\", \"src\": \"Make the sentence clearer: URLe Lilanga (1934 27 June 2005) was a Tanzanian painter and sculptor, active from the late 1970s and until the early years of the 21st century.\", \"tgt\": \"URLe Lilanga (1934 27 June 2005) was a Tanzanian painter and sculptor, active from the late 1970s and until the early 21st century.\" } \\nWe will use the src and tgt fields from each example, which correspond to the user’s prompt and the writing assistant’s response, respectively. Instead of using the full dataset, we will use a subset focused on making text coherent: 927 total conversations.\\nTo format the dataset for the Python SDK, we create a .jsonl where each JSON object is a conversation containing a series of messages.\\nA System message in the beginning, acting as the preamble that guides the whole conversation\\nMultiple pairs of User and Chatbot messages, representing the conversation that takes place between a human user and a chatbot\\nFor more detail on best practices for formatting your dataset, check out the documentation . Here is a preview of the prepared dataset:\\n{\\'messages\\': [{\\'role\\': \\'System\\', \\'content\\': \\'You are a writing assistant that helps the user write coherent text.\\' }, {\\'role\\': \\'User\\', \\'content\\': \\'Make the text more coherent: It lasted for 60 minutes. It featured the three men taking questions from a studio audience.\\' }, {\\'role\\': \\'Chatbot\\', \\'content\\': \\'Lasting for 60 minutes, it featured the three men taking questions from a studio audience.\\' } ] } \\nStep 2: Fine-Tune the Model\\nWe kick off a fine-tuning job by navigating to the fine-tuning tab of the Dashboard. Under \"Chat\", click on \"Create a Chat model\".\\nCohere Dashboard\\nNext, upload the .jsonl file you just created as the training set by clicking on the \"TRAINING SET\" button. When ready, click on \"Review data\" to proceed to the next step.\\nUploading the training set\\nThen, you\\'ll see a preview of how the model will ingest your data. It shows the total number of conversations, the total number of turns, and the average number of turns per conversation. If anything is wrong with the data, the page will also provide suggested changes to fix the data file. Otherwise, if everything looks good, you can proceed to the next step.\\nReviewing your fine-tuning data\\nNext, you\\'ll see an estimated cost of fine-tuning, followed by a page where you\\'ll provide a nickname to your model. We used coedit-coherence as the nickname for our model. This page also allows you to provide custom values for the hyperparameters used during training, but we\\'ll keep them at the default values for now.\\nStarting a training job\\nOnce you have filled in a name, click on \"Start training\" to kick off the fine-tuning process. This will navigate you to a page where you can monitor the status of the model. A model that has finished fine-tuning will show the status as READY.\\nA model has finished fine-tuning when its status is READY\\nStep 3: Evaluate the Fine-Tuned Model\\nOnce the model has completed the fine-tuning process, it’s time to evaluate its performance.\\nWith Test Data\\nWhen you\\'re ready to use the fine-tuned model, navigate to the API tab. There, you\\'ll see the model ID that you should use when callingco.chat().\\nGet the model ID from the API tab\\nIn the following code, we supply a message from the test dataset to both the pre-trained and fine-tuned models for comparison.\\nuser_message = \"Make the text coherent: Pimelodella kronei is a species of three-barbeled catfish endemic to Brazil. Discovered by the German naturalist Sigismund Ernst Richard Krone, Pimelodella kronei was the first troglobitic species described in Brazil, but several others have been described later.\" # Desired response: Pimelodella kronei is a species of three-barbeled catfish endemic to Brazil. Discovered by the German naturalist Sigismund Ernst Richard Krone, it was the first troglobitic fish described in Brazil, but several others have been described later. preamble = \"You are a writing assistant that helps the user write coherent text.\" # Get default model response response_pretrained=co.chat( message=user_message, preamble=preamble, ) # Get fine-tuned model response response_finetuned = co.chat( message=user_message, model=\\'acb944bb-fb49-4c29-a15b-e6a245a7bdf9-ft\\', preamble=preamble, ) print(f\"Default response: {response_pretrained.text}\",\"\\\\n-----\") print(f\"Fine-tuned response: {response_finetuned.text}\") \\nFor this example, the output appears as follows:\\nDefault response: The three-barbeled catfish, Pimelodella kronei, is exclusive to Brazil. It was Sigismund Ernst Richard Krone, a German naturalist, who first discovered this remarkable species. Notably, P. kronei was also the initial troglobitic species to be identified in Brazil, though other such species have since been uncovered. Would you like to know more about the discovery of this species or its natural habitat? ----- Fine-tuned response: Pimelodella kronei, a species of three-barbeled catfish endemic to Brazil, was discovered by the German naturalist Sigismund Ernst Richard Krone. It was the first troglobitic species described in Brazil, but several others have been described later. \\nIn this example, both models provide reasonable answers that are an improvement over the user’s original text. However, the fine-tuned model’s response better matches the style of the fine-tuning data, because it is more succinct: both the fine-tuned model’s response and the desired response have 36 words, whereas the response from the pre-trained model is about 25% (10 words) longer.\\nThis is just one example, but you can compare responses to more prompts in the notebook.\\nIn the Chat Context\\nWe have demonstrated that the fine-tuned model can provide good answers to individual questions. But it is also a competent participant in longer, multi-turn conversations.\\nTo see this, we will borrow from the code in the Building a Chatbot chapter to build a simple chat interface. The only change we need to make is to supply the model nickname when using co.chat_stream() to ensure that we are chatting with the model we just fine-tuned.\\n# Create a conversation ID import uuid conversation_id = str(uuid.uuid4()) print(\\'Starting the chat. Type \"quit\" to end.\\\\n\\') while True: # User message message = input(\"User: \") # Typing \"quit\" ends the conversation if message.lower() == \\'quit\\': print(\"Ending chat.\") break # Chatbot response stream = co.chat_stream(message=message, model=\\'acb944bb-fb49-4c29-a15b-e6a245a7bdf9-ft\\', preamble=preamble, conversation_id=conversation_id) print(\"Chatbot: \", end=\\'\\') for event in stream: if event.event_type == \"text-generation\": print(event.text, end=\\'\\') print(\"\\\\n\",\"-\"*50) \\nWe can use the code above to have a conversation with the fine-tuned model.\\nStarting the chat. Type \"quit\" to end. User: Hello Chatbot: Hello, how can I help you today? -------------------------------------------------- User: I\\'m fine. Can I ask you for help with some tasks? Chatbot: I am happy to help you with any tasks that you need help with. I can also provide information about any topic that you would like to know more about. -------------------------------------------------- User: Make this more coherent: Manuel now has to decide-will he let his best friend be happy with her Prince Charming. Or will he fight for the love that has kept him alive for the last 16 years? Chatbot: Manuel now has to decide-will he let his best friend be happy with her Prince Charming, or will he fight for the love that has kept him alive for the last 16 years? -------------------------------------------------- User: Help me with this one - She left Benaras. Conditions back home were bad. Chatbot: She left Benaras because conditions back home were bad. -------------------------------------------------- User: What\\'s a good time to visit London Chatbot: A good time to visit London is in the spring or fall, when the weather is mild and the city is not too crowded. The best time to visit London depends on what you want to see and do. If you want to see the sights and do some shopping, a good time to visit London is in the spring, when the weather is mild and the city is not too crowded. If you want to see the sights and do some sightseeing, a good time to visit London is in the fall, when the weather is mild and the city is not too crowded. -------------------------------------------------- User: Could you help with this please: Make the text coherent: Critically the album has not been as well received as other Browne recordings. It remains his only album to date to reach number 1 on the Billboard chart. Chatbot: Critically the album has not been as well received as other Browne recordings, but it remains his only album to date to reach number 1 on the Billboard chart. -------------------------------------------------- User: quit Ending chat. \\nNote the fine-tuned model is still able to respond to prompts like “Hello”, “I’m fine. Can I ask you for help with some tasks?”, and “What’s a good time to visit London” instead of strictly following the fine-tuning objective of editing text.\\nThe model also did a good job with context switching; it can hold a conversation when the user switches from friendly greetings, to a request for writing help, to travel planning, and finally back to writing assistance. It can also infer when the user is asking for help with making a text coherent, even if it is not explicitly stated (e.g., “Help me with this one”) or if the request is buried slightly (e.g., with “Could you help me with this please”).\\nIf you’re interested in further improving model performance, explore refining data quality or iterating on hyperparameters, as described in the documentation.\\nConclusion\\nIn this chapter, you learned how to fine-tune a model for the Chat endpoint on a custom dataset. You saw how fine-tuning allows you to tailor a chatbot to a specific use case and give it a particular style. As described in the documentation, you learned how to prepare the fine-tuning data, start a fine-tuning job, and understand the results.'),\n",
       " Document(metadata={'title': 'Introduction to RAG', 'source': 'https://cohere.com/llmu/introduction-to-rag'}, page_content=\"Introduction to RAGQualified\\nNote: This chapter covers an introduction to retrieval-augmented generation (RAG). If you want to explore this topic further and learn how to implement RAG with the Cohere API, we have a dedicated module on RAG.\\nIn this module on text generation, we have so far explored LLM chatbots that only have access to the data they have been trained on, or their internal knowledge. In many applications, particularly for enterprise use, a chatbot needs to also be able to access external knowledge for it to be useful.\\nCohere’s Chat endpoint comes with a retrieval-augmented generation (RAG) feature that makes it possible to connect to external knowledge bases and deliver more accurate responses.\\nWhat is RAG?\\nTo understand RAG, we will work with the example of a company that wants to deploy a chatbot as an intelligent knowledge assistant. For the chatbot to be useful, it will need to be connected to the company’s knowledge base.\\nConnecting the Chat endpoint with external knowledge\\nThis allows the chatbot to have the correct context when responding to requests, such as summarizing a meeting transcript, extracting information from the company wiki, and assisting a customer support agent in responding to a customer inquiry. Without access to the company’s knowledge base, the chatbot will not be able to perform these types of tasks successfully.\\nRAG solves the lack of specific knowledge problem\\nThe company will also likely need the chatbot to respond to time-sensitive prompts and provide up-to-date answers. For example, suppose an employee asks the chatbot about a recent public event. A baseline LLM is trained with data that is current up to a certain cut-off time. Without accessing external data, the model relies on the most recent information it has been trained on (assuming the specific information is available in the training data). In this situation, the lack of recency in the training data would produce an inadequate answer.\\nRAG solves the lack of recency problem\\nRAG consists of two parts: a retrieval system and an augmented generation system. Let’s take a look at how they work.\\nRetrieval\\nThe first part of RAG is to retrieve the right information needed to respond to a user query. Given a user message (1), the Chat endpoint queries an external knowledge base with the relevant queries (2), and finally retrieves the query results (3).\\nThe retrieval part of RAG: Given a user message, the endpoint retrieves information from an external knowledge base\\nRetrieval applies to any system that can fetch relevant documents based on a user message. Cohere offers two ways to build RAG systems with the Chat endpoint: connectors mode and documents mode.\\nConnectors Mode\\nAs the name implies, connectors are ways of connecting to data sources. These data sources could be internal documents, document databases, the broader internet, or any other data source which can inform the model’s replies. When the Chat endpoint is called in connectors mode, it connects to the associated data sources and automatically handles document retrieval.\\nDevelopers can leverage pre-built connectors to various data sources or even build their own. Also, there is a web search connector managed by Cohere which runs searches against a browser in safe mode. Developers can use it immediately without any additional configuration or deployment.\\nFor example, suppose we are working with a RAG system that uses Cohere’s managed web search connector, and say you input this query: “Who was the keynote speaker at the AI conference last week?”. Given that the response requires a fact from a recent event, the chatbot triggers a retrieval of this information using the web search API. It gets back the information it requires, such as a few website snippets containing the details about the conference.\\nAn example of retrieving information via web search\\nIn an enterprise setting, data is distributed across multiple platforms and datastores. With connectors mode, you can use multiple connectors at the same time to get the system to retrieve and synthesize information from various data sources.\\nConnectors mode allows you to use multiple connectors to connect to multiple datastores\\nDocuments Mode\\nWith documents mode, a developer directly provides the model with source documents that it can use to ground its replies. In this case, these documents first need to be ingested. Typically, this involves chunking large documents into smaller chunks, turning these chunks into text embeddings (also called vectors), and storing these embeddings in a vector store.\\nDocuments a inregested before retrieval\\nConnectors mode automatically handles document ingestion and is excellent for those who prefer a more hands-off approach. However, documents mode allows developers to customize the RAG stack's components to fit specific needs.\\nAugmented Generation\\nThe second part of RAG is augmented generation. Here, the prompt is augmented with the information retrieved from the retrieval step. The prompt is now grounded with the best information to provide the user with an accurate and helpful response.\\nThe chatbot responds to the user query, now having the augmented prompt as its context.\\nThe augmented generation part of RAG: The Chat endpoint uses the retrieved information to provide a grounded response\\nCohere’s Chat endpoint also provides citations to indicate the parts of the retrieved documents on which the response was grounded.\\nCitations provide verifiable references for the generated content\\nCitations provide a critical benefit by delivering the generated content with verifiable references, enhancing the credibility and trustworthiness of the presented information, and allowing users to further explore responses for a deeper understanding.\\nConclusion\\nIn this chapter, we looked at enhancing LLM chatbots with RAG, a key component of the Chat endpoint that makes it possible to connect the API to external data for augmented generation. To continue your learning, we encourage you to take a look at LLMU’s Retrieval-Augmented Generation (RAG) module to learn how to build your own RAG systems.\"),\n",
       " Document(metadata={'title': 'Deploying with Streamlit', 'source': 'https://cohere.com/llmu/deploy-streamlit'}, page_content='Deploying with StreamlitQualified\\nWe’ll show only selected code snippets in this article, but you can find the complete code in this repository.\\nIn this module, we will explore various options for deploying LLM-powered applications. And we\\'ll begin with one of the fastest and simplest options to get started – Streamlit.\\nWeb development is a multi-stage process, and front-end development usually comes back towards the end. While it may seem like front-end development should be one of the quickest parts of the process, it often turns out to be quite a lengthy endeavor.\\nEnter Streamlit. It provides a very fast way to build front-end web applications with very little hassle. In this article, we’ll see how we can quickly prototype our Startup Idea Generator idea, deploy it as a web app and share it with the world!\\nBut let’s go back to the start and see how we might approach this. We can break the plan into three key steps, as follows:\\nBuild the back-end with Cohere\\nBuild the front-end with Streamlit\\nDeploy with Streamlit Cloud\\nStep 1: Build the back-end with Cohere\\nBuild the startup idea generator\\nThe first thing we need to do is build our main feature—the startup idea generator. For this, we use the Chat endpoint , which generates text given an input called a prompt.\\nCreating prompts, or prompt engineering, does require some experimentation. This is also good news - it means there’s huge room for you to innovate and apply your own creativity in designing prompts that get the best out of the endpoint. Read this documentation if you are interested in learning more.\\nIn our case, we create a prompt containing an instruction and a few examples of an industry and its corresponding startup idea.\\nWe then build a function that leverages Cohere’s Python SDK to take in user input and return the generated text, and the code looks as follows.\\ndef generate_idea(industry): prompt = f\"\"\" Generate a startup idea given the industry. # Examples Industry: Workplace Startup Idea: A platform that generates slide deck contents automatically based on a given outline Industry: Home Decor Startup Idea: An app that calculates the best position of your indoor plants for your apartment Industry: Healthcare Startup Idea: A hearing aid for the elderly that automatically adjusts its levels and with a battery lasting a whole week Industry: Education Startup Idea: An online primary school that lets students mix and match their own curriculum based on their interests and goals Industry: {industry}\"\"\" # Call the Cohere Chat endpoint response = co.chat( model=\\'command-r\\', message=prompt, preamble=\"\") return response.text.replace(\"Startup Idea: \", \"\") \\nDoing a quick test, entering “Public Transportation” returns the following startup idea: “A small electronic device that notifies users when they have to get off the train or bus”. Looking good so far.\\nBuild the startup name generator\\nGenerating startup ideas is great, but it would make the app much more exciting if we could also suggest a startup name for each idea. Using a similar approach as before, we create a new prompt containing an instruction and a few examples of a startup idea description and its corresponding startup name.\\nWe create another function that takes in a startup idea as the input and returns the generated startup name. The code looks as follows.\\ndef generate_name(idea): prompt= f\"\"\" Generate a startup name and name given the startup idea. # Examples Startup Idea: A platform that generates slide deck contents automatically based on a given outline Startup Name: Deckerize Startup Idea: An app that calculates the best position of your indoor plants for your apartment Startup Name: Planteasy Startup Idea: A hearing aid for the elderly that automatically adjusts its levels and with a battery lasting a whole week Startup Name: Hearspan Startup Idea: An online primary school that lets students mix and match their own curriculum based on their interests and goals Startup Name: Prime Age Startup Idea: {idea}\"\"\" # Call the Cohere Chat endpoint response = co.chat( model=\\'command-r\\', message=prompt, preamble=\"\") return response.text.replace(\"Startup Name: \", \"\") \\nDoing a quick test, entering the public transportation startup idea we got earlier returns the following startup name - “Beepro”. Not bad.\\nStep 2: Build the front-end with Streamlit\\nBuild the basic components\\nNow that the text generation part is working, let’s create the front end with Streamlit. Streamlit is an open-source Python library that makes it easy to create and share custom web apps. Instead of hours or even days, you can get a basic front end up and running and deploy it in minutes.\\nThe following is our front-end code using Streamlit, which gets us a basic working app in just a few lines.\\nst.title(\"🚀 Startup Idea Generator\") form = st.form(key=\"user_settings\") with form: industry_input = st.text_input(\"Industry\", key = \"industry_input\") generate_button = form.form_submit_button(\"Generate Idea\") if generate_button: startup_idea = generate_idea(industry_input) startup_name = generate_name(startup_idea) st.markdown(\"##### \" + startup_name) st.write(startup_idea) \\nIt uses a number of features from the Streamlit API, as follows:\\nA form via the st.form() control flow\\nst.title() for the header title\\nst.text_input() for the user text input\\nA submit button that comes with st.form()\\nst.markdown() and st.write() to display the generated text.\\nWe can deploy this Streamlit app by running the following command at the terminal (in this example, our code above is stored in app.py).\\nRunning the app locally renders the following.\\nA basic version of the app\\nAdd some interactivity\\nNow that we have a basic version of the app running, let’s see how we can enhance it. One way is to give users greater control when generating ideas. Let’s add a couple of options to add interactivity to the app.\\nThe first one is straightforward. We want to let users generate more than one idea in one go. For this, we’ll use the st.slider() widget to let users input the number of generations to make. We’ll set the maximum number of generations to 10.\\nThe second is a bit more interesting. With the Chat endpoint, we can use the temperature parameter to control the randomness of the model output. The value can range between 0 and 1. Lower values tend to generate more “predictable” output, while higher values tend to generate more “creative” output. The sweet spot is typically between 0 and 1, and for our app, we’ll just add a small buffer and set the range to be between 0.1 and 0.9. We’ll name this user setting Creativity.\\nFor this, we’ll also use st.slider() to let the users control the temperature value. We’ll also need to modify the generate_idea() and generate_name() functions to accept the temperature argument, to be passed to the Chat API call. The following is an example with generate_idea().\\ndef generate_idea(industry, temperature): ... response = co.chat( model=\\'command-r\\', message=prompt, temperature=temperature, preamble=\"\") ... \\nLet’s also add a couple more things. First, we’ll use st.progress() to show a progress bar as the idea generation takes place. Second, we’ll add a check to the Industry user input to make it a required field.\\nAltogether, the completed front-end Streamlit code looks like the following, about 20 lines.\\nst.title(\"🚀 Startup Idea Generator\") form = st.form(key=\"user_settings\") with form: # User input - Industry name industry_input = st.text_input(\"Industry\", key = \"industry_input\") # Create a two-column view col1, col2 = st.columns(2) with col1: # User input - The number of ideas to generate num_input = st.slider(\"Number of ideas\", value = 3, key = \"num_input\", min_value=1, max_value=10) with col2: # User input - The \\'temperature\\' value representing the level of creativity creativity_input = st.slider(\"Creativity\", value = 0.5, key = \"creativity_input\", min_value=0.1, max_value=0.9) # Submit button to start generating ideas generate_button = form.form_submit_button(\"Generate Idea\") if generate_button: if industry_input == \"\": st.error(\"Industry field cannot be blank\") else: my_bar = st.progress(0.05) st.subheader(\"Startup Ideas\") for i in range(num_input): st.markdown(\"\"\"---\"\"\") startup_idea = generate_idea(industry_input,creativity_input) startup_name = generate_name(startup_idea,creativity_input) st.markdown(\"##### \" + startup_name) st.write(startup_idea) my_bar.progress((i+1)/num_input) \\nAnd with that, the final front-end now looks like the following.\\nThe final version of the app\\nStep 3: Deploy with Streamlit Cloud\\nNow that our app is ready, let’s deploy it to the web so everyone can start interacting with it. You can deploy the app via your own hosting option, but there’s also a quick and hassle-free way to do it — via Streamlit Cloud.\\nStreamlit Cloud launches apps directly from a GitHub repository. So, before you can deploy your app, do the following steps.\\nPush your code to a GitHub repository\\nSign up for Streamlit Cloud\\nConnect your Streamlit Cloud to the GitHub repository\\nOnce you have completed the steps, on your Streamlit Cloud page, click on New app.\\nYou’ll be brought to the Deploy an app page, where you can select the repository, the branch and the main file path i.e. the code we wrote earlier. You will also need to add your Cohere API key in the Advanced settings… link (more information here).\\nAnd once that is done, click Deploy!. Wait a few minutes for Streamlit to deploy it, and the app is live. Now literally anyone on the web can access it!\\nLet’s now take the app for a spin and, say, we want to generate three startup ideas in the productivity industry.\\nLet’s first try with a low Creativity setting, of say 0.2, which should give us ideas that are more predictable and possibly more proven. The outcome? It generates ideas revolving around the usual suspects — to-do, time management, and scheduling.\\nNext, we try with a high Creativity setting, of say 0.8, which should result in ideas that are more creative but are probably bordering on being too ambitious. This time, the ideas are a lot more diverse — travel planning, diagram drawing, and personal finance.\\nHere are the screenshots of these generated ideas.\\nExample outputs generated with different Creativity settings, 0.2 (left) and 0.8 (right)\\nAnd that concludes our startup idea generator prototype, powered by Cohere and deployed with Streamlit.\\nConclusion\\nThis example demonstrated how using Cohere and Streamlit, you can quickly turn your language AI product idea into a basic prototype. It’s an amazing feeling to be able to deploy a web app without spending long, frustrating hours building a front-end.'),\n",
       " Document(metadata={'title': 'Deploying with FastAPI', 'source': 'https://cohere.com/llmu/deploy-fastapi'}, page_content='Deploying with FastAPIQualified\\nThe source code used for this example is available in our GitHub repository.\\nFastAPI is a web framework for building APIs with Python. It is designed to be fast, simple, and easy to use. It is also designed to be flexible for a wide variety of use cases.\\nA web framework is a set of tools and libraries that simplify the process of building web applications. It provides a high-level interface that allows developers to build web applications without having to worry about low-level details such as network protocols and data storage.\\nDeveloping with FastAPI has many benefits, but the major one is as advertised in its name – it is fast. It\\'s both fast in performance (on par with NodeJS and Go) and in development time (increasing the speed to develop features by about 200% to 300%). You can read more about these in its documentation.\\nIn this chapter, we\\'ll build a simple sentiment analysis API with FastAPI and Cohere\\'s Classify endpoint.\\nIt involves the following steps.\\nStep 1: Setup\\nStep 2: Create a sentiment classifier\\nStep 3: Create a FastAPI endpoint\\nStep 4: Call the endpoint\\nStep 1: Setup\\nFirst, we create a Python file. Let\\'s name it main.py.\\nNext, we import FastAPI and Cohere, as well as Pydantic, for structuring inputs to the API.\\npip install cohere fastapi \"uvicorn[standard]\" \\nfrom fastapi import FastAPI from pydantic import BaseModel, conlist import cohere from cohere import ClassifyExample # Setup the Cohere client co = cohere.Client(\"COHERE_API_KEY\") \\nUvicorn serves as the default server for FastAPI, providing an asynchronous interface, while Pydantic provides a way to define data schemas using plain Python classes with type hints.\\nStep 2: Create a Sentiment Classifier\\nTo create a sentiment classifier, we need to provide the Classify endpoint with a list of labeled examples of the texts to classify and the expected classes.\\nThe Classify endpoint requires a minimum of two examples per class, but the more examples, the better it performs. In the example below, we have three classes to classify text into – positive, negative, and neutral – and five examples per class, making it a total of fifteen examples.\\nWe feed these examples to the co.classify() method via the examples argument. We also provide the actual list of texts we want to classify via the input argument. The other thing we need to define is the type of model to be used via the model argument, which needs to be one of the embedding models. The code is shown below.\\nIf you need more information about the endpoint, visit the Classify endpoint documentation.\\nexamples=[ClassifyExample(text=\"The order came 5 days early\", label=\"positive\"), ClassifyExample(text=\"The item exceeded my expectations\", label=\"positive\"), ClassifyExample(text=\"I ordered more for my friends\", label=\"positive\"), ClassifyExample(text=\"I would buy this again\", label=\"positive\"), ClassifyExample(text=\"I would recommend this to others\", label=\"positive\"), ClassifyExample(text=\"The package was damaged\", label=\"negative\"), ClassifyExample(text=\"The order is 5 days late\", label=\"negative\"), ClassifyExample(text=\"The order was incorrect\", label=\"negative\"), ClassifyExample(text=\"I want to return my item\", label=\"negative\"), ClassifyExample(text=\"The item\\'s material feels low quality\", label=\"negative\"), ClassifyExample(text=\"The item was nothing special\", label=\"neutral\"), ClassifyExample(text=\"I would not buy this again but it wasn\\'t a waste of money\", label=\"neutral\"), ClassifyExample(text=\"The item was neither amazing or terrible\", label=\"neutral\"), ClassifyExample(text=\"The item was okay\", label=\"neutral\"), ClassifyExample(text=\"I have no emotions towards this item\", label=\"neutral\")] response = co.classify( inputs=product_reviews.reviews, examples=examples) \\nStep 3: Create a FastAPI Endpoint\\nNow let\\'s create a FastAPI wrapper around that code to extend the model as an API endpoint.\\nFirst, we create a ProductReviews class, which is a Pydantic model that defines the structure of the request body expected by the prediction endpoint (which we\\'ll create after this). Specifically, it specifies that the request body must contain a field named reviews, which is a list of strings (conlist(str, min_length=1)). Each string in the list represents a product review. The min_length=1 constraint ensures that the list contains at least one review.\\nNext, we create an endpoint that we call predictiontogether with a function that callspredict_sentiment. This endpoint will receive the user’s inputted list of strings and invoke the function.\\nWe take the code to call the Classify endpoint from the previous step and put it inside the function.\\napp = FastAPI() class ProductReviews(BaseModel): reviews: conlist(str, min_length=1) @app.post(\"/prediction\") def predict_sentiment(product_reviews: ProductReviews): examples=[ClassifyExample(text=\"The order came 5 days early\", label=\"positive\"), ClassifyExample(text=\"The item exceeded my expectations\", label=\"positive\"), ClassifyExample(text=\"I ordered more for my friends\", label=\"positive\"), ClassifyExample(text=\"I would buy this again\", label=\"positive\"), ClassifyExample(text=\"I would recommend this to others\", label=\"positive\"), ClassifyExample(text=\"The package was damaged\", label=\"negative\"), ClassifyExample(text=\"The order is 5 days late\", label=\"negative\"), ClassifyExample(text=\"The order was incorrect\", label=\"negative\"), ClassifyExample(text=\"I want to return my item\", label=\"negative\"), ClassifyExample(text=\"The item\\'s material feels low quality\", label=\"negative\"), ClassifyExample(text=\"The item was nothing special\", label=\"neutral\"), ClassifyExample(text=\"I would not buy this again but it wasn\\'t a waste of money\", label=\"neutral\"), ClassifyExample(text=\"The item was neither amazing or terrible\", label=\"neutral\"), ClassifyExample(text=\"The item was okay\", label=\"neutral\"), ClassifyExample(text=\"I have no emotions towards this item\", label=\"neutral\")] response = co.classify( inputs=product_reviews.reviews, examples=examples) return response.classifications \\nStep 4: Call the Endpoint\\nWe can now test the endpoint locally. Switch your terminal working directory to the location of your saved Python file, then input the following shell command. This brings up a server on your localhost.\\nLet\\'s test with these two text inputs and get the predicted classes from the model.\\nThe product was faulty Customer support was very responsive \\nThere are a couple of options to call the endpoint. One way is to run a cURL command on your terminal.\\ncurl -X \\'POST\\' \\\\ \\'http://127.0.0.1:8000/prediction\\' \\\\ -H \\'accept: application/json\\' \\\\ -H \\'Content-Type: application/json\\' \\\\ -d \\'{ \"reviews\": [ \"The product was faulty\", \"Customer support was very responsive\" ] }\\' \\nAnother way is to use FastAPI\\'s built-in documentation feature. It offers easy documentation based on OpenAPI standards showcasing API endpoints, sample values, responses, and their types.\\nFor this, go to the URL provided when you run the shell command earlier to bring up the server and add docs as the path (For example, 127.0.0.1:8000/docs).\\nClick the Try it out button and add comma-separated text to the Request body section\\nClick Execute. A sample curl command of your request is shown, along with the API call results. Here, the response contains the predicted class for each input text. It also provides its confidence scores against each possible class, with the predicted class having the highest confidence.\\nAs an example, in the screenshot below, we can see that The product was faulty is classified as having a negative sentiment with a 0.985 confidence score, while Customer support was very responsiveis classified as having a positive sentiment with a 0.972 confidence score.\\nIn this chapter, we built a simple sentiment analysis API with FastAPI and Cohere\\'s Classify endpoint.'),\n",
       " Document(metadata={'title': 'Deploying on Google Sheets with Google Apps Script', 'source': 'https://cohere.com/llmu/deploy-google-sheets'}, page_content='Deploying on Google Sheets with Google Apps ScriptQualified\\nThe source code used for this example is available in our GitHub repository.\\nThis chapter demonstrates how to use Cohere and Google Apps Script, a scripting language based on JavaScript, to build two custom functions that add large language model (LLM) capabilities to Google Sheets documents. We\\'ll be classifying the sentiment and generating summaries of movie reviews.\\nIt involves the following steps.\\nStep 1: Add training examples required for text classification.\\nStep 2: Create a custom Apps Script function for text classification.\\nStep 3: Create a custom Apps Script function for text summarization.\\nStep 4: Run text classification and summarization on a new input.\\nSetup\\nWe’ve already created a file to run the demo of this project. Open the file in Google Sheets and follow the instructions described in the Instructions worksheet.\\nOnce you have completed the steps, you will be able to run the demo directly using the script provided in the repository. Let\\'s understand how it is created.\\nStep 1: Add Training Examples Required for Text Classification\\nThe first step is to create a set of examples needed by the Classify endpoint to be able to perform a classification task.\\nThe Classify endpoint requires a minimum of two training examples per class, though more training examples are generally better. You can find the examples we\\'ll use for running movie reviews sentiment analysis in the Classifier Examples worksheet. It contains five examples for each class – positive and negative.\\nStep 2: Create a Custom Apps Script Function for Text Classification\\nNow we are going to create the script for the first task: sentiment analysis. In Google Sheets, go to Extensions and then go to Apps Script, which will take us to the Apps Script environment.\\nNext, we’ll use the Classify endpoint to build our text classification function, starting with the code below. It is a basic function definition taking in two parameters. The first one (s_args) is the range of cells for the training examples we defined in the previous section. The second one (s_input) is the new text we want to label or classify.\\nfunction classify(s_args, s_input) { } \\nLet’s now add the logic between the curly braces. First, we extract the training examples from s_args and package them into something that the Classify endpoint can ingest.\\nNext, we call the Classify endpoint to classify an input text, given the package of training examples. We pass the data package (such as authentication headers and other metadata) to the endpoint. Then, we store the response in a named response object.\\nNext, we parse the response and extract the classification and confidence level.\\nFinally, we output the parsed response back into the cell where we call the function.\\nfunction classify(s_args, s_input) { // we start by extracting the training data from the specified range in s_args and packaging it into a prompt let inputs = [s_input]; let examples = []; for (let i = 0; i< s_args.length; i++) { //this is assuming label is always the 2nd parameter examples.push({\"text\" : s_args[i][0] , \"label\" : s_args[i][1]}) } const payload_for_classification = { examples, inputs, \"model\" : \"embed-english-v2.0\", }; const options = { \\'headers\\' : { \\'Authorization\\' : \\'Bearer \\' + TOKEN }, \\'method\\' : \\'post\\', \\'contentType\\': \\'application/json\\', \\'payload\\' : JSON.stringify(payload_for_classification) }; const response = UrlFetchApp.fetch(\\'https://api.cohere.ai/v1/classify\\', options); const responseContent = response.getContentText(); const responseContentText = JSON.parse(response.getContentText()); const classificationContent = responseContentText.classifications[0]; const ourPrediction = classificationContent.prediction; const confidence = (classificationContent.confidence * 100).toFixed(2) + \"%\"; let displayString = \"Prediction: \" + ourPrediction.toUpperCase() + \" review\\\\n\" + \"Confidence Level: \" + confidence + \"\\\\n\"; return displayString; } \\nStep 3: Create a Custom Apps Script Function for Text Summarization\\nThe methodology for text summarization is similar to the one used for text classification in the previous section, except we’ll be using the Chat endpoint instead.\\nWe create a simple prompt asking the endpoint to provide a summary in about ten words. We are defining three parameters for the endpoint call.\\nmessage - the prompt for the LLM\\ntemperature - we keep the value closer to 0 to make the output more predictable (0.2 in this example)\\nmodel- we\\'ll use command-r\\nFinally, we parse the response and output the summary.\\nfunction summarize(cell_value) { let raw = JSON.stringify({ \"message\": \"Summarize this in about 10 words: \" + cell_value, \"temperature\": 0.2, \"model\": \"command-r\", }); let requestOptions = { \\'method\\': \\'post\\', \\'muteHttpExceptions\\': true, \\'contentType\\': \\'application/json\\', \\'headers\\': { \\'Authorization\\': \\'Bearer \\' + TOKEN }, \\'payload\\': raw, redirect: \\'follow\\' }; let response = UrlFetchApp.fetch(\"https://api.cohere.ai/v1/chat\", requestOptions) let responseContentTxt = JSON.parse(response.getContentText()); let summarizedTxt = \"SUMMARY: \" + responseContentTxt.text + \"\\\\n\"; return summarizedTxt; } \\nFinally, we save the code we have added for text classification and summarization.\\nStep 4: Run Text Classification and Summarization on a New Input\\nNow, we get to the part we’ve been waiting for – classifying and summarizing text on Google Sheets.\\nFor this, we’ll move back to the Google Sheets view and go to the Demo worksheet. It has three columns, as shown below.\\nThe first column is the input text, the movie review that we want to classify its sentiment and summarize. Here, we have a review of The Batman.\\nIn the second column, we call our text classification function as follows.\\n=classify(\\'Classifier Examples\\'!$A$3:$B$12,A3)\\nRecall that this function takes in two parameters. The first one is the list of training examples. Our training examples are provided in the Classifier Examples tab between cells A3 and B12.\\nThe second parameter is A3, the review of The Batman. When we hit enter, it predicts the class, as shown in the screenshot above.\\nThe third column calls the summarize function.\\n=summarize(A3)\\nThis takes cell the movie review in A3 and outputs the summary.\\nConclusion\\nDealing with numbers in a spreadsheet has always been easy, but the same couldn’t be said of text — until now. With Cohere’s natural language processing capabilities, you can supercharge your spreadsheets, saving time and effort.'),\n",
       " Document(metadata={'title': 'Deploying as a Chrome Extension', 'source': 'https://cohere.com/llmu/deploy-chrome-extension'}, page_content='Deploying as a Chrome ExtensionQualified\\nThis chapter is contributed by Leila Chan Currie. The source code used for this example is available in our GitHub repository.\\nThis tutorial demonstrates how to create Condense, a Google Chrome extension that summarizes the text content of a web page.\\nIt involves the following steps:\\nStep 1: Create a function for text display\\nStep 2: Create a function for text summarization\\nStep 3: Create a function for text capturing\\nStep 4: Create a function for Cohere API key entry\\nStep 5: Create a Chrome extension manifest file\\nStep 6: Load the extension to Google Chrome\\nStep 7: Run the extension on Google Chrome\\nThe source code used for this example is available in our GitHub repository. This tutorial uses Javascript.\\nStep 1: Create a Function for Text Display\\nThe first step is to create a folder for this extension, which we’ll call condense. Then create another folder called scripts and inside it, a file called content.js. This file will contain the main logic of this extension.\\nFirst, we’ll create a function for text display, which will display the generated summary of a web page at the top of the page. This function, display(text), creates the elements and styling for the header and the summary. Finally, it inserts the header immediately before the HTML body.\\n//Display the text at the top of the page function display(text) { //Create a purple header header = document.createElement(\"div\"); header.style.backgroundColor = \"#d18ee2\"; header.style.padding = \"5px\"; //Write the text with a bit of styling and add it to the header tldr = document.createElement(\"p\"); tldr.textContent = text; tldr.style.margin = \"10px 100px\"; tldr.style.fontSize = \"medium\"; tldr.style.color = \"white\"; tldr.style.textAlign = \"center\"; tldr.style.fontFamily = \"Verdana, Geneva, sans-serif\"; header.appendChild(tldr); //Insert the header immediately before the HTML body document.body.parentNode.insertBefore(header, document.body); } \\nStep 2: Create a Function for Text Summarization\\nNext, we’ll create a function, summarize(text), to perform the text summarization, leveraging Cohere’s Chat endpoint. This function calls the Chat endpoint by defining several parameters, such as message (used to direct the model to summarize the contents of the webpage), preamble (set to an empty string to make the model give a brief response), and temperature (we set it to a low value to make the outputs less random). Refer to the API reference to learn more about the parameters of the Chat endpoint.\\nThis function then takes the endpoint response, response.text, and sends it to the display(text) function we created in the previous section.\\n//Fetch the summary for the given text and display it function summarize(text) { // Use the stored API of user chrome.storage.sync.get(\\'apiKey\\', key => { // Set up the request to send to the endpoint options = { \"method\": \"POST\", \"headers\": { \"accept\": \"application/json\", \"content-type\": \"application/json\", \"authorization\": \"Bearer \" + key.apiKey }, /* These are the chat endpt paramters. Try playing around with them and reloading the extension to see how they affect the summarization behaviour. Reference: https://docs.cohere.com/reference/chat */ \"body\": JSON.stringify({ \"message\": \"Generate a summary of this webpage: \" + text, \"preamble\": \"\", \"temperature\": 0.1 }) }; fetch(\\'https://api.cohere.ai/v1/chat\\', options) .then((response) => response.json()) .then((response) => { if (response.text === undefined) { /* If there is no summary in the endpoint response, display whatever error message it returned */ display(\"There was an error: \" + response.message); } else { // Otherwise, display the summary display(\"tl;dr: \" + response.text); } }); }); } \\nStep 3: Create a Function for Text Capturing\\nNow we create a function that feeds the text needed by the summarize(text) function. Here, getVisibleText() returns only the visible text from the page. It uses jQuery selectors (requires jQuery v3.7.0 to be added to the scripts folder) to try to find the page\\'s main body of content, often in a content or main element. It also defines a fallback to using the whole body content.\\n// Returns true if the given element is not visible on the page function isHidden(el) { var style = window.getComputedStyle(el); return ((style.display === \\'none\\') || (style.visibility === \\'hidden\\')) } // Returns only the visible text from the page function getVisibleText() { /* Using jQuery selectors, try to find the main body of content of the page, often in a content or main element. Fall back to using the whole body which is ~universal. */ var body = document.querySelector(\\'body\\') if (document.querySelector(\\'#content\\')) { body = document.querySelector(\\'#content\\'); } if (document.main) { body = document.querySelector(\\'main\\'); } var allTags = body.getElementsByTagName(\\'*\\'); let visibleText = []; var nChars = 0; // Select all visible text in the body, up to charLimit for (var i = 0, max = allTags.length; i < max; i++) { var elem = allTags[i]; if (!isHidden(elem)) { var text = $(elem).contents().filter(function() { return this.nodeType == Node.TEXT_NODE; }); if (text === undefined || text.length == 0) { continue; } text = text[0].nodeValue nChars += text.length + 1; // for newline if (nChars < charLimit) { visibleText.push(text); } else { break } } } // Separate all the text elements with a newline return visibleText.join(\\'\\\\n\\'); } \\nThe following code block stitches together the three steps we created to capture visible text from a web page, summarize it, and display it at the top.\\n// This code block runs when pages are loaded. chrome.storage.sync.get(\\'apiKey\\', key => { if (key.apiKey === undefined) { // If there is no saved API key, tell the user how to add one display(\"Please set an API key in co:ndense > Options\"); } else { // If there is a key, we can use it to summarize the page const truncatedVisibleText = getVisibleText(); /* During the dev process, it is helpful to be able to see exactly what text is being summarized */ console.log(truncatedVisibleText); summarize(truncatedVisibleText); } }); \\nStep 4: Create a Function for Cohere API Key Entry\\nWe also need to create a user interface for users to add a Cohere API key before they can run the extension. For this, we create a new folder called options and create two files, options.html and options.js. For brevity, the code blocks are not included here, but you can refer to the repository for the full code.\\nStep 5: Create a Chrome Extension Manifest File\\nThe manifest file is required by a Chrome extension, describing how the extension is configured. It is a JSON file placed in the extension\\'s root directory. It also contains metadata about the extension, such as its name, version, and author.\\nSo let’s create a manifest.json file at the root of our folder. The contents of the file are shown below. It contains the metadata, permissions, description of the UI for adding the API key, and the scripts to run.\\n// This file describes how the extension is configured. { // Metadata \"name\": \"co:ndense\", \"version\": \"1.0\", \"description\": \"Summarizes web pages\", \"manifest_version\": 3, // - activeTab gives us access to the currently active tab of user // - scripting gives us permission to run our code // - storage allows us to store the user API key \"permissions\": [\"activeTab\", \"scripting\", \"storage\"], // This describes the UI for user options \"options_ui\": { // This page contains the UI \"page\": \"options/options.html\", // The options will open as a pop-up, not a new tab \"open_in_tab\": false }, \"content_scripts\": [{ // scripts/content.js contains the main logic of extension. // It relies on jQuery so we need to configure the path to that too. \"js\": [\"scripts/content.js\", \"scripts/jquery-3.7.0.min.js\"], // This extension will run on all webpages by default. \"matches\": [ \"<all_urls>\" ], // Do not run the extension on these specific sites \"exclude_globs\": [ // The header breaks cursor alignment in Google Docs \"*://docs.google.com/*\" ] }] } \\nStep 6: Load the Extension to Google Chrome\\nThe code package is now complete, and we can load the extension to Chrome. For this, take the following steps:\\nGo to chrome://extensions/.\\nAt the top right, turn on Developer mode.\\nClick Load unpacked at the top left.\\nFind and select the folder where the code package is located. The extension will now appear in the list of extensions. Make sure it is enabled using the button on the bottom right.\\nSelect Details on the extension. Click Extension Options and then add an API key\\nStep 7: Run the Extension on Google Chrome\\nWe are now ready to run the extension. Open a new tab in Chrome and then click the Extension icon at the top right (look for the jigsaw piece icon).\\nThen go to any web page. After a short while, a summary of the page will be displayed at the top! (look for the purple banner)\\nConclusion\\nThe Chrome extension provides developers with a platform to build applications that enrich the web browsing experience. And the ease of access to large language models like Cohere’s unlocks new types of applications that enable more efficient and natural language-based interactions with the browser.'),\n",
       " Document(metadata={'title': 'Deploying with Databutton', 'source': 'https://cohere.com/llmu/deploy-databutton'}, page_content='Deploying with DatabuttonQualified\\nThis chapter is contributed by Cohere’s Community Champion, Elle Neal.\\nDatabutton is an all-in-one online workspace designed to streamline the process of creating, deploying, and managing data apps. It supports Streamlit-based applications, but it also comes with additional features to make the process of developing and deploying applications simple. For example, Pages allows you to create multipage UIs for your users, Jobs enables scheduling of Python code, Libraries provides a place to write reusable code across your app, and Data Storage offers a simple put/get data store for various types of data.\\nThis tutorial demonstrates creating a topic modeling application using Databutton.\\nIt involves the following steps:\\nStep 1: Load and embed data\\nStep 2: Create a user interface with Streamlit code\\nStep 3: Cluster data to identify groups\\nStep 4: Auto-generate cluster label names\\nStep 5: Deploy and share application\\nSetup\\nCreate a free account with Databutton.\\nCreate a new app by clicking on ‘New Blank App`on the home screen.\\nSet the app’s configurations in theConfiguresection.\\nInSecrets, add the Cohere API key.\\nIn Installed packages, add the following packages: cohere, scikit-learn, hdbscan, umap-learn, setuptools, plotly, matplotlib, and datasets.\\nStep 1: Load and Embed Data\\nAs a default, this app will use Amazon’s MASSIVE dataset, which contains a list of commands that people give to their AI-based personal assistant (e.g., Alexa).\\nWe’ll build a few helper functions:\\nTo generate embeddings via the Embed endpoint\\nTo reduce the dimensions of these embeddings to two for plotting purposes\\nTo save the embeddings into a JSON file in Databutton’s data storage\\nThe user needs to generate the embeddings of the dataset only once. This provides two outputs that are saved to Databutton’s data storage: the embeddings as a JSON file and the embeddings’ reduced dimensions that are appended to the dataset’s DataFrame.\\nAs the file is now saved to storage, the user can access the data throughout the application without having to perform this task again. There is no need to worry about caching the dataset for the user; with one line of code, you can return the data to the user anywhere within the application.\\nThe following are the helper functions mentioned above.\\n# Function to generate Cohere embeddings def embed_text(texts): embeddings = \\\\[] for i in range(0, len(texts), 90): batch = texts[i:i+90] output = co.embed( model=\"embed-english-v3.0\", input_type=\"search_document\", texts=batch) embeddings.extend(output.embeddings) time.sleep(60) # accounting for Cohere trial API key rate limit return embeddings \\n# Function to reduce dimensionality of embeddings using umap def reduce_dimensionality(embeddings): reducer = umap.UMAP() umap_embeddings = reducer.fit_transform(embeddings) return umap_embeddings[:, 0], umap_embeddings[:, 1] \\n# Function to save embeddings into a json file in Databutton data storage def save_embeddings_to_json(df): # Create a dictionary where each key is the index of the DataFrame and each value is the corresponding embedding embeddings_dict = df[\\'embedding\\'].to_dict() \\nStep 2: Create a User Interface with Streamlit\\nWe’ll build the user interface using the Streamlit API, which goes through the following steps when a user interacts with the application.\\nUser uploads a CSV file. Or, they can opt for the sample dataset (Amazon’s MASSIVE dataset).\\nText embeddings are generated using the Embed endpoint, and the embeddings will be reduced to two dimensions using UMAP (Uniform Manifold Approximation and Projection).\\nThe embeddings will be saved to Databutton storage as a JSON file.\\nThe reduced embeddings will be saved to Databutton’s data storage as a new CSV file.\\nThe following is what the user interface will look like at this point.\\nThe reduced embeddings are now saved as a new file. We can copy the code snippet to import and use the DataFrame anywhere in our application. Here is an example of how you call the data as a DataFrame:\\n# Call the embeddings data as a DataFrame df = db.storage.dataframes.get(key=”reduced.csv”) \\nStep 3: Cluster Data to Identify Groups\\nThis next stage in the workflow uses machine learning to perform text clustering — finding clusters in the data that share similar properties.\\nWe will create a user interface that provides interactivity at a few crucial points along the process of text clustering. This allows users to make decisions based on the results of initial analyses. Here are the step-by-step instructions:\\nLoad the DataFrame: The script will load the DataFrame previously saved in Databutton’s storage.\\nExtract UMAP coordinates: The script will extract the 2D UMAP coordinates that were previously computed. These coordinates are reduced representations of your data.\\nDetermine the optimal number of clusters: The script will compute the sum of squared errors (SSE) for a range of potential numbers of clusters. This is part of the process to use the k-means algorithm, which requires specifying the number of clusters beforehand. The script will plot an elbow plot, which can be used to select the optimal number of clusters — look for the “elbow” in the curve where adding more clusters doesn’t significantly decrease SSE.\\nCreate the elbow plot: The script will then visualize the SSE for different numbers of clusters as an elbow plot. This visualization will help you in choosing the optimal number of clusters.\\nUser selection of clusters: You will select the number of clusters based on the elbow plot. The selection is done using a slider in the Streamlit app.\\nRun k-means algorithm: With the chosen number of clusters, the script will run the k-means algorithm, which assigns each point in your data to one of the clusters.\\nPlot the clusters: The script will create a scatter plot of your data points, colored by their assigned cluster. This visualization gives you a spatial representation of how the algorithm has classified your data.\\nCalculate cluster centers and distances from centroids: The script calculates the centroid (or geometric center) of each cluster. It then computes how far each point in your data is from its cluster’s centroid. We will need this in the next step of the process where we label our clusters.\\nDisplay and save the results: The script will display the DataFrame that now includes cluster labels and distances from centroids. If you’re satisfied with the results, you can save this labeled data back into Databutton’s storage for later use.\\nimport databutton as db import streamlit as st from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score import matplotlib.pyplot as plt import numpy as np import plotly.express as px from scipy.spatial import distance # 1: Load DataFrame df = db.storage.dataframes.get(key=\"reduced.csv\") # 2: Extract UMAP coordinates umapx = df[\\'umap_x\\'] umapy = df[\\'umap_y\\'] umap_coords = np.column_stack((umapx, umapy)) # 3: Define a range of potential clusters and compute SSE clusters = range(2, 10) # You may want to modify this range sse = \\\\[] for k in clusters: kmeans = KMeans(n_clusters=k, random_state=0) kmeans.fit(umap_coords) sse.append(kmeans.inertia_) # 4: Plot the elbow plot fig, ax = plt.subplots(figsize=(10, 5)) plt.plot(clusters, sse, \\'bx-\\') plt.xlabel(\\'k (number of clusters)\\') plt.ylabel(\\'Sum of Squared Errors (SSE)\\') plt.title(\\'Elbow Plot For Optimal Number of Clusters\\') st.pyplot(fig) # 5: User selects number of clusters based on elbow plot n_clusters = st.slider(\\'Number of Clusters\\', min_value=2, max_value=10, value=2) # 6: Run KMeans with optimal number of clusters kmeans_model = KMeans(n_clusters=n_clusters, random_state=0) df[\\'cluster_labels\\'] = kmeans_model.fit_predict(umap_coords) # 7: Plotting the clusters fig = px.scatter(df, x=\\'umap_x\\', y=\\'umap_y\\', color=\\'cluster_labels\\', hover_data=[\\'utt\\']) st.plotly_chart(fig) # 8: Calculate cluster centers and distances from centroids centroids = df.groupby(\\'cluster_labels\\')\\\\[[\\'umap_x\\', \\'umap_y\\']].mean().reset_index() def calc_distance(row): centroid = centroids\\\\[centroids[\\'cluster_labels\\'] == row[\\'cluster_labels\\']] centroid_coords = (centroid[\\'umap_x\\'].values[0], centroid[\\'umap_y\\'].values[0]) row_coords = (row[\\'umap_x\\'], row[\\'umap_y\\']) return distance.euclidean(row_coords, centroid_coords) df[\\'distance_from_centroid\\'] = df.apply(calc_distance, axis=1) # 9: Display and save the results selected_cluster = st.selectbox(\\'Select a cluster label\\', df[\\'cluster_labels\\'].unique()) temp_df = df\\\\[[\\'utt\\', \\'cluster_labels\\', \\'distance_from_centroid\\']] st.write(temp_df\\\\[temp_df[\\'cluster_labels\\'] == selected_cluster]) if st.button(\"Save Labelled Data\"): db.storage.dataframes.put(key=\"cluster.csv\", df=df) \\nStep 4: Auto-Generate Cluster Names\\nFinally we’ll use the Chat endpoint to suggest a name for each cluster based on examples sampled from the clustered data. We’ll also generate the keywords for each cluster using the TF-IDF algorithm.\\nWe use the utterance_prompt as the prompt to the Chat endpoint to generate descriptive labels for the data clusters.\\nutterance_prompt = \"\"\" These are clusters of commands given to an AI-based personal assistant. Each cluster represents a specific type of task or query that users often ask their personal assistant to perform. A list of keywords summarizing the collection is included, along with the name of the cluster. The name of each cluster should be a brief, precise description of the common theme within the utterances. --- Cluster #0 Sample utterances from this cluster: - status for the pizza delivery from pizza hut - find and order rasgulla of janta sweet home pvt ltd - i will be at pizza hut in ten minutes and will stay there for next forty minutes arrange an uber for me that can drop me home Keywords for utterances in this cluster: pizza, delivery, uber, order Cluster name: Food Delivery --- Cluster #1 Sample utterances from this cluster: - show me where i can find a train - can you show me the directions to go museum of flight in seattle - please book train ticket to new york Keywords for utterances in this cluster: train, directions, museum, book, ticket Cluster name: Travel and Directions --- Cluster #2 Sample utterances from this cluster: - get route for los angles from here - nearest restaurants available at this time - i want you to book a train ticket for me Keywords for utterances in this cluster: route, los angeles, restaurants, time, book, train, ticket Cluster name: Route Navigation and Reservations --- Cluster #3 Sample utterances from this cluster: \"\"\" \\nWe create several helper functions for processing the DataFrame, generating keywords, creating labels, and displaying information to the user. The extract_top_n_words function generates the most relevant keywords for each cluster. The generate_label function uses an AI model to generate a descriptive label for each cluster. The generate_keywords_and_label function wraps up these processes for each cluster and updates the DataFrame accordingly. The present_cluster_data function is used to present the information about each cluster to the user.\\n# Function to generate a name for each cluster def generate_label(customer_service_prompt, text_series): # Initialize Cohere model COHERE_API_KEY = db.secrets.get(name=\"COHERE_API_KEY\") co = cohere.Client(COHERE_API_KEY) text_list = text_series.tolist() formatted_text_list = \"\" for text in text_list: formatted_text_list += \"- \" + text + \"\\\\\\\\n\" prompt=customer_service_prompt + formatted_text_list response = co.chat( model=\\'command-r\\', message=prompt, max_tokens=800, temperature=0.2, k=0) return response.text, prompt \\n# Function to generate keywords for each cluster def extract_top_n_words(vectorizer, tfidf_matrix, n=10): \"\"\" Given a TfidfVectorizer and a TF-IDF matrix, return the `n` words with the highest TF-IDF scores. \"\"\" # Get the names of the words #feature_names = vectorizer.get_feature_names_out() # Sum tfidf frequency of each term through documents summed_tfidf = np.sum(tfidf_matrix, axis=0) # Connecting term to its sums frequency words_freq = [(word, summed_tfidf[0, idx]) for word, idx in vectorizer.vocabulary_.items()] words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True) # Return the n words with highest tfidf return [word[0] for word in words_freq[:n]] \\n# Helper function to generate the cluster name and keywords @st.cache_resource def generate_keywords_and_label(df, cluster, utterance_prompt): # Filter the DataFrame for each cluster df_cluster = df\\\\[df[\\'cluster_labels\\'] == cluster] # Generate the TF-IDF matrix vectorizer = TfidfVectorizer(stop_words=\\'english\\') tfidf_matrix = vectorizer.fit_transform(df_cluster[\\'utt\\']) # Extract the top N keywords from each cluster keywords = extract_top_n_words(vectorizer, tfidf_matrix, n=10) # Generate a summary label using the AI model prompt = utterance_prompt + \"\\\\nKeywords for messages in this cluster: \" + \\', \\'.join(keywords) + \"\\\\n\" summary, prompt = generate_label(prompt, df_cluster[\\'utt\\'].sample(n=5)) # Extract cluster name from AI generated label start = summary.find(\"Cluster name:\") + len(\"Cluster name:\") end = summary.find(\"\\\\n\", start) cluster_name = summary[start:end].strip() # Update original DataFrame with generated label and keywords df.loc[df[\\'cluster_labels\\'] == cluster, \\'label\\'] = cluster_name df.loc[df[\\'cluster_labels\\'] == cluster, \\'keywords\\'] = \\', \\'.join(keywords) return df, keywords, cluster_name # Helper function to present cluster data def present_cluster_data(df, cluster, keywords, label): df_cluster = df\\\\[df[\\'cluster_labels\\'] == cluster] st.markdown(f\"**Cluster {cluster}**\") st.markdown(f\"**Generated Keywords:** {\\', \\'.join(keywords)}\") st.markdown(f\"**AI Proposed Label:** {label}\") st.DataFrame(df_cluster[[\\'utt\\', \\'distance_from_centroid\\']]) \\nAnd finally, putting the steps together for cluster names and keyword generation, as well as adding the user interaction steps, we have the following:\\nData loading: The DataFrame is loaded from a Databutton storage with a key of \"cluster.csv\".\\nCluster processing: For each unique cluster in the DataFrame, the generate_keywords_and_label function is called to generate relevant keywords and an AI-generated label. These are added to the DataFrame. Then, the present_cluster_data function is used to display this information to the user.\\nUser interactions: The user is given the option to rename the AI-generated label for each cluster. If the user enters a new label, the DataFrame is updated with this new label.\\nSaving changes: Finally, the user can click a button to save their changes to the DataFrame. When the “Save changes” button is clicked, the updated DataFrame is saved back to the Databutton storage with a new key of \"labeled_cluster.csv\".\\nThese are reflected in the corresponding code block.\\nfrom sklearn.feature_extraction.text import TfidfVectorizer import databutton as db import streamlit as st import pandas as pd import cohere import numpy as np import time # helper functions... # Load Data df = db.storage.dataframes.get(key=\"cluster.csv\") # Initialize an empty dictionary to hold cluster labels cluster_labels = {} # Define the TF-IDF vectorizer vectorizer = TfidfVectorizer(stop_words=\"english\") clusters = df[\"cluster_labels\"].unique() if ( st.button(\"Generate AI Labels\", key=\"labelling\", type=\"primary\") or st.session_state.load_state ): st.session_state.load_state = True for cluster in clusters: df, keywords, label = generate_keywords_and_label(df, cluster, utterance_prompt) present_cluster_data(df, cluster, keywords, label) # Add user interaction to rename the label state_key = f\"user_label_{cluster}\" new_label = st.text_input( f\"Enter a new label for cluster {cluster} (leave empty to keep the AI proposed label)\", value=st.session_state.get(state_key, label), key=state_key, ) if new_label != label: df.loc\\\\[df[\"cluster_labels\"] == cluster, \"label\"] = new_label # For each cluster, find the utterance that is closest to the centroid for cluster in df[\"cluster_labels\"].unique(): min_distance_idx = df[df[\"cluster_labels\"] == cluster][ \"distance_from_centroid\" ].idxmin() df.loc[min_distance_idx, \"closest_centroid_utt\"] = df.loc[ min_distance_idx, \"utt\" ] # Create the scatter plot fig = px.scatter( df, x=\"umap_x\", y=\"umap_y\", color=\"cluster_labels\", hover_data=[\"utt\", \"label\"] ) # Add labels to the points that are closest to the centroid in each cluster for i in range(len(df)): if df.iloc[i][\"utt\"] == df.iloc[i][\"closest_centroid_utt\"]: fig.add_annotation( x=df.iloc[i][\"umap_x\"], y=df.iloc[i][\"umap_y\"], text=df.iloc[i][\"label\"] ) # Display the plot st.plotly_chart(fig) save = st.button(\"Save changes\", type=\"primary\") if save: # Reset the input session states after save. Also reset the button state st.session_state.load_state = False for key in st.session_state.keys(): if key.startswith(\\'user_label_\\'): del st.session_state[key] db.storage.dataframes.put(key=\"labeled_cluster.csv\", df=df) st.write(\"Labelled Data Saved\") \\nNow your app is ready to deploy and share with a few clicks.\\nConclusion\\nDeploying with Databutton enables the usage of Streamlit functionality with additional backend features, making it a more robust solution for larger or more complex projects. Its all-in-one, online workspace simplifies the process of application development, deployment, and management.'),\n",
       " Document(metadata={'title': 'What is Semantic Search?', 'source': 'https://cohere.com/llmu/what-is-semantic-search'}, page_content=\"What is Semantic Search?What is Semantic Search?\\nThis chapter comes with a notebook where you get to build a simple semantic search model to answer queries from a small dataset. And if you'd like a more advanced semantic search Colab, check this one here!\\nWhat is Semantic Search? - YouTube\\nWhat is Not Semantic Search?\\nBefore we learn semantic search, let’s see what is not semantic search. Before semantic search, the most popular way of searching was keyword search. Imagine that you have a list of many sentences, which are the responses. When you ask a question (query), keyword search looks for the sentence (response) with the largest number of words in common with the query. For example, consider the following query and set of responses:\\nQuery: Where is the world cup?\\nResponses:\\nThe world cup is in Qatar.\\nThe sky is blue.\\nThe bear lives in the woods.\\nAn apple is a fruit.\\nWith keyword search, you can notice that the responses have the following number of words in common with the query:\\nResponses:\\nThe world cup is in Qatar. (4 words in common)\\nThe sky is blue. (2 words in common)\\nThe bear lives in the woods. (2 words in common)\\nAn apple is a fruit. (1 word in common)\\nIn this case, the winning response is number 1, “The world cup is in Qatar”. This is the correct response, luckily. However, this won’t always be the case. Imagine if there was another response:\\nWhere in the world is my cup of coffee?\\nThis response has 5 words in common with the query, so it would win if it was in the list of responses. This is unfortunate, since this is not the correct response.\\nWhat can we do? We can improve keyword search, by removing stop words such as “the”, “and”, “is”, etc. We can also use methods like TF-IDF in order to tell apart relevant from non-relevant words. However, as you may imagine, there will always be cases in which, due to the ambiguity of the language, synonyms, and other roadblocks, keyword search will fail to find the best response. So we move on to the next algorithm, one that has performed very well: Semantic search.\\nIn short, semantic search works as follows:\\nIt uses a text embedding to turn words into vectors (lists of numbers).\\nUses similarity to find the vector among the responses which is the most similar to the vector corresponding to the query.\\nOutputs the response corresponding to this most similar vector.\\nIn this post, we’ll learn all these steps in detail. First, let’s look at text embeddings. If you need to brush up on these, check out this article.\\n💡\\nIf you enjoy this content, be sure to check out more in LLM University!\\nHow to Search Using Text Embeddings?\\nAn embedding is a way to assign to each sentence (or more generally, to each text fragment, which can be as short as a word or as long as a full article), a vector, which is a list of numbers. The Cohere embedding model used in the codelab for this post returns a vector of length 4096. This is a list of 4096 numbers (other Cohere embeddings, such as the multilingual one, return smaller vectors, for example, of length 768). A very important property of embeddings is that similar pieces of text get assigned to similar lists of numbers. For example, the sentence “Hello, how are you?” and the sentence “Hi, what’s up?” will be assigned lists of similar numbers, whereas the sentence “Tomorrow is Friday” will be assigned a list of numbers that are quite different from the two previous ones.\\nIn the next image, there is an example of an embedding. For visual simplicity, this embedding assigns to each sentence, a vector of length 2 (a list of two numbers). These numbers are plotted in the graph in the right, as coordinates. For example, the sentence “The world cup is in Qatar” gets assigned to the vector (4, 2), so it gets plotted in the point with coordinates 4 (horizontal) and 2 (vertical).\\nIn this image, all the sentences are located as points in the plane. Visually, you can identify that the query (represented by the trophy) is closest to the response “The world cup is in Qatar”, represented by the soccer ball. The other queries (represented by a cloud, a bear, and an apple) are much farther. Thus, semantic search would return the response “The world cup is in Qatar”, which is the correct response.\\nBut before we get any further, let’s actually use a real-life text embedding to search in a small dataset. The following dataset has four queries, and their four corresponding responses.\\nDataset:\\nQueries:\\nWhere does the bear live?\\nWhere is the world cup?\\nWhat color is the sky?\\nWhat is an apple?\\nResponses\\nThe bear lives in the woods\\nThe world cup is in Qatar\\nThe sky is blue\\nAn apple is a fruit\\nWe can use the Cohere text embedding to encode these 8 sentences. That would give us 8 vectors of length 4096, but we can use some dimensionality reduction algorithms to bring those down to length 2. Just like before, this means we can plot the sentences in the plane with 2 coordinates. The plot is below.\\nNotice that each query is closest to its corresponding response. That means that if we used semantic search to search for the response to each of these 4 queries, we would get the correct response.\\nHowever, here’s a caveat. In the above example, we used Euclidean distance, which is simply distance in the plane. This can be generalized to vectors of 4096 entries as well (using the Pythagorean theorem). However, that’s not the ideal way to compare pieces of text. The way that is most commonly used and that has given the best results is similarity, which we study in the next section.\\nUsing Similarity to Find the Best Document\\nSimilarity is a way to tell if two pieces of text are similar or different. This uses text embeddings. If you’d like to brush up on similarity, check out this article. In this article, two types of similarity used in semantic search are described:\\nDot product similarity\\nCosine similarity\\nFor now, let’s join them into one notion, and let’s say that similarity is a number assigned to each pair of documents, with the following properties:\\nThe similarity between a piece of text and itself is a very high number.\\nThe similarity between two very similar pieces of text is a high number.\\nThe similarity between two different pieces of text is a small number.\\nFor this article, we’ll use cosine similarity, which has the extra property that the values it returns are between 0 and 1. The similarity between a piece of text and itself is always 1, and the lowest value that a similarity can take is 0 (when two pieces of text are really dissimilar).\\nNow, in order to perform semantic search, all you have to do is calculate the similarity between the query and every pair of sentences, and return the sentence with the highest similarity. Let’s do an example. Below is a plot of the cosine similarities between the 8 sentences in the above dataset.\\nIn this plot, the scale is given at the right. Notice the following properties:\\nThe diagonal is full of 1’s (as the similarity between each sentence and itself is 1).\\nThe similarities between each sentence and its corresponding response are around 0.7.\\nThe similarities between any other pair of sentences are lower values.\\nThis means, if you were to search for the answer to, for example, the query “What is an apple?”, semantic search would look at the penultimate row on the table, and notice that the closest sentences are “What is an apple?” (with similarity 1), and “An apple is a fruit” (with similarity around 0.7). The system would eliminate the same query from the list, as it wouldn’t want to respond with the same question that is given. Thus, the winning response would be “An apple is a fruit”, which is the correct response.\\nThere is a hidden algorithm here that we didn’t mention, but is very important: The nearest neighbors algorithm. In short, this algorithm finds the nearest neighbor of a point in a dataset. In this case, the algorithm found the nearest neighbor of the sentence “What is an apple?”, and the response was the sentence “An apple is a fruit”. In the next section, you’ll learn more about nearest neighbors.\\nNearest Neighbors - Pros and Cons, and How to Fix Them\\nNearest neighbors is a very simple and useful algorithm, normally used for classification. More generally, it’s called k-nearest neighbors (knn), where k is any number. If the task at hand is classification, k-nearest neighbors will simply look at the k closest neighbors or a particular data point, and assign the data point the most common label amongst the neighbors. For example, if the task at hand is to classify a sentence as happy or sad (sentiment analysis), what 3-nearest neighbors would do is to look at the 3 closest neighbors to the sentence (in some embedding), and see if their majority (2) are happy or sad. That’s the label it assigns to the sentence.\\nAs you can see, k-nearest-neighbors is exactly what we’ve been doing for semantic search in this article. Given a query, you look for the closest neighbor in the embedding, and that’s the response to the query. In the current examples, that method has worked well. However, k-nearest-neighbors is not the fastest algorithm. The reason for this is that in order to find the neighbors of one point, one needs to calculate the distances between that point and all the other points in the dataset, and then find the smallest one. As you can see in the image below, in order to find the closest neighbor to the sentence “Where is the world cup?”, we had to calculate 8 distances, one for each other data point.\\nWhen dealing with massive archives, however, we can optimize for performance by slightly adjusting the algorithm to become approximate-k-nearest-neighbors. In particular, in search, there are several improvements that can speed up this process quite a lot. Here are two of them:\\nInverted File Index (IVD): Consists of clustering similar documents, then searching in the clusters that are closest to the query.\\nHierarchical Navigable Small World (HNSW): Consists on starting with a few points, and searching there. Then adding more points at each iteration, and searching in each new space.\\nMultilingual Search\\nAs you may have noticed, the performance of semantic search depends on the strength of the embedding. Therefore, any superpowers of the embedding may translate into superpowers of semantic search. The new multilingual embedding, by Cohere, is an embedding that supports more than 100 languages. In short, the embedding will send any piece of text in any of these languages, to a vector (this time of length 768). Similar pieces of text will be sent to similar vectors. Therefore, one can search using a query in any language, and the model will search for answers in all the other languages.\\nIn the image below, you can see an example of multilingual embedding. The embedding sends each sentence to a vector of length 4096, but, just like in the previous example, a projection is used to send this vector to one of length 2. These 2 entries in the vector are used as coordinates in the plane for the plot.\\nIn this plot, we have 4 sentences in English, together with their direct translations in Spanish and French.\\nEnglish:\\nThe bear lives in the woods.\\nThe world cup is in Qatar.\\nAn apple is a fruit.\\nThe sky is blue.\\nSpanish:\\nEl oso vive en el bosque.\\nEl mundial es en Qatar.\\nUna manzana es una fruta.\\nEl cielo es azul.\\nFrench:\\nL’ours vit dans la forêt.\\nLa coupe du monde est au Qatar.\\nUne pomme est un fruit.\\nLe ciel est bleu.\\nAs you can see in the plot, the multilingual model has located each sentence and its two translations very close together. Here’s a video if you’d like to learn more about multilingual embeddings and search.\\nAre Embeddings and Similarity Enough? (No)\\nIn this article, you’ve seen how effective a search system can be when it consists of a solid embedding, plus a search based in similarity. But is this the end of the story? Unfortunately (or fortunately?) no. It turns out that only using these two tools can lead to some mishaps. Luckily, these are mishaps that we can fix. Here is an example. Let’s extend our initial dataset a bit, by adding some more responses to the world cup question. Consider the following sentences.\\nQuery: “Where is the world cup?”\\nResponses:\\nThe world cup is in Qatar\\nThe world cup is in the moon\\nThe previous world cup was in Russia\\nWhen we locate them in the embedding above, they are all close, as expected. However, the closest sentence to the query is not response 1 (the correct answer), but response 3. This response (“The previous world cup is in Russia”) is a correct statement, and semantically close to the question, but it’s not the answer to the question. Response 2 (“The world cup is in the moon”) is a completely wrong answer, but also semantically close to the query. As you can see in the embedding, it’s dangerously close to the query, meaning it is likely that bogus answers like these will be the top results using semantic search.\\nHow do we fix this? There are many ways to improve search performance so that the actual response the model returns is ideal, or at least close to ideal. One of them is multiple negative ranking loss: Having positive pairs (query, response) and several other negative pairs (query, wrong response). Training the model to reward positive pairs, and punish negative pairs.\\nIn the current example, we would take a positive (query, response) pair, such as this one:\\n(Where is the world cup?, The world cup is in Qatar.)\\nWe would also take several negative (query, response) pairs, such as:\\n(Where is the world cup?, The world cup is in the moon)\\n(Where is the world cup?, The previous world cup was in Russia)\\n(Where is the world cup?, The world cup is in 2022.)\\nBy training the model to respond negatively to bad (query, response) pairs, the model is more likely to give the correct answer to a query.\\nNow the question is, how do we train the model to do this? This is a topic for a future article. Other search topics we’ll be exploring in the future are the following, so stay tuned!\\nSemantic search, vector databases\\nMultilingual semantic search\\nSemantic search for long documents\\nRe-ranking endpoint\\nBillion-scale semantic search\\nSemantic search over semi-structured data\\nBulk encoding embeddings\\nConclusion\\nSemantic search is a very powerful way to search for an answer to a particular question. It is an improvement over keyword search, since it actually compares the semantic meaning of the query and the responses. In order to do this, it uses a text embedding, and it ranks the responses based on their similarity to the query.\"),\n",
       " Document(metadata={'title': 'Keyword Search', 'source': 'https://cohere.com/llmu/keyword-search'}, page_content='Keyword SearchQualified\\nThis chapter comes with a notebook, and we encourage you to follow it along as you read the chapter.\\nIn the previous chapter, you learned about the difference between keyword search and dense retrieval. In this lab, you’ll learn to use keyword search to query a large dataset of Wikipedia articles. Later in this same module, you’ll be able to improve your results with dense retrieval and rerank, on the same wikipedia dataset, and even be able to combine the search results with a generative model in order to generate answers in the form of sentences!\\nUsing a Vector Database\\nWe\\'ll set up a vector database for use in this and the next chapter. In order to store the Wikipedia dataset query, we’ll use the Weaviate vector database, which will give us a range of benefits. In simple terms, a vector database is a place where one can store data objects and vector embeddings, and be able to access them and perform operations easily. For example, finding the nearest neighbors of a vector in a dataset is a lengthy process, which is sped up significantly by using a vector database. This is done with the following code.\\nimport weaviate import cohere # Add your Cohere API key here # You can obtain a key by signing up in https://dashboard.cohere.com/ or https://docs.cohere.com/reference/key cohere_api_key = \"COHERE_API_KEY\" co = cohere.Client(cohere_api_key) # Connect to the Weaviate demo databse containing 10M wikipedia vectors # This uses a public READ-ONLY Weaviate API key auth_config = weaviate.auth.AuthApiKey(api_key=\"76320a90-53d8-42bc-b41d-678647c6672e\") client = weaviate.Client( url=\"https://cohere-demo.weaviate.network/\", auth_client_secret=auth_config, additional_headers={ \"X-Cohere-Api-Key\": cohere_api_key, } ) \\nQuerying the Wikipedia Dataset Using Keyword Matching\\nTo use keyword matching, we’ll first define the following function for keyword search. In this function, we’ll tell the vector database what properties we want from each retrieved document. We’ll also filter them to the English language (using results_lang), but feel free to explore searching in other languages as well!\\ndef keyword_search(query, results_lang=\\'en\\', num_results=10): properties = [\"text\", \"title\", \"url\", \"views\", \"lang\", \"_additional {distance}\"] where_filter = { \"path\": [\"lang\"], \"operator\": \"Equal\", \"valueString\": results_lang } response = ( client.query.get(\"Articles\", properties) .with_bm25( query=query ) .with_where(where_filter) .with_limit(num_results) .do() ) result = response[\\'data\\'][\\'Get\\'][\\'Articles\\'] return result \\nWe’ll use two search queries, of varying difficulty.\\nSimple query: “Who discovered penicillin?”\\nHard query: “Who was the first person to win two Nobel prizes?”\\nThe responses for these queries are “Alexander Fleming”, and “Marie Curie”. Now let’s see how keyword search does. Here are the top three results for each query (some results are repeated, so let’s look at the three top distinct ones).\\nQuery 1: “Who discovered penicillin?”\\nResponses:\\nPenicillin\\nAntibiotic\\nAlexander Fleming\\nAs you can see, keyword search did quite well. All three articles contain the answer, and in particular, the third one is the correct response: Alexander Fleming.\\nNow let’s see how it did with the more complicated query.\\nQuery 2: “Who was the first person to win two Nobel prizes?”\\nResponses:\\nNeutrino\\nWestern culture\\nReality television\\nThis time, keyword search was very far from finding the answer. If you explore the articles, you may notice that they contain several mentions of words such as “first”, “person”, “Nobel”, “prizes”, but none of them have any information on the first person to win two Nobel prizes. In fact, the neutrino article mentions a scientist who won two Nobel prizes, but this wasn’t the first person to achieve this feat.\\nConclusion\\nAs you can see, keyword search can be good for queries, like “Who discovered penicillin?”, in which you’d expect the answers to have a lot of words in common with the query. More specifically, if an article contains the words “discovered,” and “penicillin”, it’s also likely to contain the fact that Alexander Fleming discovered it.\\nWith harder queries like “Who was the first person to win two Nobel prizes?”, keyword search doesn’t do well. The reason is that the words in the query would appear in many instances without necessarily talking about something as specific as the first person who won two Nobel prizes. By matching words, we haven’t yet exploited the semantics of the sentence. A model that understands what we mean by “the first person to win two Nobel prizes” would be able to find the answer, which is exactly what dense retrieval does (see the next section).\\nAs you can see, keyword search can be good for some queries, like “Who discovered penicillin”, in which you’d expect the answers to have a lot of words in common with the query. More specifically, if an article contains the word “discovered” and \"penicillin\", it’s also likely to contain the fact that Alexander Fleming was the one who discovered it.\\nKeyword search can have a harder time with queries like “Who was the first person to win two Nobel prizes”, because there can be many articles which contain these words, yet not contain the answer. There can be articles that have words like \"first\", \"person\", \"win\", \"two\", and have nothing to do with the query. Moreover, there can be many articles about work that led to a Nobel prize which do not necessarily mention that Marie Curie was the first person to win two Nobel prizes.\\nLater in this module, you\\'ll learn two methods to improve keyword search. One is by creating a search system that actually understands the semantics of the queries and responses, and is able to match them based on their meaning and not based on the words contained. This is called semantic search, and you saw it in high level in the previous lesson (more specifically, the method you\\'ll learn is called dense retrieval, which is one type of semantic search). The other method is reranking, which is able to surface the pairs of queries and documents that are the most relevant to each other. In this way, you can use keyword search to retrieve, say, hundreds of articles with matching keywords, and then use reranking to surface the best ones.'),\n",
       " Document(metadata={'title': 'Dense Retrieval', 'source': 'https://cohere.com/llmu/dense-retrieval'}, page_content='Dense RetrievalQualified\\nThis chapter uses the same notebook as the previous chapter, and we encourage you to follow it along as you read the chapter.\\nIn the previous chapter, you used keyword search to query Wikipedia, using two different queries:\\nWho discovered penicillin?\\nWho was the first person to win two Nobel prizes?\\nKeyword search did well with the first query, but not with the second one. The reason is that the second one has very popular words, which may appear in many documents that don’t necessarily contain the answer to the question. A way to obtain better results is to have the model ‘understand’ what the question is actually asking. This is where semantic search comes into play. Semantic search is the ability to search by meaning – not just keyword matching. Language models enable two of the main ways of semantic search: Dense retrieval, and reranking. The concepts of dense retrievals were illustrated in a chapter earlier in this module, so feel free to go back and take a look if you’d like a refresher. In this chapter, you\\'ll see dense retrieval in action, with the same Wikipedia dataset and the same queries that you used for keyword search.\\nQuerying the Dataset Using Dense Retrieval\\nDense retrieval uses a text embedding in order to search for documents that are similar to a query. If you’d like to learn more about embeddings, please take a look at the embeddings chapter. Embeddings assign a vector (long list of numbers) to each piece of text. One of the main properties in an embedding is that similar pieces of text go to similar vectors.\\nIn short, dense retrieval consists of the following:\\nFinding the embedding vector corresponding to the query\\nFinding the embedding vectors corresponding to each of the responses (in this case, Wikipedia articles)\\nRetrieving the response vectors that are closest to the query vector in the embedding\\nDense retrieval finds the closest documents to the query in the embedding\\nTo use dense retrieval, we’ll first define the following function which uses the Weaviate client we defined in the previous chapter. Just like with keyword search, we’ll tell the vector database what properties we want from each retrieved document, and filter them to the English language (using results_lang).\\ndef dense_retrieval(query, results_lang=\\'en\\', num_results=10): nearText = {\"concepts\": [query]} properties = [\"text\", \"title\", \"url\", \"views\", \"lang\", \"_additional {distance}\"] # To filter by language where_filter = { \"path\": [\"lang\"], \"operator\": \"Equal\", \"valueString\": results_lang } response = ( client.query .get(\"Articles\", properties) .with_near_text(nearText) .with_where(where_filter) .with_limit(num_results) .do() ) result = response[\\'data\\'][\\'Get\\'][\\'Articles\\'] return result \\nChunking the Articles\\nThis process of finding the closest documents to a query in an embedding will yield good results. However, articles may be very long and this could make things complicated. In order to have more granularity, we’ll split the articles by paragraph. This means that we’ll find the embedding vector corresponding to each paragraph of each article in the Wikipedia dataset. In that way, when the model retrieves the answer, it will actually output the paragraph that it found the most similar to the query, as well as the article in which this paragraph belongs.\\nWikipedia articles get chunked by paragraph, and each chunk gets assigned an embedding vector\\nBack to Querying the Dataset\\nLet’s review the two queries we used in the previous chapter.\\nSimple query: “Who discovered penicillin?”\\nHard query: “What was the first person to win two Nobel prizes?”\\nNow, let’s look at the three top results for each query using dense retrieval. Recall that the responses here are at the paragraph level, so the model may sometimes retrieve the same article several times by outputting different paragraphs from the same article.\\nQuery 1: “Who discovered penicillin?”\\nResponses:\\nAlexander Fleming: “Sir Alexander Fleming (6 August 1881 - 11 March 1995) was a Scottish physician and microbiologist …”\\nPenicillin: “Penicillin was discovered in 1928 by Scottish scientist Alexander Fleming …”\\nPenicillin: “The term “penicillin” is defined as the natural product of “Penicillium” mould with antimicrobial activity. It was coined by Alexander Fleming ...”\\nAs you can see, dense retrieval did quite well by finding paragraphs that contain the exact answer. Now, let’s see how it did with the more complicated query.\\nQuery 2: “Who was the first person to win two Nobel prizes?”\\nResponses:\\nNobel prize in literature: “The Nobel prize in literature can be shared by two individuals …”\\nNobel prize: “Although posthumous nominations are not presently permitted, …”\\nNobel prize: “Few people have received two Nobel prizes. Marie Curie received the Physics prize …”\\nMarie Curie: “Marie Curie was the first woman to win a Nobel prize, the first person to win two Nobel prizes, …”\\nAs you can see, dense retrieval did much better than keyword search here. The second, third, and fourth results are in the correct documents (Nobel prize and Marie Curie), and in fact, the third and fourth results are in a paragraph which explicitly contains the answer. The reason for this is that the embedding captures the semantics of the text, and is able to see if two pieces of text have a similar meaning, even if they don’t necessarily share many words in common.\\nSearching in Other Languages\\nAs you may have noticed, the dense_retrieval function has a parameter called results_lang (see code lab). This parameter determines the language in which the search results are outputted. It is defaulted to English (\\'en\\') , but for this demo, it can also be set to German (\\'de\\'), French (\\'fr\\'), Spanish (\\'es\\'), Italian (\\'it\\'), Japanese (\\'ja\\'), Arabic (\\'ar\\'), (Simplified) Chinese (\\'zh\\'), Korean (\\'ko\\'), and Hindi (\\'hi\\'). However, the Cohere multilingual embedding handles over 100 languages.\\nFor the first example, let\\'s search for results to the English query \"Who was the first person to win two Nobel prizes\" in Arabic. The line of code is the following:\\narabic_results = dense_retrieval(hard_query, results_lang=\\'ar\\') \\nAs you can see in the lab, the top 3 are the following:\\nNobel Prize جائزة نوبل\\nList of Nobel Laureates قائمة الحاصلين على جائزة نوبل\\nWomen in Society امرأة\\nNow let\\'s search for the French results to a query in Spanish. The query is \"Quién descubrió la penicilina?\" (\"Who discovered penicillin?\").\\nspanish_query = \"Quien descubrio la penicilina?\" french_results = dense_retrieval(spanish_query, results_lang=\\'fr\\') \\nThe results are the following\\nPénicilline\\nPénicilline\\nAlexander Fleming\\nConclusion\\nDense retrieval is a search method that uses a text embedding to find the semantically closest answers to a query. In our Wikipedia example, dense retrieval did quite well in the three queries. In the next chapter you’ll learn how to improve search results even further, with the Reranking method.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bda9b3-8692-4ca1-8b1d-208d2eb035ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8d576c-db59-4ef8-af14-6cddb28efc68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "466"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4800ce9-3a31-4d0f-bcb4-051a7df16c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10708"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[10].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518ab78d-8c9d-4698-aee9-7286ba49292c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dfe8c1-e00f-4252-ac0b-dc70814eac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for doc in docs:\n",
    "    x = len(doc.page_content)\n",
    "    lengths.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b44d68-93a5-4258-ba0b-8bce983dcef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[466,\n",
       " 4113,\n",
       " 8898,\n",
       " 21332,\n",
       " 20412,\n",
       " 10834,\n",
       " 11794,\n",
       " 16466,\n",
       " 10873,\n",
       " 31397,\n",
       " 10708,\n",
       " 15184,\n",
       " 14255,\n",
       " 9727,\n",
       " 16093,\n",
       " 21687,\n",
       " 22435,\n",
       " 12602,\n",
       " 9143,\n",
       " 6448,\n",
       " 16075,\n",
       " 10785,\n",
       " 11162,\n",
       " 12920,\n",
       " 12701,\n",
       " 10930,\n",
       " 15367,\n",
       " 24705,\n",
       " 8766,\n",
       " 15962,\n",
       " 13198,\n",
       " 6271,\n",
       " 3163,\n",
       " 11542,\n",
       " 7033,\n",
       " 5904,\n",
       " 10372,\n",
       " 19277,\n",
       " 8207,\n",
       " 6243,\n",
       " 10894,\n",
       " 11707,\n",
       " 6131,\n",
       " 10639,\n",
       " 7842,\n",
       " 6490,\n",
       " 9152,\n",
       " 17299,\n",
       " 14546,\n",
       " 6265,\n",
       " 6943]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c22ae8-f21b-4599-ba18-abf2f312354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000, chunk_overlap=200, length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d505a0fa-afe4-4cf2-9c29-92ebe8e4a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_split = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88008ac6-8142-49ea-8222-6a3d9d2c3ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'LLM University | Cohere', 'source': 'https://cohere.com/llmu'}, page_content='LLM University | CohereQualified\\nWelcome to LLM University, your premier learning destination for mastering Enterprise AI technologies. Designed for developers and technical professionals, our hub offers comprehensive resources, expert-led courses, and step-by-step guides to help you start building quickly and stay ahead in the rapidly evolving AI landscape.\\nJoin us to enhance your skills, drive innovation, and unlock the full potential of AI in your enterprise.'),\n",
       " Document(metadata={'title': 'Reranking', 'source': 'https://cohere.com/llmu/reranking'}, page_content='RerankingQualified\\nThis chapter uses the same notebook as the previous chapter.\\nIn previous chapters, you learned keyword search and dense retrieval, and you were able to apply them by querying a large Wikipedia article dataset. You noticed that keyword search performed well with some queries, and not so well with others. Dense retrieval, on the other hand, worked well with all the queries.\\nFor both, keyword search and dense retrieval, and in fact, for any other search mechanism we use, there is a very powerful method called reranking, which can enhance it. With Cohere, you can perform reranking using the Rerank endpoint.\\nReranking works as follows: For each pair (query, response), it assigns a relevance score. As the name hints, relevance scores are high for pairs in which the response is relevant to the query, and low otherwise. In this chapter, you’ll learn how to use Reranking to improve the wikipedia search results you found previously in this module.\\nUsing Rerank to Improve Keyword Search\\nReranking is a very powerful method which can significantly boost any existing search system. In short, reranking takes a query and a response, and outputs a relevance score between them. In that way, one can use any search system to surface a number of documents that can potentially contain the answer to a query, and then sort them using the Rerank endpoint.\\nThe results from any search system get reranked based on their relevance to the query\\nRemember that the results we obtained for the query “Who was the first person to win two Nobel prizes” using the keyword_search function were the following (for the full text, please check out the Colab notebook):\\nQuery: “Who was the first person to win two Nobel prizes?”\\nResponses:\\nNeutrino\\nWestern culture\\nReality television'),\n",
       " Document(metadata={'title': 'Reranking', 'source': 'https://cohere.com/llmu/reranking'}, page_content=\"Query: “Who was the first person to win two Nobel prizes?”\\nResponses:\\nNeutrino\\nWestern culture\\nReality television\\nThese could contain the answer somewhere in the document, but they are certainly not the best documents for this query. Let’s dig in a bit more, and find the first 100 results. To save space, I’ll only note the top 20 titles.\\nNeutrino\\nWestern culture\\nReality television\\nPeter Mullan\\nIndiana Pacers\\nWilliam Regal\\nNobel Prize\\nNobel Prize\\nNobel Prize\\nNoble gas\\nNobel Prize in Literature\\nD.C. United\\nNobel Prize in Literature\\n2021-2022 Manchester United F.C. season\\nNobel Prize\\nNobel Prize\\nZach LaVine\\n2011 Formula One World Championship\\n2021-2022 Manchester United F.C. season\\nChristians\\nOk, there’s a high chance that the answer is there. Let’s see if reranking can help us find it. The following function calls the Rerank endpoint. Its inputs are the query, the responses, and the number of responses we’d like to retrieve.\\ndef rerank_responses(query, responses, num_responses=3): reranked_responses = co.rerank( query = query, documents = responses, top_n = num_responses, model = 'rerank-english-v3.0', return_documents=True ) return reranked_responses \\nRerank will output the result, as well as the relevance score. Let’s look at the top 3 results.\\nQuery: “Who was the first person to win two Nobel prizes?”\\nResponses:\\nNobel Prize: “Five people have received two Nobel Prizes. Marie Curie received the …”\\nRelevance score: 1.00\\nNobel Prize: “In terms of the most prestigious awards in STEM fields, only a small …”\\nRelevance score: 0.97\\nNobel Prize in Literature: “There are also prizes for honouring the lifetime achievement of writers …”\\nRelevance score: 0.87\\nWell, that certainly improved the keyword search results! Even though the third result doesn’t work, the first two retrieved the correct article that contains the answer. Notice that the relevance score for both is close to 1.\\nConclusion\"),\n",
       " Document(metadata={'title': 'Reranking', 'source': 'https://cohere.com/llmu/reranking'}, page_content='Conclusion\\nReranking is a very useful method to find the most relevant responses to a particular query. It is very useful as a way to improve keyword search for dense retrieval. In this lab, we used it to vastly improve the results of keyword search, by first using keyword search to retrieve 100 potential documents that may contain the answer, and then using the Rerank endpoint to retrieve the top 3 among those. We encourage you to try reranking to improve the other searches we performed in the previous labs, and check your results!'),\n",
       " Document(metadata={'title': 'Generating Answers', 'source': 'https://cohere.com/llmu/generating-answers'}, page_content=\"Generating AnswersQualified\\nThis chapter uses the same notebook as the previous chapter.\\nIn this module, you’ve learned how to search and retrieve information from large databases in very effective ways. In this chapter, you’ll learn how to combine this with a generative model, in order to get an answer in sentence format, instead of a list of search results.\\nLarge Language Models, as you know, are very good at answering questions, but they are prone to some limitations, such as incorrect information, or even hallucinations. A good way to fix this is to enhance an LLM with a search mechanism.\\nIn short, this combination is done in the following way:\\nGiven a query, the search mechanism retrieves one or more documents containing the answer.\\nThese documents are given to the large language model, and it is instructed to generate an answer based on that information.\\nI like to imagine this the following way. If I have a question about thermodynamics, I can pick a random friend of mine, and ask them that question. They may or may not get the answer wrong. But if I go and search a few chapters in books about thermodynamics, I give them to my friend, and then I ask them to answer the question based on that, they are much more likely to answer the question correctly.\\nIn this chapter, we'll compare two ways a large language model can answer a question. The first one is by feeding the question to the generative model, and obtaining an answer. This will generate an answer that could be correct, but it may not. This is equivalent to asking your friend a thermodynamics question.\\nA generative model receives a query and outputs a response. This response may be inaccurate.\\nThe second one starts by using a search system to retrieve documents where the answer to the query is likely to appear. Then we feed the question and the documents to a generative model, and prompt it to answer the question using these documents. This yields a more accurate response.\"),\n",
       " Document(metadata={'title': 'Generating Answers', 'source': 'https://cohere.com/llmu/generating-answers'}, page_content='The query is first given to a search system, which retrieves documents which are likely to contain the answer. Then the query and the documents (context) are fed to the generative model, for a more accurate response.\\nGenerating Answers (Without Search)\\nLet’s first use a generative model to answer a slightly harder question — without search. We are trying to find out how many people won more than one Nobel prize. So, we ask the model the following query.\\nQuery: “How many people have won more than one Nobel prize?”\\nThe answer to this question is five: Marie Curie, Linus Pauling, John Bardeen, Frederick Sanger, and Karl Barry Sharpless.\\nThe way to ask this to the model is with the following line of code, which calls the co.chat endpoint.\\nprediction_without_search = [ co.chat( message=query, max_tokens=50, ) for _ in range(5) ] \\nWe call the endpoint five times to get five responses. The max_tokens parameter determines the length of the answer (which is why some answers appear truncated).\\nResponses:\\nMarie Skłodowska-Curie, a Polish and naturalized French physicist and chemist, won the Nobel Prize twice. She received the Nobel Prize in Physics in 1903 along with her husband Pierre Curie and Henri Becquerel for their pioneering work\\nMarie Skłodowska-Curie, a Polish physicist and chemist, won the Nobel Prize twice: once in physics and once in chemistry. She was awarded the Nobel Prize in Physics in 1903 along with her husband, Pierre Curie, and\\nMarie Skłodowska-Curie, a Polish and naturalized French physicist and chemist, won the Nobel Prize in Physics in 1903 and the Nobel Prize in Chemistry in 1911. She is the only person to have won multiple\\nMarie Skłodowska-Curie, a Polish and naturalized French physicist and chemist, won the Nobel Prize twice. She received the Nobel Prize in Physics in 1903 along with her husband, Pierre Curie, and Henri Becquerel for their'),\n",
       " Document(metadata={'title': 'Generating Answers', 'source': 'https://cohere.com/llmu/generating-answers'}, page_content='Marie Skłodowska-Curie, a Polish and naturalized French physicist and chemist, is the only person to have won multiple Nobel prizes. She won the Nobel Prize in Physics in 1903 and the Nobel Prize in Chemistry in 19\\nThese answers sound like they could be correct, but they’re all wrong. One reason for this is that transformers are good at talking and understanding sentiment and nuisances of the language, etc., but not so good at storing information. As a matter of fact, storing information inside the nodes of the neural network is not something that we can (or should!) fully trust.\\nInstead, let’s first search for the answer using what we’ve learned in the previous sections of this post.\\nSearching Answers\\nIn order to find the answer to this question in the Wikipedia dataset (the one we’ve been working with throughout this post), we can use the same dense_retrieval function that we used before. For simplicity, we will only use dense retrieval without Rerank, but we invite you to add it to the lab and see how the results improve!\\nresponses = dense_retrieval(query, num_results=20) \\nThis retrieves the top 20 articles, with their corresponding paragraphs. Here are the top three (remember that the search is done by finding the most similar paragraphs to the query, so some articles may appear several times with different paragraphs).\\nResponses:\\nNobel Peace Prize: “, the Peace prize has been awarded to 110 individuals and 27 organizations …”\\nNobel Prize: “The strict rule against awarding a prize to more than three people is also controversial …”\\nNobel Prize: “The prize ceremonies take place annually …”\\nNext, we’ll feed these 20 paragraphs to a generative model, and instruct it to answer the question in sentence format.\\nGenerating an Answer from the Search Results'),\n",
       " Document(metadata={'title': 'Generating Answers', 'source': 'https://cohere.com/llmu/generating-answers'}, page_content='Next, we’ll feed these 20 paragraphs to a generative model, and instruct it to answer the question in sentence format.\\nGenerating an Answer from the Search Results\\nIn order to get the generative model to answer a question based on a certain context, we need to create a prompt. And in this prompt, we need to give it a command and a context. The context will be the concatenation of all the paragraphs retrieved in the search step, which we can obtain using this line of code:\\ncontext = [r[\\'text\\'] for r in responses] \\nThe array context contains a lot of text, and, given the good results we’ve been obtaining with search mechanisms, we are fairly confident that somewhere in this text lies the answer to our original question. Now, we invoke the Chat endpoint. The prompt we’ll use is the following.\\nprompt = f\"\"\" Use the information provided below to answer the questions at the end. If the answer to the question is not contained in the provided information, say \"The answer is not in the context\". --- Context information: {context} --- Question: How many people have won more than one Nobel prize? \"\"\" \\nIn other words, we’ve prompted the model to answer the question, but only from information coming from the context array. And if the information is not there, we are prompting the model to state that the answer is not in the context. The following line of code will run the prompt. As before, we generate 5 answers, and max_tokens controls the length of each answer.\\nprediction_with_search = [ co.chat( message=prompt, max_tokens=50) for _ in range(5)] \\nThe five responses we get are the following (just like before, they are truncated):\\nFive people have won multiple Nobel prizes, according to the information provided. However, the source also states that another individual, Lê Đức Thọ, refused to accept the prize, and is therefore not counted among the winners. This brings the total number of multiple'),\n",
       " Document(metadata={'title': 'Generating Answers', 'source': 'https://cohere.com/llmu/generating-answers'}, page_content='Five people have won multiple Nobel prizes, according to the information provided. However, the source also states that another individual, Lê Đức Thọ, refused to accept the prize, and is therefore not counted among the winners. This brings the total number of multiple\\nFive people have won multiple Nobel prizes, according to the information provided. However, the source also states that another individual, Lê Đức Thọ, refused to accept the prize, and is therefore not counted among the winners. This brings the total number of multiple\\nFive people have won multiple Nobel prizes, according to the information provided. However, the source also states that another individual, Linus Pauling, was awarded two prizes but these were of different types, in different years. Marie Curie is the only person to\\nFive people have won multiple Nobel prizes, according to the information provided. However, the source also states that another individual, Lê Đức Thọ, refused to accept the prize, and is therefore not counted among the winners. This brings the total number of multiple\\nAs you can see, this improved the quality of the answer. It got the right number of people who received more than one Nobel prize, which is 5.\\nConclusion\\nGenerative models are prone to hallucinations. For example, when asked a question, they may answer with an incorrect answer. In this chapter you learned to power a generative model with search, in order to generate more accurate answers and reduce the chance of hallucinations.'),\n",
       " Document(metadata={'title': 'Constructing Prompts', 'source': 'https://cohere.com/llmu/constructing-prompts'}, page_content='Constructing PromptsQualified\\nContents\\nIntroduction\\nSetting Up\\nInstruction\\nSpecifics\\nContext\\nFormat\\nExamples\\nChain of Thought\\nSteps\\nConclusion\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nWhen working with large language models (LLMs), the prompt is the key to getting the desired response. A well-designed prompt will result in useful and accurate responses from a model and will considerably improve your experience interacting with it.\\nPrompts can be as simple as a one-liner, or they can be as complex as multiple layers of specific information. The more specific your command is, the more likely you will get exactly what you need from the model. We’ll look at some tips and ideas for constructing the commands in your prompt to help you get to your intended outcome. We’ll focus on the broad patterns without going into the long-tail list of techniques and tricks.\\nSetup\\nFirst, let’s install the Cohere Python SDK, get the Cohere API key, and set up the client.\\n! pip install cohere import cohere co = cohere.Client(\"COHERE_API_KEY\") # Your Cohere API key \\nLet’s also define a function to take a prompt and a temperature value and then call the Chat endpoint, which is how we can access the Command model.\\u200b\\u200b Here, we select the model type to be command.\\nWe set a default temperature value of 0, which nudges the response to be more predictable and less random. Throughout this article, you’ll see different temperature values being used in different situations. Increasing the temperature value tells the model to generate less predictable responses and instead be more “creative.”\\nThis function prints the text response generated by the model.\\ndef generate_text(prompt, temp=0): response = co.chat_stream( message=prompt, model=\"command-r-plus\", temperature=temp) for event in response: if event.event_type == \"text-generation\": print(event.text, end=\\'\\') \\nInstruction\\nAdding basic instructions to a prompt'),\n",
       " Document(metadata={'title': 'Constructing Prompts', 'source': 'https://cohere.com/llmu/constructing-prompts'}, page_content='Instruction\\nAdding basic instructions to a prompt\\nWhile prompts can morph into something very lengthy and complex, it doesn’t have to be that way all the time. At its core, prompting a Command model is about sending an instruction to a text generation model and getting a response back. Hence, the smallest unit of a perfectly complete prompt is a short line of instruction to the model.\\nLet’s say we want to generate a product description for a wireless headphone. Here’s an example prompt, where we create a variable for the user to input some text and merge that into the main prompt.\\nuser_input = \"a wireless headphone product named the CO-1T\" prompt = f\"\"\"Write a creative product description for {user_input}\"\"\" generate_text(prompt, temp=0.5) \\nThe model returns the following sample response, which does the job we asked for.\\nLooking for a wireless headphone option that won\\'t break the bank? Look no further than the CO-1T! These sleek headphones offer crystal clear sound and a comfortable, snug fit. With their Bluetooth capabilities, you can enjoy your favorite tunes without any distractions, whether you\\'re on the go or just relaxing at home. The CO-1T is also designed with a built-in microphone, making it easy to switch between listening to music and answering calls with minimal effort. But it doesn\\'t stop there – these headphones have an incredible battery life that will last you all day long, so you don\\'t have to worry about charging them constantly. Overall, the CO-1T offers a convenient, versatile, and affordable solution to all your wireless headphone needs. Meanwhile, the design is a stylish blend of black, gray, and silver that boasts a professional and modern look. \\nSpecifics\\nAdding specific details to a prompt'),\n",
       " Document(metadata={'title': 'Constructing Prompts', 'source': 'https://cohere.com/llmu/constructing-prompts'}, page_content='Specifics\\nAdding specific details to a prompt\\nA simple and short prompt can get you started, but in most cases, you’ll need to add specificity to your instructions. A generic prompt will return a generic response, and in most cases, that’s not what we want. In the same way that specific instructions will help humans do our job well, a model needs to be supplied with specific details to guide its response.\\nGoing back to the previous prompt, the generated product description was great, but what if we wanted it to include specific things, such as its features, who it is designed for, and so on? We can adjust the prompt to take more inputs from the user, like so:\\nuser_input_product = \"a wireless headphone product named the CO-1T\" user_input_keywords = \\'\"bluetooth\", \"wireless\", \"fast charging\"\\' user_input_customer = \"a software developer who works in noisy offices\" user_input_describe = \"benefits of this product\" prompt = f\"\"\"Write a creative product description for {user_input_product}, \\\\ with the keywords {user_input_keywords} for {user_input_customer}, and describe {user_input_describe}.\"\"\" generate_text(prompt, temp=0.5) \\nIn the example above, we pack the additional details of the prompt in a single paragraph. Alternatively, we can also compose it to be more structured, like so:\\nuser_input_product = \"a wireless headphone product named the CO-1T\" user_input_keywords = \\'\"bluetooth\", \"wireless\", \"fast charging\"\\' user_input_customer = \"a software developer who works in noisy offices\" user_input_describe = \"benefits of this product\" prompt = f\"\"\"Write a creative product description for {user_input_product}. Keywords: {user_input_keywords} Audience: {user_input_customer} Describe: {user_input_describe}\"\"\" generate_text(prompt, temp=0.5)'),\n",
       " Document(metadata={'title': 'Constructing Prompts', 'source': 'https://cohere.com/llmu/constructing-prompts'}, page_content=\"And here’s an example response. This time, the product description is tailored more specifically to our desired target customer, includes the key features that we specified, and sprinkles benefit statements throughout — all coming from the instruction we added to the prompt.\\nAre you a software developer struggling to focus in the hustle and bustle of noisy offices? The CO-1T wireless headphones are the ideal solution for you! With its Bluetooth connectivity and wireless design, you can enjoy uninterrupted music and podcast streaming from your devices, whether you're working or kicking back. One of the CO-1T's standout features is its fast charging capabilities, ensuring that you're never without your trusted noise-canceling companion. A quick recharge can provide you with a bustling day's worth of power, so you're never left lacking when inspiration (or your power bar) runs low. Designed with functionality and form in mind, the CO-1T delivers crisp and clear audio while its ergonomic design showcases a sleek and stylish look. Comfortable ear pads that won't irritate or pinch you guarantee comfort over long periods of use. Take control of your audio experience and stay focused on the tasks at hand with the CO-1T wireless headphones. Whether you're tuning out distractions or simply kicking back with some music, this Bluetooth headphone is the ideal companion for programmers and audiophiles alike. \\nThere are many other angles to add specificity to a prompt. Here are some examples:\\nStyle: Telling the model to provide a response that follows a certain style or framework. For example, instead of asking the model to “Generate an ad copy for a wireless headphone product” in the generic sense, we ask it to follow a certain style, such as “Generate an ad copy for a wireless headphone product, following the AIDA Framework – Attention, Interest, Desire, Action.”\"),\n",
       " Document(metadata={'title': 'Constructing Prompts', 'source': 'https://cohere.com/llmu/constructing-prompts'}, page_content='Tone: Adding a line mentioning how the tone of a piece of text should be, such as professional, inspirational, fun, serious, and so on. For example, “Tone: casual”\\nPersona: Telling the model to act like a certain persona helps to add originality and quality to the response. For example, “You are a world-class content marketer. Write a product description for…”\\nLength: Telling the model to generate text with a specific length, be it in words, paragraphs, and others. This helps guide the model to be verbose, concise, or somewhere in between. For example, “Write in three paragraphs the benefits of …”\\nContext\\nAdding contextual information to a prompt\\nWhile LLMs excel in text generation tasks, they struggle in context-aware scenarios. Here’s an example. If you were to ask the model for the top qualities to look for in wireless headphones, it will duly generate a solid list of points. But if you were to ask it for the top qualities of the CO-1T headphone, it will not be able to provide an accurate response because it doesn’t know about it (CO-1T is a hypothetical product we just made up for illustration purposes).\\nIn real applications, being able to add context to a prompt is key because this is what enables personalized generative AI for a team or company. It makes many use cases possible, such as intelligent assistants, customer support, and productivity tools, that retrieve the right information from a wide range of sources and add it to the prompt.\\nThis is a whole topic on its own, but to provide some idea, this demo shows an example of information retrieval in action. In this article though, we’ll assume that the right information is already retrieved and added to the prompt.\\nHere’s an example where we ask the model to list the features of the CO-1T wireless headphone without any additional context:\\nuser_input =\"What are the key features of the CO-1T wireless headphone\" prompt = user_input generate_text(prompt, temp=0)'),\n",
       " Document(metadata={'title': 'Constructing Prompts', 'source': 'https://cohere.com/llmu/constructing-prompts'}, page_content='user_input =\"What are the key features of the CO-1T wireless headphone\" prompt = user_input generate_text(prompt, temp=0) \\nThis generates a response that the model makes up since it doesn’t have any information to refer to.\\nThe CO-1T wireless headphone is a high-quality, comfortable, and durable headphone that is designed for use with a variety of devices. It features a sleek and modern design, a comfortable and secure fit, and a high-quality sound. The CO-1T is also equipped with a variety of features, including a built-in microphone, a multi-function button, and a rechargeable battery. \\nAnd here’s the same request to the model, this time with the product description of the product added as context.\\ncontext = \"\"\"Think back to the last time you were working without any distractions in the office. That\\'s right...I bet it\\'s been a while. \\\\ With the newly improved CO-1T noise-cancelling Bluetooth headphones, you can work in peace all day. Designed in partnership with \\\\ software developers who work around the mayhem of tech startups, these headphones are finally the break you\\'ve been waiting for. With \\\\ fast charging capacity and wireless Bluetooth connectivity, the CO-1T is the easy breezy way to get through your day without being \\\\ overwhelmed by the chaos of the world.\"\"\" user_input = \"What are the key features of the CO-1T wireless headphone\" prompt = f\"\"\"{context} Given the information above, answer this question: {user_input}\"\"\" generate_text(prompt, temp=0) \\nNow, the model accurately lists the features of the model.\\nThe CO-1T wireless headphones have noise-cancellation, fast charging capacity, and Bluetooth connectivity. \\nFormat\\nAdding output format requirements to a prompt\\nSo far, we saw how to get the model to generate responses that follow certain styles or include specific information. But we can also get the model to generate responses in a certain format. Let’s look at a couple of them: markdown tables and JSON strings.'),\n",
       " Document(metadata={'title': 'Constructing Prompts', 'source': 'https://cohere.com/llmu/constructing-prompts'}, page_content='Here, the task is to extract information from a list of invoices. Instead of providing the information in plain text, we can prompt the model to generate a table that contains all the information required.\\nprompt=\"\"\"Turn the following information into a table with columns Invoice Number, Merchant Name, and Account Number. Bank Invoice: INVOICE #0521 MERCHANT ALLBIRDS ACC XXX3846 Bank Invoice: INVOICE #6781 MERCHANT SHOPPERS ACC XXX9877 Bank Invoice: INVOICE #0777 MERCHANT CN TOWER ACC XXX3846 \"\"\" generate_text(prompt, temp=0) \\nThe response will come in the form of a markdown table.\\nHere is a table formatted with the columns of Invoice Number, Merchant Name, and Account Number filling in the previous information: | Invoice Number | Merchant Name | Account Number | | --- | --- | --- | | INVOICE #0521 | ALLBIRDS | XXX3846 | | INVOICE #6781 | SHOPPERS | XXX9877 | | INVOICE #0777 | CN TOWER | XXX3846 | Let me know if you\\'d like me to make any modifications to this table or provide you with any further assistance! \\nAnother useful format is JSON, which we can modify the prompt as follows.\\nprompt=\"\"\"Turn the following information into a JSON string with the following keys: Invoice Number, Merchant Name, and Account Number. Bank Invoice: INVOICE #0521 MERCHANT ALLBIRDS ACC XXX3846 Bank Invoice: INVOICE #6781 MERCHANT SHOPPERS ACC XXX9877 Bank Invoice: INVOICE #0777 MERCHANT CN TOWER ACC XXX3846 \"\"\" generate_text(prompt, temp=0) \\nThis returns the following response.\\nCertainly, here is the JSON format of the three bank invoices: ```json [ { \"invoice_number\": \"INVOICE #0521\", \"merchant_name\": \"ALLBIRDS\", \"account_number\": \"XXX3846\" }, { \"invoice_number\": \"INVOICE #6781\", \"merchant_name\": \"SHOPPERS\", \"account_number\": \"XXX9877\" }, { \"invoice_number\": \"INVOICE #0777\", \"merchant_name\": \"CN TOWER\", \"account_number\": \"XXX3846\" } ] \\nExamples\\nAdding examples to a prompt'),\n",
       " Document(metadata={'title': 'Constructing Prompts', 'source': 'https://cohere.com/llmu/constructing-prompts'}, page_content='Examples\\nAdding examples to a prompt\\nAll our prompts so far use what is called zero-shot prompting, which means that we are providing instruction without any example. But in many cases, it is extremely helpful to provide examples to the model to guide its response. This is called few-shot prompting.\\nFew-shot prompting is especially useful when we want the model response to follow a certain style or format. Also, sometimes it is hard to explain what you want in an instruction, and easier to show examples.\\nLet’s use an example task, where a model should take a request coming from a human and rephrase it into the most accurate utterance that an AI virtual assistant should use. The example data is taken from this paper (Einolghozati, et al. 2020).\\nWe’ll use this example request: “Send a message to Alison to ask if she can pick me up tonight to go to the concert together”. Given that request, we should expect the rephrased utterance to be something like: “Can you pick me up tonight to go to the concert together?”\\nFirst, let’s generate a response without giving the model an example. Here’s the prompt:\\nprompt=\"\"\"Turn the following message to a virtual assistant into the correct action: Send a message to Alison to ask if she can pick me up tonight to go to the concert together\"\"\" generate_text(prompt, temp=0) \\nThe response we get is not wrong, but it doesn’t follow the style that we need, which is a simple one-line rephrasing of the original request. Instead, it generates an email!\\nOk, I will send a message to Alison asking if she can pick you up tonight to go to the concert together. Please provide me with the details of your inquiry so I can assist you better! \\nNow, let’s modify the prompt by adding a few examples of how we expect the output to be.'),\n",
       " Document(metadata={'title': 'Constructing Prompts', 'source': 'https://cohere.com/llmu/constructing-prompts'}, page_content='Now, let’s modify the prompt by adding a few examples of how we expect the output to be.\\nuser_input = \"Send a message to Alison to ask if she can pick me up tonight to go to the concert together\" prompt=f\"\"\"Turn the following message to a virtual assistant into the correct action: Message: Ask my aunt if she can go to the JDRF Walk with me October 6th Action: can you go to the jdrf walk with me october 6th Message: Ask Eliza what should I bring to the wedding tomorrow Action: what should I bring to the wedding tomorrow Message: Send message to supervisor that I am sick and will not be in today Action: I am sick and will not be in today Message: {user_input}\"\"\" generate_text(prompt, temp=0) \\nThis time, the style of the response is exactly how we want it.\\ncan you pick me up tonight to go to the concert together? \\nChain of Thought\\nOne specific way to provide examples in a prompt is to show responses that include a reasoning step. This way, we are asking the model to “think” first rather than going straight to the response. In tasks involving mathematical questions, for example, there is a huge difference between directly giving the answer and adding a reasoning step in between.\\nThis concept is called chain of thought prompting, introduced by Wei et al. Let’s look at an example from the paper which illustrates this idea.\\nFirst let’s look at a prompt without a chain of thought. It contains one example of a question followed by the answer, without any intermediate calculation step. It also contains the new question we want to answer.'),\n",
       " Document(metadata={'title': 'Constructing Prompts', 'source': 'https://cohere.com/llmu/constructing-prompts'}, page_content='prompt=f\"\"\"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. \\\\ How many tennis balls does he have now? A: The answer is 11. --- Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? A: The answer is 9. --- Q: A box has 10 balls and a half of the balls are red balls. How many red balls are in the box if 4 red balls are added? A:\"\"\" generate_text(prompt, temp=0) \\nWe get the following response, which is an incorrect answer (12). And notice that the response gives the answer first, similar to the example shown.\\nThe answer is 12. There are 5 balls that are red and 5 balls that are not red, and adding 4 red balls would make it 9 red and 6 not red. \\nNow, let’s repeat that, this time with a chain of thought. Now, the example answer contains a reasoning step, describing the calculation logic to get to the final answer, before giving the final answer.\\nprompt=f\"\"\"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. \\\\ How many tennis balls does he have now? A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. \\\\ The answer is 11. --- Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? A: The cafeteria started with 23 apples. They used 20 to make lunch, so they have 23 - 20 = 3 apples. They bought 6 more apples, so they have 3 + 6 = 9 apples. \\\\ The answer is 9. --- Q: A box has 12 balls and a half of the balls are red balls. How many red balls are in the box if 4 red balls are added? A:\"\"\" generate_text(prompt, temp=0) \\nAnd we get the correct answer this time (10), with the response following the style of the example given.\\nThere are 12 / 2 = 6 red balls in the box. When 4 more red balls are added, the number of red balls becomes 6 + 4 = 10. The answer is 10. \\nSteps\\nAdding generation steps to a prompt'),\n",
       " Document(metadata={'title': 'Constructing Prompts', 'source': 'https://cohere.com/llmu/constructing-prompts'}, page_content='There are 12 / 2 = 6 red balls in the box. When 4 more red balls are added, the number of red balls becomes 6 + 4 = 10. The answer is 10. \\nSteps\\nAdding generation steps to a prompt\\nTo steer the model toward generating higher-quality responses, it can be helpful to add instructions for the model to generate intermediate steps before generating the final output. The information generated during these steps helps enrich the model’s context before it generates the final response.\\nThere could be another scenario where we specifically need the response to contain specific information. In this case, we can specify what we need in the prompt.\\nLet’s use an example of generating startup ideas. We can get the model to directly generate an idea for a given industry, like so:\\nuser_input = \"education\" prompt = f\"\"\"Generate a startup idea for this industry: {user_input}\"\"\" generate_text(prompt, temp=0.5) \\nThis generates the following response, which is reasonable, but perhaps not exactly what we are looking for.\\nHere is a startup idea for the education industry: \"MathsMadeEasy\" Description: \"MathsMadeEasy\" is an innovative K-12 ed-tech startup that aims to revolutionize the way young students learn mathematics. With a vision to eliminate math-related anxiety and foster a deep understanding of mathematical concepts, our platform offers a holistic learning experience that engages, empowers, and excites students on their mathematical journey. Approach & Features: 1. Gamified Learning: MathsMadeEasy infuses gaming elements into its interactive math lessons, turning repetitions into fun challenges and keeping students motivated to achieve their goals. 2. Personalized Learning: Using adaptive algorithms... \\nAlternatively, we can ask the model to generate information in steps, such as describing the problem to be solved and the target audience experiencing this problem.'),\n",
       " Document(metadata={'title': 'Constructing Prompts', 'source': 'https://cohere.com/llmu/constructing-prompts'}, page_content='Alternatively, we can ask the model to generate information in steps, such as describing the problem to be solved and the target audience experiencing this problem.\\nuser_input = \"education\" prompt = f\"\"\"Generate a startup idea for this industry: {user_input} First, describe the problem to be solved. Next, describe the target audience of this startup idea. Next, describe the startup idea and how it solves the problem for the target audience. Next, provide a name for the given startup. Use the following format: Industry: <the given industry> The Problem: <the given problem> Audience: <the given target audience> Startup Idea: <the given idea> Startup Name: <the given name>\"\"\" generate_text(prompt, temp=0.9) \\nThis provides a richer description of the startup idea.\\nIndustry: Education The Problem: Many people today struggle to develop meaningful relationships where they can cultivate connections and get support for their educational needs. Audience: Recent college graduates and young professionals seeking community and educational support. Startup Idea: Online community forum and networking platform specifically curating mentorship and guidance on continuing education, relevant career skills, and personal development. Startup Name: \"Nurture Nest\" Our platform offers a diverse range of discussion forums... \\nConclusion\\nIn this article, we looked at some techniques for constructing prompts for the Command model. A prompt can be as simple as a single line of instruction, though the more specific the prompt is, the higher the level of quality and accuracy you can expect from the response. Each building block added a prompt provides a different type of lever to enhance the quality of the response.'),\n",
       " Document(metadata={'title': 'Use Case Patterns', 'source': 'https://cohere.com/llmu/use-case-patterns'}, page_content='Use Case PatternsQualified\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nCohere’s Command model is an instruction-following text generation model trained to follow user commands. It is trained to be instantly useful in practical applications, covering a wide range of use cases.\\nHaving said that, if you are a developer just starting with this technology, it can be daunting to try to comprehend what’s possible. Large language models (LLMs) like the Command model are general-purpose and can be applied in infinite ways, but if one can’t recognize the patterns where they can be useful, it can feel overwhelming.\\nIn this chapter, we’ll go through several broad use case categories for the Command model. Though they won’t cover all the possible ways that you can use the model, they are good starting points for understanding the patterns of tasks where the model works well.\\nWe’ll go through the following use cases:\\nWriting\\nQuestion Answering\\nBrainstorming\\nTransforming\\nSummarizing\\nRewriting\\nExtracting\\nClassifying\\nSetting Up\\nThe examples in this post will be shown in Python. For each use case, we’ll look at some ideas on how a prompt can be constructed and the associated model settings. This blog post comes with a Google Colaboratory notebook that lets you get hands-on with the code.\\nFirst, let’s install the Cohere package, get the Cohere API key, and set up the client.\\n! pip install cohere import cohere co = cohere.Client(\"COHERE_API_KEY\") # Your Cohere API key \\nLet’s also define a function to take a prompt and a temperature value and then call the Chat endpoint, which is how we can access the Command model.\\u200b\\u200b We set a default temperature value of 0, which nudges the response to be more predictable and less random. This function returns the text response generated by the model.'),\n",
       " Document(metadata={'title': 'Use Case Patterns', 'source': 'https://cohere.com/llmu/use-case-patterns'}, page_content='def generate_text(prompt, temp=0): response = co.chat_stream( message=prompt, model=\"command-r\", temperature=temp, preamble=\"\") for event in response: if event.event_type == \"text-generation\": print(event.text, end=\\'\\') \\nOur examples will revolve around a company’s activities for launching a new wireless headphone product, such as getting the word out, managing customer interactions, and so on. For this, let’s define a text snippet containing the product description. We’ll be utilizing this snippet in several examples throughout this article.\\nproduct=\"\"\"The CO-1T is a wireless headphone product that uses Bluetooth technology to connect to your devices. \\\\ It has a long battery life and can be quickly charged using the included USB cable. The headphone is \\\\ lightweight and comfortable, ideal for long periods of use. It has a built-in microphone for making calls, \\\\ and a button to control the volume. The CO-1T is a great choice for anyone looking for a wireless headphone \\\\ product with great battery life.\"\"\" \\nWriting\\nWe’ll start with the most general type of use case, which is writing.\\nHere we can ask the model to write freeform text, for example, with this prompt: “Create an email about the launch of the wireless headphone product”. But that alone might not be very useful in practical applications because the generated text can go in very different directions. We may want to add more context and specificity to the prompt.\\nHere’s how we can do that. Let’s say we’re building an application for users to enter some bullet points and get a complete email written. We can set up the prompt in the following way: create a variable for the user to input some text and merge that, together with the product description, into the main prompt.'),\n",
       " Document(metadata={'title': 'Use Case Patterns', 'source': 'https://cohere.com/llmu/use-case-patterns'}, page_content='user_input =\"\"\" - announce product launch - create a call to action - mention live chat for support \"\"\" prompt = f\"\"\"{product} Create an email about the product above mentioning the following: {user_input} \"\"\" response = generate_text(prompt, temp=0.5) print(response) \\nAdding this context helps to guide the model in the direction we want it to go. Here is a sample response:\\nWe are thrilled to announce the launch of our newest wireless headphones, the CO-1T! These headphones are designed to offer a seamless and convenient listening experience with the use of Bluetooth technology, giving you the freedom to connect to your devices wirelessly. The CO-1T is crafted to be comfortable and lightweight, making it the perfect companion for your daily commutes or workouts. With a long-lasting battery life, you\\'ll never have to worry about running out of charge during your day. Plus, a quick charge feature ensures you\\'re always ready to go. Simply use the included USB cable for efficient charging. Control your volume and manage your calls effortlessly with the built-in microphone and easy-to-use button controls. Experience the convenience of wireless connectivity and immerse yourself in crystal-clear audio. We believe our product will provide an exceptional listening experience and enhance your daily routine. Try it out today and discover the convenience of wireless freedom! If you have any questions or need further assistance, our team is here to help. Feel free to reach out to us through our live chat feature on our website, and we\\'ll be happy to assist you. Thank you, and we hope you enjoy the CO-1T wireless headphones!'),\n",
       " Document(metadata={'title': 'Use Case Patterns', 'source': 'https://cohere.com/llmu/use-case-patterns'}, page_content='In this example, we changed the temperature value to 0.5. And throughout this article, you’ll see different temperature values being used in different situations. Increasing the temperature value tells the model to generate less predictable responses and instead be more “creative”. And since this task does require some form of creativity (expanding brief keywords into a full passage), we increased the temperature value. There is no one right value for a use case, so this is a setting you should try and iterate to get the best outcome for your task.\\nWe have looked at one example, but writing as a use case can mean many different things. So instead of beginning your prompt with “Create,” you can try other ways to extract different forms of content and formats that your application needs, such as using “Explain,” “Generate,” “Write,” “Provide,” “Design,” “Describe,” and so on.\\nQuestion Answering\\nThis use case is about answering a question that a user asks, be it in a single-turn, question answering scenario or a multi-turn, chatbot setting.\\nQuestion answering can take place in either a closed or open setting. In a closed-book question answering setting, we rely on the model to answer questions based on the general knowledge from which it has been trained. Here’s one example:\\nuser_input =\"What features should I consider when choosing a wireless headphone\" prompt = user_input response = generate_text(prompt, temp=0.5) print(response) \\nSample response:'),\n",
       " Document(metadata={'title': 'Use Case Patterns', 'source': 'https://cohere.com/llmu/use-case-patterns'}, page_content=\"There are several features to consider when choosing a wireless headphone that best suits your needs: 1. Sound Quality: This is a primary factor for many people. Look for headphones with clear and balanced audio reproduction, good bass, and a wide frequency response. Some brands are known for their audio expertise and use high-quality drivers for superior sound. 2. Noise Cancellation: Wireless headphones with active noise cancellation (ANC) technology are excellent for reducing ambient noise and enhancing focus on your music or calls. They use microphones to detect external sound and produce an opposite signal to cancel it out. 3. Connectivity: Ensure the wireless headphones have Bluetooth compatibility with your devices, such as smartphones, tablets, or laptops. Some headphones also support multi-device pairing, allowing seamless connection switching. 4. Battery Life: Longer battery life ensures you won't be frequently charged. Look for headphones that offer at least 15-20 hours of continuous playback on a single charge. Also, check the charging time and consider if the headphone supports fast charging. 5. Comfort and Design: Consider the comfort of the ear pads and the overall design. Look for ergonomic designs, adjustable headbands, and lightweight construction for extended wear. Also, choose between in-ear, on-ear, or over-the-ear models based on your preference and portability. 6. Controls and Microphone: Easy-to-use controls for playback, volume, and calls are essential. A good microphone quality is crucial for clear calls and voice commands. Some headphones have touch-sensitive controls on the earcups, while others have physical buttons. 7. Voice Assistant Integration: If you're into voice commands, look for headphones with built-in support for virtual assistants like Siri or Google Assistant. This allows you to control various functions hands-free. 8. Multi-Point Connection: This feature lets you connect your headphones simultaneously to multiple devices.\"),\n",
       " Document(metadata={'title': 'Use Case Patterns', 'source': 'https://cohere.com/llmu/use-case-patterns'}, page_content='like Siri or Google Assistant. This allows you to control various functions hands-free. 8. Multi-Point Connection: This feature lets you connect your headphones simultaneously to multiple devices. This way, you can switch between your laptop and smartphone effortlessly. 9. Water and Sweat Resistance: If you plan to use your headphones during workouts or in rainy weather, consider a model with an IPX rating for water resistance. 10. Price and Warranty: Determine your budget and check the warranty period offered by the manufacturer for any potential issues. Remember to read reviews and guides from reputable sources and consider trying them out, if possible, to ensure a comfortable and enjoyable listening experience.'),\n",
       " Document(metadata={'title': 'Use Case Patterns', 'source': 'https://cohere.com/llmu/use-case-patterns'}, page_content='This works well for a question that is open-ended and doesn’t have one correct answer. But let’s say we have a customer asking for specific information about the wireless headphone, such as its feature set. For this, we need to turn to an extractive question answering setting.\\nIn this setting, we can get the model to refer to specific knowledge bases to help it do its job well. This way, we can design a system that can handle questions that require factual responses.\\nHere, a customer asks a product question. We can append the customer’s question and the product description to the prompt, as follows.\\nuser_input =\"How do I control the sound levels\" prompt = f\"\"\"{product} Answer this question based on the context provided above: {user_input}\"\"\" generate_text(prompt, temp=0) \\nAnd the model picks up the relevant information from the prompt to provide the correct answer.\\nSample response:\\nYou can control the volume using the button built into the headphones. \\nBrainstorming\\nAnother form of writing is brainstorming, where we want the model to generate a list of options based on a given prompt. This can be for writing outlines, generating ideas, providing critical analysis, and so on. This use case forces the model to go broad and cover different perspectives of a situation.\\nIn this example, we want the model to act as an assistant to a customer support agent in identifying possible ways to troubleshoot a technical problem that a customer is facing.\\nuser_input = \"I can\\'t get the Bluetooth connection working\" prompt = f\"\"\"{product} A customer provided the following complaint about this product: {user_input}. Provide a bulleted list of possible ways to troubleshoot so we can advise the customer accordingly. \"\"\" response = generate_text(prompt, temp=0.3) print(response) \\nSample response:'),\n",
       " Document(metadata={'title': 'Use Case Patterns', 'source': 'https://cohere.com/llmu/use-case-patterns'}, page_content='Sample response:\\nHere is a list of possible ways to troubleshoot the customer\\'s problem with the Bluetooth connection for the CO-1T wireless headphones: - Suggest forgetting the existing Bluetooth connection and reinstalling it from scratch - Recommend updating the software of the device they are trying to connect to - Inquiry about whether the customer\\'s device is compatible with the headphone\\'s Bluetooth profile - Suggest a factory reset of the headphones themselves, but warn that this option should only be used if other troubleshooting steps fail. Let me know if you would like me to provide more options to troubleshoot potential problems with the CO-1T headphones. \\nTransforming\\nThe first thing that comes to mind when thinking about generative models is their ability to write a fresh piece of text, but one aspect that is rather understated is their ability to synthesize an existing piece of text.\\nOne example is transforming a passage of text into a different form, making it reusable for different purposes.\\nFor example, creating a list of Frequently Asked Questions (FAQs) about wireless headphones is crucial, but it requires some effort to create. We can cut short this process by getting the model to generate a list of FAQs based on the product description, as follows:\\nprompt =f\"\"\"Turn the following product description into a list of frequently asked questions (FAQ). Product description: {product} \"\"\" response = generate_text(prompt, temp=0) print(response) \\nSample response:'),\n",
       " Document(metadata={'title': 'Use Case Patterns', 'source': 'https://cohere.com/llmu/use-case-patterns'}, page_content=\"Sample response:\\nHere is the product description turned into an FAQ format: FAQ: Q: How does the CO-1T connect to my devices? A: The CO-1T uses Bluetooth technology to connect wirelessly to your smartphones, laptops, or other compatible devices. Q: What is the battery life like? A: The CO-1T has an excellent battery life that will last you long periods of use. It also charges quickly using the included USB cable, so you won't have to wait long to get back to listening. Q: Is it comfortable to wear? A: Absolutely! The CO-1T headphones are lightweight and designed to be comfortable for extended periods. They are perfect for listening to music, taking calls, or using them for work. Q: Does it have a microphone and volume control? A: Yes, the headphone has a built-in microphone, making it easy to take calls on the go. There's also a button that allows you to adjust the volume according to your preference. Q: Why should I choose the CO-1T over other wireless headphones? A: Besides the excellent battery life and comfortable design, the CO-1T is a great choice for anyone looking for a reliable and convenient wireless headphone option at an affordable price. It's quick to charge, easy to use, and perfect for everyday use. \\nSummarizing\\nOne popular use case for synthesizing text is summarization. Here we take a long passage of text and summarize it to its essence. These can be articles, conversation transcripts, reports, meeting notes, and so on.\\nIn this example, we create a prompt to summarize a list of customer reviews about the wireless headphone.\"),\n",
       " Document(metadata={'title': 'Use Case Patterns', 'source': 'https://cohere.com/llmu/use-case-patterns'}, page_content='In this example, we create a prompt to summarize a list of customer reviews about the wireless headphone.\\nuser_input =\"\"\"Customer reviews of the CO-1T wireless headphones: \"The CO-1T is a great pair of headphones! The design is sleek and modern, and the headphones are \\\\ very comfortable to wear. The sound quality is excellent, and I can hear every detail of my music. \\\\ The built-in microphone means I can make calls without having to take my phone out of my pocket. I \\\\ highly recommend the CO-1T to anyone looking for a great pair of wireless headphones!\" \"I\\'m very disappointed with the CO-1T. The design is nice, but the battery life is terrible. I can \\\\ only use them for a few hours before they need to be recharged. This is very inconvenient, and I\\'m \\\\ not sure if I can recommend them to anyone.\" \"The CO-1T is a mixed bag. The speaker quality is great, but the built-in microphone\\'s quality is \\\\ poor. I can hear every detail of my music, but my voice sounds distorted when I make calls. The \\\\ design is nice, and the headphones are comfortable to wear, but the battery life is only average. \\\\ I\\'m not sure if I can recommend them to anyone.\" \"\"\" prompt = f\"\"\"Summarize the following. {user_input} \"\"\" response = generate_text(prompt, temp=0) print(response) \\nExample response:\\nThe CO-1T wireless headphones have a sleek, modern design and are comfortable to wear. They offer excellent sound quality, with clear and detailed audio. However, the reviews are mixed when it comes to the built-in microphone\\'s quality, with some users reporting distortion. The battery life is a major downside, lasting only a few hours on a single charge. Overall, the CO-1T seems like a decent option for those prioritizing sound quality and comfort, but the mediocre battery life and inconsistent microphone quality might be dealbreakers for some. \\nRewriting'),\n",
       " Document(metadata={'title': 'Use Case Patterns', 'source': 'https://cohere.com/llmu/use-case-patterns'}, page_content='Rewriting\\nRewriting text is another useful use case where you need to modify some aspects of the text while maintaining its overall meaning. One example is changing the tone of a piece of text to tailor it to a specific audience. Here we want to rewrite the product description so it’s more relatable to students.\\nuser_input = \"college students\" prompt = f\"\"\"Create a version of this product description that\\'s tailored towards {user_input}. {product}\"\"\" response = generate_text(prompt, temp=0.5) print(response) \\nSample response:\\nThe CO-1T headphones are the ultimate wireless companion for any college student. With Bluetooth capabilities, you can easily connect to your laptop, phone, or tablet without the hassle of tangling cords. Study for long hours without discomfort or interruption - the lightweight design ensures comfort and the long battery life keeps you connected throughout the day. Quick charge the batteries using the included USB cable when you\\'re running short on time, and use the built-in microphone to stay connected with your friends and family. Control your audio easily with the volume button, and focus on what really matters - acing your courses! \\nAnother extremely useful way of looking at text synthesis is information extraction. Here we leverage the model’s ability to capture the context of a piece of text to extract the right information as specified by the prompt.\\nHere is an example of an email that a customer is, unfortunately, asking for a refund for the wireless headphone. We can have the model process this email by getting it to extract information, such as the product name, refund reason, and pick-up address.'),\n",
       " Document(metadata={'title': 'Use Case Patterns', 'source': 'https://cohere.com/llmu/use-case-patterns'}, page_content='user_input =\"\"\"I am writing to request a refund for a recent CO-1T purchase I made on your platform. \\\\ Unfortunately, the produce has not met my expectations due to its poor battery life. \\\\ Please arrange for the pick-up at this address: to 171 John Street, Toronto ON, M5T 1X2.\"\"\" prompt =f\"\"\"Extract the product, refund reason and pick-up address from this email: {user_input} \"\"\" response = generate_text(prompt, temp=0) print(response) \\nSample response:\\nProduct: CO-1T Refund Reason: Poor battery life Pick-up Address: 171 John Street, Toronto ON, M5T 1X2 \\nClassifying\\nOne of the most widely deployed use cases in NLP is text classification. Here, the task is to classify a piece of text into one of a few predefined classes. In this example, we want to classify incoming customer messages into one of three categories: Order, Support, or Refunds.\\nWe can create the prompt as follows.\\nuser_input =\"\"\"The battery drains fast\"\"\" prompt = f\"\"\"The following is a user message to a customer support agent. Classify the message into one of the following categories: Order, Support, or Refunds. {user_input} \"\"\" response = generate_text(prompt, temp=0) print(response) \\nHere’s a sample response where the generative model correctly classifies the text into the right category:\\nBased on the information provided in the message, this inquiry falls under the category of \"Support.\" This is an issue that the customer is encountering with a product that they have already purchased, and they are seeking assistance and guidance from the customer support agent regarding this issue. The customer is reporting a problem with a product they purchased and is asking for help. \\nAlternatively, the Classify endpoint provides a simple API for running text classification. The endpoint leverages Cohere’s embeddings models and makes it easy to add training examples and even create custom models that are specifically tailored to your task.'),\n",
       " Document(metadata={'title': 'Use Case Patterns', 'source': 'https://cohere.com/llmu/use-case-patterns'}, page_content='Here’s how we can use the Classify endpoint. It requires a minimum of two examples per class, which is passed as an argument to the API call. We have six examples altogether – two for each class.\\nfrom cohere import ClassifyExample response = co.classify( model=\\'embed-english-v3.0\\', inputs=[user_input], examples=[ClassifyExample(text=\"I can\\\\\\'t connect to the bluetooth\", label=\"Support\"), ClassifyExample(text=\"Why is max volume so low\", label=\"Support\"), ClassifyExample(text=\"When will my order arrive\", label=\"Order\"), ClassifyExample(text=\"How much is the shipping cost\", label=\"Order\"), ClassifyExample(text=\"What is your refund policy\", label=\"Refunds\"), ClassifyExample(text=\"How do I return my product\", label=\"Refunds\")]) print(response.classifications[0].predictions[0]) \\nSample response:\\nConclusion\\nThis chapter is a starting point for understanding the range of use cases you can build with the Cohere Command model. Take these use cases as the initial set of examples to further experiment on what’s possible.'),\n",
       " Document(metadata={'title': 'Chaining Prompts', 'source': 'https://cohere.com/llmu/chaining-prompts'}, page_content=\"Chaining PromptsQualified\\nContents\\nIntroduction\\nSequential\\nParallel\\nSampling\\nExploration\\nLoop\\nPerformance Considerations\\nFinal Thoughts\\nIntroduction\\nIn the previous two chapters, we looked at various ways to prompt the Command model. One thing similar about all of those examples is that they utilize a single prompt to fulfill a particular task. However, there may be other tasks where this is insufficient, and we instead have to chain a few prompts together to complete a task successfully.\\nWe can think of the previous two chapters as looking at prompts as a single unit, and this chapter will look at prompts as a combination of these units. The diagram below summarizes these three chapters.\\nA comparison of this blog post and the two previous ones (blog #1 and blog #2)\\nCombining multiple prompts to accomplish a single goal will be the focus of this chapter. We’ll look at some ideas and patterns for chaining prompts, which could come in handy when building a generative AI application.\\nSequential\\nLet’s begin with probably the most common approach for chaining prompts – in a sequence.\\nChaining prompts in a sequence\\nThe key idea is that if the task provided in a single prompt becomes too lengthy or complex and contains many different instructions, the response may not capture the details and granularity required. In this case, it would make sense to break this task into smaller subtasks that can be run one after the other. The response of one subtask becomes the prompt to the other subtask, and this chain continues until the overall task is complete.\\nLet's use story generation as an example. Suppose we have a task to generate a long story containing a dialog guided by a set of information – characters, story beats, and locations. Now, we could stuff all this information into a single prompt, but that might dilute the key details we want the dialog to contain.\"),\n",
       " Document(metadata={'title': 'Chaining Prompts', 'source': 'https://cohere.com/llmu/chaining-prompts'}, page_content='Besides, we don’t want to write all these additional details in the first place because we’d like the model to generate them for us. What we want is to provide a short summary of what we want the story to be about, which becomes our sole prompt.\\nThe diagram below summarizes the chains involved in generating the eventual dialog. First, a human enters a story summary, which becomes the prompt to generate a list of characters, which then becomes the prompt to generate the story beats, and so on, until we get to the dialog generation.\\nA summary of the different steps in the story generation task, running in a sequence\\nAnother added benefit of chaining prompts in sequences instead of using one single prompt is that we can implement human checks at each point in the sequence, and if a response does not fulfill a certain standard, we can trigger a re-run. This makes it easy to do evaluation and quality control over a complex generative AI task – by doing them in stages.\\nParallel\\nA sequential chain of prompts is needed when the subtasks depend on each other. But when they are independent, we can run them in parallel instead.\\nRunning prompts in parallel before combining them into a single output\\nLet’s say we are building an application that generates recipe ideas for a whole week and then generates a shopping list of ingredients for the user to buy. In this case, given a user input of, say, the number of meals or days, we can run the recipe generation step in parallel. The prompt might look something like the following:\\nprompt = f\\'Suggest a simple and quick recipe for {meal}. Write in JSON containing these keys \"Ingredients\" and \"Instructions\"\\' \\nNext, we’ll repeat the recipe generation across all meals. Once complete, we can consolidate the ingredients from each meal into a single shopping list that the user can use immediately.\\nprompt = f\"\"\"Consolidate the following ingredients into a single shopping list, without repetition: {ingredients}\"\"\" \\nSampling'),\n",
       " Document(metadata={'title': 'Chaining Prompts', 'source': 'https://cohere.com/llmu/chaining-prompts'}, page_content='prompt = f\"\"\"Consolidate the following ingredients into a single shopping list, without repetition: {ingredients}\"\"\" \\nSampling\\nIf a task involves logical reasoning, for example, in mathematical question answering, there may be more than one way to solve a given problem. In this case, we want to encourage the model to be more creative when exploring different solutions. But dialing up the creativity knob (i.e., “temperature”) also means that there’s a higher chance that the model will get the final answer wrong.\\nThe solution is, instead of prompting the same question to the model once, we can repeat that multiple times and choose the response with the highest majority.\\nSampling prompt responses followed by a majority vote\\nLet’s look at an example taken from a paper by Wang et al. that introduces the concept of self -consistency. \\nFirst, revisiting the previous blog post, we looked at the concept of chain-of-thought prompting introduced by Wei et al., where a model is prompted in such a way that it is encouraged to do a reasoning step before giving the final response. In those settings, however, the model is typically encouraged to do “greedy decoding,” which means biasing towards the correct and safe path. This can be done by adjusting settings like the temperature value.\\nWith self-consistency, we can build on top of the chain-of-thought approach by sampling from several paths instead of one. We also make the paths much more diverse by adjusting the settings towards being more “creative,” again using settings like temperature. We then do a majority vote out of all answers.\\nThe diagram below illustrates the self-consistency concept. It shows an example of comparing the result of a single solution, using the greedy decoding approach, and a solution that samples from multiple generations, using a more diverse decoding approach. The former didn’t get the answer correct, and the latter did.\\nAn example from Wei et al. illustrating the self-consistency concept\\nExploration'),\n",
       " Document(metadata={'title': 'Chaining Prompts', 'source': 'https://cohere.com/llmu/chaining-prompts'}, page_content=\"An example from Wei et al. illustrating the self-consistency concept\\nExploration\\nBuilding on the previous chain pattern, when a question is much more complex and requires more granular reasoning steps, it is helpful to break down the steps into pieces. At each step, we encourage the model to explore different solutions, vote for the best solution and only then continue to the next step.\\nThis is also useful in creative applications like story writing. Making the model explore different story ideas is more interesting than constraining it to a single path, elevating the final output's quality.\\nExploring different solutions, one step at a time\\nLet’s use one concrete example from Yao et al., which introduces the concept of tree of thoughts. The diagram below illustrates how this concept differs from direct input-output prompting, chain-of-thought, and self-consistency with chain-of-thought. At each step, the model generates several diverse solutions. A separate prompt is then used to evaluate these solutions and vote for the best solution. The process repeats until the final step is completed.\\nAn illustration from Yao et al. of the Tree of Thoughts approach compared to three others\\nHere’s one of the examples shown in the paper with a creative writing task. The diagram below shows the initial input containing a list of four seemingly unrelated sentences – each talking about handstands, the smell of space, sign language, and people’s perceptions. The task is to weave them into one coherent passage. It must contain four short paragraphs, each ending with the given sentence.\\nA creative writing example from Yao et al. using Tree of Thoughts\"),\n",
       " Document(metadata={'title': 'Chaining Prompts', 'source': 'https://cohere.com/llmu/chaining-prompts'}, page_content='A creative writing example from Yao et al. using Tree of Thoughts\\nIn this example, since the task is quite a challenging one, it makes sense to break down the task into two: writing the plan and writing the actual passage based on the winning plan. At each step, the model generates a few solutions, and then another prompt is used to evaluate and vote for the best solution, guiding the direction of the next step.\\nA couple of writing plan options are shown in the screenshot, and the winning option is the one that suggests using self-help as the theme to weave the sentences into a coherent passage.\\nLoop\\nIn some applications, we may need to re-run a generation step, given what happens in a subsequent step. One example is when that subsequent step is used to check if the response generated meets specific criteria, such as quality and format. This is where the loop pattern becomes useful.\\nLooping a task back to an earlier step\\nLet’s take a rephrasing task as an example. Say we have an application that takes a rude user comment and suggests a rephrasing that makes it more polite. Here, we want the LLM response to be polite and, at the same time, retain the original meaning of the comment.\\nThe prompt, taking in the user input to be rephrased, might look something like the following:\\nuser_input = \"I really don\\'t have time for this nonsense.\" prompt_rephrase = f\"\"\"Rephrase this user comment into something more polite: User comment: You don\\'t know what you\\'re talking about. Rephrased comment: I think there might be some inaccuracies in your statement. User comment: {user_input} Rephrased comment:\"\"\" \\nHere is an example response which rephrases the original user input:\\nI think we might need to set aside some time to discuss this properly. \\nNext, we create another prompt to check if the rephrased comment is similar enough to the original comment.'),\n",
       " Document(metadata={'title': 'Chaining Prompts', 'source': 'https://cohere.com/llmu/chaining-prompts'}, page_content='I think we might need to set aside some time to discuss this properly. \\nNext, we create another prompt to check if the rephrased comment is similar enough to the original comment.\\nprompt_check = f\"\"\"Below is a rude comment that has been rephrased into a polite version. The rephrased comment must maintain a similar meaning to the original comment. Check if this is true. Answer with YES or NO. Original comment: Shut up, you\\'re always wrong. Rephrased comment: Please be quiet, check your facts again. Similar meaning: YES Original comment: I can\\'t stand you. Rephrased comment: Let\\'s discuss this. Similar meaning: NO Original comment: {user_input} Rephrased comment: {user_input_rephrased} Similar meaning:\"\"\" \\nAnd if the response is “NO,” we route the task back to the rephrasing prompt and repeat until we get a “YES” response.\\nPerformance Considerations\\nPrompt chaining is a powerful concept that makes complex use cases possible when a single prompt setup is insufficient. Having said that, prompt chaining should only be considered when it’s truly necessary. The overall performance considerations need to be taken into account. \\nOne such consideration is latency. The longer the chain, the longer it takes to complete a task from start to finish. If an application is latency-sensitive, it makes sense to minimize the number of chains as much as possible. The cost factor is another consideration when designing applications that rely on prompt chaining.\\nConclusion\\nIn this chapter, we looked at several prompt-chaining patterns and examples of how they can be applied to the Command model.\\nThis is a fascinating area of prompt engineering because it opens up so much room for creativity when solving problems with LLMs. Although it comes with some performance trade-offs, balancing these considerations presents an exciting challenge for building production-ready LLM-powered applications.'),\n",
       " Document(metadata={'title': 'Evaluating Outputs', 'source': 'https://cohere.com/llmu/evaluating-llm-outputs'}, page_content='Evaluating OutputsQualified\\nContents\\nIntroduction\\nHuman Evaluation \\nReference\\nScoring\\nA/B Testing\\nLLM-Generated Evaluation\\nWord-Level Metrics\\nConclusion\\nIntroduction\\nLarge language models (LLMs) offer exciting new ways to build applications that leverage natural language as the interface. However, as impressive as this technology may be, it is crucial to evaluate the generated outputs of LLMs to ensure the quality of an application.\\nEvaluating LLM outputs is especially important because the outputs produced by these models are probabilistic – meaning the same prompt does not necessarily produce the same outputs every time. Evaluations provide a way to measure the quality level of the outputs, ensuring a great user experience.\\nIn this blog post, we look at a few techniques for evaluating the outputs generated by an LLM.\\nReal User Feedback\\nThe gold standard for evaluation is gathering actual feedback from real application users. The best way to gain insights into an application’s quality and usefulness is by collecting feedback from users who interact with the application. In contrast, the rest of the methods we’ll discuss are all proxies for understanding real user experience and behaviors.\\nThe specific tactics for gathering user feedback can come in different forms, for example:\\nExplicit feedback: By implementing features to gather user feedback, such as thumbs up/down for an output, rating the output, and more\\nImplicit feedback: By observing user behaviors, such as considering ignored outputs as negatives, analyzing time spent on the output, and more\\nThis creates a flywheel for continuously improving an application. As more users start using the application, more data becomes available to inform the effectiveness of an application, providing signals on areas for improvement.'),\n",
       " Document(metadata={'title': 'Evaluating Outputs', 'source': 'https://cohere.com/llmu/evaluating-llm-outputs'}, page_content='But, of course, the challenge is that this can happen only after we deploy the application and users have been using it for a while. So, we must also perform evaluations before an application is deployed. For this, let’s look at alternative evaluation methods.\\nHuman Evaluation\\nThe next best option is to have human annotators evaluate the outputs of an application in the pre-deployment stage. A typical evaluation approach requires building a test dataset, and evaluation is performed against this test dataset.\\nAn example human evaluation comparing two LLM outputs.\\nLet’s look at a question-answering example. Here, we have one test data point: the prompt asks a question about a headphone product given a user review. The evaluation task is to rate the response generated by the LLM. \\nGiven the following user review, answer the question. User review: The CO-1T is a great pair of headphones! The sound quality is the best out there, and I can hear every detail of my music. Question: Why is the CO-1T a great wireless headphone? \\nAnd let’s suppose that this is the generated response.\\n\"Because the audio experience is unrivaled\" \\nThere are several ways to approach evaluation for this response, as follows.\\nReference\\nHere, the evaluator would compare each test data point against the ground truth of an ideal response.\\nWith our example, the ideal response might be, “Because the sound quality is the best out there.” Based on this, the evaluator provides a Yes/No judgment on whether the generated response provides an accurate response.\\nBut this approach requires the ground truth to be constructed beforehand. Given that no two use cases are quite the same, this means having to construct ground truth for each prompt and application. Moreover, the quality of the ground truth directly affects the evaluation outcome – if not constructed correctly, it can produce misleading results.\\nScoring'),\n",
       " Document(metadata={'title': 'Evaluating Outputs', 'source': 'https://cohere.com/llmu/evaluating-llm-outputs'}, page_content='Scoring\\nHere, the evaluator would evaluate the generated response by assigning a score, such as a rating between 0 and 10. There is no ground truth as a reference, so it’s up to the evaluator to provide a verdict on the quality of an output.\\nThe score can be a single score or a set of scores, and it can be broad or granular, depending on the use case. For example, a creative writing task might require more granular scoring for different output characteristics, such as fluency, interestingness, and conciseness.\\nAn evaluation criterion can be a score along a scale, and it can also be a check against a flag. For example, a summarization task might require checking whether the output is consistent and does not generate content that doesn\\'t exist in the actual document. A specialized task might require checking for specific rules, such as the number of syllables in a haiku.\\nA/B Testing\\nHere, the evaluator would be given a pair of LLM-generated responses and asked to rate the better response. This is useful for comparing an application’s quality over different time snapshots or different sets of configurations, such as prompts and parameters.\\nWith our example, let’s suppose the evaluator is tasked to compare the following two responses to our user question above.\\n1. \"Because the audio experience is unrivaled\" 2. \"Because the microphone has the best quality\" \\nWe can probably agree that the winning response should be the first one (“Because the audio experience is unrivaled\") as it answers the question accurately, while the second response (“Because the microphone has the best quality”) talks about the microphone’s quality instead of the sound quality.\\nThe challenge with human evaluation is that it cannot be scaled efficiently. The cost and time incurred are significantly higher than the alternative methods, which can be automated.'),\n",
       " Document(metadata={'title': 'Evaluating Outputs', 'source': 'https://cohere.com/llmu/evaluating-llm-outputs'}, page_content='The challenge with human evaluation is that it cannot be scaled efficiently. The cost and time incurred are significantly higher than the alternative methods, which can be automated.\\nAnother challenge is that human evaluation is subjective – the verdict from one evaluator may not be the same as another. The example above is relatively straightforward, but in more challenging tasks, there will be more ambiguity and room for interpretation about what makes a response good or bad. Many factors can influence an evaluator’s verdict, such as expertise, style, and biases, impacting the evaluation outcome.\\nLLM-Generated Evaluation\\nAn alternative to human evaluation is to have an LLM to evaluate the output. With the same setup as above, this is done by having a prompt instructing the LLM to provide the verdict of a generated answer against the reference.\\nAn example LLM-generated evaluation comparing two LLM outputs.\\nAny human evaluation paradigms that we discussed (reference, scoring, and A/B testing) could be replicated using LLM-generated evaluation.\\nIn the example below, we use the Command model to perform an A/B testing evaluation for the same question-answering task. The model\\'s task is to choose the winning response between two responses to the question.\\n# Add text to evaluate ref_answer = \"Because the sound quality is the best out there\" gen_answers = [\"Because the audio experience is unrivaled\", \"Because the microphone has the best quality\"] # Run evaluation for gen_answer in gen_answers: prompt=f\"\"\"User review: The CO-1T is a great pair of headphones! The sound quality is the \\\\ best out there, and I can hear every detail of my music. Question: Why is the CO-1T a great headphone? Answer #1: {gen_answers[0]} Answer #2: {gen_answers[1]} Given the question about the user review, state whether Answer #1 or Answer #2 provides \\\\ the more accurate answer.\"\"\" response = co.generate(prompt=prompt,max_tokens=50, temperature=0).generations[0].text print(response)'),\n",
       " Document(metadata={'title': 'Evaluating Outputs', 'source': 'https://cohere.com/llmu/evaluating-llm-outputs'}, page_content='The generated verdict is “Answer #1”, which is what we expect the winning response should be.\\nThis approach is promising as it eliminates the cost and time constraints of human evaluation, but the jury is still out on whether it can surpass human evaluation in accuracy and quality. Its effectiveness on one task doesn’t guarantee that it will generalize to other tasks and domains, and the only way to know is by testing it on a specific application.\\nLLM-generated evaluation also faces the same subjectivity challenge as human evaluation. Many factors can affect a model’s evaluation outcome, such as the model’s overall capability, whether it’s being trained specifically to perform evaluations, the presence of training data that could introduce biases, and more.\\nWord-Level Metrics\\nAnother evaluation approach compares the reference and generated output at the word/token (or word/token group) level. Several evaluation metrics are available, such as BLEU, ROUGE, Perplexity, and BERTScore.\\nLet’s look at an example with ROUGE, originally created for evaluating summaries. It measures the number of matching “n-grams” between the reference and generated text. An N-gram is a contiguous sequence of `n` items in a text, where `n` can be 1, 2, and so on. To keep it simple, we’ll use `n=1`, also called “unigrams.” For example, in the sentence “I love cats,” the unigrams are “I,” “love,” and “cats.”\\nWe calculate the precision, recall, and F1-score of the n-grams of the question-answering task against a reference answer of “Because the sound quality is the best out there.”\\nPrecision is the ratio of the count of matching unigrams divided by the count of unigrams in the generated text\\nRecall is the ratio of the count of matching unigrams divided by the count of unigrams in the reference text\\nF1-score is calculated from precision and recall with the following formula: 2 * (precision * recall) / (precision + recall)\\nHere is an example using ROUGE:'),\n",
       " Document(metadata={'title': 'Evaluating Outputs', 'source': 'https://cohere.com/llmu/evaluating-llm-outputs'}, page_content='F1-score is calculated from precision and recall with the following formula: 2 * (precision * recall) / (precision + recall)\\nHere is an example using ROUGE:\\nfrom collections import Counter def rouge_1(reference, candidate): # Turn into unigrams reference_words = reference.split() candidate_words = candidate.split() # Compute the number of overlapping words reference_count = Counter(reference_words) candidate_count = Counter(candidate_words) overlap = sum(min(candidate_count[w], reference_count[w]) for w in candidate_count) # Compute precision, recall, and F1 score recall = overlap / len(reference_words) precision = overlap / len(candidate_words) f1 = 2 * (recall * precision) / (recall + precision) # Return resuls return {\"recall\": recall, \"precision\": precision, \"f1\": f1} for idx,gen_answer in enumerate(gen_answers): result = rouge_1(ref_answer, gen_answer) print(f\"Answer #{idx+1}\") print(f\"Precision: {result[\\'precision\\']:.2f}\") print(f\"Recall: {result[\\'recall\\']:.2f}\") print(f\"F1-Score: {result[\\'f1\\']:.2f}\") print(\"\\\\n\") \\nThis gives the following outcome.\\nAnswer #1 Precision: 0.50 Recall: 0.33 F1-Score: 0.40 Answer #2 Precision: 0.71 Recall: 0.56 F1-Score: 0.63 \\nHere, the second generated answer scored higher than the first in precision, recall, and F1-score, which is not the expected outcome. This is because it has more unigram overlaps with the reference answer, for example, with the words “best” and “quality.”\\nThis is an example where word-level metrics may fall short. They can be handy because they are easy to interpret and their implementation is fast and cheap, but they may not capture the overall meaning and accuracy when comparing two pieces of text.\\nConclusion\\nThis article looked at a few techniques for evaluating LLM outputs, from human evaluations to automated ones.'),\n",
       " Document(metadata={'title': 'Evaluating Outputs', 'source': 'https://cohere.com/llmu/evaluating-llm-outputs'}, page_content=\"Conclusion\\nThis article looked at a few techniques for evaluating LLM outputs, from human evaluations to automated ones.\\nThere is a trade-off to be considered: on the one hand, automated evaluations are much more cost and time-efficient, which makes them practical options in some cases, such as in the early prototyping stages. On the other hand, human evaluations are still the gold standard for getting the strongest signal on an application's accuracy and usefulness.\\nIt’s also worth noting that this article looks at evaluating LLM outputs in the general sense without making any assumptions about the actual task. In practice, there are other approaches not mentioned here that better suit specific tasks, such as code execution or information extraction.\\nUltimately, each evaluation approach has its potential pitfalls. An evaluation outcome can only be considered reliable if we have first understood and mitigated the associated limitations.\"),\n",
       " Document(metadata={'title': 'Validating Outputs', 'source': 'https://cohere.com/llmu/validating-llm-outputs'}, page_content='Validating OutputsQualified\\nThis chapter was written in collaboration with the Guardrails AI team.\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nIn previous chapters, we covered prompt engineering techniques to elicit the desired responses from a large language model (LLM).\\nHowever, one key property of LLMs that’s different from traditional software is that the output is probabilistic in nature. The same input (i.e., the prompt) may not always produce the same response. While this property makes it possible to build entirely new classes of natural language applications, it also means that those applications require a mechanism for validating their outputs.\\nHere’s an example. In Chapter 1, we looked at a text extraction task of turning a list of bank invoices in a text document into a JSON object containing three fields: “Invoice Number,” “Merchant Name,” and “Account Number.” For brevity, we’ll turn it into a shorter version with the document containing just one invoice, as follows.\\nprompt=\"\"\"Turn the following information into a JSON string with the following keys: Invoice Number, Merchant Name, and Account Number. Bank Invoice: INVOICE #0521 MERCHANT ALLBIRDS ACC XXX3846 \"\"\" \\nThis produced an LLM response that followed exactly what we wanted, as shown below.\\n{ \"Invoice Number\": \"0521\", \"Merchant Name\": \"Allbirds\", \"Account Number\": \"XXXX3846\" } \\nBut how do we ensure we’ll get the same response every time? Perhaps another time, the output may miss some information, such as the returning incomplete information like this one.\\n{ \"Invoice Number\": \"0521\" }'),\n",
       " Document(metadata={'title': 'Validating Outputs', 'source': 'https://cohere.com/llmu/validating-llm-outputs'}, page_content='{ \"Invoice Number\": \"0521\" } \\nThere are many other ways that a response may not match the expected structure. For example, what if the generated output contained fields we never specified? What if the document provided were more challenging to parse, causing an unclear response? What if we wanted to impose a rule that a field can only take up values within a specific range? In these cases and many others, we must add a step to validate the output.\\nIn the rest of this chapter, we’ll look at LLM output validation and examples of how to implement it using an open-source package called Guardrails AI.\\nWhen Is Output Validation Needed\\nTo build robust and production-ready LLM applications, the outputs need to be predictable. This helps to safeguard the application against unexpected behaviors and to ensure a reliable user experience. The following provides some example scenarios when output validation may be needed.\\nStructure compliance: Some LLM applications require their output to go beyond just freeform text and instead follow a specific structure containing specific types of information. We looked at a toy example earlier of a text extraction task that requires a JSON output that follows a certain format. We can extend this to synthetic data generation cases, where the generated data must meet certain criteria.\\nSafe responses: Due to their probabilistic nature, LLM applications require additional guardrails to ensure their outputs are safe, ethical, and privacy-preserving. For example, we may want to confirm that an output does not contain profanity. Alternatively, we may want to ensure that an output does not contain personally identifiable information (PII).'),\n",
       " Document(metadata={'title': 'Validating Outputs', 'source': 'https://cohere.com/llmu/validating-llm-outputs'}, page_content='Semantic similarity: In some applications, we may require the output to be similar enough to a target. For example, in text summarization tasks, we want to ensure that the summary does not deviate too far from the original document. For this, with the help of text embeddings, we want to validate that the summary and the document are similar enough semantically.\\nValid choices: We may also want the LLM to generate valid outputs per given definitions or constraints. For example, creating an LLM chess player will require the LLM output to generate only valid moves for a given board state. We could also validate that a generated piece of code is syntactically correct.\\nQuality assurance: More generally, we may want to implement a validation step to ensure an LLM output meets a certain quality standard for a respective application’s use case and provides value to users.\\nWe can bake these types of validation into an application by implementing a set of validation steps followed by an automated action to fix any gaps found. Let’s see how we can do this.\\nOutput Validation with Guardrails AI\\nGuardrails AI is a Python package that enables developers to enhance the outputs of LLms by adding structural, type, and quality assurances. Guardrails helps developers with two key activities in a validation process:\\nValidation: Performing output validation for LLMs in a similar style to the Pydantic package, which is a data validation library for Python. Some examples are validating the structure of generated JSON output, identifying bias in generated text, and checking for bugs in generated code.\\nCorrection: Performing corrective actions based on the validation results, such as asking the LLM to re-generate the response or fixing the output directly.\\nValidation and correction are two key activities in a validation process\\nImplementation-wise, the following steps are involved in incorporating Guardrails into an LLM application:'),\n",
       " Document(metadata={'title': 'Validating Outputs', 'source': 'https://cohere.com/llmu/validating-llm-outputs'}, page_content='Validation and correction are two key activities in a validation process\\nImplementation-wise, the following steps are involved in incorporating Guardrails into an LLM application:\\nCreate the output schema: The RAIL spec contains the overall schema of the LLM output, the type info for each field, the quality criteria for each field, and the corrective action to be taken if the quality criteria are not met. It also contains the prompt template and any custom code for implementing the schema. Alternatively, the output schema can also be defined using Pydantic, which we’ll use in our example later.\\nInitialize a Guard object based on the schema: The Guard class is the main entry point for using Guardrails. It is initialized using the output schema created in the previous step.\\nWrap an LLM call with the Guard object: The gd.Guard object wraps around an LLM API call to validate, structure, and correct the outputs.\\nThe key steps in output validation with Guardrails: creating the output schema, initializing a Guard object, and wrapping an LLM call with it\\nLet’s look at an example of using Guardrails in a text extraction task. The task is to extract the information from a doctor’s note into a JSON object. The following is the doctor’s note.\\ndoctors_notes = \"\"\"49 y/o Male with chronic macular rash to face & hair, worse in beard, eyebrows & nares. Itchy, flaky, slightly scaly. Moderate response to OTC steroid cream\"\"\" \\nSpecifically, we want our extracted information to contain the following fields:\\nPatient\\'s gender\\nPatient\\'s age\\nA list of symptoms, each with a severity rating and an affected area\\nA list of medications, each with information about the patient\\'s response to the medication\\nSetup\\nFirst, let’s install the packages required: cohere, guardrails-ai, and pydantic.\\npip install cohere guardrails-ai -q'),\n",
       " Document(metadata={'title': 'Validating Outputs', 'source': 'https://cohere.com/llmu/validating-llm-outputs'}, page_content='Setup\\nFirst, let’s install the packages required: cohere, guardrails-ai, and pydantic.\\npip install cohere guardrails-ai -q \\nWe\\'ll also need to download the validators required for this tutorial from Guardrails Hub, which is a place where you can find guardrails for common LLM validation use cases.\\n!guardrails hub install hub://guardrails/valid_range !guardrails hub install hub://guardrails/valid_choices \\nNext, import the necessary packages and create a Cohere client.\\nimport cohere import guardrails as gd from guardrails.hub import ValidRange, ValidChoices from pydantic import BaseModel, Field from rich import print from typing import List # Create a Cohere client co = cohere.Client(api_key=\"COHERE_API_KEY\") \\nDefine the Output Schema\\nNext, we define the output schema that defines what the LLM response should look like. As mentioned earlier, Guardrails provides an option to define the schema using Pydantic. We’ll use this option, and below is the schema we’ll use for the doctor notes extraction task.\\nclass Symptom(BaseModel): symptom: str = Field(..., description=\"Symptom that a patient is experiencing\") affected_area: str = Field( ..., description=\"What part of the body the symptom is affecting\", validators=[ValidChoices([\"Head\", \"Face\", \"Neck\", \"Chest\"], on_fail=\"reask\")] ) class CurrentMed(BaseModel): medication: str = Field(..., description=\"Name of the medication the patient is taking\") response: str = Field(..., description=\"How the patient is responding to the medication\") class PatientInfo(BaseModel): gender: str = Field(..., description=\"Patient\\'s gender\") age: int = Field(..., description=\"Patient\\'s age\", validators=[ValidRange(0, 100)]) symptoms: List[Symptom] = Field(..., description=\"Symptoms that the patient is experiencing\") current_meds: List[CurrentMed] = Field(..., description=\"Medications that the patient is currently taking\")'),\n",
       " Document(metadata={'title': 'Validating Outputs', 'source': 'https://cohere.com/llmu/validating-llm-outputs'}, page_content='In the schema, we defined a few “validators,” a Guardrails feature that lets us define the type of validation to perform. One example is ValidChoices, useful for situations where we want to enforce that a response can only be within a predefined list of items. In our example, in the symptom field, the value can only be one of head, neck, or chest. And if the generated response doesn’t fulfill this criteria, it will be re-prompted. This is shown by the on_fail setting that triggers a reask.\\nThe Guardrails documentation provides more information about the types of validators in Guardrails.\\nInitialize a Guard Object Based on the Schema\\nNext, we initialize a Guard object based on the schema we have defined.\\nFirst, we define the base instruction prompt for the LLM as follows.\\nPROMPT = \"\"\"Given the following doctor\\'s notes about a patient, please extract a dictionary that contains the patient\\'s information. ${doctors_notes} ${gr.complete_json_suffix_v2} \"\"\" \\nThen, we initialize a Guard object from the PatientInfo Pydantic model.\\n# Initialize a Guard object from the Pydantic model PatientInfo guard = gd.Guard.from_pydantic(PatientInfo, prompt=PROMPT) print(guard.base_prompt) \\nGuardrails then uses this information to construct the full prompt for the LLM, which looks like the following example.'),\n",
       " Document(metadata={'title': 'Validating Outputs', 'source': 'https://cohere.com/llmu/validating-llm-outputs'}, page_content='Given the following doctor\\'s notes about a patient, please extract a dictionary that contains the patient\\'s information. ${doctors_notes} Given below is XML that describes the information to extract from this document and the tags to extract it into. <output> <string name=\"gender\" description=\"Patient\\'s gender\"/> <integer name=\"age\" description=\"Patient\\'s age\" format=\"guardrails/valid_range: min=0 max=100\"/> <list name=\"symptoms\" description=\"Symptoms that the patient is experiencing\"> <object> <string name=\"symptom\" description=\"Symptom that a patient is experiencing\"/> <string name=\"affected_area\" description=\"What part of the body the symptom is affecting\" format=\"guardrails/valid_choices: choices=[\\'Head\\', \\'Face\\', \\'Neck\\', \\'Chest\\']\"/> </object> </list> <list name=\"current_meds\" description=\"Medications that the patient is currently taking\"> <object> <string name=\"medication\" description=\"Name of the medication the patient is taking\"/> <string name=\"response\" description=\"How the patient is responding to the medication\"/> </object> </list> </output> ONLY return a valid JSON object (no other text is necessary), where the key of the field in JSON is the `name` attribute of the corresponding XML, and the value is of the type specified by the corresponding XML\\'s tag. The JSON MUST conform to the XML format, including any types and format requests e.g. requests for lists, objects and specific types. Be correct and concise. Here are examples of simple (XML, JSON) pairs that show the expected behavior: - `<string name=\\'foo\\' format=\\'two-words lower-case\\' />` => `{\\'foo\\': \\'example one\\'}` - `<list name=\\'bar\\'><string format=\\'upper-case\\' /></list>` => `{\"bar\": [\\'STRING ONE\\', \\'STRING TWO\\', etc.]}` - `<object name=\\'baz\\'><string name=\"foo\" format=\"capitalize two-words\" /><integer name=\"index\" format=\"1-indexed\" /></object>` => `{\\'baz\\': {\\'foo\\': \\'Some String\\', \\'index\\': 1}}` \\nWrap an LLM Call with the Guard Object'),\n",
       " Document(metadata={'title': 'Validating Outputs', 'source': 'https://cohere.com/llmu/validating-llm-outputs'}, page_content='Wrap an LLM Call with the Guard Object\\nWe’re ready to run an LLM call using the Cohere Generate endpoint. For this, we wrap the LLM call with the Guard object. This means it will take care of the validation and reasking (if any) until the final generated output fulfills the defined schema.\\n# Wrap the Cohere API call with the `guard` object response = guard( co.chat, prompt_params={\"doctors_notes\": doctors_notes}, model=\\'command-r-plus\\', temperature=0, ) # Print the validated output from the LLM print(response.validated_output) \\nAnd we get the final validated output as follows.\\n{ \\'gender\\': \\'Male\\', \\'age\\': 49, \\'symptoms\\': [{\\'symptom\\': \\'Chronic macular rash, itchy, flaky, slightly scaly\\', \\'affected_area\\': \\'Face\\'}], \\'current_meds\\': [{\\'medication\\': \\'OTC steroid cream\\', \\'response\\': \\'Moderate response\\'}] } \\nBehind the scenes, Guardrails performs the validation step on the output against the schema, raises any errors if there are mismatches, and triggers a reask. We can trace the execution steps as follows.\\nThe LLM call first returned the following response. However, notice that the affected_area field returned \"Face & Head\", which did not fall within the options we had defined earlier (any of \"Head\", \"Face\", \"Neck\", or \"Chest\").\\n{ \"gender\": \"Male\", \"age\": 49, \"symptoms\": [ { \"symptom\": \"Chronic macular rash, itchy, flaky, slightly scaly\", \"affected_area\": \"Face & Head\" } ], \"current_meds\": [ { \"medication\": \"OTC steroid cream\", \"response\": \"Moderate response\" } ] } \\nGuardrails captured this discrepancy by raising a FieldReAsk object containing the incorrect value, the error message, and other additional information.'),\n",
       " Document(metadata={'title': 'Validating Outputs', 'source': 'https://cohere.com/llmu/validating-llm-outputs'}, page_content='Guardrails captured this discrepancy by raising a FieldReAsk object containing the incorrect value, the error message, and other additional information.\\n{ \\'gender\\': \\'Male\\', \\'age\\': 49, \\'symptoms\\': [ { \\'symptom\\': \\'Chronic macular rash, itchy, flaky, slightly scaly\\', \\'affected_area\\': FieldReAsk( incorrect_value=\\'Face & Head\\', fail_results=[ FailResult( outcome=\\'fail\\', metadata=None, error_message=\"Value Face & Head is not in choices [\\'Head\\', \\'Face\\', \\'Neck\\', \\'Chest\\'].\", fix_value=None ) ], path=[\\'symptoms\\', 0, \\'affected_area\\'] ) } ], \\'current_meds\\': [ {\\'medication\\': \\'OTC steroid cream\\', \\'response\\': \\'Moderate response\\'} ] } \\nBased on this information, it triggered another LLM call to re-generate the response. Here is the full prompt.'),\n",
       " Document(metadata={'title': 'Validating Outputs', 'source': 'https://cohere.com/llmu/validating-llm-outputs'}, page_content='Based on this information, it triggered another LLM call to re-generate the response. Here is the full prompt.\\nI was given the following JSON response, which had problems due to incorrect values. { \"gender\": \"Male\", \"age\": 49, \"symptoms\": [ { \"symptom\": \"Chronic macular rash, itchy, flaky, slightly scaly\", \"affected_area\": { \"incorrect_value\": \"Face & Head\", \"error_messages\": [ \"Value Face & Head is not in choices [\\'Head\\', \\'Face\\', \\'Neck\\', \\'Chest\\'].\" ] } } ], \"current_meds\": [ { \"medication\": \"OTC steroid cream\", \"response\": \"Moderate response\" } ] } Help me correct the incorrect values based on the given error messages. Given below is XML that describes the information to extract from this document and the tags to extract it into. <output> <string name=\"gender\" description=\"Patient\\'s gender\"/> <integer name=\"age\" description=\"Patient\\'s age\" format=\"guardrails/valid_range: min=0 max=100\"/> <list name=\"symptoms\" description=\"Symptoms that the patient is experiencing\"> <object> <string name=\"symptom\" description=\"Symptom that a patient is experiencing\"/> <string name=\"affected_area\" description=\"What part of the body the symptom is affecting\" format=\"guardrails/valid_choices: choices=[\\'Head\\', \\'Face\\', \\'Neck\\', \\'Chest\\']\"/> </object> </list> <list name=\"current_meds\" description=\"Medications that the patient is currently taking\"> <object> <string name=\"medication\" description=\"Name of the medication the patient is taking\"/> <string name=\"response\" description=\"How the patient is responding to the medication\"/> </object> </list> </output> ONLY return a valid JSON object (no other text is necessary), where the key of the field in JSON is the `name` attribute of the corresponding XML, and the value is of the type specified by the corresponding XML\\'s tag. The JSON MUST conform to the XML format, including any types and format requests e.g. requests for lists, objects and specific types. Be correct and concise. If you are unsure anywhere, enter `null`.'),\n",
       " Document(metadata={'title': 'Validating Outputs', 'source': 'https://cohere.com/llmu/validating-llm-outputs'}, page_content='Guardrails then generated the final validated output, which now completely fulfills the schema.\\n{ \"gender\": \"Male\", \"age\": 49, \"symptoms\": [ { \"symptom\": \"Chronic macular rash, itchy, flaky, slightly scaly\", \"affected_area\": \"Face\" } ], \"current_meds\": [ { \"medication\": \"OTC steroid cream\", \"response\": \"Moderate response\" } ] } \\nConclusion\\nIn this chapter, we looked at LLM output validation and how to implement it using Guardrails AI. Output validation is key to ensuring a generative AI application is robust and predictable enough to be deployed confidently.'),\n",
       " Document(metadata={'title': 'Getting Started with Retrieval-Augmented Generation', 'source': 'https://cohere.com/llmu/rag-start'}, page_content=\"Getting Started with Retrieval-Augmented GenerationQualified\\nWelcome to LLM University's module on Retrieval-Augmented Generation (RAG)!\\nBy the end of this module, you will be able to build RAG-powered applications by leveraging various Cohere endpoints—Chat, Embed, and Rerank. You will also learn how to use quickstart connectors, which are pre-built implementations that connect a RAG application to over 80 enterprise datastores.\\nThis module consists of the following chapters:\\nIntroduction to RAG (this chapter): Learn the basics of RAG and how to get started with RAG via the Chat endpoint.\\nRAG with Chat, Embed, and Rerank: Learn how to build a RAG-powered chatbot using the Chat, Embed, and Rerank endpoints.\\nRAG with Connectors: Learn about connectors and how to build RAG applications using the web search connector.\\nRAG with Quickstart Connectors: Learn how to connect RAG applications to datastores by leveraging Cohere’s pre-built quickstart connectors.\\nRAG over Large-Scale Data: Learn how to build RAG applications over multiple datastores and long documents.\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nContents\\nWhat Is RAG?\\nRAG with Cohere\\nTry It with Coral\\nStep-by-Step Guide\\nSetup\\nDefine the Documents\\nGenerate the Response with Citations\\nConclusion\\nWhat Is RAG?\\nWhile LLMs are good at maintaining the context of the conversation and generating responses, they can be prone to hallucinate and include factually incorrect or incomplete information in their responses.\\nRetrieval-augmented generation (RAG) is a technique that enhances the performance of LLMs by incorporating external data sources. This approach significantly reduces the hallucination issue common in LLMs. RAG enables the model to access and utilize supplementary information from external documents, thereby improving the accuracy of its responses.\"),\n",
       " Document(metadata={'title': 'Getting Started with Retrieval-Augmented Generation', 'source': 'https://cohere.com/llmu/rag-start'}, page_content=\"In a previous module, we discussed how to build a chatbot using Cohere’s Chat endpoint. In this module, we’ll discuss the endpoint's RAG capabilities. This means you can build chatbots that can connect to external documents, ground their responses on these documents, and produce inline citations in their responses.\\nThe chatbot provides helpful and verifiable responses through citations\\nHaving RAG in a chat paradigm means you can build context-aware applications that are able to both maintain the state of a conversation and generate grounded responses.\\nThe Chat endpoint adds RAG capabilities to the chat paradigm\\nRAG with Cohere\\nThe Cohere Chat endpoint comes with RAG features already integrated. This greatly simplifies the task of developing RAG-powered applications.\\nWith Cohere Chat, you get the complete set of building blocks needed to build a high-quality RAG application in the shortest time possible. We’ll cover them in depth throughout this module, but first, let’s take a quick look at some key capabilities of Cohere’s RAG solution.\\nChat interface: The RAG functionalities run on the Chat endpoint. That means everything is wrapped in a chat interface and powered by the Command model. Thus, you can build chatbots that have the full context of a conversation and are not limited to a single interaction.\\nQuery generation: With Cohere’s RAG solution, you also get an LLM that’s trained for query generation. It takes a user message and transforms it into queries that are more relevant and optimized for the retrieval process.\\nRetrieval models: Cohere Embed helps you build a high-quality semantic search system that retrieves the most relevant documents using embeddings. On top of that, Cohere Rerank helps you boost the results further by reranking the search results based on relevance.\"),\n",
       " Document(metadata={'title': 'Getting Started with Retrieval-Augmented Generation', 'source': 'https://cohere.com/llmu/rag-start'}, page_content=\"Response generation: Cohere’s RAG solution gives you an LLM that can provide the right responses to the user in different scenarios. A good RAG system should generate a grounded response based on relevant documents, but it should not do that every single time. The system also has to be able to determine whether or not any of the provided documents are relevant (and possibly decide that none are relevant), as well as decide that it can directly respond without needing any documents retrieved.\\nFine-grained citation: Each grounded response includes fine-grained citations linking back to the source documents. This makes the response easily verifiable and builds trust with the user.\\nConnector mode: What makes RAG work is having the data in the first place. In enterprises, data is typically spread across many platforms, and integrating data sources into a RAG system can be a huge challenge. Cohere Chat comes with a “connector mode,” which makes it easy to connect to multiple datastores.\\nQuickstart connectors: Cohere's quickstart connectors allow you to quickly get up and running. These over 80 pre-built connectors are ready to use, including those for Google Drive, Slack, GitHub, Elastic, Pinecone, and more.\\nAutomated document handling: One common challenge in RAG is handling long documents at scale. The Cohere API provides an option for automating document handling, from chunking up to fitting them into a prompt.\\nDocument mode: For developers who want greater control over each component of a RAG system, Cohere Chat in document mode provides the modularity and flexibility needed to design such systems.\\nTry It with Coral\\nTo see Cohere-powered RAG in action, you can try Coral, which is a conversational AI toolkit for enterprises to build RAG-enabled knowledge assistants. Coral includes some document grounding functionalities out-of-the-box, such as web search results, specific domain grounding, and PDF document support.\"),\n",
       " Document(metadata={'title': 'Getting Started with Retrieval-Augmented Generation', 'source': 'https://cohere.com/llmu/rag-start'}, page_content='Users can engage Coral by entering a prompt to find answers from across their documents. Generated responses include citations of the information sources used, which verifies their accuracy and mitigates LLM hallucinations.\\nA screenshot of Coral, Cohere\\'s conversational AI toolkit for enterprises\\nStep-by-Step Guide\\nLet’s start our exploration of RAG with a quick example.\\nWe’ll walk through how to ground an LLM’s response with information from external documents and provide document citations along with it. In this example, we’ll use a static, short list of documents. Below is a diagram that provides an overview of our simple RAG system.\\nAn overview of what we\\'ll build\\nSetup\\nFirst, let’s install and import the cohere library, and then create a Cohere client using an API key.\\nimport cohere co = cohere.Client(\"COHERE_API_KEY\") \\nDefine the Documents\\nNext, we define the documents that we want to ground an LLM’s response with, formatted as a list. In our case, each document consists of two fields: title and text.\\nThe documents list includes a list of documents with a “text” field containing the information we want the model to use. The recommended length for the snippet of each document is relatively short, 300 words or less. We recommend using field names similar to the ones we’ve included in this example (i.e., “title” and “text”), but RAG is quite flexible with respect to how you structure the documents listings. You can give the fields any names you want, and you can pass in other fields as well, such as a “date” field. All field names and field values are passed to the model.\\ndocuments = [ { \"title\": \"Tall penguins\", \"text\": \"Emperor penguins are the tallest.\"}, { \"title\": \"Penguin habitats\", \"text\": \"Emperor penguins only live in Antarctica.\"}, { \"title\": \"What are animals?\", \"text\": \"Animals are different from plants.\"} ] \\nGenerate the Response with Citations'),\n",
       " Document(metadata={'title': 'Getting Started with Retrieval-Augmented Generation', 'source': 'https://cohere.com/llmu/rag-start'}, page_content='Generate the Response with Citations\\nCohere’s RAG functionalities are part of the Chat endpoint, with the Command model as the underlying LLM. This allows developers to build chatbots that have the full context of a conversation and are not limited to a single interaction.\\nFirst, we define the message coming from the user. We’ll use a simple query, “What are the tallest living penguins?”, as an example.\\n# Get the user message message = \"What are the tallest living penguins?\" \\nThen, we pass this message as a message parameter to a Chat endpoint call. We also pass the list of documents as a documents parameter. By using the chat_stream method, the response is generated incrementally by token without having to wait for the full completion.\\n# Generate the response response = co.chat_stream(message=message, documents=documents) \\nFinally, we display the response from the model. The streamed response will return different types of objects, and for now, we are interested in the text-generation objects, which contain the generated text.\\nWe also display the citations and source documents, which we can get from the final object returned by the streamed response.\\n# Display the response citations = [] cited_documents = [] for event in response: if event.event_type == \"text-generation\": print(event.text, end=\"\") elif event.event_type == \"citation-generation\": citations.extend(event.citations) elif event.event_type == \"stream-end\": cited_documents = event.response.documents # Display the citations and source documents if citations: print(\"\\\\n\\\\nCITATIONS:\") for citation in citations: print(citation) print(\"\\\\nDOCUMENTS:\") for document in cited_documents: print(document) \\nAnd here’s the response generated by our RAG system.'),\n",
       " Document(metadata={'title': 'Getting Started with Retrieval-Augmented Generation', 'source': 'https://cohere.com/llmu/rag-start'}, page_content=\"And here’s the response generated by our RAG system. \\nThe tallest living penguins are emperor penguins, which are found only in Antarctica. CITATIONS: start=32 end=48 text='emperor penguins' document_ids=['doc_0'] start=66 end=85 text='only in Antarctica.' document_ids=['doc_1'] DOCUMENTS: {'id': 'doc_0', 'text': 'Emperor penguins are the tallest.', 'title': 'Tall penguins'} {'id': 'doc_1', 'text': 'Emperor penguins only live in Antarctica.', 'title': 'Penguin habitats'} \\nFirst, we get the actual text response from the model (see the output below).\\nThis is followed by a list of citations, which are references to specific source documents on our list that provided the information contained in specific passages within the text response. For example, the first citation indicates that the term “emperor penguins,” which appears between the 32nd and 48th characters of the response, came from the first document on the list ('doc_0').\\nFinally, we get the full list of the source documents used to generate the response.\\nConclusion\\nIn this chapter, you learned about RAG and how to get started with RAG using the Cohere Chat endpoint.\\nContinue to the next chapter to learn how to build a RAG-powered chatbot that leverages text embeddings using the Chat, Embed, and Rerank endpoints.\\nAbout Cohere’s LLM University\\nOur comprehensive curriculum aims to equip you with the skills to develop your own AI applications. We cater to learners from all backgrounds, covering everything from the basics to the most advanced topics in large language models (LLMs). Plus, you'll have the opportunity to work on hands-on exercises, allowing you to build and deploy your very own solutions. Take a course today.\"),\n",
       " Document(metadata={'title': 'How to Build a RAG-Powered Chatbot with Chat, Embed, and Rerank', 'source': 'https://cohere.com/llmu/rag-chatbot'}, page_content=\"How to Build a RAG-Powered Chatbot with Chat, Embed, and RerankQualified\\nIn this chapter, you’ll learn how to build a RAG-powered chatbot that leverages text embeddings using the Chat, Embed, and Rerank endpoints.\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nContents\\nStep-by-Step Guide \\nWhat We'll Build\\nSetup\\nCreate the Vectorstore Component \\nLoad and Chunk the Documents\\nEmbed the Document Chunks\\nIndex Document Chunks\\nImplement Retrieval \\nDense Retrieval\\nReranking\\nProcess the Documents\\nTest Retrieval\\nCreate the Chatbot Component \\nGet the User Message\\nGenerate the Queries\\nRetrieve Relevant Chunks and Generate the Response\\nDisplay the Response with Citations\\nRun the Chatbot\\nConclusion\\nIn the previous chapter, you learned how to get started with RAG using the Chat endpoint.\\nIn this chapter, you’ll learn how to add RAG applications that leverage text embeddings and how to build a chatbot using RAG in document mode and multiple Cohere endpoints.\\nIn the previous chapter, we used a short list of simple documents. In real-world applications, however, developers typically need to work with a larger volume of documents that each vary in length.\\nThis is where text embeddings can help. With embeddings, we can split these documents into smaller chunks and build a semantic search system that can retrieve the most relevant chunks to a user query based on contextual meaning, and not just keyword-matching.\\nStep-by-Step Guide\\nThere are three RAG modes available with the Cohere Chat endpoint:\\nDocument mode: Specifying the documents for the model to use when generating a response\\nConnectors mode: Connecting the endpoint with an external service that handles all the logic of document retrieval\\nQuery-generation mode: Generating one or more queries given a user message\"),\n",
       " Document(metadata={'title': 'How to Build a RAG-Powered Chatbot with Chat, Embed, and Rerank', 'source': 'https://cohere.com/llmu/rag-chatbot'}, page_content=\"Connectors mode: Connecting the endpoint with an external service that handles all the logic of document retrieval\\nQuery-generation mode: Generating one or more queries given a user message\\nIn this chapter, you’ll learn how to use RAG in document mode, which will also involve the query-generation mode. In the next chapter, you’ll learn how to use RAG in connector mode. Refer to the RAG documentation for more details.\\nWhat We'll Build\\nWe’ll build a chatbot that answers users’ questions about the contents in Cohere's documentation on prompt engineering. Let’s examine the demo application's high-level implementation plan (see the diagram below).\\nAn overview of what we'll build\\nThe steps to building a RAG-powered chatbot are summarized below:\\nSetup phase:\\nStep 0: Ingest the documents – get documents, chunk, embed, and index\\nFor each user-chatbot interaction:\\nStep 1: Get the user message\\nStep 2: Call the Chat endpoint in query-generation mode\\nIf at least one query is generated:\\nStep 3: Retrieve and rerank relevant documents\\nStep 4: Call the Chat endpoint in document mode to generate a grounded response with citations\\nIf no query is generated:\\nStep 4: Call the Chat endpoint in normal mode to generate a response\\nThroughout the conversation:\\nAppend the user-chatbot interaction to the conversation thread\\nRepeat with every interaction\\nTo build a RAG system that can effectively handle a complex corpus of documents, we’ll need to use several Cohere API endpoints, including:\\nChat: For handling the main logic of the chatbot, including turning a user message into queries, generating responses, and producing citations\\nEmbed: For turning textual documents into their embeddings representation, later to be used in retrieval (we’ll use the latest Embed v3 model)\\nRerank: For reranking the retrieved documents according to their relevance to a query (we’ll use the latest Rerank 3 model)\\nThis demo application will use Cohere’s Chat, Embed, and Rerank endpoints\"),\n",
       " Document(metadata={'title': 'How to Build a RAG-Powered Chatbot with Chat, Embed, and Rerank', 'source': 'https://cohere.com/llmu/rag-chatbot'}, page_content='Rerank: For reranking the retrieved documents according to their relevance to a query (we’ll use the latest Rerank 3 model)\\nThis demo application will use Cohere’s Chat, Embed, and Rerank endpoints\\nFor further reading, the API reference page contains a detailed description of the Chat endpoint’s input parameters and response objects.\\nSetup\\nFirst, let’s import the necessary libraries for this project. This includes cohere, hnswlib for the vector library, and unstructured for chunking the documents (more details on these later).\\npip install cohere hnswlib unstructured \\nThen, import the necessary modules from these libraries in addition to other required modules. Let’s also create a Cohere client.\\nimport cohere import uuid import hnswlib from typing import List, Dict from unstructured.partition.html import partition_html from unstructured.chunking.title import chunk_by_title co = cohere.Client(\"COHERE_API_KEY\") \\nCreate the Vectorstore Component\\nThe Vectorstore class handles the ingestion of documents into embeddings (or vectors) and the retrieval of relevant documents given a query.\\nThe Vectorstore component handles document ingestion and retrieval\\nAs an example, we’ll use the contents from Cohere\\'s documentation on prompt engineering. It consists of four web pages, each in the Python list raw_documents below. Each entry is identified by its title and URL.\\nraw_documents = [ { \"title\": \"Crafting Effective Prompts\", \"url\": \"https://docs.cohere.com/docs/crafting-effective-prompts\"}, { \"title\": \"Advanced Prompt Engineering Techniques\", \"url\": \"https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\"}, { \"title\": \"Prompt Truncation\", \"url\": \"https://docs.cohere.com/docs/prompt-truncation\"}, { \"title\": \"Preambles\", \"url\": \"https://docs.cohere.com/docs/preambles\"} ] \\nWe implement this in the Vectorstore class below, which takes the raw_documents list as input.'),\n",
       " Document(metadata={'title': 'How to Build a RAG-Powered Chatbot with Chat, Embed, and Rerank', 'source': 'https://cohere.com/llmu/rag-chatbot'}, page_content='We implement this in the Vectorstore class below, which takes the raw_documents list as input.\\nclass Vectorstore: def __init__(self, raw_documents: List[Dict[str, str]]): self.raw_documents = raw_documents self.docs = [] self.docs_embs = [] self.retrieve_top_k = 10 self.rerank_top_k = 3 self.load_and_chunk() self.embed() self.index() \\nWe also initialize a few instance attributes and methods. The attributes include self.raw_documents to represent the raw documents, self.docs to represent the chunked version of the documents, self.docs_embs to represent the embeddings of the chunked documents, and a couple of top_k parameters to be used for retrieval and reranking.\\nMeanwhile, the methods include load_and_chunk(), embed(), and index() for ingesting raw documents. As you’ll see, we will also specify a retrieve() method to retrieve relevant document chunks given a query.\\nThe document ingestion portion of the Documents component\\nLoad and Chunk the Documents\\nThe load_and_chunk() method loads the raw documents from the URL and breaks them into smaller chunks. Chunking for information retrieval is a broad topic in and of itself, with many strategies being discussed within the AI community. For our example, we’ll utilize the partition_html method from the unstructured library. Read its documentation for more information about its chunking approach.\\nEach chunk is turned into a dictionary with three fields:\\ntitle: The web page’s title\\ntext: The textual content of the chunk\\nurl: The web page’s URL\\nThis information will eventually be passed to the chatbot’s prompt for generating the response, so it’s crucial to populate relevant information into this dictionary. Note that we are not limited to these three fields. At a minimum, the Chat endpoint requires the text field, but beyond that, we can add custom fields that can provide more context about the document, such as subtitles, snippets, tags, and others.\\nThe resulting dictionaries are stored in the self.docs attribute.'),\n",
       " Document(metadata={'title': 'How to Build a RAG-Powered Chatbot with Chat, Embed, and Rerank', 'source': 'https://cohere.com/llmu/rag-chatbot'}, page_content='The resulting dictionaries are stored in the self.docs attribute.\\nclass Vectorstore: ... ... def load_and_chunk(self) -> None: \"\"\" Loads the text from the sources and chunks the HTML content. \"\"\" print(\"Loading documents...\") for raw_document in self.raw_documents: elements = partition_html(url=raw_document[\"url\"]) chunks = chunk_by_title(elements) for chunk in chunks: self.docs.append( { \"title\": raw_document[\"title\"], \"text\": str(chunk), \"url\": raw_document[\"url\"], } ) \\nEmbed the Document Chunks\\nThe embed() method generates embeddings of the chunked documents. We use the Embed endpoint and Cohere\\'s embed-english-v3.0 model. Since the endpoint has a limit of 96 documents per call, we send them in batches.\\nWith the Embed v3 model, we need to define an input_type, of which there are four options depending on the type of task. Using these input types ensures the highest possible quality for the respective tasks. Since our document chunks will be used for retrieval, we use search_document as the input_type.\\nThe resulting chunk embeddings are stored in the self.docs_embs attribute.\\nclass Vectorstore: ... ... def embed(self) -> None: \"\"\" Embeds the document chunks using the Cohere API. \"\"\" print(\"Embedding document chunks...\") batch_size = 90 self.docs_len = len(self.docs) for i in range(0, self.docs_len, batch_size): batch = self.docs[i : min(i + batch_size, self.docs_len)] texts = [item[\"text\"] for item in batch] docs_embs_batch = co.embed( texts=texts, model=\"embed-english-v3.0\", input_type=\"search_document\" ).embeddings self.docs_embs.extend(docs_embs_batch) \\nIndex Document Chunks\\nThe index() method indexes the document chunk embeddings. We build an index to store the embeddings in a structured and organized way in order to ensure efficient similarity search during retrieval.'),\n",
       " Document(metadata={'title': 'How to Build a RAG-Powered Chatbot with Chat, Embed, and Rerank', 'source': 'https://cohere.com/llmu/rag-chatbot'}, page_content='The index() method indexes the document chunk embeddings. We build an index to store the embeddings in a structured and organized way in order to ensure efficient similarity search during retrieval.\\nThere are many options available for building an index. For production environments, typically a vector database (like Weaviate or MongoDB) is required to handle the continuous process of indexing documents and maintaining the index.\\nIn our example, however, we’ll keep it simple and use a vector library instead. We can choose from many open-source projects, such as Faiss, Annoy, ScaNN, or Hnswlib, which is the one we’ll use. These libraries store embeddings in in-memory indexes and implement approximate nearest neighbor (ANN) algorithms to make similarity search efficient.\\nThe resulting document chunk embeddings are stored in the self.idx attribute.\\nclass Vectorstore: ... ... def index(self) -> None: \"\"\" Indexes the documents for efficient retrieval. \"\"\" print(\"Indexing documents...\") self.idx = hnswlib.Index(space=\"ip\", dim=1024) self.idx.init_index(max_elements=self.docs_len, ef_construction=512, M=64) self.idx.add_items(self.docs_embs, list(range(len(self.docs_embs)))) print(f\"Indexing complete with {self.idx.get_current_count()} documents.\") \\nImplement Retrieval\\nThe retrieve() method uses semantic search to retrieve relevant document chunks given a query, and it has two steps: (1) dense retrieval, (2) reranking.\\nA more detailed view of document ingestion, retrieval, and reranking\\nDense Retrieval\\nWe implement a dense retrieval system that leverages embeddings to retrieve document chunks, offering significant improvements over basic keyword-matching approaches. Embeddings can capture the contextual meaning of a document, thus enabling the retrieval of highly relevant results to the given query.\\nWe embed the query using the same embed-english-v3.0 model that we used to embed the document chunks, but this time, we set input_type=”search_query”.'),\n",
       " Document(metadata={'title': 'How to Build a RAG-Powered Chatbot with Chat, Embed, and Rerank', 'source': 'https://cohere.com/llmu/rag-chatbot'}, page_content=\"We embed the query using the same embed-english-v3.0 model that we used to embed the document chunks, but this time, we set input_type=”search_query”.\\nSearch is performed by the knn_query() method from the hnswlib library. Given a query, it returns the document chunks most similar to the query. We define the number of document chunks to return using the attribute self.retrieve_top_k=10.\\nReranking\\nAfter dense retrieval, we implement a reranking step. While our dense retrieval component is already highly capable of retrieving relevant sources, the Rerank endpoint provides an additional boost to the quality of the search results, especially for complex and domain-specific queries. It takes the search results and sorts them according to their relevance to the query.\\nWe call the Rerank endpoint with co.rerank() and pass the query and the list of document chunks to be reranked. We also define the number of top reranked document chunks to retrieve using the attribute self.rerank_top_k=3. The model we use is rerank-english-v3.0, which lets you rerank documents that contain multiple fields, in the form of JSON objects. In our case, we'll use the title and text fields for reranking.\\nThis method returns the top retrieved document chunks as a Python list docs_retrieved, so that they can be passed to the chatbot, which we’ll implement next.\"),\n",
       " Document(metadata={'title': 'How to Build a RAG-Powered Chatbot with Chat, Embed, and Rerank', 'source': 'https://cohere.com/llmu/rag-chatbot'}, page_content='This method returns the top retrieved document chunks as a Python list docs_retrieved, so that they can be passed to the chatbot, which we’ll implement next.\\nclass Vectorstore: ... ... def retrieve(self, query: str) -> List[Dict[str, str]]: \"\"\" Retrieves document chunks based on the given query. Parameters: query (str): The query to retrieve document chunks for. Returns: List[Dict[str, str]]: A list of dictionaries representing the retrieved document chunks, with \\'title\\', \\'text\\', and \\'url\\' keys. \"\"\" # Dense retrieval query_emb = co.embed( texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\" ).embeddings doc_ids = self.idx.knn_query(query_emb, k=self.retrieve_top_k)[0][0] # Reranking rank_fields = [\"title\", \"text\"] # We\\'ll use the title and text fields for reranking docs_to_rerank = [self.docs[doc_id] for doc_id in doc_ids] rerank_results = co.rerank( query=query, documents=docs_to_rerank, top_n=self.rerank_top_k, model=\"rerank-english-v3.0\", rank_fields=rank_fields ) docs_retrieved = [] for doc_id in doc_ids_reranked: docs_retrieved.append( { \"title\": self.docs[doc_id][\"title\"], \"text\": self.docs[doc_id][\"text\"], \"url\": self.docs[doc_id][\"url\"], } ) return docs_retrieved \\nProcess the Documents\\nWe can now process the raw documents. We do that by creating an instance of Vectorstore. In our case, we get a total of 136 documents, chunked from the four web URLs.\\nvectorstore = Vectorstore(raw_documents) \\nLoading documents... Embedding documents... Indexing documents... Indexing complete with 134 documents. \\nTest Retrieval\\nBefore going further, we first test the document retrieval part of the system. First, we create an instance of the Vectorstore with the raw documents that we have defined. Then, we use the retrieve method to retrieve the most relevant documents to the query \"Prompting by giving examples.\"\\nvectorstore.retrieve(\"Prompting by giving examples\")'),\n",
       " Document(metadata={'title': 'How to Build a RAG-Powered Chatbot with Chat, Embed, and Rerank', 'source': 'https://cohere.com/llmu/rag-chatbot'}, page_content='vectorstore.retrieve(\"Prompting by giving examples\") \\nAnd here’s the response. We can see that the document chunks returned are indeed highly relevant to the query we sent.\\n[{\\'title\\': \\'Advanced Prompt Engineering Techniques\\', \\'text\\': \\'Few-shot Prompting\\\\n\\\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.\\', \\'url\\': \\'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\\'}, {\\'title\\': \\'Crafting Effective Prompts\\', \\'text\\': \\'Incorporating Example Outputs\\\\n\\\\nLLMs respond well when they have specific examples to work from. For example, instead of asking for the salient points of the text and using bullet points “where appropriate”, give an example of what the output should look like.\\', \\'url\\': \\'https://docs.cohere.com/docs/crafting-effective-prompts\\'}, {\\'title\\': \\'Advanced Prompt Engineering Techniques\\', \\'text\\': \\'In addition to giving correct examples, including negative examples with a clear indication of why they are wrong can help the LLM learn to distinguish between correct and incorrect responses. Ordering the examples can also be important; if there are patterns that could be picked up on that are not relevant to the correctness of the question, the model may incorrectly pick up on those instead of the semantics of the question itself.\\', \\'url\\': \\'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\\'}] \\nRun the Chatbot\\nWe can now run the chatbot. For this, we create a generate_chat function which includes the RAG components:'),\n",
       " Document(metadata={'title': 'How to Build a RAG-Powered Chatbot with Chat, Embed, and Rerank', 'source': 'https://cohere.com/llmu/rag-chatbot'}, page_content='Run the Chatbot\\nWe can now run the chatbot. For this, we create a generate_chat function which includes the RAG components:\\nSearch query generation: For each user message, we use the endpoint’s search query generation feature to turn the message into one or more queries that are optimized for retrieval. The endpoint can even return no query, which means that a user message can be responded to directly without retrieval. This is done by calling the Chat endpoint with the search_queries_only parameter and setting it as True.\\nDocument retrieval: If there is no search query generated, we call the Chat endpoint to generate a response directly. If there is at least one, we call the retrieve method from the Vectorstore instance to retrieve the most relevant documents to each query.\\nResponse and citation generation: Finally, all the results from all queries are appended to a list and passed to the Chat endpoint for response generation. We print the response, together with the citations and the list of document chunks cited, for easy reference.\\nThe generate_chat function handles the chatbot logic, from getting the user message to generating the response'),\n",
       " Document(metadata={'title': 'How to Build a RAG-Powered Chatbot with Chat, Embed, and Rerank', 'source': 'https://cohere.com/llmu/rag-chatbot'}, page_content='The generate_chat function handles the chatbot logic, from getting the user message to generating the response\\ndef run_chatbot(message, chat_history=[]): # Generate search queries, if any response = co.chat(message=message, model=\"command-r-plus\", search_queries_only=True, chat_history=chat_history) search_queries = [] for query in response.search_queries: search_queries.append(query.text) # If there are search queries, retrieve the documents if search_queries: print(\"Retrieving information...\", end=\"\") # Retrieve document chunks for each query documents = [] for query in search_queries: documents.extend(vectorstore.retrieve(query)) # Use document chunks to respond response = co.chat_stream( message=message, model=\"command-r-plus\", documents=documents, chat_history=chat_history, ) else: response = co.chat_stream( message=message, model=\"command-r-plus\", chat_history=chat_history, ) # Print the chatbot response and citations chatbot_response = \"\" print(\"\\\\nChatbot:\") for event in response: if event.event_type == \"text-generation\": print(event.text, end=\"\") chatbot_response += event.text if event.event_type == \"stream-end\": if event.response.citations: print(\"\\\\n\\\\nCITATIONS:\") for citation in event.response.citations: print(citation) if event.response.documents: print(\"\\\\nCITED DOCUMENTS:\") for document in event.response.documents: print(document) # Update the chat history for the next turn chat_history = event.response.chat_history return chat_history \\nSearch Query Generation\\nLet\\'s take a deeper look at the search query generation feature. Based on the user message, the chatbot needs to decide if it needs to consult external information before responding. If so, the chatbot determines an optimal set of search queries to use for retrieval. When we call co.chat() with search_queries_only=True, the Chat endpoint handles this for us automatically.'),\n",
       " Document(metadata={'title': 'How to Build a RAG-Powered Chatbot with Chat, Embed, and Rerank', 'source': 'https://cohere.com/llmu/rag-chatbot'}, page_content='The generated queries can be accessed from the search_queries field of the object that is returned. To understand how this works, let’s look at a few scenarios:\\nNo query needed: Suppose we have a user message of “Hello, I need help with a report I\\'m writing”. This type of message doesn’t require any additional context from external information, so retrieval is not required. A direct chatbot response will suffice (for example: “Sure, how can I help?”). When we send this to the Chat endpoint, we get an empty search_queries result, which is what we expect.\\nOne query generated: Take this user message: \"What did the report say about the company\\'s Q4 performance?” This does require additional context as it refers to a report, hence retrieval is required. Given this message, the Chat endpoint returns the search_queries result of Q4 company performance. Here it turns the user message into a query optimized for search. Another important scenario is generating queries in the context of the conversation. Suppose there’s an ongoing conversation where the user is learning from the chatbot about deep learning. If at some point, the user asks, “Why is it important”, then the generated search_queries will become why is deep learning important, providing the much-needed context for the retrieval process.\\nMore than one query generated: What if the user message is a bit more complex, such as \"What did the report say about the company\\'s Q4 performance and its range of products and services?” This requires multiple pieces of information to be retrieved. Given this message, the Chat endpoint returns two search_queries results: Q4 company performance and company\\'s range of products and services.\\nThese scenarios highlight the adaptability of the Chat endpoint to decide on the next course of action based on a user message.\\nDocument Retrieval\\nLet\\'s take a deeper look at the document retrieval step. What happens next depends on how many search queries are returned.'),\n",
       " Document(metadata={'title': 'How to Build a RAG-Powered Chatbot with Chat, Embed, and Rerank', 'source': 'https://cohere.com/llmu/rag-chatbot'}, page_content=\"Document Retrieval\\nLet's take a deeper look at the document retrieval step. What happens next depends on how many search queries are returned.\\nIf search queries are returned\\nIf the chatbot response contains at least one search query, we call the retrieve() method from the Vectorstore class instance to retrieve document chunks that are relevant to the queries.\\nThen, we call the Chat endpoint to generate a response, adding a documents parameter to the call to pass the relevant document chunks.\\nIf no search queries are returned\\nMeanwhile, if the chatbot response doesn’t contain any search queries, then it doesn’t require information retrieval. To generate the response, we call the Chat endpoint another time, passing the user message and without needing to add any sources to the call.\\nIn either case, we also pass the chat_history parameter, which retains the interactions between the user and the chatbot in the same conversation thread. We also use the chat_stream endpoint so we can stream the chatbot response to the application.\\nResponse and Citation Generation\\nLet's take a deeper look at the response generation step. The chatbot response includes a stream of events, such as the generated text and citations followed by a final object which contains the sources used by the chatbot along with other details.\\nTo display the response, we use the text-generation events from the response stream.\\nThe citation-generation events indicate the spans of text from the retrieved document chunks on which the response is grounded. Here is one example:\\nstart=382 end=397 text='similar vectors' document_ids=['doc_0', 'doc_2'] \\nThe format of each citation is:\\nstart: The starting point of a span where one or more documents are referenced\\nend: The ending point of a span where one or more documents are referenced\\ntext: The text representing this span\"),\n",
       " Document(metadata={'title': 'How to Build a RAG-Powered Chatbot with Chat, Embed, and Rerank', 'source': 'https://cohere.com/llmu/rag-chatbot'}, page_content='start: The starting point of a span where one or more documents are referenced\\nend: The ending point of a span where one or more documents are referenced\\ntext: The text representing this span\\ndocument_ids: The IDs of the document chunks being referenced (doc_0 being the ID of the first document chunk passed to the documents creating parameter in the endpoint call, and so on)\\nThe final response object includes a list of the document chunks, which we access from the documents attribute.\\nExample conversation\\nHere’s an example of a conversation that happens over a few turns:\\n# Turn # 1 chat_history = run_chatbot(\"Hello, I have a question\") \\nChatbot: Of course! I am here to help. Please go ahead with your question, and I will do my best to assist you. \\n# Turn # 2 chat_history = run_chatbot(\"What\\'s the difference between zero-shot and few-shot prompting\", chat_history)'),\n",
       " Document(metadata={'title': 'How to Build a RAG-Powered Chatbot with Chat, Embed, and Rerank', 'source': 'https://cohere.com/llmu/rag-chatbot'}, page_content='# Turn # 2 chat_history = run_chatbot(\"What\\'s the difference between zero-shot and few-shot prompting\", chat_history) \\nRetrieving information... Chatbot: Zero-shot prompting involves asking the model to perform a task without providing any examples. On the other hand, few-shot prompting is a technique where the model is provided with a few relevant and diverse examples of the task being performed before asking the specific question to be answered. These examples help steer the model toward a high-quality solution and condition it to the expected response type and style. CITATIONS: start=0 end=19 text=\\'Zero-shot prompting\\' document_ids=[\\'doc_0\\'] start=29 end=95 text=\\'asking the model to perform a task without providing any examples.\\' document_ids=[\\'doc_0\\'] start=115 end=133 text=\\'few-shot prompting\\' document_ids=[\\'doc_0\\'] start=159 end=217 text=\\'model is provided with a few relevant and diverse examples\\' document_ids=[\\'doc_0\\'] start=246 end=297 text=\\'before asking the specific question to be answered.\\' document_ids=[\\'doc_0\\'] start=318 end=364 text=\\'steer the model toward a high-quality solution\\' document_ids=[\\'doc_0\\'] start=369 end=422 text=\\'condition it to the expected response type and style.\\' document_ids=[\\'doc_0\\'] CITED DOCUMENTS: {\\'id\\': \\'doc_0\\', \\'text\\': \\'Few-shot Prompting\\\\n\\\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.\\', \\'title\\': \\'Advanced Prompt Engineering Techniques\\', \\'url\\': \\'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\\'} \\n# Turn # 3 chat_history = run_chatbot(\"How would the latter help?\", chat_history)'),\n",
       " Document(metadata={'title': 'How to Build a RAG-Powered Chatbot with Chat, Embed, and Rerank', 'source': 'https://cohere.com/llmu/rag-chatbot'}, page_content='# Turn # 3 chat_history = run_chatbot(\"How would the latter help?\", chat_history) \\nRetrieving information... Chatbot: Few-shot prompting can vastly improve the quality of the model\\'s completions. Providing a few relevant and diverse examples helps steer the model toward a high-quality solution by conditioning it to the expected response type and style. CITATIONS: start=23 end=77 text=\"vastly improve the quality of the model\\'s completions.\" document_ids=[\\'doc_2\\'] start=90 end=123 text=\\'few relevant and diverse examples\\' document_ids=[\\'doc_0\\'] start=130 end=176 text=\\'steer the model toward a high-quality solution\\' document_ids=[\\'doc_0\\'] start=180 end=236 text=\\'conditioning it to the expected response type and style.\\' document_ids=[\\'doc_0\\'] CITED DOCUMENTS: {\\'id\\': \\'doc_2\\', \\'text\\': \\'Advanced Prompt Engineering Techniques\\\\n\\\\nSuggest Edits\\\\n\\\\nThe previous chapter discussed general rules and heuristics to follow for successfully prompting the Command family of models. Here, we will discuss specific advanced prompt engineering techniques that can in many cases vastly improve the quality of the model’s completions. These include how to give clear and unambiguous instructions, few-shot prompting, chain-of-thought (CoT) techniques, and prompt chaining.\\', \\'title\\': \\'Advanced Prompt Engineering Techniques\\', \\'url\\': \\'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\\'} {\\'id\\': \\'doc_0\\', \\'text\\': \\'Few-shot Prompting\\\\n\\\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.\\', \\'title\\': \\'Advanced Prompt Engineering Techniques\\', \\'url\\': \\'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\\'}'),\n",
       " Document(metadata={'title': 'How to Build a RAG-Powered Chatbot with Chat, Embed, and Rerank', 'source': 'https://cohere.com/llmu/rag-chatbot'}, page_content='# Turn # 4 chat_history = run_chatbot(\"What do you know about 5G networks?\", chat_history) \\nRetrieving information... Chatbot: Sorry, I don\\'t have any information about 5G networks. Can I help you with anything else? There are a few observations worth pointing out: Direct response: For user messages that don’t require retrieval (“Hello, I have a question”), the chatbot responds directly without requiring retrieval. Citation generation: For responses that do require retrieval (\"What\\'s the difference between zero-shot and few-shot prompting\"), the endpoint returns the response together with the citations. These are fine-grained citations, which means they refer to specific spans of the generated text. State management: The endpoint maintains the state of the conversation via the chat_history parameter, for example, by correctly responding to a vague user message such as \"How would the latter help?\" Response synthesis: The model can decide if none of the retrieved documents provide the necessary information to answer a user message. For example, when asked the question, “What do you know about 5G networks”, the chatbot retrieves external information from the index. However, it doesn’t use any of the information in its response as none of it is relevant to the question. \\nIn the conversation above, notice a few observations that reflect the different components of what we built:\\nDirect response: For user messages that don’t require retrieval (“Hello, I have a question”), the chatbot responds directly without requiring retrieval.\\nCitation generation: For responses that do require retrieval (“What’s the difference between word and sentence embeddings”), the endpoint returns the response together with the citations.\\nState management: The endpoint maintains the state of the conversation via the conversation_id parameter, for example, by being able to correctly respond to a vague user message of “And what are their similarities”'),\n",
       " Document(metadata={'title': 'How to Build a RAG-Powered Chatbot with Chat, Embed, and Rerank', 'source': 'https://cohere.com/llmu/rag-chatbot'}, page_content='Response synthesis: The model can decide if none of the retrieved documents provide the necessary information required to answer a user message. For example, when asked the question, “What do you know about 5G networks”, the chatbot goes on and retrieves external information from the index. However, it doesn’t use any of the information in its response as none of them is relevant to the question.\\nHere are the contents of the chat history.\\nprint(\"Chat history:\") for c in chat_history: print(c, \"\\\\n\") \\nChat history: message=\\'Hello, I have a question\\' tool_calls=None role=\\'USER\\' message=\\'Of course! I am here to help. Please go ahead with your question, and I will do my best to assist you.\\' tool_calls=None role=\\'CHATBOT\\' message=\"What\\'s the difference between zero-shot and few-shot prompting\" tool_calls=None role=\\'USER\\' message=\\'Zero-shot prompting involves asking the model to perform a task without providing any examples. On the other hand, few-shot prompting is a technique where the model is provided with a few relevant and diverse examples of the task being performed before asking the specific question to be answered. These examples help steer the model toward a high-quality solution and condition it to the expected response type and style.\\' tool_calls=None role=\\'CHATBOT\\' message=\\'How would the latter help?\\' tool_calls=None role=\\'USER\\' message=\"Few-shot prompting can vastly improve the quality of the model\\'s completions. Providing a few relevant and diverse examples helps steer the model toward a high-quality solution by conditioning it to the expected response type and style.\" tool_calls=None role=\\'CHATBOT\\' message=\\'What do you know about 5G networks?\\' tool_calls=None role=\\'USER\\' message=\"Sorry, I don\\'t have any information about 5G networks. Can I help you with anything else?\" tool_calls=None role=\\'CHATBOT\\' \\nConclusion'),\n",
       " Document(metadata={'title': 'How to Build a RAG-Powered Chatbot with Chat, Embed, and Rerank', 'source': 'https://cohere.com/llmu/rag-chatbot'}, page_content=\"Conclusion\\nIn this chapter, you learned how to build a RAG-powered chatbot with the Chat endpoint. With access to a collection of documents, the chatbot is able to provide contextually relevant responses to user requests, along with verifiable citations.\\nWe used the Chat endpoint in document mode. This mode highlights the modularity of the endpoint, giving developers the flexibility to customize each component of the system.\\nAn alternative to this is connectors mode. It abstracts away some of the steps we saw in the documents mode, which makes it simpler to build applications. It also makes it easy to connect to enterprise data sources and do that at scale.\\nContinue to the next chapter to learn about connectors and how to build RAG applications using the web search connector.\\nAbout Cohere’s LLM University\\nOur comprehensive curriculum aims to equip you with the skills to develop your own AI applications. We cater to learners from all backgrounds, covering everything from the basics to the most advanced topics in large language models (LLMs). Plus, you'll have the opportunity to work on hands-on exercises, allowing you to build and deploy your very own solutions. Take a course today.\\nThis LLMU module consists of the following chapters:\\nIntroduction to RAG\\nRAG with Chat, Embed, and Rerank (this chapter)\\nRAG with Connectors\\nRAG with Quickstart Connectors\\nRAG over Large-Scale Data\"),\n",
       " Document(metadata={'title': 'How to Build RAG Applications With Connectors', 'source': 'https://cohere.com/llmu/rag-connectors'}, page_content='How to Build RAG Applications With ConnectorsQualified\\nIn this chapter, you’ll learn about connectors and how to build RAG applications using the web search connector.\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nContents\\nWhat Are Connectors?\\nStep-by-Step Guide\\nSetup\\nCreate the Chatbot Component\\nRun the chatbot\\nConclusion\\nIn the previous chapter, we built the chatbot using the Chat endpoint’s document mode. Document mode provides developers with the flexibility to customize each component of a RAG stack.\\nThere is another way to build RAG systems with the Chat endpoint, which is through the connector mode. Connector mode simplifies the development of RAG systems by abstracting away some of the complexities.\\nWe’ll explore connectors over the next three chapters:\\nIn this chapter (Chapter 3), we’ll discuss how to get started quickly with connectors using the web search connector. It’s a connector managed by Cohere, and because of that, we can focus on connector usage instead of implementation.\\nIn Chapter 4, we’ll implement one of Cohere’s quickstart connectors, which are over 80 pre-built connectors that you can use to connect to popular enterprise datastores.\\nIn Chapter 5, we’ll see how to use connectors at scale, specifically on multiple datastores and long documents.\\nWhat Are Connectors?\\nConnectors are independent REST APIs that can be used in a RAG workflow to provide secure, real-time access to private data.\\nIn enterprises, data lives in many different places. The ability of enterprises to realize the full value of RAG rests on their ability to bring these data sources together. Cohere’s build-your-own connectors framework enables developers to develop a connector to any datastore that offers an accompanying search API.\\nCohere’s connectors framework simplifies connecting RAG systems to datastores'),\n",
       " Document(metadata={'title': 'How to Build RAG Applications With Connectors', 'source': 'https://cohere.com/llmu/rag-connectors'}, page_content=\"Cohere’s connectors framework simplifies connecting RAG systems to datastores\\nAt a high level, here’s what connectors do. When the Chat endpoint calls a connector, what happens is that the endpoint is sending a query to the search endpoint of that connector. The connector will then return the list of documents that it deems the most relevant to the query.\\nThe build-your-own connectors framework allows developers to build any logic behind a connector. For example, you can define the retrieval implementation—whether it’s running a semantic similarity search over a vector database, searching over an existing full-text search engine, or utilizing the existing search APIs of platforms like Google Drive or Notion.\\nAdditionally, in connector mode, most of the RAG building blocks are taken care of by the endpoint. This includes deciding whether to retrieve information, generating queries, retrieving documents, chunking and reranking documents (post-retrieval), and generating the response.\\nRecall that in the previous chapter (document mode), we implemented the following steps.\\nStep 1: Get the user message\\nStep 2: Call the Chat endpoint in query-generation mode\\nIf at least one query is generated:\\nStep 3: Retrieve and rerank relevant documents\\nStep 4: Call the Chat endpoint in document mode to generate a grounded response with citations\\nIf no query is generated:\\nStep 4: Call the Chat endpoint in normal mode to generate a direct response\\nIn connector mode, this is simplified to the following two steps.\\nStep 1: Get the user message\\nStep 2: Call the Chat endpoint in connector mode to generate a response (this can be either a grounded response with citations or a direct response)\\nStep-by-Step Guide\\nBelow is a diagram that provides an overview of what we’ll build. We’ll build a RAG chatbot that can search the web, retrieve relevant results to a user query, and generate grounded responses to the query.\\nAn overview of what we'll build\\nSetup\"),\n",
       " Document(metadata={'title': 'How to Build RAG Applications With Connectors', 'source': 'https://cohere.com/llmu/rag-connectors'}, page_content='An overview of what we\\'ll build\\nSetup\\nFirst, let’s install and import the cohere library, and then create a Cohere client using an API key.\\nimport uuid import cohere from cohere import ChatConnector from typing import List co = cohere.Client(\"COHERE_API_KEY\") \\nCreate the Chatbot Component\\nThe change from document mode to connector mode requires just one change to the Chat endpoint, which is swapping the documents parameter with the connectors parameter.\\nHere’s how it looks with the web search connector. We supply the connector id, which is web-search as an argument to the connectors parameter.\\nresponse = co.chat_stream(message=\"What is LLM university\", connectors = [ChatConnector(id=\"web-search)]) \\nThe one line of code above is enough to get a full RAG-enabled response—the response text, the citations, and the source documents, which in this case are snippets from the most relevant information available on the web based on a given user message.\\nBut in order to run this in a multi-turn chatbot scenario, we need to build the chatbot component. The good news is that we can adapt the chatbot we built in the previous chapter.\\nThere are a few changes to make, including:\\nRemove the query generation logic (done by the endpoint)\\nRemove the retrieval logic (done by the endpoint)\\nChange the Chatbot initialization to use connectors instead\\nUse the connectors parameter instead of documents in the Chat endpoint call'),\n",
       " Document(metadata={'title': 'How to Build RAG Applications With Connectors', 'source': 'https://cohere.com/llmu/rag-connectors'}, page_content='Remove the retrieval logic (done by the endpoint)\\nChange the Chatbot initialization to use connectors instead\\nUse the connectors parameter instead of documents in the Chat endpoint call\\nclass Chatbot: def __init__(self, connectors: List[str]): \"\"\" Initializes an instance of the Chatbot class. \"\"\" self.conversation_id = str(uuid.uuid4()) self.connectors = [ChatConnector(id=connector) for connector in connectors] def run(self): \"\"\" Runs the chatbot application. \"\"\" while True: # Get the user message message = input(\"User: \") # Typing \"quit\" ends the conversation if message.lower() == \"quit\": print(\"Ending chat.\") break else: # If using Google Colab, remove this line to avoid printing the same thing twice print(f\"User: {message}\") # If using Google Colab, remove this line to avoid printing the same thing twice # Generate response response = co.chat_stream( message=message, model=\"command-r-plus\", conversation_id=self.conversation_id, connectors=self.connectors, ) # Print the chatbot response, citations, and documents print(\"\\\\nChatbot:\") citations = [] cited_documents = [] # Display response for event in response: if event.event_type == \"text-generation\": print(event.text, end=\"\") elif event.event_type == \"citation-generation\": citations.extend(event.citations) elif event.event_type == \"stream-end\": cited_documents = event.response.documents # Display citations and source documents if citations: print(\"\\\\n\\\\nCITATIONS:\") for citation in citations: print(citation) print(\"\\\\nDOCUMENTS:\") for document in cited_documents: print({\\'id\\': document[\\'id\\'], \\'snippet\\': document[\\'snippet\\'][:400] + \\'...\\', \\'title\\': document[\\'title\\'], \\'url\\': document[\\'url\\']}) print(f\"\\\\n{\\'-\\'*100}\\\\n\") \\nRun the Chatbot\\nAnd that’s about it. We are now ready to run the chatbot.\\nFirst we define the connector to use, which is web-search. Next, we create an instance of the Chatbot class using the connector, and then we run the chatbot.'),\n",
       " Document(metadata={'title': 'How to Build RAG Applications With Connectors', 'source': 'https://cohere.com/llmu/rag-connectors'}, page_content='First we define the connector to use, which is web-search. Next, we create an instance of the Chatbot class using the connector, and then we run the chatbot.\\n# Define the connector connectors = [\"web-search\"] # Create an instance of the Chatbot class chatbot = Chatbot(connectors) # Run the chatbot chatbot.run() \\nAnd we get the same type of response as we’ve seen in the previous chapter – the text response followed by the citations and source documents used.'),\n",
       " Document(metadata={'title': 'How to Build RAG Applications With Connectors', 'source': 'https://cohere.com/llmu/rag-connectors'}, page_content='User: What is Cohere\\'s LLM University Chatbot: Cohere\\'s LLM University (LLMU) is a set of comprehensive learning resources for anyone interested in natural language processing (NLP), from beginners to advanced learners. The curriculum covers everything from the basics of LLMs to the most advanced topics, including generative AI. The course is designed to give learners a solid foundation in NLP and help them develop their own applications. CITATIONS: start=24 end=30 text=\\'(LLMU)\\' document_ids=[\\'web-search_0\\', \\'web-search_1\\'] start=36 end=75 text=\\'set of comprehensive learning resources\\' document_ids=[\\'web-search_1\\'] start=101 end=134 text=\\'natural language processing (NLP)\\' document_ids=[\\'web-search_0\\', \\'web-search_1\\'] start=141 end=172 text=\\'beginners to advanced learners.\\' document_ids=[\\'web-search_0\\', \\'web-search_1\\'] start=177 end=187 text=\\'curriculum\\' document_ids=[\\'web-search_0\\', \\'web-search_1\\'] start=215 end=229 text=\\'basics of LLMs\\' document_ids=[\\'web-search_0\\', \\'web-search_1\\'] start=237 end=283 text=\\'most advanced topics, including generative AI.\\' document_ids=[\\'web-search_1\\'] start=326 end=349 text=\\'solid foundation in NLP\\' document_ids=[\\'web-search_0\\', \\'web-search_1\\'] start=364 end=395 text=\\'develop their own applications.\\' document_ids=[\\'web-search_0\\', \\'web-search_1\\'] DOCUMENTS: {\\'id\\': \\'web-search_0\\', \\'snippet\\': \\'Guides and ConceptsAPI ReferenceRelease NotesApplication ExamplesLLMU\\\\n\\\\nCoralDashboardDocumentationPlaygroundCommunityLog In\\\\n\\\\nCoralDashboardDocumentationPlaygroundCommunityLog In\\\\n\\\\nWelcome to LLM University!\\\\n\\\\nWelcome to LLM University by Cohere!\\\\n\\\\nWe’re so happy that you’ve chosen to learn Natural Language Processing and Large Language Models with us.\\\\n\\\\nOur comprehensive curriculum aims to give you a ...\\', \\'title\\': \\'LLM University (LLMU) | Cohere\\', \\'url\\': \\'https://docs.cohere.com/docs/llmu\\'} {\\'id\\': \\'web-search_1\\', \\'snippet\\': \"Introducing LLM University — Your Go-To Learning Resource for NLP🎓\\\\n\\\\nDiscover our comprehensive NLP curriculum at'),\n",
       " Document(metadata={'title': 'How to Build RAG Applications With Connectors', 'source': 'https://cohere.com/llmu/rag-connectors'}, page_content='\\'url\\': \\'https://docs.cohere.com/docs/llmu\\'} {\\'id\\': \\'web-search_1\\', \\'snippet\\': \"Introducing LLM University — Your Go-To Learning Resource for NLP🎓\\\\n\\\\nDiscover our comprehensive NLP curriculum at LLM University. From the fundamentals of LLMs all the way to the most advanced topics, including generative AI\\\\n\\\\nWe\\'re excited to announce the launch of LLM University (LLMU), a set of comprehensive learning resources for anyone interested in natural language processing (NLP), from begin...\", \\'title\\': \\'Introducing LLM University — Your Go-To Learning Resource for NLP🎓\\', \\'url\\': \\'https://txt.cohere.com/llm-university/\\'} ---------------------------------------------------------------------------------------------------- Ending chat.'),\n",
       " Document(metadata={'title': 'How to Build RAG Applications With Connectors', 'source': 'https://cohere.com/llmu/rag-connectors'}, page_content=\"Conclusion\\nIn this chapter, you learned about the concept of connectors and how to build a RAG-powered chatbot using connectors. In particular, we used the web search connector, which is a Cohere-managed connector that you can use immediately.\\nContinue to the next chapter to learn how to connect RAG applications to datastores by leveraging Cohere’s pre-built quickstart connectors.\\nAbout Cohere’s LLM University\\nOur comprehensive curriculum aims to equip you with the skills to develop your own AI applications. We cater to learners from all backgrounds, covering everything from the basics to the most advanced topics in large language models (LLMs). Plus, you'll have the opportunity to work on hands-on exercises, allowing you to build and deploy your very own solutions. Take a course today.\\nThis LLMU module consists of the following chapters:\\nIntroduction to RAG\\nRAG with Chat, Embed, and Rerank\\nRAG with Connectors (this chapter)\\nRAG with Quickstart Connectors\\nRAG over Large-Scale Data\"),\n",
       " Document(metadata={'title': 'How to Build RAG Applications With Quickstart Connectors', 'source': 'https://cohere.com/llmu/rag-quickstart-connectors'}, page_content='How to Build RAG Applications With Quickstart ConnectorsQualified\\nIn this chapter, you’ll learn how to connect RAG applications to datastores by leveraging Cohere’s pre-built quickstart connectors.\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook. Note: To run the notebook, you must first deploy your own Google Drive connector as a web-based REST API (we’ll cover the steps in this chapter – here is the implementation code).\\nContents\\nWhat Are Quickstart Connectors?\\nStep-by-Step Guide\\nSetup\\nBuild and Deploy the Connector \\nEnable Google Drive Access\\nConfigure the Connector\\nAdd Authentication\\nDeploy the Connector\\nTest the Connector\\nRegister the Connector\\nUse the Connector\\nConclusion\\nIn the previous chapter, you learned about the concept of connectors and how to build a RAG-powered chatbot using connectors. In particular, we used the web search connector, a Cohere-managed connector.\\nIn this chapter, you’ll learn about how to build your own connector using one of 80+ pre-built quickstart connectors. We’ll use it to connect a chatbot to a Google Drive, enabling the chatbot to use the Google Drive API to find answers to a user’s question by searching documents in the Google Drive.\\nWhat Are Quickstart Connectors?\\nCohere’s build-your-own connectors framework enables developers to build a connector to any datastore that offers an accompanying search API. However, connecting to multiple data sources still requires effort to build the connectors. Developers will need to learn about the API of each platform, whether it is Google Drive, Slack, Notion, or GitHub, before they can build the connector. As the number of data sources increases, this becomes a bottleneck.\\nQuickstart connectors solve this problem. Developers can use pre-built implementations of over 80 connectors immediately without having to build them themselves. They can either use these quickstart connectors directly or adapt them to their organization’s needs.'),\n",
       " Document(metadata={'title': 'How to Build RAG Applications With Quickstart Connectors', 'source': 'https://cohere.com/llmu/rag-quickstart-connectors'}, page_content='Cohere’s quickstart connectors are open-sourced and available in our connectors repository. Here are a few examples:\\nVector databases: Weaviate, Qdrant, Pinecone, Milvus\\nFull-text search engines: Elastic, Opensearch, Vespa, Solr\\nCollaboration: Slack, Linear, Asana, Jira, Trello, Miro\\nFile management: Google Drive, Dropbox, Box\\nData warehouse: Snowflake, Amazon Redshift\\nContent management: Readme, Wordpress, Medium\\nand many more\\nStep-by-Step Guide\\nTo illustrate how quickstart connectors work, let’s build an example RAG-powered chatbot and connect the Cohere Chat endpoint to Google Drive.\\nAn overview of what we\\'ll build\\nHere are some additional references:\\nChat endpoint API reference page, containing a detailed description of the Chat endpoint’s input parameters and response objects\\nConnectors documentation\\nFirst, let’s install and import the cohere library, and then create a Cohere client using an API key.\\nSetup\\nFirst, let’s install and import the cohere library, and then create a Cohere client using an API key.\\nimport cohere from cohere import ChatConnector import uuid from typing import List co = cohere.Client(\"COHERE_API_KEY\") \\nBuild and Deploy the Connector\\nWe’ll need to first build a Google Drive connector and deploy it as a web-based REST API.\\nEnable Google Drive Access\\nFirst, we need to enable access to Google Drive. The Google Drive connector supports two types of authentication: Service Account and OAuth. In this example, we’ll use the Service Account option.\\nThe steps are as follows:\\nCreate a project in Google Cloud Console.\\nCreate a service account and activate the Google Drive API in the Google Cloud Console.\\nCreate a service account key and download the credentials file as JSON. We’ll need to use this later. The credentials file should look like this:'),\n",
       " Document(metadata={'title': 'How to Build RAG Applications With Quickstart Connectors', 'source': 'https://cohere.com/llmu/rag-quickstart-connectors'}, page_content='Create a service account key and download the credentials file as JSON. We’ll need to use this later. The credentials file should look like this:\\n{ \"type\": \"service_account\", \"project_id\": \"{project id}\", \"private_key_id\": \"{private_key_id}\", \"private_key\": \"{private_key}\", \"client_email\": \"{client_email}\", \"client_id\": \"{client_id}\", \"auth_uri\": \"{auth_uri}\", \"token_uri\": \"{token_uri}\", \"auth_provider_x509_cert_url\": \"{auth_provider_x509_cert_url}\", \"client_x509_cert_url\": \"{client_x509_cert_url}\", \"universe_domain\": \"{universe_domain}\" } \\nOnce you are done with these steps, go to any Google Drive account and share the folder(s) you want to search with the service account email address. You can find the email address in Google Cloud Console > Service Accounts > Your project. As an example, we\\'ll use the contents from LLM University: Prompt Engineering which explains the techniques of prompting LLMs. It consists of five web pages, which we’ll download and store as five documents in Google Drive.\\nConfigure the Connector\\nNext, we clone the quickstart connector repository and choose the Google Drive connector from the list of options. Here’s a quick look at what’s inside the Google Drive connector:\\napp: This module handles the API endpoints and authentication. It defines the search function, which is the endpoint for the search API. It also handles the extraction of the access token from the request headers.\\nasync_download: This module is responsible for downloading files from Google Drive asynchronously.\\nprovider: This module interacts with the Google Drive API. It defines the search function, which performs a search on Google Drive using the provided query and access token. It also processes the search results, extracting the necessary information and downloading the file contents using the async_download module.\\napi.yaml file: This OpenAPI specification describes the API endpoints, request bodies, responses, and security schemes.'),\n",
       " Document(metadata={'title': 'How to Build RAG Applications With Quickstart Connectors', 'source': 'https://cohere.com/llmu/rag-quickstart-connectors'}, page_content='api.yaml file: This OpenAPI specification describes the API endpoints, request bodies, responses, and security schemes.\\nWe then create an .env file to define the environment variables (use the .env-template file provided).\\nFinally, we define the GDRIVE_SERVICE_ACCOUNT_INFO. For this, we convert the service account key we created earlier to a string through json.dumps(credentials) and use this string as the environment variable.\\nAdd Authentication\\nOur next step involves adding another environment variable, GDRIVE_CONNECTOR_API_KEY. This is required for connector authentication purposes.\\nImplementing authentication and authorization methods to protect your web services is crucial. It ensures that only legitimate users can access your service, protects data from being accessed or modified by unauthorized individuals, and prevents service abuse.\\nCohere supports a few authentication/authorization options: service-to-service authentication, OAuth 2.0, and pass-through. Find more details about how to implement these in the documentation.\\nIn service-to-service authentication, Cohere sends requests to our connector with this connector API key. You can generate an API key using libraries like secrets.\\nimport secrets secrets.token_urlsafe(32) \\nFinally, when registering the API as a connector, we include this connector API key. As a result, Cohere will pass the key in every search request to the connector. We’ll see how to register a connector shortly.\\nDeploy the Connector\\nNext, we deploy the connector API as a web service that can be accessed via the Internet. There are many options available, including cloud service providers, platform-as-a-service (PaaS) providers, and more. We won’t cover deploying the API in this article, but if you are looking for an example, refer to this Render template for deploying a Poetry/Flask application or this guide on Docker deployment.\\nTo make the API compatible as a connector, ensure the following:'),\n",
       " Document(metadata={'title': 'How to Build RAG Applications With Quickstart Connectors', 'source': 'https://cohere.com/llmu/rag-quickstart-connectors'}, page_content='To make the API compatible as a connector, ensure the following:\\nExpose an endpoint that will return the retrieval results, which in our case is the search endpoint\\nThis endpoint must return a list of dictionaries called results\\nEach dictionary item can contain any number of fields, with the minimum being the text field\\nWith the quickstart connectors, these have already been implemented.\\nTest the Connector\\nNow, we can test if the API is working by making a curl request to the search endpoint.\\ncurl --request POST \\\\ --url https://demo-conn-gdrive.myurl.com/search \\\\ --header \\'Authorization: Bearer YOUR_CONNECTOR_API_KEY\\' \\\\ --header \\'Content-Type: application/json\\' \\\\ --data \\'{\"query\": \"word embeddings\"}\\' \\nA successful response will return data containing some metadata followed by the fields that a connector is configured to provide (in this case, text, title, and url).\\n{ \"results\": [ { \"editedBy\": \"Meor Amer\", \"id\": \"10x9mJOnEr62hg1IFxgAtD1aIFS4NXJ2l5Lt-UhJXLVg\", \"mimeType\": \"application/vnd.google-apps.document\", \"modifiedTime\": \"2023-12-01T07:49:27.196Z\", \"text\": \"\\\\ufeffEvaluating Outputs\\\\r\\\\nIn this chapter, you\\'ll learn about the different techniques for evaluating LLM outputs.Introduction\\\\r\\\\nLarge language models (LLMs) offer exciting new ways to build applications that leverage natural language as the interface ...\", \"title\": \"Evaluating Outputs\", \"url\": \"https://docs.google.com/document/d/10x9mJOnEr62hg1IFxgAtD1aIFS4NXJ2l5Lt-UhJXLVg/edit?usp=drivesdk\" }, { \"editedBy\": \"Meor Amer\", \"id\": \"1wngAfCJY1IgD6H__4AkQXFfymKUpSeJL13TItbigdyA\", \"mimeType\": \"application/vnd.google-apps.document\", \"modifiedTime\": \"2023-12-01T07:50:02.204Z\", \"text\": \"\\\\ufeffValidating Outputs\\\\r\\\\nIn this chapter ...\", \"title\": \"Validating Outputs\", \"url\": \"https://docs.google.com/document/d/1wngAfCJY1IgD6H__4AkQXFfymKUpSeJL13TItbigdyA/edit?usp=drivesdk\" }, ... } ] } \\nRegister the Connector'),\n",
       " Document(metadata={'title': 'How to Build RAG Applications With Quickstart Connectors', 'source': 'https://cohere.com/llmu/rag-quickstart-connectors'}, page_content='Register the Connector\\nNext, we register the Google Drive connector as a connector with Cohere. We do this by sending a POST request to the Cohere API. We’ll need to provide the following information:\\nThe Cohere API key\\nThe name we want to call this connector\\nThe URL of the connector API’s search endpoint\\nThe connector API key\\nHere’s an example request:\\ncurl --request POST \\\\\\\\\\\\\\\\ --url \\'<https://api.cohere.ai/v1/connectors>\\' \\\\\\\\\\\\\\\\ --header \\'Authorization: Bearer {Cohere API key}\\' \\\\\\\\\\\\\\\\ --header \\'Content-Type: application/json\\' \\\\\\\\\\\\\\\\ --data \\'{ \"name\":\"demo-conn-gdrive\", \"url\":\"<https://demo-conn-gdrive.example.com/search>\", \"service_auth\": { \"type\": \"bearer\", \"token\": \"{Connector API Key}\" } }\\' \\nAnd with that, we have successfully registered the API as a connector.\\nUse the Connector\\nOur new connector is now ready to use. To create a chatbot, we can reuse the same exact code from the previous chapter.'),\n",
       " Document(metadata={'title': 'How to Build RAG Applications With Quickstart Connectors', 'source': 'https://cohere.com/llmu/rag-quickstart-connectors'}, page_content='Use the Connector\\nOur new connector is now ready to use. To create a chatbot, we can reuse the same exact code from the previous chapter.\\nclass Chatbot: def __init__(self, connectors: List[str]): \"\"\" Initializes an instance of the Chatbot class. \"\"\" self.conversation_id = str(uuid.uuid4()) self.connectors = [ChatConnector(id=connector) for connector in connectors] def run(self): \"\"\" Runs the chatbot application. \"\"\" while True: # Get the user message message = input(\"User: \") # Typing \"quit\" ends the conversation if message.lower() == \"quit\": print(\"Ending chat.\") break else: # If using Google Colab, remove this line to avoid printing the same thing twice print(f\"User: {message}\") # If using Google Colab, remove this line to avoid printing the same thing twice # Generate response response = co.chat_stream( message=message, model=\"command-r-plus\", conversation_id=self.conversation_id, connectors=self.connectors, ) # Print the chatbot response, citations, and documents print(\"\\\\nChatbot:\") citations = [] cited_documents = [] # Display response for event in response: if event.event_type == \"text-generation\": print(event.text, end=\"\") elif event.event_type == \"citation-generation\": citations.extend(event.citations) elif event.event_type == \"stream-end\": cited_documents = event.response.documents # Display citations and source documents if citations: print(\"\\\\n\\\\nCITATIONS:\") for citation in citations: print(citation) print(\"\\\\nDOCUMENTS:\") for document in cited_documents: print({\\'id\\': document[\\'id\\'], \\'text\\': document[\\'text\\'][:50] + \\'...\\'}) print(f\"\\\\n{\\'-\\'*100}\\\\n\") \\nAnd when running the chatbot, we define the connector we have created.\\n# Define the connector connectors = [\"demo-conn-gdrive-6bfrp6\"] # Create an instance of the Chatbot class chatbot = Chatbot(connectors) # Run the chatbot chatbot.run() \\nHere’s an example conversation:'),\n",
       " Document(metadata={'title': 'How to Build RAG Applications With Quickstart Connectors', 'source': 'https://cohere.com/llmu/rag-quickstart-connectors'}, page_content='User: What is prompt engineering Chatbot: Prompt engineering is a process of constructing prompts for a large language model (LLM) to get a desired response. It involves techniques such as adding specific details, context, output format requirements, and examples to a prompt to guide the LLM\\'s response. Prompt engineering also involves chaining multiple prompts together to accomplish a complex task and validating and evaluating the outputs of LLMs to ensure the quality and accuracy of the responses. CITATIONS: start=35 end=55 text=\\'constructing prompts\\' document_ids=[\\'demo-conn-gdrive-6bfrp6_0\\', \\'demo-conn-gdrive-6bfrp6_1\\', \\'demo-conn-gdrive-6bfrp6_2\\', \\'demo-conn-gdrive-6bfrp6_3\\', \\'demo-conn-gdrive-6bfrp6_4\\'] start=62 end=88 text=\\'large language model (LLM)\\' document_ids=[\\'demo-conn-gdrive-6bfrp6_0\\', \\'demo-conn-gdrive-6bfrp6_3\\'] start=98 end=115 text=\\'desired response.\\' document_ids=[\\'demo-conn-gdrive-6bfrp6_0\\', \\'demo-conn-gdrive-6bfrp6_3\\'] start=154 end=170 text=\\'specific details\\' document_ids=[\\'demo-conn-gdrive-6bfrp6_0\\'] start=172 end=179 text=\\'context\\' document_ids=[\\'demo-conn-gdrive-6bfrp6_0\\'] start=181 end=207 text=\\'output format requirements\\' document_ids=[\\'demo-conn-gdrive-6bfrp6_0\\'] start=213 end=221 text=\\'examples\\' document_ids=[\\'demo-conn-gdrive-6bfrp6_0\\'] start=237 end=262 text=\"guide the LLM\\'s response.\" document_ids=[\\'demo-conn-gdrive-6bfrp6_0\\'] start=296 end=330 text=\\'chaining multiple prompts together\\' document_ids=[\\'demo-conn-gdrive-6bfrp6_1\\'] start=364 end=374 text=\\'validating\\' document_ids=[\\'demo-conn-gdrive-6bfrp6_3\\'] start=379 end=389 text=\\'evaluating\\' document_ids=[\\'demo-conn-gdrive-6bfrp6_4\\'] start=424 end=431 text=\\'quality\\' document_ids=[\\'demo-conn-gdrive-6bfrp6_4\\'] start=436 end=444 text=\\'accuracy\\' document_ids=[\\'demo-conn-gdrive-6bfrp6_3\\'] DOCUMENTS: {\\'id\\': \\'demo-conn-gdrive-6bfrp6_0\\', \\'text\\': \"\\\\ufeffConstructing Prompts\\\\r\\\\nIn this chapter, you\\'ll lea...\"} {\\'id\\': \\'demo-conn-gdrive-6bfrp6_1\\', \\'text\\': \"\\\\ufeffChaining Prompts\\\\r\\\\nIn this'),\n",
       " Document(metadata={'title': 'How to Build RAG Applications With Quickstart Connectors', 'source': 'https://cohere.com/llmu/rag-quickstart-connectors'}, page_content='DOCUMENTS: {\\'id\\': \\'demo-conn-gdrive-6bfrp6_0\\', \\'text\\': \"\\\\ufeffConstructing Prompts\\\\r\\\\nIn this chapter, you\\'ll lea...\"} {\\'id\\': \\'demo-conn-gdrive-6bfrp6_1\\', \\'text\\': \"\\\\ufeffChaining Prompts\\\\r\\\\nIn this chapter, you\\'ll learn a...\"} {\\'id\\': \\'demo-conn-gdrive-6bfrp6_2\\', \\'text\\': \"\\\\ufeffUse Case Patterns\\\\r\\\\nIn this chapter, you\\'ll learn ...\"} {\\'id\\': \\'demo-conn-gdrive-6bfrp6_3\\', \\'text\\': \"\\\\ufeffValidating Outputs\\\\r\\\\nIn this chapter, you\\'ll learn...\"} {\\'id\\': \\'demo-conn-gdrive-6bfrp6_4\\', \\'text\\': \"\\\\ufeffEvaluating Outputs\\\\r\\\\nIn this chapter, you\\'ll learn...\"} ---------------------------------------------------------------------------------------------------- Ending chat.'),\n",
       " Document(metadata={'title': 'How to Build RAG Applications With Quickstart Connectors', 'source': 'https://cohere.com/llmu/rag-quickstart-connectors'}, page_content=\"Conclusion\\nIn this chapter, you learned how to build your own connector for Google Drive, one of 80+ pre-built quickstart connectors available.\\nContinue to the next chapter to learn how to build RAG applications over multiple datastores and long documents.\\nAbout Cohere’s LLM University\\nOur comprehensive curriculum aims to equip you with the skills to develop your own AI applications. We cater to learners from all backgrounds, covering everything from the basics to the most advanced topics in large language models (LLMs). Plus, you'll have the opportunity to work on hands-on exercises, allowing you to build and deploy your very own solutions. Take a course today.\\nThis LLMU module consists of the following chapters:\\nIntroduction to RAG\\nRAG with Chat, Embed, and Rerank\\nRAG with Connectors\\nRAG with Quickstart Connectors (this chapter)\\nRAG over Large-Scale Data\"),\n",
       " Document(metadata={'title': 'How to Build RAG Applications Over Large-Scale Data', 'source': 'https://cohere.com/llmu/rag-large-scale-data'}, page_content='How to Build RAG Applications Over Large-Scale DataQualified\\nIn this chapter, you’ll learn how to build RAG applications over multiple datastores and long documents.\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook. Note: To run the notebook, you must first deploy your own Google Drive connector as a web-based REST API (we covered the steps in the previous chapter).\\nContents\\nStep-by-Step Guide\\nSetup\\nUsing Multiple Connectors\\nHandling Long and Large Volume of Documents \\nChunking\\nReranking\\nInterleaving\\nPrompt Building\\nConclusion\\nIn the previous chapter, you learned how to build your own Google Drive connector, which is one of 80+ pre-built quickstart connectors available.\\nIn this chapter, you’ll learn how to use connectors at scale, such as connecting to multiple datastores, working with large volumes of documents, and handling long documents. Enterprises need a RAG system that can efficiently handle vast amounts of data from diverse sources, and in this chapter, you’ll learn about how this can be automated with the Chat endpoint.\\nIn an enterprise setting, data is distributed across multiple platforms and datastores. The real value of using connectors comes from being able to use multiple connectors at the same time. This way, we are maximizing the RAG system’s potential as an intelligent knowledge assistant, giving it access to various data sources, so it can synthesize the information from all these data sources.\\nStep-by-Step Guide\\nLet’s now look at an example of using the two connectors we used in the previous two chapters: Google Drive and web search.\\nAn overview of what we\\'ll build\\nSetup\\nFirst, let’s install and import the cohere and other necessary libraries, and then create a Cohere client using an API key.\\nimport cohere from cohere import ChatConnector import uuid from typing import List, Dict co = cohere.Client(\"COHERE_API_KEY\") \\nUsing Multiple Connectors'),\n",
       " Document(metadata={'title': 'How to Build RAG Applications Over Large-Scale Data', 'source': 'https://cohere.com/llmu/rag-large-scale-data'}, page_content='import cohere from cohere import ChatConnector import uuid from typing import List, Dict co = cohere.Client(\"COHERE_API_KEY\") \\nUsing Multiple Connectors\\nIn the previous two chapters, we only examined examples where one connector was defined at a time. However, the Chat endpoint can accept multiple connectors and retrieve information from all the defined connectors.\\nTo create a chatbot, we can reuse the same exact code we used in the previous chapter.\\nclass Chatbot: def __init__(self, connectors: List[str]): \"\"\" Initializes an instance of the Chatbot class. \"\"\" self.conversation_id = str(uuid.uuid4()) self.connectors = [ChatConnector(id=connector) for connector in connectors] def run(self): \"\"\" Runs the chatbot application. \"\"\" while True: # Get the user message message = input(\"User: \") # Typing \"quit\" ends the conversation if message.lower() == \"quit\": print(\"Ending chat.\") break else: # If using Google Colab, remove this line to avoid printing the same thing twice print(f\"User: {message}\") # If using Google Colab, remove this line to avoid printing the same thing twice # Generate response response = co.chat_stream( message=message, model=\"command-r-plus\", conversation_id=self.conversation_id, connectors=self.connectors, ) # Print the chatbot response, citations, and documents print(\"\\\\nChatbot:\") citations = [] cited_documents = [] # Display response for event in response: if event.event_type == \"text-generation\": print(event.text, end=\"\") elif event.event_type == \"citation-generation\": citations.extend(event.citations) elif event.event_type == \"stream-end\": cited_documents = event.response.documents # Display citations and source documents if citations: print(\"\\\\n\\\\nCITATIONS:\") for citation in citations: print(citation) print(\"\\\\nDOCUMENTS:\") for document in cited_documents: print({\\'id\\': document[\\'id\\'], \\'text\\': document.get(\\'text\\', document.get(\\'snippet\\', \\'\\'))[:50] + \\'...\\'}) # \"text\" for Gdrive, \"snippet\" for web search print(f\"\\\\n{\\'-\\'*100}\\\\n\")'),\n",
       " Document(metadata={'title': 'How to Build RAG Applications Over Large-Scale Data', 'source': 'https://cohere.com/llmu/rag-large-scale-data'}, page_content='And when running the chatbot, we define the connectors we want the endpoint to retrieve information from.\\nThe Chatbot class has already been prepared to accept multiple connectors.\\nclass Chatbot: def __init__(self, connectors: List[str]): ... self.connectors = [{\"id\": c} for c in connectors] ... \\nAnd what’s actually sent as the connectors parameter in the endpoint call will be the following. \\nresponse = co.chat( message=message, connectors=[ChatConnector(id=\"demo-conn-gdrive-6bfrp6\"), ChatConnector(id=\"web-search\")] ... ) \\nWhen creating the Chatbot instance, we define the connector IDs as a list of strings.\\n# Define connectors connectors = [\"demo-conn-gdrive-6bfrp6\", \"web-search\"] # Create an instance of the Chatbot class chatbot = Chatbot(connectors) # Run the chatbot chatbot.run() \\nHere’s an example conversation. The connector uses information retrieved from both sources, as can be seen in the list of source documents.'),\n",
       " Document(metadata={'title': 'How to Build RAG Applications Over Large-Scale Data', 'source': 'https://cohere.com/llmu/rag-large-scale-data'}, page_content='User: What is chain of thought prompting Chatbot: Chain-of-Thought (CoT) prompting is a technique used to guide Large Language Models (LLMs) to follow a reasoning process when dealing with complex problems. This is done by providing the model with a few examples where the step-by-step reasoning is clearly laid out. The model is then expected to follow that \"chain of thought\" reasoning to get to the correct answer. CoT prompting is a prompt engineering technique that aims to improve language models\\' performance on tasks requiring logic, calculation and decision-making by structuring the input prompt in a way that mimics human reasoning. To construct a chain-of-thought prompt, a user typically appends an instruction such as \"Describe your reasoning in steps\" or \"Let\\'s think step by step\" to the end of their query to a large language model (LLM). This encourages the model to generate intermediate steps before providing a final answer. CITATIONS: start=17 end=22 text=\\'(CoT)\\' document_ids=[\\'web-search_0\\', \\'web-search_1\\', \\'web-search_2\\', \\'web-search_3\\', \\'web-search_5\\', \\'web-search_7\\', \\'web-search_8\\', \\'demo-conn-gdrive-6bfrp6_11\\', \\'demo-conn-gdrive-6bfrp6_12\\'] start=56 end=61 text=\\'guide\\' document_ids=[\\'web-search_3\\', \\'web-search_4\\', \\'web-search_7\\'] start=62 end=83 text=\\'Large Language Models\\' document_ids=[\\'web-search_0\\', \\'web-search_2\\', \\'web-search_3\\', \\'web-search_4\\', \\'web-search_5\\', \\'web-search_7\\', \\'demo-conn-gdrive-6bfrp6_11\\'] start=84 end=90 text=\\'(LLMs)\\' document_ids=[\\'web-search_0\\', \\'web-search_2\\', \\'web-search_3\\', \\'web-search_4\\', \\'web-search_5\\', \\'web-search_7\\', \\'demo-conn-gdrive-6bfrp6_11\\'] start=94 end=120 text=\\'follow a reasoning process\\' document_ids=[\\'web-search_1\\', \\'web-search_3\\', \\'web-search_4\\', \\'web-search_7\\'] start=139 end=156 text=\\'complex problems.\\' document_ids=[\\'web-search_3\\', \\'web-search_5\\', \\'web-search_7\\'] start=200 end=212 text=\\'few examples\\' document_ids=[\\'web-search_1\\', \\'web-search_3\\', \\'web-search_5\\', \\'web-search_7\\', \\'demo-conn-gdrive-6bfrp6_11\\']'),\n",
       " Document(metadata={'title': 'How to Build RAG Applications Over Large-Scale Data', 'source': 'https://cohere.com/llmu/rag-large-scale-data'}, page_content='\\'web-search_5\\', \\'web-search_7\\'] start=200 end=212 text=\\'few examples\\' document_ids=[\\'web-search_1\\', \\'web-search_3\\', \\'web-search_5\\', \\'web-search_7\\', \\'demo-conn-gdrive-6bfrp6_11\\'] start=223 end=266 text=\\'step-by-step reasoning is clearly laid out.\\' document_ids=[\\'web-search_1\\', \\'web-search_3\\', \\'web-search_4\\', \\'web-search_5\\', \\'web-search_7\\', \\'demo-conn-gdrive-6bfrp6_11\\'] start=297 end=337 text=\\'follow that \"chain of thought\" reasoning\\' document_ids=[\\'web-search_3\\', \\'web-search_5\\'] start=341 end=367 text=\\'get to the correct answer.\\' document_ids=[\\'web-search_3\\', \\'web-search_4\\', \\'web-search_5\\'] start=388 end=416 text=\\'prompt engineering technique\\' document_ids=[\\'web-search_4\\', \\'web-search_5\\'] start=430 end=466 text=\"improve language models\\' performance\" document_ids=[\\'web-search_4\\'] start=486 end=524 text=\\'logic, calculation and decision-making\\' document_ids=[\\'web-search_4\\'] start=528 end=556 text=\\'structuring the input prompt\\' document_ids=[\\'web-search_4\\'] start=571 end=594 text=\\'mimics human reasoning.\\' document_ids=[\\'web-search_4\\'] start=684 end=718 text=\\'\"Describe your reasoning in steps\"\\' document_ids=[\\'web-search_4\\', \\'demo-conn-gdrive-6bfrp6_11\\'] start=722 end=748 text=\\'\"Let\\\\\\'s think step by step\"\\' document_ids=[\\'web-search_1\\', \\'web-search_3\\', \\'web-search_5\\', \\'web-search_8\\', \\'demo-conn-gdrive-6bfrp6_11\\', \\'demo-conn-gdrive-6bfrp6_12\\'] start=846 end=864 text=\\'intermediate steps\\' document_ids=[\\'web-search_0\\', \\'web-search_1\\', \\'web-search_2\\', \\'web-search_3\\', \\'web-search_4\\', \\'web-search_5\\', \\'web-search_7\\', \\'demo-conn-gdrive-6bfrp6_11\\'] DOCUMENTS: {\\'id\\': \\'web-search_0\\', \\'text\\': \\'Skip to main content\\\\n\\\\nWe gratefully acknowledge su...\\'} {\\'id\\': \\'web-search_1\\', \\'text\\': \\'General Tips for Designing Prompts\\\\n\\\\nChain-of-Thoug...\\'} {\\'id\\': \\'web-search_2\\', \\'text\\': \\'BlogDocsCommunityHackAPrompt Playground\\\\n\\\\nLanguage ...\\'} {\\'id\\': \\'web-search_3\\', \\'text\\': \\'We now support using Microsoft Azure hosted OpenAI...\\'} {\\'id\\': \\'web-search_5\\', \\'text\\': \\'Comprehensive Guide to Chain-of-Thought'),\n",
       " Document(metadata={'title': 'How to Build RAG Applications Over Large-Scale Data', 'source': 'https://cohere.com/llmu/rag-large-scale-data'}, page_content='Playground\\\\n\\\\nLanguage ...\\'} {\\'id\\': \\'web-search_3\\', \\'text\\': \\'We now support using Microsoft Azure hosted OpenAI...\\'} {\\'id\\': \\'web-search_5\\', \\'text\\': \\'Comprehensive Guide to Chain-of-Thought Prompting\\\\n...\\'} {\\'id\\': \\'web-search_7\\', \\'text\\': \\'ResourcesArticleChain-of-Thought Prompting: Helpin...\\'} {\\'id\\': \\'web-search_8\\', \\'text\\': \\'Skip to main content\\\\n\\\\nScan this QR code to downloa...\\'} {\\'id\\': \\'demo-conn-gdrive-6bfrp6_11\\', \\'text\\': \"\\\\ufeffConstructing Prompts\\\\r\\\\nIn this chapter, you\\'ll lea...\"} {\\'id\\': \\'demo-conn-gdrive-6bfrp6_12\\', \\'text\\': \"\\\\ufeffUse Case Patterns\\\\r\\\\nIn this chapter, you\\'ll learn ...\"} {\\'id\\': \\'web-search_4\\', \\'text\\': \\'Tech Accelerator What is generative AI? Everything...\\'} ---------------------------------------------------------------------------------------------------- Ending chat.'),\n",
       " Document(metadata={'title': 'How to Build RAG Applications Over Large-Scale Data', 'source': 'https://cohere.com/llmu/rag-large-scale-data'}, page_content='Handling Long and Large Volume of Documents\\nWith all these documents coming from various connectors, you may be asking a couple of questions:\\nHow to handle long documents? Connecting to multiple connectors means having to deal with various APIs, each with its own way of providing documents. Some may return a complete document with tens or hundreds of pages. There are a couple of problems with this. First, stuffing a long document into an LLM prompt means its context limit will be reached, resulting in an error. Second, even if the context limit is not reached, the LLM response will likely not be very good because it is getting a lot of irrelevant information from a long document instead of specific chunks from the document that are the most relevant.\\nHow to handle multiple documents from multiple connectors and queries? For a specific connector, the retrieval and reranking implementation is within the developer’s control. But with multiple connectors, that is not possible because these documents are aggregated at the Chat endpoint. As the number of connectors increases, this becomes a bigger problem because we don’t have control over the relevancy of the documents sent to the LLM prompt. And then there is the same problem of possible context length limits being reached. Furthermore, if more than one query is generated, the number of documents retrieved will multiply with the same number.\\nThe Chat endpoint solves these problems with its automated chunking and reranking process. Let’s see how it’s done.\\nNote that for this to happen, the prompt_truncation parameter should be set as AUTO (default) and not OFF.\\nChunking\\nThe command-r family of models supports a large context length (128k tokens), offering ample room for retrieved documents. However, in the scenario where this context length is exceeded, the automated chunking feature will be activated.'),\n",
       " Document(metadata={'title': 'How to Build RAG Applications Over Large-Scale Data', 'source': 'https://cohere.com/llmu/rag-large-scale-data'}, page_content='The first step is to split every document sent by the connectors into smaller chunks. Each chunk is between 100 and 400 words, and sentences are kept intact where possible.\\nChunking the retrieved documents\\nReranking\\nThe Chat endpoint then uses the Rerank endpoint to take all the chunked documents from all connectors and rerank them based on contextual relevance to the query.\\nReranking the chunked documents\\nThis will be independent for each query and connector. For example, let’s say that a user asks the question, “What is AI and how can enterprises use it?” resulting in two queries generated by the endpoint: “What is AI?” and “How can enterprises use AI?” Also, let’s assume that there are two connectors: “web search” and “notion.”\\nThis means that there will be four lists of chunked documents (two queries for two connectors), each to be reranked separately.\\nThe reranking step takes the top 20 chunks from each list and drops the rest.\\nInterleaving\\nThe reranked documents from the different lists are then interleaved into one list.\\nInterleaving the reranked chunks\\nWith our example above, let’s say that these are the four lists of reranked documents:\\nWeb search results (“What is AI”): web_ai_1, web_ai_2, web_ai_3\\nNotion search results (“What is AI”): notion_ai_1, notion_ai_2, notion_ai_3\\nWeb search results (“How can enterprises use AI”): web_enterprise_1, web_enterprise_2, web_enterprise_3\\nNotion search results (“How can enterprises use AI”): notion_enterprise_1, notion_enterprise_2, notion_enterprise_3\\nThe documents will be interleaved in a list in this order:\\nDocuments: web_ai_1, notion_ai_1, web_enterprise_1, notion_enterprise_1, web_ai_2, notion_ai_2, web_enterprise_2, notion_enterprise_2, web_ai_3, notion_ai_3, web_enterprise_3, notion_enterprise_3\\nThis list is what gets sent to the LLM prompt.\\nPrompt Building'),\n",
       " Document(metadata={'title': 'How to Build RAG Applications Over Large-Scale Data', 'source': 'https://cohere.com/llmu/rag-large-scale-data'}, page_content=\"This list is what gets sent to the LLM prompt.\\nPrompt Building\\nBy setting the prompt_truncation parameter by setting it to AUTO, some elements from chat_history and documents will be dropped in an attempt to construct a prompt that fits within the model's context length limit.\\nDocuments and chat history will be iteratively added until the prompt is too long. This prompt will be passed to the Command model for response generation.\\nConclusion\\nIn this chapter, you learned how to use connectors at scale. The Chat endpoint allows you to define multiple connectors in an endpoint call, and will aggregate the retrieved documents from these connectors. You can also leverage the automated handling of long documents and large volumes of documents, where the endpoint takes care of chunking, reranking, and interleaving of documents, as well as prompt building.\\nAbout Cohere’s LLM University\\nOur comprehensive curriculum aims to equip you with the skills to develop your own AI applications. We cater to learners from all backgrounds, covering everything from the basics to the most advanced topics in large language models (LLMs). Plus, you'll have the opportunity to work on hands-on exercises, allowing you to build and deploy your very own solutions. Take a course today.\\nThis LLMU module consists of the following chapters:\\nIntroduction to RAG\\nRAG with Chat, Embed, and Rerank\\nRAG with Connectors\\nRAG with Quickstart Connectors\\nRAG over Large-Scale Data (this chapter)\"),\n",
       " Document(metadata={'title': 'From RAG to Tool Use', 'source': 'https://cohere.com/llmu/from-rag-to-tool-use'}, page_content=\"From RAG to Tool UseQualified\\nContents\\nIntroduction\\nRAG\\nTool Use\\nDecision-making\\nAction\\nFunction Calling\\nCitations\\nConclusion\\nIntroduction\\nWhat does it take to turn an application that can answer questions like:\\n“How many employees does Microsoft have?”\\n…into one that can automate tasks like:\\n“Create a plot of the number of employees at the three tech companies with the highest market cap in the United States in 2024.”?\\nThe answer is to transition from a retrieval-augmented generation (RAG) to a tool use approach.\\nSuppose we are asking a human assistant to perform these two tasks. For the first task, simply entering the exact question into a web search engine would provide the answer.\\nAs for the second task, they would have to:\\nBreak down the task into smaller parts and perform them in a sequence (first, find the three companies, and only then find the employee number for each)\\nPerform an action multiple times (multiple web searches)\\nNot only perform information retrieval (web search) but also perform some kind of action (creating a plot)\\nThis planning, and then the execution of every task, can be performed with tool use.\\nTool use can automate the tasks required to fulfill a user's request \\nIn this module, we’ll learn how to leverage tool use by building:\\nA RAG assistant that can query structured data about sales and product\\nA calendar assistant that can check and schedule appointments\\nA data analyst that’s capable of answering questions like the above\\nRAG\\nTo understand what tool use is, it’s helpful to start by understanding RAG. We’ll only do a brief recap here as we covered RAG in detail in Module 7.\\nRAG gives LLMs the capability to ground their responses in external text data. This means we can build applications that are more robust against hallucination, can access recent events, and are personalized to an enterprise’s data and context.\"),\n",
       " Document(metadata={'title': 'From RAG to Tool Use', 'source': 'https://cohere.com/llmu/from-rag-to-tool-use'}, page_content=\"With RAG, an LLM retrieves unstructured text documents from external data sources to provide useful and accurate responses to user queries.\\nRAG utilizes text documents from external data sources\\nA common setup is to store these documents in a vector database as text embeddings. During query time, we use search techniques, such as semantic search, to retrieve the documents most relevant to a user's query.\\nThese documents are then augmented to the LLM prompt, acting as the context to inform and guide the LLM's response to the user.\\nThe document retrieval system can be set up for any source that contains textual, unstructured data. Examples would be a company's meeting notes stored on Google Drive, its internal wiki stored in Notion, its customer conversations on Intercom, PDF documents, and so on.\\nIn a RAG workflow, documents are used to guide an LLM's response\\nTool use takes the RAG concept to the next level. It extends the principles of RAG, where external systems are used to guide the response of an LLM, but by leveraging a much bigger set of tools than what’s possible with RAG — as we’ll see in this module.\\nWith the RAG example earlier, we are already using a type of tool — the document retriever. But tools can come in many forms beyond just that.\\nIf there is a way to communicate and send information to and from a service via an API, that service can become a tool for an LLM to utilize. It can be a web search engine, a SQL database, a weather forecast service, or a Python interpreter, to name a few.\\nSo, instead of retrieving information from unstructured documents (typical RAG), we can retrieve information from any source (tool use). Plus, we'll see later on that this tool use setup allows us to not only retrieve information from external tools, but also take actions through them.\\nIn a tool use workflow, results from tool calls are used to guide an LLM's response\\nDecision-Making\"),\n",
       " Document(metadata={'title': 'From RAG to Tool Use', 'source': 'https://cohere.com/llmu/from-rag-to-tool-use'}, page_content='In a tool use workflow, results from tool calls are used to guide an LLM\\'s response\\nDecision-Making\\nThe concept of tool use expands on LLMs\\' capabilities of acting as a reasoning and decision-making engine.\\nWith a standard RAG setup, an LLM needs to make one type of decision. Given a user query, it has to decide whether it needs external data to respond or can respond directly. With Cohere’s Chat endpoint, this is done using the search query generation feature.\\nFor example, \"How do I build a great company?\" doesn\\'t usually require retrieving a specific piece of data, but \"What was Apple\\'s revenue in 2023?\" does.\\nDecision-making in RAG\\nWith tool use, the model needs to make two types of decisions.\\nThe first is similar to RAG, like in the example above, which is a yes/no decision on whether a user query can be answered directly or requires some external context\\nThe second is if tools are required, which tools would those be (and how to call them)\\nIn a tool use setting, a model is often given several tools to pick from. For example, let\\'s say that those are a web search engine and a Python interpreter.\\nSo, if a user asks, \"What were Apple\\'s and Google\\'s revenue in 2023?\" Then, the model would determine that only the web search engine is needed.\\nBut if the question is \"What were Apple\\'s and Google\\'s revenue in 2023, and plot them on a bar chart?\" then the model would determine that both the web search engine and the Python interpreter are needed.\\nDecision-making in tool use\\nThis concept of routing a request to only the relevant tools can enhance the quality of a model’s response. In a naive RAG setting, the application would aggregate retrieved documents from all the different sources it is connected to. This may contribute to noise from less relevant documents.\\nAdditionally, it doesn’t take into consideration that, given a data source\\'s nature, it might be less or more relevant to a query compared to the other data sources.'),\n",
       " Document(metadata={'title': 'From RAG to Tool Use', 'source': 'https://cohere.com/llmu/from-rag-to-tool-use'}, page_content=\"Additionally, it doesn’t take into consideration that, given a data source's nature, it might be less or more relevant to a query compared to the other data sources.\\nFor example, suppose an application is connected to two data sources, one to access public data (the web) and the other to private data (a company’s internal Notion knowledge base). Given the question of “Who were the largest car manufacturers in 2023?”, the information is more likely to be available publicly, and it makes more sense to search just the web rather than both.\\nA naive RAG system does not offer this routing capability. Moving to a tool use paradigm solves this.\\nTool use enables routing a request to only the relevant tools\\nAction\\nA standard RAG setup is read-only. The RAG application can only retrieve documents from the source (or a read action), but it doesn't need to make any changes in the opposite direction (or a write action).\\nA standard RAG setup is read-only\\nTool use enables developers to build applications that can take actions. That is, doing both read and write operations on an external system.\\nFor example, if it is a calendar service, the tool use approach not only enables the application to look up a person’s availability (read) but also create new events on the calendar (write).\\nTool use enables developers to build applications that can take actions\\nThe ability to read, write, and call APIs means that an LLM is more connected to the external world, can take a broader range of actions, and is more flexible overall.\\nTool use enables LLMs to be more connected to the external world\\nFunction Calling\\nIf tool use is the what of LLM capability extension, function calling is the how.\"),\n",
       " Document(metadata={'title': 'From RAG to Tool Use', 'source': 'https://cohere.com/llmu/from-rag-to-tool-use'}, page_content='Tool use enables LLMs to be more connected to the external world\\nFunction Calling\\nIf tool use is the what of LLM capability extension, function calling is the how.\\nWhile tool use is a broader concept that encompasses the use of various external systems and tools to enhance LLM capabilities, function calling, on the other hand, is a specific implementation of tool use, focusing on the technical details of picking the tools to call and generating the payload to call the tool.\\nFunction calling is the process of tool selection and payload generation to perform specific tasks within a program. It is a standardized way for an LLM to interface with tools to send and receive information.\\nEvery tool can be represented as a function with a schema that informs the LLM about what the tool does, what parameters the tool accepts, what these parameters do, and the types of the parameters. These are used by the LLM to make tool use decisions. We’ll go deeper into function calling in the coming chapters.\\nFunction calling is a specific implementation of tool use\\nCitations\\nTool use is such a natural extension of RAG that many concepts we see in RAG can be ported over to the tool use scenario.\\nOne such way is generating citations. A big part of building a successful RAG application is implementing a citation mechanism to trace LLM responses back to the source documents. This allows an LLM’s responses to be easily verifiable, building trust with the user.\\nIn the same way, you need the same citation mechanism when tool results are used to augment an LLM’s response. The range of tools is now much wider, but the need to trace the response to its source remains.\\nThrough Cohere’s Command R family of models, you get citation generation as a built-in feature in RAG without having to do prompt engineering or fine-tuning. And the same goes for tool use!'),\n",
       " Document(metadata={'title': 'From RAG to Tool Use', 'source': 'https://cohere.com/llmu/from-rag-to-tool-use'}, page_content=\"Through Cohere’s Command R family of models, you get citation generation as a built-in feature in RAG without having to do prompt engineering or fine-tuning. And the same goes for tool use!\\nWhen using Command R/R+ to generate responses, it will generate fine-grained citations referencing the tool results being used to ground its response. Here, fine-grained means specific spans of the text generated based on results from one or more tools — more specific than just at the response or even sentence level.\\nCohere's Command R/R+ models produce fine-grained citations to their responses\\nConclusion\\nThis was a quick introduction to the concept of tool use, starting with using RAG as the starting point. In the coming chapters, we’ll look at hands-on examples and concrete use cases of how tool use is applied in actual applications.\\nIn Chapter 2, we’ll look at the building blocks of a tool use system and how they all work together in a workflow. We’ll also look at our first code example by building a RAG assistant that can query structured sales data.\"),\n",
       " Document(metadata={'title': 'Tool Use Anatomy', 'source': 'https://cohere.com/llmu/tool-use-anatomy'}, page_content='Tool Use AnatomyQualified\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nContents\\nIntroduction\\nSetting Up the Tools \\nStep 1: Create the Tool\\nStep 2: Define Tool Schema\\nStep 3: Create Custom Preamble (Optional)\\nRunning the tool workflow \\nStep 1: Get User Message\\nStep 2: Generate Tool Calls\\nStep 3: Get Tool Results\\nStep 4: Generate Response and Citations\\nStreaming the Response\\nTool Use Modes \\nSingle-Step\\nSingle-Step, Parallel\\nMulti-Step\\nMulti-Step, Parallel\\nIntroduction\\nIn this chapter, we’ll dissect the key components of a tool use system and what a tool use workflow looks like. And we’ll do that with a concrete code example.\\nTo build an intuition of how tool use works, let’s start with a simple example. We’ll look at a use case of a RAG assistant that can query the sales database of an e-commerce company.\\nIn summary, there are two parts to building a tool use application:\\nSetting up the tools\\nRunning the tool use workflow\\nFirst, let’s install the Cohere Python SDK and set up the Cohere client.\\n# pip install cohere import json import cohere co = cohere.Client(api_key=\"YOUR_COHERE_API_KEY\") # Get your API key: https://dashboard.cohere.com/api-keys \\nThe pre-requisite, or Step 0, before we can run a tool use workflow, is to set up the tools. We can break this further into two steps:\\nCreating the tool\\nDefining the tool schema\\nThe pre-requisite of running a tool use workflow is to set up the tools\\nLet’s create a function to query a sales database called daily_sales_report and represent it as a tool. For simplicity, it contains a mock database containing just three data entries and the logic to return the data given a user query.'),\n",
       " Document(metadata={'title': 'Tool Use Anatomy', 'source': 'https://cohere.com/llmu/tool-use-anatomy'}, page_content='In this example, we are defining a Python function as the tool. But more broadly, the tool can be any function or service that can receive and send data. It could be an email service, an SQL database, a vector database, a weather data service, a sports data service, a web search engine, or even another LLM, just to give a few examples.\\ndef daily_sales_report(day: str) -> dict: \"\"\" Function to retrieve the sales report for the given day \"\"\" # Mock database containing daily sales reports sales_database = { \\'2023-09-28\\': {\\'total_sales_amount\\': 5000,\\'total_units_sold\\': 100}, \\'2023-09-29\\': {\\'total_sales_amount\\': 10000,\\'total_units_sold\\': 250}, \\'2023-09-30\\': {\\'total_sales_amount\\': 8000,\\'total_units_sold\\': 200} } report = sales_database.get(day, {}) if report: return { \\'date\\': day, \\'summary\\': f\"Total Sales Amount: {report[\\'total_sales_amount\\']}, Total Units Sold: {report[\\'total_units_sold\\']}\" } else: return {\\'date\\': day, \\'summary\\': \\'No sales data available for this day.\\'} functions_map = { \"daily_sales_report\": daily_sales_report } \\nNext, we define the tool schema for the sales database tool. This schema is what will be passed to the Cohere API when running a tool use workflow. The schema must contain the following fields:\\nname: the name of the tool.\\ndescription: a description of what the tool is and what it is used for.\\nparameter_definitions: a list of parameters that the tool accepts. For each parameter, we need to define the following fields:\\ndescription (details about the parameter)\\ntype (the parameter’s data type, e.g., str, int, and so on)\\nrequired (either True or False) fields\\nThis schema informs the LLM about what the tool does, and the LLM decides whether to use a particular tool based on it. Therefore, the more descriptive and specific the schema, the more likely the LLM will make the right tool call decisions.'),\n",
       " Document(metadata={'title': 'Tool Use Anatomy', 'source': 'https://cohere.com/llmu/tool-use-anatomy'}, page_content='Let’s name our tool daily_sales_report and populate the schema. The function has only one parameter, day, which requires the caller to pass a date in the YYYY-MM-DD string format.\\ntools = [ { \"name\": \"daily_sales_report\", \"description\": \"Connects to a database to retrieve overall sales volumes and sales information for a given day.\", \"parameter_definitions\": { \"day\": { \"description\": \"Retrieves sales data for this day, formatted as YYYY-MM-DD.\", \"type\": \"str\", \"required\": True } } } ] \\nStep 3: Create Custom Preamble (Optional)\\nAn optional step is to add a custom preamble, which is the LLM’s system message.\\nThe recommended approach is to use two H2 Markdown headers: \"Task & Context\" and \"Style Guide\" in the exact order (read the complete guide).\\nIt’s a completely optional step, though it’s likely needed if we want to create a robust and reliable application. Also note that the preamble is not related to the tool setup that we covered earlier, rather it’s part of the instruction to the LLM.\\nLet’s create a custom preamble as follows:\\npreamble = \"\"\"## Task & Context You help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user\\'s needs as best you can, which will be wide-ranging. ## Style Guide Unless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling. \"\"\" \\nThe tool is now ready to use. We can think of a tool use system as consisting of four components:\\nThe user\\nThe application\\nThe LLM\\nThe tools\\nAt its most basic, these four components interact in a workflow through four steps:\\nStep 1: Get user message. The LLM gets the user message (via the application).\\nStep 2: Generate tool calls. The LLM decides which tools to call (if any) and generates the tool calls.'),\n",
       " Document(metadata={'title': 'Tool Use Anatomy', 'source': 'https://cohere.com/llmu/tool-use-anatomy'}, page_content='Step 1: Get user message. The LLM gets the user message (via the application).\\nStep 2: Generate tool calls. The LLM decides which tools to call (if any) and generates the tool calls.\\nStep 3: Get tool results. The application executes the tools, and the results are sent to the LLM.\\nStep 4: Generate response and citations. The LLM generates the response and citations back to the user.\\nAn overview of the four-step tool use workflow\\nLet’s walk through these four steps using a code example. In this first example, we’ll use the simplest possible scenario where:\\nThere is only one tool (the sales database)\\nTool calling happens only once (and only one tool is called)\\nThere is only one turn in the conversation (no conversation memory preserved)\\nStep 1: Get User Message\\nStep 1: Getting the user message\\nIn the first step, the application receives the user\\'s message. As an example, we’ll use “Can you provide a sales summary for 29th September 2023?”\\nmessage = \"Can you provide a sales summary for 29th September 2023?\" \\nStep 2: Generating the tool calls\\nDuring this step:\\nThe application calls the Chat endpoint to generate the list of tool calls. We add the following parameters:\\nmessage - the user message.\\nmodel - the model of choice. Here we use Command R+.\\npreamble - the custom preamble.\\ntools - the list of tools we defined earlier (only one in this example).\\nforce_single_step - we define it as True. This forces the endpoint to run tool calls only once and not over a sequence of runs (we’ll see how the opposite – multi-step – works in Chapter 4).\\nThe endpoint sends back a list of tool calls to be made (if it determines that tools are required).\\nHere’s the code for running this step.'),\n",
       " Document(metadata={'title': 'Tool Use Anatomy', 'source': 'https://cohere.com/llmu/tool-use-anatomy'}, page_content='The endpoint sends back a list of tool calls to be made (if it determines that tools are required).\\nHere’s the code for running this step.\\nmodel = \"command-r-plus\" # Initial response to the user message response = co.chat( message=message, model=model, preamble=preamble, tools=tools, force_single_step=True ) tool_calls = response.tool_calls print(\"Tool calls:\\\\n\") for i, t in enumerate(tool_calls): print(f\"#{i+1}\\\\nTool: {t.name}\\\\nParameters: {t.parameters}\\\\n\") \\nAnd here’s the list of tool calls (only one in our case). The model determines that to answer the user question, it needs to call the daily_sales_report tool with the parameter \\'day\\': \\'2023-09-29\\' – as we’d expect.\\nTool calls: #1 Tool: daily_sales_report Parameters: {\\'day\\': \\'2023-09-29\\'} \\nStep 3: Getting the tool results\\nDuring this step, the application performs the function calling. It calls the necessary tools based on the tool call payloads given by the endpoint. In our case, it calls the daily_sales_report function.\\nHere’s the code for running this step.\\ntool_results = [] for tc in tool_calls: tool_output = functions_map[tc.name](**tc.parameters) tool_call = {\"name\": tc.name, \"parameters\": tc.parameters} tool_results.append({\"call\": tool_call, \"outputs\": [tool_output]}) print(\"Tool results:\\\\n\") for i, t in enumerate(tool_results): print(f\"#{i+1}\\\\nTool call: {t[\\'call\\']}\\\\nOutputs: {t[\\'outputs\\']}\\\\n\") \\nAnd here are the tool results (again, only one in our case). The tool successfully returns the correct data on the sales amount and units sold.\\nTool results: #1 Tool call: {\\'name\\': \\'daily_sales_report\\', \\'parameters\\': {\\'day\\': \\'2023-09-29\\'}} Outputs: [{\\'date\\': \\'2023-09-29\\', \\'summary\\': \\'Total Sales Amount: 10000, Total Units Sold: 250\\'}] \\nStep 4: Generate Response and Citations\\nStep 4: Generating the response and citations\\nWe have now come to the fourth and final step: response generation. By this time, the tool call has already been executed, and the result has been returned to the LLM.'),\n",
       " Document(metadata={'title': 'Tool Use Anatomy', 'source': 'https://cohere.com/llmu/tool-use-anatomy'}, page_content='We have now come to the fourth and final step: response generation. By this time, the tool call has already been executed, and the result has been returned to the LLM.\\nDuring this step, the application calls the Chat endpoint to generate the response to the user. Compared to the call in Step 2, here are the parameter changes required:\\nmessage - we put a blank value \"\" as this is a call to generate the chatbot’s response rather than to get a new user message (which doesn’t change)\\ntool_results - here we introduce this parameter as a way to pass the tool results generated in the previous step\\nchat_history - we use this parameter to pass the chat_history value from Step 2’s response. It contains the context required to generate the response – the user message and the tool call(s) made – as shown below.\\n# response.chat_history [Message_User(message=\\'Can you provide a sales summary for 29th September 2023?\\', tool_calls=None, role=\\'USER\\'), Message_Chatbot(tool_calls=[ToolCall(name=\\'daily_sales_report\\', parameters={\\'day\\': \\'2023-09-29\\'})], role=\\'CHATBOT\\')] \\nHere’s the code for running this step:\\n# Generate response response = co.chat( message=\"\", model=model, preamble=preamble, tools=tools, tool_results=tool_results, chat_history=response.chat_history ) print(\"Final response:\") print(response.text) print(\"=\"*50) if response.citations: print(\"\\\\nCitations:\") for citation in response.citations: print(citation) print(\"\\\\nCited Documents:\") for document in response.documents: print(document) \\nAnd here’s the final response from the LLM, which accurately answers the user’s question (total sales amount: 10,000, and total number of units sold: 250).\\nIt also generates fine-grained citations, which are included with Command R+ out of the box. Here, we see the model generating two citations, one for each specific span in its response, where it uses the tool result to answer the question.'),\n",
       " Document(metadata={'title': 'Tool Use Anatomy', 'source': 'https://cohere.com/llmu/tool-use-anatomy'}, page_content=\"For example, the first citation shows that the total sales amount was 10,000 portion of the response comes from the daily_sales_report tool. It also provides the values of the start and end character numbers that can be used for front-end development purposes to render citations easily and elegantly.\\nFinal response: On 29 September 2023, we sold 250 units, totalling 10,000 in sales. ================================================== Citations: start=30 end=39 text='250 units' document_ids=['daily_sales_report:0:2:0'] start=51 end=66 text='10,000 in sales' document_ids=['daily_sales_report:0:2:0'] Cited Documents: {'date': '2023-09-29', 'id': 'daily_sales_report:0:2:0', 'summary': 'Total Sales Amount: 10000, Total Units Sold: 250', 'tool_name': 'daily_sales_report'} \\nThe screenshot below, taken from Cohere’s Chat UI, shows an example of displaying fine-grained citations on a user interface. In this example, a web search tool is used to find the capital of Ontario. The response includes highlighting Toronto as the span that cites the web search results.\\nAn example of displaying fine-grained citations on a user interface, taken from Cohere’s Chat UI\\nStreaming the Response\\nWe can stream the response from the Chat endpoint for each generated token instead of having to wait for the full response. To enable streaming, we need to change the endpoint call from co.chat to co.chat_stream.\\nThe code from Step 4 would then change to the following. The difference now is we are capturing the response events as the endpoint streams them.\"),\n",
       " Document(metadata={'title': 'Tool Use Anatomy', 'source': 'https://cohere.com/llmu/tool-use-anatomy'}, page_content='The code from Step 4 would then change to the following. The difference now is we are capturing the response events as the endpoint streams them.\\n# Generate response response = co.chat_stream( message=\"\", model=model, preamble=preamble, tools=tools, tool_results=tool_results, chat_history=response.chat_history ) print(\"Final response:\\\\n\") chatbot_response = \"\" for event in response: if event.event_type == \"text-generation\": print(event.text, end=\"\") chatbot_response += event.text if event.event_type == \"stream-end\": if event.response.citations: print(\"\\\\n\\\\nCitations:\") for citation in event.response.citations: print(citation) if event.response.documents: print(\"\\\\nCited Documents\") for document in event.response.documents: print(document) \\nA good tool use API must be able to handle different scenarios, from simple to complex LLM tool calls. This translates to being able to call tools in different modes: single-step, multi-step, and parallel.\\nThe good news is that Cohere’s API supports all these modes. We’ll walk through them in detail in the coming two chapters, but for now, let’s briefly look at what these different modes are.\\nCohere’s API supports different modes of tool use\\nSingle-Step\\nThis is the most straightforward mode, so let\\'s start here. An LLM calls a single tool, the tool returns the result, and the result gets appended to the prompt.\\nReferring to the four-step workflow above, this means that Steps 2 (generating the tool calls) and 3 (getting the tool results) happen exactly once.\\nThis is the scenario that we saw in the sales database example earlier.\\nTo illustrate this further, let\\'s assume we have two tools:\\nA web search tool: It takes a query and returns relevant search results from the web\\nA Python interpreter: It takes a block of Python code, runs it, and returns the output\\nSuppose a user asks, \"What was Apple\\'s revenue in 2023?\"\\nTo answer this, a single web search of \"Apple 2023 revenue\" will suffice.\\nSingle-step tool use\\nSingle-Step, Parallel'),\n",
       " Document(metadata={'title': 'Tool Use Anatomy', 'source': 'https://cohere.com/llmu/tool-use-anatomy'}, page_content='Suppose a user asks, \"What was Apple\\'s revenue in 2023?\"\\nTo answer this, a single web search of \"Apple 2023 revenue\" will suffice.\\nSingle-step tool use\\nSingle-Step, Parallel\\nThe LLM can also decide that more than one tool call is required, which means calling tools in parallel. This can be calling the same tool multiple times or different tools for any number of calls.\\nWe’ll see an example of this in Chapter 3.\\nSuppose the user now asks, \"What were Apple\\'s and Google\\'s revenue in 2023?\"\\nTo answer this, two web searches of \"Apple 2023 revenue\" and \"Google 2023 revenue\" are needed.\\nSingle-step, parallel tool use\\nMulti-Step\\nThis is needed when more than one tool is required, and the output of one tool is needed as the input to the other tool.\\nIn other words, tool-calling needs to happen in a sequence.\\nReferring to the four-step workflow above, this means that Steps 2 (generating the tool calls) and 3 (getting the tool results) can happen multiple times before moving on to Step 4 (generating the response).\\nWe’ll see examples of this in Chapters 4 and 5.\\nSuppose the user now asks, \"Plot Apple\\'s 2023 revenue on a bar chart\".\\nThis requires first a web search for Apple\\'s revenue, and then using the search result to generate the Python code for plotting.\\nMulti-step tool use\\nMulti-Step, Parallel\\nTool calling can run in parallel in a multi-step scenario as it can in the single-step case.\\nWe’ll see examples of this in Chapters 4 and 5.\\nLet\\'s say the user now asks, \"Plot Apple\\'s and Google\\'s 2023 revenue on a bar chart\".\\nThis requires two parallel web search calls — one each for Apple and Google — and then running the Python plotting code in the second step.\\nMulti-step, parallel tool use\\nConclusion\\nIn this chapter, we looked at the components of a tool use system, the tool use workflow, state management in tool use, and different modes of tool use.\\nWe also looked at a code example using a single-step scenario. But it was a basic example that doesn’t address some questions like:'),\n",
       " Document(metadata={'title': 'Tool Use Anatomy', 'source': 'https://cohere.com/llmu/tool-use-anatomy'}, page_content='We also looked at a code example using a single-step scenario. But it was a basic example that doesn’t address some questions like:\\nHow to set up a system with more than one tool?\\nDoes the assistant always need to call tools?\\nHow does the assistant work in a chat setting consisting of multiple turns?\\nIn Chapter 3, we’ll address these questions and improve on the code example in this chapter.'),\n",
       " Document(metadata={'title': 'Multi-Step Tool Use', 'source': 'https://cohere.com/llmu/multi-step-tool-use-2'}, page_content='Multi-Step Tool UseQualified\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nContents\\nIntroduction\\nReAct-Style Prompting\\nMulti-Step Tool Use Workflow \\nPlanning and Execution\\nSelf-Correction\\nSetting Up the Tools \\nStep 1: Create Tools\\nStep 2: Define Tool Schema\\nStep 3: Create Custom Preamble (Optional)\\nMulti-Step Tool Use\\nTool Planning\\nMulti-Step, Parallel Tool Use\\nState Management (Memory)\\nConclusion\\nIntroduction\\nIn Chapter 2, we briefly discussed that multi-step tool use happens when more than one tool is required and the output of one tool is needed as the input to another tool.\\nIn other words, tool-calling needs to happen in a sequence.\\nThis is especially true in complex workflows where tasks have to be broken down into a sequence of steps.\\nLet\\'s say a user asks, \"Plot Apple\\'s 2023 revenue on a bar chart\".\\nThis requires first a web search for Apple\\'s revenue and then generating the right Python code using the search results. In the same way as a single-step scenario, multi-step tool use can also happen in parallel.\\nMulti-step tool use serves tasks where tool-calling needs to happen in a sequence\\nIn this chapter, we’ll focus on multi-step tool use and formalize our understanding through code examples.\\nWe’ll build a calendar assistant that can check a user’s calendar and create new appointments based on available times. This use case requires tool calls to happen in sequence, i.e., multi-step. It is also an example of using tools for performing not just a read operation (retrieving information) but also a write operation (taking actions).\\nReAct-Style Prompting\\nA common approach for multi-step tool use implementation is using the ReAct-style of prompting.'),\n",
       " Document(metadata={'title': 'Multi-Step Tool Use', 'source': 'https://cohere.com/llmu/multi-step-tool-use-2'}, page_content='ReAct-Style Prompting\\nA common approach for multi-step tool use implementation is using the ReAct-style of prompting.\\nThe ReAct framework (Yao et al., 2022) is a technique for improving the accuracy and reliability of responses from LLMs. It combines \"reasoning\" and \"acting,\" inspired by the natural human ability to learn and make decisions by blending these two elements. It allows an LLM to create a plan to solve a task and then execute the plan step by step in an iterative process, where it reflects and adjusts its plan after each action it takes. By integrating both reasoning and acting within LLMs, ReAct guides them to produce verbal reasoning paths and specific actions for a given task.\\nThe framework follows a series of Thought-Action-Observation reasoning sequences. Given a user message, the LLM first generates a thought, which includes the plan of what it needs to do. Then, it generates an action, which is a call to an external tool. The result of this call is then passed to its observation step. It will repeat this sequence until a task has been fulfilled.\\nCohere’s multi-step tool workflow uses a style similar to the ReAct framework. The prompt is comprised of:\\nThe preamble, consisting of the system and custom (if defined) preambles\\nThe list of available tools, taken from the tool schema defined by the developer\\nThe user message\\nAnd finally, the instructions to run the ReAct-style workflow (to be described next)\\nPlanning and Execution\\nYou can view the complete prompt to the model by enabling return_prompt to True when calling the Chat endpoint. But for our purpose, let’s just focus on the portion with the instructions to run the ReAct-style workflow.'),\n",
       " Document(metadata={'title': 'Multi-Step Tool Use', 'source': 'https://cohere.com/llmu/multi-step-tool-use-2'}, page_content='... Carefully perform the following instructions, in order, starting each with a new line. Firstly, You may need to use complex and advanced reasoning to complete your task and answer the question. Think about how you can use the provided tools to answer the question and come up with a high level plan you will execute. Write \\'Plan:\\' followed by an initial high level plan of how you will solve the problem including the tools and steps required. Secondly, Carry out your plan by repeatedly using actions, reasoning over the results, and re-evaluating your plan. Perform Action, Observation, Reflection steps with the following format. Write \\'Action:\\' followed by a json formatted action containing the \"tool_name\" and \"parameters\" Next you will analyze the \\'Observation:\\', this is the result of the action. After that you should always think about what to do next. Write \\'Reflection:\\' followed by what you\\'ve figured out so far, any changes you need to make to your plan, and what you will do next including if you know the answer to the question. ... (this Action/Observation/Reflection can repeat N times) Thirdly, Decide which of the retrieved documents are relevant to the user\\'s last input by writing \\'Relevant Documents:\\' followed by comma-separated list of document numbers. If none are relevant, you should instead write \\'None\\'. Fourthly, Decide which of the retrieved documents contain facts that should be cited in a good answer to the user\\'s last input by writing \\'Cited Documents:\\' followed a comma-separated list of document numbers. If you dont want to cite any of them, you should instead write \\'None\\'. Fifthly, Write \\'Answer:\\' followed by a response to the user\\'s last input in high quality natural english. Use the retrieved documents to help you. Do not insert any citations or grounding markup. Finally, Write \\'Grounded answer:\\' followed by a response to the user\\'s last input in high quality natural english. ...'),\n",
       " Document(metadata={'title': 'Multi-Step Tool Use', 'source': 'https://cohere.com/llmu/multi-step-tool-use-2'}, page_content='The first instruction is to perform a “planning” step consisting of an initial high-level plan of how the model will solve the problem, including the tools and steps required.\\nThis is followed by actually carrying out the plan by repeatedly using actions (tool calls), with every step going through an Action-Observation-Reflection sequence.\\nWith the Cohere API, this sequence of reasoning is captured as an assistant’s tool plan before generating the actual tool calls, as we’ll see in the code example.\\nCohere’s multi-step tool use workflow uses a style similar to the ReAct framework\\nSelf-Correction\\nWhat’s not explicitly mentioned in the prompt but something the model can handle is self-correction.\\nSo far, we have assumed that every tool call will return a successful result. But that does not necessarily have to be the case. Calling external functions could instead return errors, such as missing information, unsuccessful attempts, and many others.\\nFor example, when calling a web search as a tool call, a timeout error might occur for some reason. That means that the tool result returned will not contain the required information to generate a response.\\nIn this kind of situation, the model will be able to reflect on this returned result and take a different course of action than what was originally planned. In this case, instead of immediately going to the next task it had planned, it might attempt to rerun the same step that returned the error.\\nAnd with that, let’s start building our calendar assistant.\\nLet’s first set up the Cohere client. We’ll also need to import the json module for use later.\\n# pip install cohere import json import cohere co = cohere.Client(api_key=\"YOUR_COHERE_API_KEY\") # Get your API key: https://dashboard.cohere.com/api-keys \\nLet’s now create two tools:'),\n",
       " Document(metadata={'title': 'Multi-Step Tool Use', 'source': 'https://cohere.com/llmu/multi-step-tool-use-2'}, page_content='# pip install cohere import json import cohere co = cohere.Client(api_key=\"YOUR_COHERE_API_KEY\") # Get your API key: https://dashboard.cohere.com/api-keys \\nLet’s now create two tools:\\nA function called list_calendar_events to list the existing calendar events based on a given date. For simplicity, we are not going to connect to an actual calendar. Instead, we’ll include a mock events record, which is a simple list of events in a day.\\nA function called create_calendar_event to create a new calendar event based on the provided date, time, and duration. Again, to keep things simple, we will not make actual changes to a database but instead will just return a simple success message for illustration purposes.\\nSetting up the tools\\ndef list_calendar_events(date: str): events = [{\"start\": \"8:00\", \"end\": \"8:59\"}, {\"start\": \"9:00\", \"end\": \"9:59\"}, {\"start\": \"11:00\", \"end\": \"11:59\"},{\"start\": \"12:00\", \"end\": \"12:59\"}] return { \"existing_events\": events } def create_calendar_event(date: str, time: str, duration: int): return { \"is_success\": True, \"message\": f\"Created a {duration} hour long event at {time} on {date}\" } functions_map = { \"list_calendar_events\": list_calendar_events, \"create_calendar_event\": create_calendar_event } \\nNext, we define the tool schema for the two tools.'),\n",
       " Document(metadata={'title': 'Multi-Step Tool Use', 'source': 'https://cohere.com/llmu/multi-step-tool-use-2'}, page_content='Next, we define the tool schema for the two tools.\\ntools = [ { \"name\": \"list_calendar_events\", \"description\": \"Returns a list of existing events for the specified date, including the start time and end time for each event.\", \"parameter_definitions\": { \"date\": { \"description\": \"the date to list events for, formatted as mm/dd/yy\", \"type\": \"str\", \"required\": True } } }, { \"name\": \"create_calendar_event\", \"description\": \"Creates a new calendar event of the specified duration at the specified time and date. A new event cannot be created on the same time as an existing event.\", \"parameter_definitions\": { \"date\": { \"description\": \"the date on which the event starts, formatted as mm/dd/yy\", \"type\": \"str\", \"required\": True }, \"time\": { \"description\": \"the time of the event, formatted using 24h military time formatting\", \"type\": \"str\", \"required\": True }, \"duration\": { \"description\": \"the number of hours the event lasts for\", \"type\": \"float\", \"required\": True } } } ] \\nStep 3: Create Custom Preamble (Optional)\\nLet’s also create a custom preamble. The important part here is specifying the date since we are not implementing any date-handling with the list_calendar_events tool. What we’re doing is we are nudging the model to specify this date in the create_calendar_event tool call, which requires a date field.\\npreamble=\"\"\"## Task & Context You are a calendar assistant who helps people schedule events on their calendar. You must make sure that a new event does not overlap with any existing event. Today is Thursday, May 23, 2024 \"\"\" \\nLet’s reuse the run_assistant function we created in the previous chapter. Here’s a recap of what it does.\\nGet the user message (Step 1)\\nCall the Chat endpoint for tool call generation (Step 2)\\nIf the response contains at least one tool call, execute the tool call(s) and get the tool results (Step 3)\\nRepeat Steps 2 and 3 until there are no more tool calls\\nGenerate the final response with citations (Step 4)'),\n",
       " Document(metadata={'title': 'Multi-Step Tool Use', 'source': 'https://cohere.com/llmu/multi-step-tool-use-2'}, page_content='Repeat Steps 2 and 3 until there are no more tool calls\\nGenerate the final response with citations (Step 4)\\nAn overview of the four-step tool use workflow\\nThe good news is that it is already compatible with multi-step tool use because we have already implemented Steps 2 and 3 to run in a loop to handle zero, one, or multiple steps of tool calls.\\nBut there is one change we need to make. Here, we remove the force_single_step argument as it is False by default. This tells the API to enable a multi-step workflow and use the right prompt template to run the workflow.\\nmodel = \"command-r-plus\" def run_assistant(message, chat_history=[]): # Step 1: Get user message print(f\"Question:\\\\n{message}\") print(\"=\"*50) # Step 2: Generate tool calls (if any) response = co.chat( message=message, model=model, preamble=preamble, tools=tools, chat_history=chat_history ) while response.tool_calls: tool_calls = response.tool_calls if response.text: print(\"Tool plan:\") print(response.text,\"\\\\n\") print(\"Tool calls:\") for call in tool_calls: print(f\"Tool name: {call.name} | Parameters: {call.parameters}\") print(\"=\"*50) # Step 3: Get tool results tool_results = [] for tc in tool_calls: tool_call = {\"name\": tc.name, \"parameters\": tc.parameters} tool_output = functions_map[tc.name](**tc.parameters) tool_results.append({\"call\": tool_call, \"outputs\": [tool_output]}) # Step 4: Generate response and citations response = co.chat( message=\"\", model=model, preamble=preamble, tools=tools, tool_results=tool_results, chat_history=response.chat_history ) # Update the chat history chat_history = response.chat_history # Print final response print(\"Final response:\") print(response.text) print(\"=\"*50) # Print citations (if any) if response.citations: print(\"Citations:\") for citation in response.citations: print(citation) print(\"\\\\nCited Documents:\") for document in response.documents: print(document) print(\"=\"*50) return chat_history'),\n",
       " Document(metadata={'title': 'Multi-Step Tool Use', 'source': 'https://cohere.com/llmu/multi-step-tool-use-2'}, page_content='Let’s now ask the assistant the first question, starting with a simple one about the number of meetings for the day. This requires just a single step of tool calling to check existing calendar events.\\nchat_history = run_assistant(\"How many meetings do I have today\") \\nThat’s exactly what the assistant does, and it gives the correct answer.\\nQuestion: How many meetings do I have today ================================================== Tool plan: I will use the \\'list_calendar_events\\' tool to find out how many meetings the user has today. Tool calls: Tool name: list_calendar_events | Parameters: {\\'date\\': \\'05/23/2024\\'} ================================================== Final response: You have four meetings today. ================================================== Citations: start=9 end=22 text=\\'four meetings\\' document_ids=[\\'list_calendar_events:0:2:0\\'] Cited Documents: {\\'existing_events\\': \\'[{\"end\":\"8:59\",\"start\":\"8:00\"},{\"end\":\"9:59\",\"start\":\"9:00\"},{\"end\":\"11:59\",\"start\":\"11:00\"},{\"end\":\"12:59\",\"start\":\"12:00\"}]\\', \\'id\\': \\'list_calendar_events:0:2:0\\', \\'tool_name\\': \\'list_calendar_events\\'} ================================================== \\nLet’s now try to ask a question that requires multi-step tool calling — this one asking the assistant to book an appointment. To complete this task, the assistant will have to first query the existing events and then use the information to create a new event that wouldn’t cause a conflict.\\nchat_history = run_assistant(\"Create an hour-long appointment for the first available free slot after 9am\") \\nHere’s the assistant’s response. In the first step, it calls the list_calendar_events tool to get a list of existing events.\\nIn the second step, it calls the create_calendar_event tool to create a new event. It creates a new event at 10 a.m., which is indeed the first available slot after 9 a.m.'),\n",
       " Document(metadata={'title': 'Multi-Step Tool Use', 'source': 'https://cohere.com/llmu/multi-step-tool-use-2'}, page_content=\"In the second step, it calls the create_calendar_event tool to create a new event. It creates a new event at 10 a.m., which is indeed the first available slot after 9 a.m.\\nQuestion: Create an hour-long appointment for the first available free slot after 9am ================================================== Tool plan: I will first check the user's calendar for 23/05/2024 to see if there are any free slots after 9am. I will then create an hour-long appointment for the first available free slot. Tool calls: Tool name: list_calendar_events | Parameters: {'date': '05/23/2024'} ================================================== Tool plan: The user's calendar shows that they have a free slot from 10:00 to 10:59. I will now create an hour-long appointment for this time slot. Tool calls: Tool name: create_calendar_event | Parameters: {'date': '05/23/2024', 'duration': 1, 'time': '10:00'} ================================================== Final response: I've created a 1-hour long appointment for 10:00 on 23/05/2024. ================================================== Citations: start=15 end=62 text='1-hour long appointment for 10:00 on 23/05/2024' document_ids=['create_calendar_event:0:4:0'] Cited Documents: {'id': 'create_calendar_event:0:4:0', 'is_success': 'true', 'message': 'Created a 1 hour long event at 10:00 on 05/23/2024', 'tool_name': 'create_calendar_event'} ================================================== \\nAnd here’s a look at the chat history. Compared to the single-step scenario, it contains multiple pairs of CHATBOT - TOOL messages, with each pair being one step in the sequence of tool calls. In this particular case, we have two of them.\"),\n",
       " Document(metadata={'title': 'Multi-Step Tool Use', 'source': 'https://cohere.com/llmu/multi-step-tool-use-2'}, page_content='message=\\'Create an hour-long appointment for the first available free slot after 9am\\' tool_calls=None role=\\'USER\\' message=\"I will first check the user\\'s calendar for 23/05/2024 to see when they are free after 9am. I will then create an hour-long appointment for the first available slot.\" tool_calls=[ToolCall(name=\\'list_calendar_events\\', parameters={\\'date\\': \\'05/23/2024\\'})] role=\\'CHATBOT\\' tool_results=[ToolResult(call=ToolCall(name=\\'list_calendar_events\\', parameters={\\'date\\': \\'05/23/2024\\'}), outputs=[{\\'existing_events\\': [{\\'end\\': \\'8:59\\', \\'start\\': \\'8:00\\'}, {\\'end\\': \\'9:59\\', \\'start\\': \\'9:00\\'}, {\\'end\\': \\'11:59\\', \\'start\\': \\'11:00\\'}, {\\'end\\': \\'12:59\\', \\'start\\': \\'12:00\\'}]}])] role=\\'TOOL\\' message=\\'The user has an appointment from 9am to 9:59am. I will now create an hour-long appointment for the first available slot after this.\\' tool_calls=[ToolCall(name=\\'create_calendar_event\\', parameters={\\'date\\': \\'05/23/2024\\', \\'duration\\': 1, \\'time\\': \\'10:00\\'})] role=\\'CHATBOT\\' tool_results=[ToolResult(call=ToolCall(name=\\'create_calendar_event\\', parameters={\\'date\\': \\'05/23/2024\\', \\'duration\\': 1, \\'time\\': \\'10:00\\'}), outputs=[{\\'is_success\\': True, \\'message\\': \\'Created a 1 hour long event at 10:00 on 05/23/2024\\'}])] role=\\'TOOL\\' message=\"I\\'ve created an hour-long appointment for 10am on 23/05/2024.\" tool_calls=None role=\\'CHATBOT\\' \\nComparing the events within a single chat turn for single-step and multi-step scenarios\\nThe chat history also highlights another difference between the single-step and multi-step scenarios.\\nNotice that in the multi-step example above, the CHATBOT turns generate a textual response at each tool-calling step (look for the message) before making the actual tool calls.\\nLet’s call this the model’s tool plan. These intermediate responses are the model’s internal reasoning logic, which guides its next course of action.'),\n",
       " Document(metadata={'title': 'Multi-Step Tool Use', 'source': 'https://cohere.com/llmu/multi-step-tool-use-2'}, page_content='Let’s call this the model’s tool plan. These intermediate responses are the model’s internal reasoning logic, which guides its next course of action.\\nThis goes back to the ReAct framework discussed earlier. The first of these plans is the assistant’s initial planning step, which lists its set of actions to take. The subsequent responses consist of its reflection on the current state and decision on the next immediate actions.\\nHere’s how the tool plan in multi-step looks again in the chat history.\\n... message=\\'The user has an appointment from 9am to 9:59am. I will now create an hour-long appointment for the first available slot after this.\\' tool_calls=[ToolCall(name=\\'create_calendar_event\\', parameters={\\'date\\': \\'05/23/2024\\', \\'duration\\': 1, \\'time\\': \\'10:00\\'})] role=\\'CHATBOT\\' ... \\nMeanwhile, in the single-step case, the model doesn’t generate any tool plan (message=None) but instead generates the tool calls directly.\\n... message=None tool_calls=[ToolCall(name=\\'daily_sales_report\\', parameters={\\'day\\': \\'2023-09-29\\'})] role=\\'CHATBOT\\' ... \\nMulti-step tool use contains tool plans, which single-step doesn\\'t\\nLet’s now look at an example of multi-step, parallel tool use. Let’s ask the assistant to create two separate events on the same day.\\nchat_history = run_assistant(\"Book two hour-long appointments for any available time between 8am to 6pm\") \\nAnd here’s the assistant’s response. It follows the same two steps as in the earlier example: first, checking the existing events, and second, creating the new events.\\nThe difference this time is that since it needs to create two events, it calls create_calendar_event twice within the same step. Here, the assistant makes the correct judgment and tool calls are independent of each other, which means they can be done in parallel.'),\n",
       " Document(metadata={'title': 'Multi-Step Tool Use', 'source': 'https://cohere.com/llmu/multi-step-tool-use-2'}, page_content=\"Question: Create two hour-long appointments for any available time between 8am to 6pm ================================================== Tool plan: I will first list the events for 23/05/2024. Then, I will create two hour-long appointments for any available time between 8am and 6pm. Tool calls: Tool name: list_calendar_events | Parameters: {'date': '05/23/2024'} ================================================== Tool plan: The available times between 8am and 6pm are: 10am-11am, 1pm-6pm. I will now create two hour-long appointments for the user during these times. Tool calls: Tool name: create_calendar_event | Parameters: {'date': '05/23/2024', 'duration': 1, 'time': '10:00'} Tool name: create_calendar_event | Parameters: {'date': '05/23/2024', 'duration': 1, 'time': '13:00'} ================================================== Final response: I have created two hour-long appointments for you today: one at 10am and another at 1pm. ================================================== Citations: start=64 end=68 text='10am' document_ids=['create_calendar_event:0:4:0'] start=84 end=87 text='1pm' document_ids=['create_calendar_event:1:4:0'] Cited Documents: {'id': 'create_calendar_event:0:4:0', 'is_success': 'true', 'message': 'Created a 1 hour long event at 10:00 on 05/23/2024', 'tool_name': 'create_calendar_event'} {'id': 'create_calendar_event:1:4:0', 'is_success': 'true', 'message': 'Created a 1 hour long event at 13:00 on 05/23/2024', 'tool_name': 'create_calendar_event'} ================================================== \\nState Management (Memory)\\nIn the previous chapter, we saw how state management works with the Chat endpoint from the context of single-step tool use.\\nMulti-step tool use works the same way. The chat history for each turn consists of the following messages, and will accumulate with every new turn.\\nThe USER message\\nFollowed by the CHATBOT message with the list of tool calls\\nFollowed the TOOL message with the list of tool results\"),\n",
       " Document(metadata={'title': 'Multi-Step Tool Use', 'source': 'https://cohere.com/llmu/multi-step-tool-use-2'}, page_content='The USER message\\nFollowed by the CHATBOT message with the list of tool calls\\nFollowed the TOOL message with the list of tool results\\nFinally, followed by the CHATBOT message with the final response to the user\\nThe difference here is, as we’ve seen, that each turn can have multiple pairs of CHATBOT - TOOL messages, representing its multi-step nature. The other difference is the presence of the tool plans, which are not present in the single-step case.\\nMaintaining the state over multiple chat turns (multi-step tool use)\\nConclusion\\nIn this chapter, we learned about multi-step tool use by creating a calendar assistant that can check and create events on behalf of a user. We saw how the multi-step approach enables agents to perform a complex task via repeated sequences of planning, action, observation, and reflection. We also saw how all this can be done using the Cohere native API.\\nThere is another way you can run multi-step tool use with Cohere, and that is via open-source frameworks. We have worked with the Langchain team to make it simple for developers to build Cohere-backed agents on Langchain.\\nIn Chapter 5, we’ll learn how to use Langchain to create a multi-step agent.'),\n",
       " Document(metadata={'title': 'Single-Step Tool Use', 'source': 'https://cohere.com/llmu/single-step-tool-use'}, page_content='Single-Step Tool UseQualified\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nContents\\nIntroduction\\nSetting Up the Tools \\nStep 1: Create Tools\\nStep 2: Define Tool Schema\\nStep 3: Define the Preamble (Optional)\\nSingle-Step Tool Use\\nSingle-Step, Parallel Tool Use\\nDirectly Answering\\nState Management (Memory)\\nConclusion\\nIntroduction\\nIn the previous chapter, we saw that tool use can run in two different modes: single-step and multi-step.\\nIn a single-step tool use scenario, an LLM calls a single tool or many tools in parallel. The tools then return the results, which the LLM uses to generate its response.\\nIn single-step tool use, an LLM calls a single tool or many tools in parallel\\nWe also went through a code example of single-step tool use. However, that example was a basic one. We haven’t yet seen these cases:\\nMultiple tools: The example only had one tool, but what if the model had multiple tools at its disposal? How does the model reason over which tools to use at a particular time?\\nParallel tool calls: We had only seen a single tool call in a single step, but what does it look like if more than one tool call is required?\\nWhen not to use tools: What if a question can, and should, be answered directly by a model without needing a tool?\\nTool use in a chat setting: What if the user had follow-up questions, i.e., how does the assistant maintain the context of a conversation in a multi-turn chat setting?\\nIn this chapter, let’s go deeper into single-step tool use and address the questions above.\\nLet’s first set up the Cohere client. We’ll also need to import the json module for use later.\\n# pip install cohere import json import cohere co = cohere.Client(api_key=\"YOUR_COHERE_API_KEY\") # Get your API key: https://dashboard.cohere.com/api-keys \\nLet’s now create two tools:\\nA function to query the sales database called query_daily_sales_report, which includes a mock sales database (same as the previous chapter)'),\n",
       " Document(metadata={'title': 'Single-Step Tool Use', 'source': 'https://cohere.com/llmu/single-step-tool-use'}, page_content='Let’s now create two tools:\\nA function to query the sales database called query_daily_sales_report, which includes a mock sales database (same as the previous chapter)\\nA function to query the product catalog called query_product_catalog, which includes a mock product catalog (a new one)\\nSetting up the tools\\ndef daily_sales_report(day: str) -> dict: \"\"\" Function to retrieve the sales report for the given day \"\"\" # Mock database containing daily sales reports sales_database = { \\'2023-09-28\\': {\\'total_sales_amount\\': 5000,\\'total_units_sold\\': 100}, \\'2023-09-29\\': {\\'total_sales_amount\\': 10000,\\'total_units_sold\\': 250}, \\'2023-09-30\\': {\\'total_sales_amount\\': 8000,\\'total_units_sold\\': 200} } report = sales_database.get(day, {}) if report: return { \\'date\\': day, \\'summary\\': f\"Total Sales Amount: {report[\\'total_sales_amount\\']}, Total Units Sold: {report[\\'total_units_sold\\']}\" } else: return {\\'date\\': day, \\'summary\\': \\'No sales data available for this day.\\'} def product_database(category: str) -> dict: \"\"\" Function to retrieve products for the given category \"\"\" # Mock product catalog product_catalog = { \\'Electronics\\': [ {\\'product_id\\': \\'E1001\\', \\'name\\': \\'Smartphone\\', \\'price\\': 500, \\'stock_level\\': 20}, {\\'product_id\\': \\'E1002\\', \\'name\\': \\'Laptop\\', \\'price\\': 1000, \\'stock_level\\': 15}, {\\'product_id\\': \\'E1003\\', \\'name\\': \\'Tablet\\', \\'price\\': 300, \\'stock_level\\': 25}, ], \\'Clothing\\': [ {\\'product_id\\': \\'C1001\\', \\'name\\': \\'T-Shirt\\', \\'price\\': 20, \\'stock_level\\': 100}, {\\'product_id\\': \\'C1002\\', \\'name\\': \\'Jeans\\', \\'price\\': 50, \\'stock_level\\': 80}, {\\'product_id\\': \\'C1003\\', \\'name\\': \\'Jacket\\', \\'price\\': 100, \\'stock_level\\': 40}, ] } products = product_catalog.get(category, []) return { \\'category\\': category, \\'products\\': products } functions_map = { \"daily_sales_report\": daily_sales_report, \"product_database\": product_database } \\nNext, we define the tool schema for the two tools. Both functions accept one parameter, day and category respectively.'),\n",
       " Document(metadata={'title': 'Single-Step Tool Use', 'source': 'https://cohere.com/llmu/single-step-tool-use'}, page_content='Next, we define the tool schema for the two tools. Both functions accept one parameter, day and category respectively.\\ntools = [ { \"name\": \"daily_sales_report\", \"description\": \"Connects to a database to retrieve overall sales volumes and sales information for a given day.\", \"parameter_definitions\": { \"day\": { \"description\": \"Retrieves sales data for this day, formatted as YYYY-MM-DD.\", \"type\": \"str\", \"required\": True } } }, { \"name\": \"product_database\", \"description\": \"A database that contains information about all the products of this company, including categories, prices, and stock levels.\", \"parameter_definitions\": { \"category\": { \"description\": \"Retrieves product information data for all products in this category.\", \"type\": \"str\", \"required\": True } } } ] \\nStep 3: Create Custom Preamble (Optional)\\nWe keep the same preamble as in the previous chapter.\\npreamble = \"\"\"## Task & Context You help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user\\'s needs as best you can, which will be wide-ranging. ## Style Guide Unless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling. \"\"\" \\nLet’s now take the code used in the previous chapter and automate the four-step tool use workflow. Let’s put the code in a function called run_assistant. Here’s what it does:\\nGets the user message (Step 1)\\nCalls the Chat endpoint for tool call generation (Step 2)\\nIf the response contains at least one tool call, executes the tool call(s) and gets the tool results (Step 3)\\nGenerates the final response with citations (Step 4)\\nAn overview of the four-step tool use workflow'),\n",
       " Document(metadata={'title': 'Single-Step Tool Use', 'source': 'https://cohere.com/llmu/single-step-tool-use'}, page_content='Generates the final response with citations (Step 4)\\nAn overview of the four-step tool use workflow\\nThe main difference from the previous chapter is that this time, we put Steps 2 and 3 in a loop, checking if the model’s response contains tool calls. This allows the assistant to handle any number of tool call steps (zero, one, or multiple) for a given user message.\\nThe other key difference is we are also enabling the assistant to retain the state (or memory) of a conversation, which allows it to handle multi-turn chat scenarios. We do this by having the run_assistant function take the chat history from the previous turns as an argument and return the updated chat history.'),\n",
       " Document(metadata={'title': 'Single-Step Tool Use', 'source': 'https://cohere.com/llmu/single-step-tool-use'}, page_content='model = \"command-r-plus\" def run_assistant(message, chat_history=[]): # Step 1: Get user message print(f\"Question:\\\\n{message}\") print(\"=\"*50) # Step 2: Generate tool calls (if any) response = co.chat( message=message, model=model, preamble=preamble, tools=tools, chat_history=chat_history, force_single_step=True ) while response.tool_calls: tool_calls = response.tool_calls if response.text: print(\"Tool plan:\") print(response.text,\"\\\\n\") print(\"Tool calls:\") for call in tool_calls: print(f\"Tool name: {call.name} | Parameters: {call.parameters}\") print(\"=\"*50) # Step 3: Get tool results tool_results = [] for tc in tool_calls: tool_call = {\"name\": tc.name, \"parameters\": tc.parameters} tool_output = functions_map[tc.name](**tc.parameters) tool_results.append({\"call\": tool_call, \"outputs\": [tool_output]}) # Step 4: Generate response and citations response = co.chat( message=\"\", model=model, preamble=preamble, tools=tools, tool_results=tool_results, chat_history=response.chat_history ) # Update the chat history chat_history = response.chat_history # Print final response print(\"Final response:\") print(response.text) print(\"=\"*50) # Print citations (if any) if response.citations: print(\"Citations:\") for citation in response.citations: print(citation) print(\"\\\\nCited Documents:\") for document in response.documents: print(document) print(\"=\"*50) return chat_history \\nLet’s now ask the assistant the first question, which is the same as in the previous chapter.\\nchat_history = run_assistant(\"Can you provide a sales summary for 29th September 2023?\") \\nThe assistant correctly identifies that, out of the two tools available, query_daily_sales_report is sufficient to answer the question. And it answers the question correctly.'),\n",
       " Document(metadata={'title': 'Single-Step Tool Use', 'source': 'https://cohere.com/llmu/single-step-tool-use'}, page_content='The assistant correctly identifies that, out of the two tools available, query_daily_sales_report is sufficient to answer the question. And it answers the question correctly.\\nQuestion: Can you provide a sales summary for 29th September 2023? ================================================== Tool calls: Tool name: daily_sales_report | Parameters: {\\'day\\': \\'2023-09-29\\'} ================================================== Final response: On 29 September 2023, we made a total of 10,000 in sales from 250 units. ================================================== Citations: start=32 end=56 text=\\'total of 10,000 in sales\\' document_ids=[\\'daily_sales_report:0:2:0\\'] start=62 end=72 text=\\'250 units.\\' document_ids=[\\'daily_sales_report:0:2:0\\'] Cited Documents: {\\'date\\': \\'2023-09-29\\', \\'id\\': \\'daily_sales_report:0:2:0\\', \\'summary\\': \\'Total Sales Amount: 10000, Total Units Sold: 250\\', \\'tool_name\\': \\'daily_sales_report\\'} ================================================== \\nIn this example, the model requires a single step to answer the question, and within that single step, it requires just one tool call.\\nAs mentioned, the model can decide that more than one tool is required to provide a response. This means calling multiple tools in parallel within the same step. This can either be:\\nCalling different tools in parallel\\nCalling the same tool multiple times in parallel\\nOr a combination of both\\nDifferent types of parallel tool calls\\nTo illustrate this, let’s try to ask the model another question. Let’s ask about the sales summary of two different dates and also about the stock level information of a product category.\\nchat_history = run_assistant(\"Can you provide a sales summary for 28th and 29th September 2023 as well as the stock level of the products in the \\'Electronics\\' category?\") \\nThis time, the model generates three tool calls in parallel:\\nTwo queries to the sales database, one for each date\\nAnd one query to the product catalog'),\n",
       " Document(metadata={'title': 'Single-Step Tool Use', 'source': 'https://cohere.com/llmu/single-step-tool-use'}, page_content='This time, the model generates three tool calls in parallel:\\nTwo queries to the sales database, one for each date\\nAnd one query to the product catalog\\nThese are exactly what are needed to answer the question. And it correctly answers the question.'),\n",
       " Document(metadata={'title': 'Single-Step Tool Use', 'source': 'https://cohere.com/llmu/single-step-tool-use'}, page_content=\"Question: Can you provide a sales summary for 28th and 29th September 2023 as well as the stock level of the products in the 'Electronics' category? ================================================== Tool calls: Tool name: daily_sales_report | Parameters: {'day': '2023-09-28'} Tool name: daily_sales_report | Parameters: {'day': '2023-09-29'} Tool name: product_database | Parameters: {'category': 'Electronics'} ================================================== Final response: On 28 September 2023, the total sales amount was 5000, with 100 units sold. The following day, 29 September 2023, the total sales amount was 10000, with 250 units sold. Here is the stock level of the products in the 'Electronics' category: - Smartphone (E1001) - 20 - Laptop (E1002) - 15 - Tablet (E1003) - 25 ================================================== Citations: start=3 end=20 text='28 September 2023' document_ids=['daily_sales_report:0:2:0'] start=26 end=53 text='total sales amount was 5000' document_ids=['daily_sales_report:0:2:0'] start=60 end=74 text='100 units sold' document_ids=['daily_sales_report:0:2:0'] start=95 end=112 text='29 September 2023' document_ids=['daily_sales_report:1:2:0'] start=118 end=146 text='total sales amount was 10000' document_ids=['daily_sales_report:1:2:0'] start=153 end=167 text='250 units sold' document_ids=['daily_sales_report:1:2:0'] start=244 end=267 text='Smartphone (E1001) - 20' document_ids=['product_database:2:2:0'] start=270 end=289 text='Laptop (E1002) - 15' document_ids=['product_database:2:2:0'] start=292 end=311 text='Tablet (E1003) - 25' document_ids=['product_database:2:2:0'] Cited Documents: {'date': '2023-09-28', 'id': 'daily_sales_report:0:2:0', 'summary': 'Total Sales Amount: 5000, Total Units Sold: 100', 'tool_name': 'daily_sales_report'} {'date': '2023-09-29', 'id': 'daily_sales_report:1:2:0', 'summary': 'Total Sales Amount: 10000, Total Units Sold: 250', 'tool_name': 'daily_sales_report'} {'category': 'Electronics', 'id':\"),\n",
       " Document(metadata={'title': 'Single-Step Tool Use', 'source': 'https://cohere.com/llmu/single-step-tool-use'}, page_content='{\\'date\\': \\'2023-09-29\\', \\'id\\': \\'daily_sales_report:1:2:0\\', \\'summary\\': \\'Total Sales Amount: 10000, Total Units Sold: 250\\', \\'tool_name\\': \\'daily_sales_report\\'} {\\'category\\': \\'Electronics\\', \\'id\\': \\'product_database:2:2:0\\', \\'products\\': \\'[{\"name\":\"Smartphone\",\"price\":500,\"product_id\":\"E1001\",\"stock_level\":20},{\"name\":\"Laptop\",\"price\":1000,\"product_id\":\"E1002\",\"stock_level\":15},{\"name\":\"Tablet\",\"price\":300,\"product_id\":\"E1003\",\"stock_level\":25}]\\', \\'tool_name\\': \\'product_database\\'} =================================================='),\n",
       " Document(metadata={'title': 'Single-Step Tool Use', 'source': 'https://cohere.com/llmu/single-step-tool-use'}, page_content='Directly Answering\\nA key attribute of tool use systems is the model’s ability to choose the right tools for a task. This includes the ability to decide to not use any tool, and instead, respond to a user message directly.\\nThis could happen if a user’s question inherently can be answered directly or none of the available tools are helpful to answer the question.\\nComparing the sequence of events of using tools and directly answering\\nLet’s look at the first scenario. The question below asks about building a great company. It is a rather general question that a good LLM would be able to answer directly without needing any additional help.\\nchat_history = run_assistant(\"Give me 3 concise tips on how to build a great company\") \\nAnd here’s the response. Notice that the model doesn’t trigger any tool calls, but instead goes directly to answering the question.\\nQuestion: Give me 3 concise tips on how to build a great company ================================================== Final response: 1. Start with a strong foundation: Build a solid business plan, secure adequate funding, and establish a clear mission and vision. 2. Focus on people: Hire and retain talented employees by creating a positive company culture and offering competitive benefits. 3. Stay agile: Adapt to market changes and be open to innovation. Stay connected to your customers\\' needs and wants and be willing to pivot when necessary. ================================================== \\nLet’s look at the other scenario. The question below asks about the company\\'s employee count, which is a specific piece of information that an LLM would not possess and requires context from an external tool.\\nchat_history = run_assistant(\"How many employees does this company have?\") \\nBut because none of the available tools can provide this information, the model doesn’t attempt any tool call. Instead, it responds directly to the user question, mentioning that it doesn’t have the information needed to answer the question.'),\n",
       " Document(metadata={'title': 'Single-Step Tool Use', 'source': 'https://cohere.com/llmu/single-step-tool-use'}, page_content='Question: How many employees does this company have? ================================================== Final response: I don\\'t have any information about the company\\'s number of employees. Is there anything else I can help you with? ================================================== \\nState Management (Memory)\\nWhen we created the run_assistant function, we already enabled it to handle multi-turn chat scenarios. The function represents one turn of a conversation. For each turn, it accepts the latest chat history of a conversation and returns the updated one once the turn has been completed.\\nThis can continue for any number of turns, the limit being the model’s maximum context length (with Command R/R+, that’s 128k tokens).\\nThe chat history for each turn consists of:\\nThe USER message\\nFollowed by the CHATBOT message with the list of tool calls\\nFollowed by the TOOL message with the list of tool results\\nFinally, followed by the CHATBOT message with the final response to the user\\nMaintaining the state over multiple chat turns (single-step tool use)\\nThese four entries in the chat history directly map to the four steps in the tool use workflow. Here’s an example chat history taken from the first user message example earlier.\\n# Print chat history for turn in chat_history: print(turn,\"\\\\n\") \\nmessage=\\'Can you provide a sales summary for 29th September 2023?\\' tool_calls=None role=\\'USER\\' message=None tool_calls=[ToolCall(name=\\'daily_sales_report\\', parameters={\\'day\\': \\'2023-09-29\\'})] role=\\'CHATBOT\\' tool_results=[ToolResult(call=ToolCall(name=\\'daily_sales_report\\', parameters={\\'day\\': \\'2023-09-29\\'}), outputs=[{\\'date\\': \\'2023-09-29\\', \\'summary\\': \\'Total Sales Amount: 10000, Total Units Sold: 250\\'}])] role=\\'TOOL\\' message=\\'On 29 September 2023, we sold 250 units, totalling a sales amount of 10,000.\\' tool_calls=None role=\\'CHATBOT\\' \\nAll this becomes part of the prompt to a chatbot at a particular turn, giving it the full context of a conversation.'),\n",
       " Document(metadata={'title': 'Single-Step Tool Use', 'source': 'https://cohere.com/llmu/single-step-tool-use'}, page_content='All this becomes part of the prompt to a chatbot at a particular turn, giving it the full context of a conversation.\\nLet’s try it out, starting with the same question about the sales summary.\\nchat_history = run_assistant(\"Can you provide a sales summary for 29th September 2023?\") \\nThis gives the same answer as before.\\nQuestion: Can you provide a sales summary for 29th September 2023? ================================================== Tool calls: Tool name: daily_sales_report | Parameters: {\\'day\\': \\'2023-09-29\\'} ================================================== Final response: On 29 September 2023, the total sales amount was 10,000 and the total number of units sold was 250. ================================================== Citations: start=26 end=55 text=\\'total sales amount was 10,000\\' document_ids=[\\'daily_sales_report:0:2:0\\'] start=64 end=98 text=\\'total number of units sold was 250\\' document_ids=[\\'daily_sales_report:0:2:0\\'] Cited Documents: {\\'date\\': \\'2023-09-29\\', \\'id\\': \\'daily_sales_report:0:2:0\\', \\'summary\\': \\'Total Sales Amount: 10000, Total Units Sold: 250\\', \\'tool_name\\': \\'daily_sales_report\\'} ================================================== \\nLet’s now ask a follow-up question, a rather vague question that a model would not be able to answer without the context of the previous turn. Here, we pass the first turn’s chat history to the run_assistant function.\\nchat_history = run_assistant(\"What about the 28th?\", chat_history) \\nThe model is able to infer that “the 28th” is likely referring to September 28th 2023, because of what was asked in the previous turn.'),\n",
       " Document(metadata={'title': 'Single-Step Tool Use', 'source': 'https://cohere.com/llmu/single-step-tool-use'}, page_content='The model is able to infer that “the 28th” is likely referring to September 28th 2023, because of what was asked in the previous turn.\\nQuestion: What about the 28th? ================================================== Tool calls: Tool name: daily_sales_report | Parameters: {\\'day\\': \\'2023-09-28\\'} ================================================== Final response: On 28 September 2023, the total sales amount was 5,000 and the total number of units sold was 100. ================================================== Citations: start=26 end=54 text=\\'total sales amount was 5,000\\' document_ids=[\\'daily_sales_report:0:6:0\\'] start=63 end=97 text=\\'total number of units sold was 100\\' document_ids=[\\'daily_sales_report:0:6:0\\'] Cited Documents: {\\'date\\': \\'2023-09-28\\', \\'id\\': \\'daily_sales_report:0:6:0\\', \\'summary\\': \\'Total Sales Amount: 5000, Total Units Sold: 100\\', \\'tool_name\\': \\'daily_sales_report\\'} ================================================== \\nLet’s continue the conversation.\\nchat_history = run_assistant(\"How many units were sold over both days\", chat_history) \\nAgain, the model is able to infer what “both days” are from the chat context.'),\n",
       " Document(metadata={'title': 'Single-Step Tool Use', 'source': 'https://cohere.com/llmu/single-step-tool-use'}, page_content='Let’s continue the conversation.\\nchat_history = run_assistant(\"How many units were sold over both days\", chat_history) \\nAgain, the model is able to infer what “both days” are from the chat context.\\nQuestion: How many units were sold over both days ================================================== Tool calls: Tool name: daily_sales_report | Parameters: {\\'day\\': \\'2023-09-29\\'} Tool name: daily_sales_report | Parameters: {\\'day\\': \\'2023-09-28\\'} ================================================== Final response: Combined, 28 and 29 September 2023 saw 350 units sold. ================================================== Citations: start=39 end=53 text=\\'350 units sold\\' document_ids=[\\'daily_sales_report:0:10:0\\', \\'daily_sales_report:1:10:0\\'] Cited Documents: {\\'date\\': \\'2023-09-29\\', \\'id\\': \\'daily_sales_report:0:10:0\\', \\'summary\\': \\'Total Sales Amount: 10000, Total Units Sold: 250\\', \\'tool_name\\': \\'daily_sales_report\\'} {\\'date\\': \\'2023-09-28\\', \\'id\\': \\'daily_sales_report:1:10:0\\', \\'summary\\': \\'Total Sales Amount: 5000, Total Units Sold: 100\\', \\'tool_name\\': \\'daily_sales_report\\'} ================================================== \\nHere’s a look at the chat history. It consists of a set of USER, CHATBOT, and TOOL messages appended in the right sequence of turns, providing the model with the right context each time it generates a new response.'),\n",
       " Document(metadata={'title': 'Single-Step Tool Use', 'source': 'https://cohere.com/llmu/single-step-tool-use'}, page_content=\"message='Can you provide a sales summary for 29th September 2023?' tool_calls=None role='USER' message=None tool_calls=[ToolCall(name='daily_sales_report', parameters={'day': '2023-09-29'})] role='CHATBOT' tool_results=[ToolResult(call=ToolCall(name='daily_sales_report', parameters={'day': '2023-09-29'}), outputs=[{'date': '2023-09-29', 'summary': 'Total Sales Amount: 10000, Total Units Sold: 250'}])] role='TOOL' message='On 29 September 2023, we sold 250 units, totalling a revenue of 10,000.' tool_calls=None role='CHATBOT' message='What about the 28th?' tool_calls=None role='USER' message=None tool_calls=[ToolCall(name='daily_sales_report', parameters={'day': '2023-09-28'})] role='CHATBOT' tool_results=[ToolResult(call=ToolCall(name='daily_sales_report', parameters={'day': '2023-09-28'}), outputs=[{'date': '2023-09-28', 'summary': 'Total Sales Amount: 5000, Total Units Sold: 100'}])] role='TOOL' message='On 28 September 2023, we sold 100 units, totalling a revenue of 5,000.' tool_calls=None role='CHATBOT' message='How many units were sold over both days' tool_calls=None role='USER' message=None tool_calls=[ToolCall(name='daily_sales_report', parameters={'day': '2023-09-29'}), ToolCall(name='daily_sales_report', parameters={'day': '2023-09-28'})] role='CHATBOT' tool_results=[ToolResult(call=ToolCall(name='daily_sales_report', parameters={'day': '2023-09-29'}), outputs=[{'date': '2023-09-29', 'summary': 'Total Sales Amount: 10000, Total Units Sold: 250'}]), ToolResult(call=ToolCall(name='daily_sales_report', parameters={'day': '2023-09-28'}), outputs=[{'date': '2023-09-28', 'summary': 'Total Sales Amount: 5000, Total Units Sold: 100'}])] role='TOOL' message='Over the two days, we sold 350 units.' tool_calls=None role='CHATBOT'\"),\n",
       " Document(metadata={'title': 'Single-Step Tool Use', 'source': 'https://cohere.com/llmu/single-step-tool-use'}, page_content=\"Here, the four entries in the chat history directly map to the four-step workflow. It starts with the USER message, the CHATBOT message with the list of tool calls, the TOOL message with the list of tool results, and the CHATBOT message with the final response to the user.\\nConclusion\\nIn this chapter, we added another tool to the structured data RAG assistant from the previous chapter. We also looked at the different dynamics this introduces, such as calling tools in parallel and being able to decide on the right tool(s) for a given user message.\\nHowever, all the examples so far have been confined to a single-step scenario. For example, “Create a plot of the number of employees at the three tech companies with the highest market cap in the United States in 2024.” requires a number of tool calls that happen in sequence. These types of tasks can’t be handled with single-step tool use.\\nAn assistant's potential can only be truly unlocked when it can reason over complex tasks that require multiple steps of tool-calling. We’ll learn about exactly that in the next two chapters. \\nLet’s now turn our focus to multi-step tool use in Chapter 4.\"),\n",
       " Document(metadata={'title': 'Tool Use on LangChain', 'source': 'https://cohere.com/llmu/tool-use-on-langchain'}, page_content='Tool Use on LangChainQualified\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nContents\\nIntroduction\\nUsing Cohere with LangChain\\nDefining Tools \\nTavily Search\\nPython REPL\\nCreating a ReAct Agent\\nMulti-Step Tool Use\\nConclusion\\nIntroduction\\nThe previous chapter showed how to implement multi-step tool use using the native Cohere API. A similar implementation is also available on LangChain. \\nIn this chapter, we’ll look at how to use LangChain to create an assistant powered by multi-step tool use.\\nLangChain is an open-source framework for building applications based on large language models (LLMs). It provides tools and APIs that simplify the process of building LLM-driven applications like chatbots and virtual agents.\\nIt provides abstractions for the various components needed to build LLM-powered applications. This includes the ReAct-style implementation of LLM agents and a collection of ready-to-use tools to simplify the development process.\\nIn this chapter, we’ll build a data analyst that can research the web, get up-to-date data, and perform data visualization using Python.\\nUsing Cohere with LangChain\\nWith LangChain, the four-step tool-use workflow remains the same. The only difference is that interfacing with the LLM and tools is done via the LangChain API, as illustrated in the diagram below.\\nInterfacing with the LLM and tools is done via the LangChain API\\nLangChain makes it easy to create tools, with different options available:\\nBuilt-in tools: Ready implementations of tools that can be used immediately (see full list). We’ll use two of them in this chapter.\\nToolkits: Collections of tools that are designed to be used together for specific tasks (see full list). Some examples are toolkits for working with SQL databases, Gmail, or GitHub.\\nCustom tools: There’s the option to build our own tools, such as those we created in the previous chapters.\\nLet’s get started. First, we need to install a few LangChain libraries.'),\n",
       " Document(metadata={'title': 'Tool Use on LangChain', 'source': 'https://cohere.com/llmu/tool-use-on-langchain'}, page_content='Custom tools: There’s the option to build our own tools, such as those we created in the previous chapters.\\nLet’s get started. First, we need to install a few LangChain libraries.\\npip install langchain langchain-cohere langchain-experimental \\nTo build our data analysis agent, we’ll be using two tools, both of which are LangChain built-in tools:\\nTavily Search - for performing web search\\nPython REPL - for executing Python code\\nTavily Search\\nThe Tavily Search is used for performing web search queries. Given a query, it returns a list of relevant document snippets from the internet. It requires an API key which you can get here under the free plan.\\nWe access Tavily Search via the langchain_community library. The steps to define the tool are shown below.\\nfrom langchain_community.tools.tavily_search import TavilySearchResults os.environ[\\'TAVILY_API_KEY\\'] = \"<INSERT TAVILY API KEY HERE>\" internet_search = TavilySearchResults() internet_search.name = \"internet_search\" internet_search.description = \"Returns a list of relevant document snippets for a textual query retrieved from the internet.\" from langchain_core.pydantic_v1 import BaseModel, Field class TavilySearchInput(BaseModel): query: str = Field(description=\"Query to search the internet with\") internet_search.args_schema = TavilySearchInput \\nPython REPL\\nNext, we’ll create the Python REPL to execute the Python code. For our example, we’ll need it to plot data on a chart.\\nWe access the Python REPL via the langchain_experimental library. The steps to define the tool are shown below.'),\n",
       " Document(metadata={'title': 'Tool Use on LangChain', 'source': 'https://cohere.com/llmu/tool-use-on-langchain'}, page_content='We access the Python REPL via the langchain_experimental library. The steps to define the tool are shown below.\\nfrom langchain.agents import Tool from langchain_experimental.utilities import PythonREPL python_repl = PythonREPL() repl_tool = Tool( name=\"python_repl\", description=\"Executes python code and returns the result. The code runs in a static sandbox without interactive mode, so print output or save output to a file.\", func=python_repl.run, ) repl_tool.name = \"python_interpreter\" # from langchain_core.pydantic_v1 import BaseModel, Field class ToolInput(BaseModel): code: str = Field(description=\"Python code to execute.\") repl_tool.args_schema = ToolInput \\nCreating a ReAct Agent\\nIn the previous chapter, we saw that the native Cohere API uses a ReAct-style approach for implementing multi-step tool use. As a quick recap, the agent workflow starts with a planning step and is followed by a series of Action-Observation-Reflection sequences until a task is deemed complete.\\nThe same approach is used when using Cohere with LangChain, which can be utilized via the create_cohere_react_agent module.\\nfrom langchain.agents import AgentExecutor from langchain_cohere.react_multi_hop.agent import create_cohere_react_agent from langchain_core.prompts import ChatPromptTemplate \\nThe overall implementation to create an agent in LangChain is pretty simple:\\nCreate the prompt using Cohere’s template\\nCreate an agent by passing the model type, list of tools, and the prompt\\nCreate an agent executor using AgentExecutor\\n# Create the prompt prompt = ChatPromptTemplate.from_template(\"{input}\") # Create the ReAct agent agent = create_cohere_react_agent( llm=chat, tools=[internet_search, repl_tool], prompt=prompt, ) agent_executor = AgentExecutor(agent=agent, tools=[internet_search, repl_tool], verbose=True) \\nThe agent is now ready to use. We can run it using agent_executor.invoke.'),\n",
       " Document(metadata={'title': 'Tool Use on LangChain', 'source': 'https://cohere.com/llmu/tool-use-on-langchain'}, page_content='The agent is now ready to use. We can run it using agent_executor.invoke.\\nLet’s ask the agent a question that requires it to run a multi-step sequence: “Create a plot of the number of full-time employees at the three tech companies with the highest market cap in the United States in 2024.”\\nIf a human assistant were given this query, they would need to do a few steps in the right order:\\nFirst, find the three tech companies with the highest market cap in the United States in 2024\\nNext, find the number of full-time employees at each of the companies\\nFinally, create a plot based on these numbers\\nLet’s see how the agent performs.\\nagent_executor.invoke({ \"input\": \"Create a plot of the number of full time employees at the 3 tech companies with the highest market cap in the United States in 2024.\", }) \\nThe whole response is pretty long, so let’s break down the responses into the different steps the agent takes.\\nFirst, it generates a plan of what it needs to perform: “First, I will search for the three tech companies with the highest market cap in the US in 2024. Then, I will search for the number of full-time employees at each of these companies, and plot the data using Python.”\\nThen, it executes its first action: using the internet search tool to find the three companies and display the results.'),\n",
       " Document(metadata={'title': 'Tool Use on LangChain', 'source': 'https://cohere.com/llmu/tool-use-on-langchain'}, page_content='Then, it executes its first action: using the internet search tool to find the three companies and display the results.\\n> Entering new AgentExecutor chain... First, I will search for the three tech companies with the highest market cap in the US in 2024. Then, I will search for the number of full-time employees at each of these companies, and plot the data using Python. {\\'tool_name\\': \\'internet_search\\', \\'parameters\\': {\\'query\\': \\'top 3 tech companies highest market cap US 2024\\'}} [{\\'url\\': \\'<https://www.fool.com/research/largest-companies-by-market-cap/>\\', \\'content\\': \"It\\'s the most valuable automaker in the world and has the world\\'s best-selling car in the Model Y.\\\\\\\\nTesla is most famous for its vehicles, and it\\'s second only to China\\'s BYD Company (OTC:BYDDY) among the largest EV companies in terms of manufacturing. While it\\'s most famous for Windows, Microsoft also has a diverse selection of products and services that has helped to build on its success, including:\\\\\\\\nMicrosoft has been the world\\'s largest company before, and it briefly surpassed Apple for the biggest market cap in 2021. Walmart\\\\\\\\nWalmart (NYSE:WMT) may not have the largest market cap, but it is No. 1 in terms of revenue, and it’s the largest retailer in the world. Microsoft\\\\\\\\nConsidering the popularity of the Windows operating system, it’s no surprise that Microsoft (NASDAQ:MSFT) has consistently ranked as one of the largest companies in the world. Although the top spot has changed hands on multiple occasions, Apple has spent the most time there and is currently the most valuable company in the world.\\\\\\\\n\"}, {\\'url\\': ...(truncated) \\nUsing the results, the agent generates a reflection step that identifies the list of companies from its results. It then executes its next action: using the internet search tool to find the number of employees of the first company in the list, Microsoft.'),\n",
       " Document(metadata={'title': 'Tool Use on LangChain', 'source': 'https://cohere.com/llmu/tool-use-on-langchain'}, page_content=\"I have found that the three tech companies with the highest market cap in the US in 2024 are Microsoft, Apple and NVIDIA. Now, I will search for the number of full-time employees at each of these companies and plot the data. {'tool_name': 'internet_search', 'parameters': {'query': 'Microsoft full time employees 2024'}} [{'url': '<https://www.statista.com/statistics/273475/number-of-employees-at-the-microsoft-corporation-since-2005/>', 'content': 'Digital & Trend reports\\\\\\\\nOverview and forecasts on trending topics\\\\\\\\nIndustry & Market reports\\\\\\\\nIndustry and market insights and forecasts\\\\\\\\nCompanies & Products reports\\\\\\\\nKey figures and rankings about companies and products\\\\\\\\nConsumer & ...(truncated) \\nHaving received the results, it moves to the next company in the list.\\n{'tool_name': 'internet_search', 'parameters': {'query': 'Apple full time employees 2024'}} [{'url': '<https://www.macrotrends.net/stocks/charts/AAPL/apple/number-of-employees>', 'content': 'Employee Count. Interactive chart of Apple ...(truncated) \\nAnd then, the third and final company on the list.\\n{'tool_name': 'internet_search', 'parameters': {'query': 'NVIDIA full time employees 2024'}} [{'url': '<https://www.macrotrends.net/stocks/charts/NVDA/nvidia/number-of-employees>', 'content': 'Employee Count. Interactive chart of NVIDIA (NVDA) annual worldwide employee count from 2010 to 2024. NVIDIA total number of employees in 2024 was 29,600, a 12.99% increase \\nNow that it has retrieved all the data it needs, it can proceed with the next action: writing and executing the Python code to plot this data.\\nIt generates a code block as the parameter for calling the Python REPL tool. Here, it opts to use matplotlib as the library to plot the data and finally save it as a PNG file.\"),\n",
       " Document(metadata={'title': 'Tool Use on LangChain', 'source': 'https://cohere.com/llmu/tool-use-on-langchain'}, page_content='It generates a code block as the parameter for calling the Python REPL tool. Here, it opts to use matplotlib as the library to plot the data and finally save it as a PNG file.\\nI have found the following data on the number of full-time employees at each company: Microsoft: 221,000 Apple: 161,000 NVIDIA: 29,600 Now, I will write and execute Python code to plot this data. {\\'tool_name\\': \\'python_interpreter\\', \\'parameters\\': {\\'code\\': \\'import matplotlib.pyplot as plt\\\\\\\\n\\\\\\\\n# Data on the number of full-time employees at each company\\\\\\\\ncompanies = [\\\\\\\\n \"Microsoft\", \"Apple\", \"NVIDIA\"\\\\\\\\n]\\\\\\\\nfull_time_employees = [221000, 161000, 29600]\\\\\\\\n\\\\\\\\n# Plot the data\\\\\\\\nplt.bar(companies, full_time_employees)\\\\\\\\nplt.xlabel(\"Company\")\\\\\\\\nplt.ylabel(\"Number of Full Time Employees\")\\\\\\\\nplt.xticks(rotation=45)\\\\\\\\nplt.tight_layout()\\\\\\\\nplt.savefig(\"tech_companies_market_cap_employees.png\")\\'}} \\nNow that it has completed the task, it generates the final response to the user. It contains a text commentary on the number of employees and a rendering of the plot it had created.\\nAs with the previous chapters, it also generates fine-grained citations that refer to the tools it used to generate the response.'),\n",
       " Document(metadata={'title': 'Tool Use on LangChain', 'source': 'https://cohere.com/llmu/tool-use-on-langchain'}, page_content=\"As with the previous chapters, it also generates fine-grained citations that refer to the tools it used to generate the response.\\nRelevant Documents: 0,1,3,4,5,7,8,9,10,11,12,15,18,20 Cited Documents: 1,3,4,7,8,10,11,15,18,20 Answer: Here is a plot showing the number of full-time employees at the three US tech companies with the highest market cap in 2024: ![Number of Full Time Employees]('tech_companies_market_cap_employees.png') The companies with the highest number of full-time employees are Microsoft with 221,000, Apple with 161,000 and NVIDIA with 29,600. Grounded answer: Here is a plot showing the number of full-time employees at the three US tech companies with the highest market cap in 2024: <co: 20>! [Number of Full Time Employees]('tech_companies_market_cap_employees.png')</co: 20> The companies with the highest number of full-time employees are <co: 1,4>Microsoft</co: 1,4> with <co: 7,8>221,000</co: 7,8>, <co: 1,3,4>Apple</co: 1,3,4> with <co: 10,11>161,000</co: 10,11> and <co: 1,4>NVIDIA</co: 1,4> with <co: 15,18>29,600</co: 15,18>. \\nThe matplotlib plot generated through the Python REPL \\nConclusion\\nIn this chapter, we looked at how to build a multi-step agent using LangChain. This lets us build ReAct-style agents that can reason over multiple steps, similar to the one we built with the native Cohere API in the previous chapter.\\nThis agent also had access to the two LangChain built-in tools: Tavily Search and Python REPL. The built-in tools and toolkit simplify the process of creating tool use applications, so the developer can focus on building the application’s core logic.\\nThat concludes the LLM University module on tool use, for now. There is still so much to discuss on the subject of agents and tool use, so be sure to keep an eye on LLM University for new modules on this topic in future.\"),\n",
       " Document(metadata={'title': 'Introduction to Cohere on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-bedrock'}, page_content=\"Introduction to Cohere on Amazon BedrockQualified\\nContents\\nIntroduction\\nCohere's Model Deployment Options\\nWhat Is Amazon Bedrock?\\nFeatures of Amazon Bedrock\\nCohere Models on Amazon Bedrock \\nList of Models\\nPricing Mechanism\\nExample Use Cases\\nGetting Started with Amazon Bedrock \\nStep 1: Set Up Amazon Bedrock\\nStep 2: Choose Your Models\\nStep 3: Test on the Playground\\nConclusion\\nIntroduction\\nThe most common way to access Cohere’s large language models (LLMs) is through the Cohere platform, which is fully managed by Cohere and accessible through an API.\\nBut that’s not the only way to access Cohere’s models. In an enterprise setting, companies might require more control over where and how the models are hosted. The good news is that Cohere provides enterprises with flexibility and choice around how they want to deploy the models.\\nIn this module, Cohere on AWS, you’ll learn how to deploy Cohere’s LLMs through Amazon Web Services (AWS). In this first chapter of the module, you’ll get an overview of Amazon Bedrock, AWS’s fully managed service for deploying foundational models (FMs).\\nCohere's Model Deployment Options\\nSpecifically, Cohere offers four types of deployment options:\\nCohere’s SaaS Platform and API\\nThis is the fastest and easiest way to start using Cohere’s models. The models are hosted on Cohere infrastructure and available on our public SaaS platform (which provides an API data opt-out), which is fully managed by Cohere.\\nCloud AI Services (Managed)\\nThese managed services enable enterprises to access Cohere’s models while easily integrating the service and dealing with sensitive data. In this scenario, Cohere’s models are hosted on the cloud provider’s infrastructure. Cohere is cloud-agnostic, meaning you can deploy our models through any cloud provider. Examples include Amazon Bedrock, Amazon SageMaker, Google Vertex AI, OCI Generative AI, and Azure AI.\\nPrivate Deployment - Cloud\"),\n",
       " Document(metadata={'title': 'Introduction to Cohere on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-bedrock'}, page_content=\"Private Deployment - Cloud\\nPrivate deployments (cloud) allow enterprises to deploy the Cohere stack privately on cloud platforms. With AWS, Cohere’s models can be deployed in an enterprise’s AWS Cloud environment via their own VPC (EC2, EKS).\\nPrivate Deployment - On-Premises\\nPrivate deployments (on-premises) allow enterprises to deploy the Cohere stack privately on their own compute.\\nAmazon Bedrock (introduced in this chapter) and Amazon SageMaker (introduced in the next chapter) fall under the second category (Cloud AI Services).\\nAn overview of Cohere’s four deployment options\\nWhat Is Amazon Bedrock?\\nAmazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs).\\nUsing Amazon Bedrock, you can customize the models with your data using techniques such as fine-tuning and retrieval-augmented generation (RAG), and build agents that execute tasks (tool use) using your enterprise systems and data sources.\\nAmazon Bedrock is serverless, so you don't have to manage any infrastructure. You can securely integrate generative AI capabilities into your applications using the AWS services that you are already familiar with.\\nFeatures of Amazon Bedrock\\nThe following are some key features that come with Bedrock.\\nModel fine-tuning\\nBedrock allows companies to adapt Cohere’s models to meet their individual needs. It enables developers to fine-tune a model’s performance and optimize it for specific use cases, thus empowering you to build and scale generative AI applications.\\nData security and privacy\"),\n",
       " Document(metadata={'title': 'Introduction to Cohere on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-bedrock'}, page_content='Data security and privacy\\nAmazon Bedrock helps to ensure that your data stays under your control. When you fine-tune a foundation model, we base it on a private copy of that model. This means that your data is not shared with model providers, and it is not used to improve the base models. You can use AWS PrivateLink to establish private connectivity from your Amazon Virtual Private Cloud (VPC) to Amazon Bedrock without exposing your VPC to internet traffic. Finally, Bedrock is in scope for common compliance standards, including ISO, SOC, and CSA STAR Level 2, it is HIPAA eligible, and customers can use Bedrock in compliance with the GDPR.\\nProvisioned throughput\\nBedrock makes it easy to get started with Cohere’s models without worrying about the underlying infrastructure as that is handled by AWS. That said, if you want greater control over a model’s performance, you can opt for the provisioned throughput option. When you configure Provisioned Throughput for a model, you receive a level of throughput at a fixed cost.\\nKnowledge bases\\nThis fully managed capability helps you implement the entire retrieval-augmented generation (RAG) workflow, from ingestion to retrieval and prompt augmentation, without having to build custom integrations to data sources and manage data flows. Session context management is built in, so your app can readily support multi-turn conversations.\\nGuardrails\\nGuardrails for Amazon Bedrock evaluates user inputs and FM responses based on use case-specific policies, and it provides an additional layer of safeguards regardless of the underlying FM. You can create multiple guardrails, each configured with a different combination of controls, and use them across different applications and use cases.\\nModel evaluation\\nAmazon Bedrock supports model evaluation jobs. The results of a model evaluation job allow you to compare model outputs and then choose the model best suited for your downstream generative AI applications.'),\n",
       " Document(metadata={'title': 'Introduction to Cohere on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-bedrock'}, page_content='Note: the list of features will continue to evolve and expand. To keep up with the latest, visit the Bedrock website.\\nCohere Models on Amazon Bedrock\\nList of Models\\nThe most up-to-date list of available Cohere’s models is on the Bedrock website, but here is the list at the time of writing:\\nGenerative models:\\nCommand R+\\nCommand R\\nCommand\\nCommand Light\\nEmbeddings models:\\nEmbed (English)\\nEmbed (Multilingual)\\nPricing Mechanism\\nThe pricing for Cohere models on Bedrock at the time of writing falls under two types:\\nOn-demand and batch\\nThis applies to default Cohere models on Bedrock. Pricing is based on tokens and broken down into:\\nPrice per 1,000 input tokens\\nPrice per 1,000 output tokens\\nCustomization (fine-tuning)\\nThis applies to fine-tuned Cohere models on Bedrock. Pricing is broken down into:\\nPrice to train 1,000 tokens\\nPrice to store each custom model per month\\nPrice to infer from a custom model per model unit per hour (with no-commit Provisioned Throughput pricing)\\nTo get the most updated pricing mechanism with further details, visit the Bedrock pricing page.\\nExample Use Cases\\nThe following are some example use cases that you can build on top of Cohere’s models on Bedrock.\\nGenerative AI: Build applications that can write product descriptions, help draft emails, suggest example press releases, and much more.\\nChat: Combine the power of text generation with a conversational interface. Build powerful chatbots and interactive knowledge assistants that give users accurate answers from connected enterprise data, citing sources if required.\\nSemantic Search and Retrieval: Easily build powerful search solutions in English and over 100 other languages using Cohere’s industry-leading Embed model. Cohere provides the Embed model, connectors to common enterprise data sources, and customization tools to maximize search relevance by domain.'),\n",
       " Document(metadata={'title': 'Introduction to Cohere on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-bedrock'}, page_content='RAG: Enhance the performance of generative AI applications by enabling a model to access and utilize supplementary information from external documents, thereby improving the accuracy of its responses. The RAG approach significantly reduces the hallucination issue common in LLMs.\\nGetting Started with Amazon Bedrock\\nWe’ll start implementing actual use cases in later chapters, but for now, let’s first get set up with Amazon Bedrock.\\nStep 1: Set Up Amazon Bedrock\\nTo get started, follow the steps outlined on the Bedrock page, and in particular, be sure to do the following:\\nSign up for an AWS account\\nCreate an administrative user\\nGrant programmatic access\\nGet console and playground access\\nAdd model access\\nStep 2: Choose Your Models\\nAfter completing the step above, you’ll reach the page where you can select models to access. To access Cohere’s models, locate the list of Cohere’s models and select the ones you want.\\nA screenshot of Bedrock’s model selection page\\nStep 3: Test on the Playground\\nIn Step 1, you established console and playground access. The Bedrock playground provides a console environment that allows you to experiment with running inference on different models and with different configurations before deciding to use them in your application. You can test out the models, such as Command, and get a feel for their capabilities. Enter a prompt, adjust the configurations (if needed), and you’ll get the output generated.\\nA screenshot of the Bedrock playground\\nConclusion\\nThis chapter introduced Cohere’s deployment options: its SaaS platform, Cloud AI platforms, virtual private cloud, and private deployments. We also introduced Amazon Bedrock, a fully managed service by AWS that hosts foundational models, including Cohere’s LLMs. We also went through the steps to get set up with Bedrock.\\nIn Chapter 2, you’ll learn about another way to access Cohere’s models on AWS: Amazon SageMaker.'),\n",
       " Document(metadata={'title': 'Introduction to Cohere on Amazon SageMaker', 'source': 'https://cohere.com/llmu/co-aws-sagemaker'}, page_content='Introduction to Cohere on Amazon SageMakerQualified\\nContents\\nIntroduction\\nFeatures of Amazon SageMaker\\nCohere Models on Amazon SageMaker \\nList of Models\\nPricing Mechanism\\nGetting Started with Amazon SageMaker\\nConclusion\\nIntroduction\\nIn the previous chapter, we discussed using Amazon Bedrock to access Cohere’s models. Amazon SageMaker provides another option.\\nAmazon SageMaker is a fully managed service where you can build, train, and deploy ML models at scale using tools like notebooks, debuggers, profilers, pipelines, MLOps, and more — all in one integrated development environment (IDE).\\nFor each of the available models, Cohere ships to Amazon Elastic Container Registry (AWS ECR) the elements required to run a model replica. This includes a container image and the model weights.\\nYou can subscribe to Cohere’s models via AWS Marketplace. Once subscribed, you can create a new inference endpoint deployment from SageMaker. This will create a deployment of a model replica and an associate compute instance that runs the replica within your VPC. The model will be available for communication through your inference endpoint.\\nWhen you use a VPC interface endpoint, communication between your VPC and the SageMaker API or Runtime is conducted entirely and securely within an AWS network.\\nAn overview of the Amazon SageMaker solution\\nDifferences Between Amazon Bedrock and SageMaker\\nWhat is the difference between SageMaker and Bedrock? While Bedrock is a platform that’s focused on foundational models (FMs), SageMaker is a platform that caters to a much broader range of machine learning (ML) models.\\nAnother key difference is that Bedrock offers a simpler way to get started using Cohere’s models, while SageMaker offers greater control over the underlying infrastructure hosting the models.\\nThe table below summarizes the key differences between Bedrock and SageMaker.\\nModuleTitleColab'),\n",
       " Document(metadata={'title': 'Introduction to Cohere on Amazon SageMaker', 'source': 'https://cohere.com/llmu/co-aws-sagemaker'}, page_content='The table below summarizes the key differences between Bedrock and SageMaker.\\nModuleTitleColab\\nScope\\tFocused on foundation models and generative AI applications\\tCaters to a much broader range of machine learning models and applications\\t\\nEase of Use\\tSimple API for foundation models, easy to integrate and experiment\\tRelatively more involved setup requiring more steps to get started\\t\\nControl\\tLimited control over infrastructure and model updates, managed entirely by AWS\\tGreater control over model versions, endpoints, and security configurations\\t\\nPricing\\tPer-token pricing based on tokens processed, as well as separate compute/storage time used for fine-tuning\\tPer-hour pricing based on software and infrastructure licensing\\t\\nFeatures of Amazon SageMaker\\nSageMaker offers a wide range of services for building, training, and deploying machine learning models. It provides fully managed infrastructure, tools, and workflows to help engineers and data scientists.\\nHere are some of the relevant features in the context of working with foundational models:\\nSageMaker Studio: A single, web-based interface for end-to-end ML development, SageMaker Studio provides a fully integrated development environment (IDE) for machine learning, including a visual interface for all ML development activities.\\nSageMaker Jumpstart: An ML hub that forms a part of the SageMaker Studio experience, SageMaker Jumpstart accelerates ML with built-in algorithms, pre-trained models, and pre-built solutions for common use cases, enabling one-click deployment. Jumpstart also allows users to share ML models and notebooks within their organizations.\\nAutomatic scaling: SageMaker supports automatic scaling for hosted models, dynamically adjusting the number of instances provisioned based on workload changes to optimize resource usage.\\nData labeling: SageMaker Ground Truth is a service that helps build high-quality training datasets for machine learning models by combining human and automatic labeling.'),\n",
       " Document(metadata={'title': 'Introduction to Cohere on Amazon SageMaker', 'source': 'https://cohere.com/llmu/co-aws-sagemaker'}, page_content=\"Data labeling: SageMaker Ground Truth is a service that helps build high-quality training datasets for machine learning models by combining human and automatic labeling.\\nCohere Models on Amazon SageMaker\\nList of Models\\nAt the time of writing, the following Cohere models are available on SageMaker.\\nGenerative models:\\nCommand R+\\nCommand R\\nCommand\\nCommand Light\\nEmbeddings models:\\nEmbed v3 (English & Multilingual)\\nEmbed Light v3 (English & Multilingual)\\nRerank 2 and 3 (English & Multilingual)\\nClassification (English & Multilingual)\\nTo get the most updated list of available models on SageMaker, visit Cohere’s listing on the AWS Marketplace.\\nPricing Mechanism\\nThe pricing for Cohere models on SageMaker is broken down into two portions:\\nSoftware: Hourly pricing that can vary by instance type\\nInfrastructure: Hourly pricing that can vary by region and instance type\\nAdditionally, you can choose from two inference options — real-time inference and batch transform — each with different pricing.\\nGetting Started with Amazon SageMaker\\nStep 1: Set Up Amazon SageMaker\\nTo get set up with SageMaker, follow the steps outlined in the Cohere documentation, which take you through the prerequisites and getting access to Cohere models.\\nIn order to successfully subscribe to Cohere’s offerings on SageMaker, you’ll need the right identity and access management (IAM) permissions, which is described in the documentation linked above.\\nStep 2: Get Model Access\\nThe guide above also includes the steps for subscribing to Cohere’s models.\\nStep 3: Test on SageMaker Studio\\nYou can test out the models on SageMaker Studio, which offers a hosted environment in which to access the models on Jupyter notebooks. To do this, follow these steps:\\nIn the AWS Console, go to Amazon SageMaker and click Studio.\\nThen, click Open Studio. If you don't see this option, you must first set up a SageMaker domain.\\nA new JupyterLab tab will open. Look for Prebuilt and automated solutions and click JumpStart.\"),\n",
       " Document(metadata={'title': 'Introduction to Cohere on Amazon SageMaker', 'source': 'https://cohere.com/llmu/co-aws-sagemaker'}, page_content=\"Then, click Open Studio. If you don't see this option, you must first set up a SageMaker domain.\\nA new JupyterLab tab will open. Look for Prebuilt and automated solutions and click JumpStart.\\nA list of models will appear. In the Foundation Models: Text Generation category, look for Cohere Command and then click View notebook.\\nThis will open up a sample notebook to get started with the model.\\nConclusion\\nThis chapter explored Amazon SageMaker, another platform that hosts Cohere's models. Unlike Amazon Bedrock, which focuses on foundational models, SageMaker caters to a wide range of machine learning models, providing greater control over infrastructure. We also looked at SageMaker's features, including SageMaker Studio, SageMaker Jumpstart, automatic scaling, and data labeling. The chapter concluded with a guide on getting started with SageMaker, including setting up, subscribing to Cohere's models, and testing on SageMaker Studio.\\nIn Chapter 3, you’ll learn how to use Cohere Command on Amazon Bedrock for text generation use cases.\"),\n",
       " Document(metadata={'title': 'Text Generation Using Cohere Command on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-generation'}, page_content=\"Text Generation Using Cohere Command on Amazon BedrockQualified\\nContents\\nIntroduction\\nCode Walkthrough \\nSetup\\nCreate Contextual Information\\nCreate the Function to Generate Text\\nText Generation\\nText Summarization\\nText Rewriting\\nText Extraction\\nBuilding a Chatbot\\nConclusion\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nIntroduction\\nLarge language models (LLMs) enable a long list of text generation use cases. For instance, in business writing, they can assist with creating compelling narratives or drafting precise reports. LLMs are also useful in answering questions accurately and efficiently, drawing from a vast knowledge base. Additionally, they can summarize vast amounts of information, making it easier to glean key insights. Another noteworthy application is rewriting text to, for example, enhance readability or alter the tone to suit different audiences.\\nCohere's Command family of models, available on Amazon Bedrock, are powerful LLMs that offer these capabilities. This includes cutting-edge models like Command R and Command R+, which we’ll be using in our code examples in this chapter.\\nFor example, a major luxury goods retailer wanted to help its client advisors, reducing the time advisors needed to find and research products. It engaged Cohere to create a tool that could help advisors suggest products to customers based on demographics, purchase history, and personal preferences.\\nUsing retrieval-augmented generation (RAG), Cohere Command and Rerank were integrated into the customer’s website, product catalog, inventory, and customer service apps to build a conversational virtual shopping assistant.\\nImplementing a virtual shopping assistant at a major luxury goods retailer\\nIn this chapter, we'll explore how to use Cohere's Command R+ model on Amazon Bedrock. For the most updated list of Cohere models available on Amazon Bedrock, visit the Amazon Bedrock website.\\nCode Walkthrough\"),\n",
       " Document(metadata={'title': 'Text Generation Using Cohere Command on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-generation'}, page_content='Code Walkthrough\\nLet’s walk through how to use Command R+ to power a range of tasks in a customer support agent scenario, including text generation, summarization, rewriting, and extraction. We’ll also use the Chat endpoint to build a simple chatbot that can maintain the memory of a conversation that runs over multiple turns.\\nAn overview of what we\\'ll cover in the code walkthrough\\nSetup\\nFirst, let\\'s install and import the necessary libraries and set up our Cohere client using the cohere SDK. To use Bedrock, we create a BedrockClient by passing the necessary AWS credentials.\\nimport cohere co = cohere.BedrockClient( aws_region=\"YOUR_AWS_REGION\", aws_access_key=\"YOUR_AWS_ACCESS_KEY_ID\", aws_secret_key=\"YOUR_AWS_SECRET_ACCESS_KEY\", aws_session_token=\"YOUR_AWS_SESSION_TOKEN\", ) \\nCreate Contextual Information\\nBefore we begin, let\\'s create some context to use in our text generation tasks. In this example, we\\'ll use a set of technical support frequently asked questions (FAQs) as our context.'),\n",
       " Document(metadata={'title': 'Text Generation Using Cohere Command on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-generation'}, page_content='Before we begin, let\\'s create some context to use in our text generation tasks. In this example, we\\'ll use a set of technical support frequently asked questions (FAQs) as our context.\\n# Technical support FAQ faq_tech_support = \"\"\"- Question: How do I set up my new smartphone with my mobile plan? - Answer: - Insert your SIM card into the device. - Turn on your phone and follow the on-screen setup instructions. - Connect to your mobile network and enter your account details when prompted. - Download and install any necessary apps or updates. - Contact customer support if you need further assistance. - Question: My internet connection is slow. How can I improve my mobile data speed? - Answer: - Check your signal strength and move to an area with better coverage. - Restart your device and try connecting again. - Ensure your data plan is active and has sufficient data. - Consider upgrading your plan for faster speeds. - Question: I can\\'t connect to my mobile network. What should I do? - Answer: - Check your SIM card is inserted correctly and not damaged. - Restart your device and try connecting again. - Ensure your account is active and not suspended. - Check for any network outages in your area. - Contact customer support for further assistance. - Question: How do I set up my voicemail? - Answer: - Dial your voicemail access number (usually provided by your carrier). - Follow the prompts to set up your voicemail greeting and password. - Record your voicemail greeting and save it. - Test your voicemail by calling your number and leaving a message. - Question: I\\'m having trouble sending text messages. What could be the issue? - Answer: - Check your signal strength and move to an area with better coverage. - Ensure your account has sufficient credit or an active plan. - Restart your device and try sending a message again. - Check your message settings and ensure they are correct. - Contact customer support if the issue persists.\"\"\" \\nCreate the Function to Generate Text'),\n",
       " Document(metadata={'title': 'Text Generation Using Cohere Command on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-generation'}, page_content='Create the Function to Generate Text\\nNow, let\\'s define a function to generate text using the Command R+ model on Bedrock. We’ll use this function a few times throughout.\\nThis function takes a user message and generates the response via the chat endpoint. We also define the model as cohere.command-r-plus-v1:0. Note that this is how the model is named on Bedrock, compared to command-r-plus for the same model via the native Cohere platform.\\ndef generate_text(message): response = co.chat(message=message, model=\"cohere.command-r-plus-v1:0\") return response \\nText Generation\\nLet’s look at the first use case, which is in the broad category of text generation. Given a prompt, the model will generate a corresponding response.\\nSuppose we have a customer support agent who responds to customer inquiries using the help of an LLM to craft accurate responses. For this, we pass the technical support FAQs as context to the prompt for the LLM. We then structure the prompt to include the instruction, context (FAQs), and the customer inquiry.\\nWe then run the generate_text function on the prompt and get the response. The textual content of the response is stored in the response.text field.\\ninquiry = \"I\\'ve noticed some fluctuations in my mobile network\\'s performance recently.The connection seems stable most of the time, but every now and then, I experience brief periods of slow data speeds. It happens a few times a day and is quite inconvenient.\" prompt = f\"\"\"Use the FAQs below to provide a concise response to this customer inquiry. # Customer inquiry {inquiry} # FAQs {faq_tech_support}\"\"\" response = generate_text(prompt) print(response.text)\\nAnd here’s the response. It uses the information available from the FAQs and adds its own style for responding.'),\n",
       " Document(metadata={'title': 'Text Generation Using Cohere Command on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-generation'}, page_content='And here’s the response. It uses the information available from the FAQs and adds its own style for responding.\\nResponse to customer inquiry: Brief slowdowns in data speed can be frustrating. Here are some quick steps to improve your mobile data performance: 1. Check your signal strength: Move to an area with better coverage if needed. 2. Restart your device: A simple restart can often improve connectivity. 3. Data plan status: Ensure your data plan is active and you have sufficient data allocated for faster speeds. 4. Consider a plan upgrade: If the issue persists, consider contacting your provider to discuss faster speed plans. Additionally, checking for any network outages in your area and ensuring your SIM card is inserted correctly can also help maintain a stable connection. If the issue continues, reach out to your mobile provider\\'s customer support for further guidance and troubleshooting. \\nText Summarization\\nAnother type of use case is text summarization. Now, let\\'s summarize the customer inquiry into a single sentence. We add an instruction to the prompt and then pass the inquiry to the prompt.\\nprompt=f\"\"\"Summarize this customer inquiry into one short sentence. Inquiry: {inquiry}\"\"\" response = generate_text(prompt) print(response.text) \\nAnd here’s the response. As specified in the instruction, it generates a one-sentence summary about the inquiry.\\nI experience intermittent periods of slow data speeds a few times a day on my mobile network. \\nText Rewriting\\nAnother useful use case is rewriting or transforming a piece of text to suit a different need. Typically, we want to modify some aspects of the text while maintaining its overall meaning.\\nIn our example, we want to turn the response from the customer support agent, which is done via chat, into an email format that can be sent to the customer as a follow-up. It follows a similar format for building the prompt: specifying the instruction followed by adding the necessary context.'),\n",
       " Document(metadata={'title': 'Text Generation Using Cohere Command on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-generation'}, page_content='prompt=f\"\"\"Rewrite this customer support agent response into an email format, ready to send to the customer. If you\\'re experiencing brief periods of slow data speeds or difficulty sending text messages and connecting to your mobile network, here are some troubleshooting steps you can follow: 1. Check your signal strength - Move to an area with better coverage. 2. Restart your device and try connecting again. 3. Ensure your account is active and not suspended. 4. Contact customer support for further assistance. (This can include updating your plan for better network performance.) Did these steps help resolve the issue? Let me know if you need further assistance.\"\"\" response = generate_text(prompt) print(response.text) \\nAnd here’s the response, which is an email version of the customer support agent’s response.\\nSubject: Troubleshooting Guide for Slow Data and Network Issues Dear [Customer Name], I hope this email finds you well. I understand that experiencing slow data speeds and network connectivity issues can be frustrating. Here are some detailed troubleshooting steps to help resolve these problems: - Signal Strength: - Please check your device\\'s signal strength by looking at the signal bars displayed on your screen. - If you\\'re in an area with weak coverage, try moving to a different location with better reception. You can also try going outdoors or near a window to improve signal strength. - Device Restart: - Simply restart your device. This step can often resolve minor glitches and connectivity issues. Turn your device off, wait for a few seconds, and ... (truncated for brevity)'),\n",
       " Document(metadata={'title': 'Text Generation Using Cohere Command on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-generation'}, page_content='With text extraction, we can pull out certain pieces of information from a document. In our example, we want to generate a support ticket and populate it with information about the interaction, such as the category of inquiry, product type, and the ticket status. Additionally, we can ask the model to generate the output in a specific format, such as markdown, JSON, HTML, and others. In our example, we ask the model to generate a JSON string.\\nprompt=f\"\"\"Extract the following as a JSON from the text provided below: - category (Options: technical support, promotions, billing) - product (Options: broadband, mobile, TV) - status (Options: open, closed) # Customer Inquiry {inquiry} # Customer Support Agent\\'s Response If you\\'re experiencing brief periods of slow data speeds or difficulty sending text messages and connecting to your mobile network, here are some troubleshooting steps you can follow: 1. Check your signal strength - Move to an area with better coverage. 2. Restart your device and try connecting again. 3. Ensure your account is active and not suspended. 4. Contact customer support for further assistance. (This can include updating your plan for better network performance.) Did these steps help resolve the issue? Let me know if you need further assistance.\"\"\" response = generate_text(prompt) print(response.text) \\nAnd here’s the response, formatted in JSON as instructed.\\n{ \"category\": \"technical support\", \"product\": \"mobile\", \"status\": \"open\" } \\nBuilding a Chatbot\\nAll our examples so far run on a single-turn interaction. But with the Chat endpoint, we can build a chatbot that keeps a memory of all previous interactions. This allows developers to build chatbot-style applications that maintain the state of a conversation.'),\n",
       " Document(metadata={'title': 'Text Generation Using Cohere Command on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-generation'}, page_content='The following is an implementation of a simple chatbot in a customer support setting, where the chatbot acts as a helpful customer support agent. We create a function called run_chatbot to implement these steps and print out the key events and messages along the way. This function also optionally accepts the chat history as an argument to keep the state in a multi-turn conversation.\\nFor this, we introduce a couple of additional parameters to the Chat endpoint:\\npreamble: A preamble contains instructions to help steer a chatbot’s response toward specific characteristics, such as a persona, style, or format. Here, we are using a simple preamble: “You are a helpful customer support agent that assists customers of a mobile network service.”\\nchat_history: We store the history of a conversation between a user and the chatbot as a list, append every new conversation turn, and pass this information to the next endpoint call.\\n# Define a preamble preamble = \"\"\"## Task and Context You are a helpful customer support agent that assists customers of a mobile network service.\"\"\" # Run the chatbot def run_chatbot(message, chat_history=[]): response = co.chat(message=message, model=\"cohere.command-r-plus-v1:0\", preamble=preamble, chat_history=chat_history) print(response.text) chat_history = response.chat_history return chat_history\\nAnd here’s an example conversation that runs over a few turns.\\nTurn #1:\\nchat_history = run_chatbot(\"Hi. I\\'ve noticed some fluctuations in my mobile network\\'s performance recently.\") \\nI\\'m sorry to hear that you\\'ve been experiencing issues with your mobile network\\'s performance. Can you tell me a little more about the specific problems you\\'ve been facing? For example, are you experiencing dropped calls, slow data speeds, or something else entirely? The more information you can provide, the better I\\'ll be able to assist you in troubleshooting the issue.\\nTurn #2:'),\n",
       " Document(metadata={'title': 'Text Generation Using Cohere Command on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-generation'}, page_content='Turn #2:\\nchat_history = run_chatbot(\"At times, the data speed is very poor. What should I do?\", chat_history)\\nI\\'m sorry to hear that you\\'re experiencing issues with your data speed. Here are a few troubleshooting steps you can try to improve your mobile data performance: - Check your device\\'s signal: Weak or fluctuating signals can lead to poor data speeds. Try moving to a different location or adjusting your device\\'s position to see if that improves your signal strength. - Restart your device: Sometimes, ... (truncated for brevity)\\nTurn #3:\\nchat_history = run_chatbot(\"Thanks, I\\'ll try these.\", chat_history)\\nYou\\'re welcome! If you have any other questions or concerns, feel free to reach out. I\\'m here to help. \\nHere\\'s what is contained in the chat history after a few turns.\\nprint(\"Chat history:\") for turn in chat_history: print(turn, \"\\\\n\")'),\n",
       " Document(metadata={'title': 'Text Generation Using Cohere Command on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-generation'}, page_content='Here\\'s what is contained in the chat history after a few turns.\\nprint(\"Chat history:\") for turn in chat_history: print(turn, \"\\\\n\")\\nChat history: message=\"Hi. I\\'ve noticed some fluctuations in my mobile network\\'s performance recently.\" tool_calls=None role=\\'USER\\' message=\"I\\'m sorry to hear that you\\'ve been experiencing issues with your mobile network\\'s performance. Can you tell me a little more about the specific problems you\\'ve been facing? For example, are you experiencing dropped calls, slow data speeds, or something else entirely? The more information you can provide, the better I\\'ll be able to assist you in troubleshooting the issue.\" tool_calls=None role=\\'CHATBOT\\' message=\\'At times, the data speed is very poor. What should I do?\\' tool_calls=None role=\\'USER\\' message=\"I\\'m sorry to hear that you\\'re experiencing issues with your data speed. Here are a few troubleshooting steps you can try to improve your mobile data performance: \\\\n\\\\n- Check your device\\'s signal: Weak or fluctuating signals can lead to poor data ... (truncated for brevity) \" tool_calls=None role=\\'CHATBOT\\' message=\"Thanks, I\\'ll try these.\" tool_calls=None role=\\'USER\\' message=\"You\\'re welcome! If you have any other questions or concerns, feel free to reach out. I\\'m here to help.\" tool_calls=None role=\\'CHATBOT\\' \\nConclusion\\nIn this chapter, we explored how to use Command R+ on Amazon Bedrock for various text generation and manipulation tasks. We covered text generation, summarization, rewriting, and extraction, demonstrating how these capabilities can be applied to real-world scenarios.\\nIf you want to learn more LLM use cases, visit our LLMU chapter on use case patterns.\\nIn Chapter 4, you’ll learn how to perform semantic search using the Cohere Embed model on Amazon Bedrock.'),\n",
       " Document(metadata={'title': 'Semantic Search Using Cohere Embed on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-search'}, page_content=\"Semantic Search Using Cohere Embed on Amazon BedrockQualified\\nContents\\nIntroduction\\nCode Walkthrough \\nSetup\\nDownload Dataset\\nPre-Process Dataset\\nEmbed and Index Documents\\nSend Query and Retrieve Documents\\nConclusion\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nIntroduction\\nEnterprises have a lot of data at their disposal, but much of it is hard to tap into. This is because a good chunk of this data is unstructured, and digging into it to find insights isn't straightforward. Traditional methods, like keyword matching, can only do so much and they often fail to capture the full context and depth of a document.\\nThis is where we introduce the concept of text embeddings. Essentially, text embeddings are numerical representations created by language models that convert text into vectors. They capture and encode the context of a document. These vectors store a wealth of context about the documents they represent, opening up the possibility of various applications, from semantic search and retrieval-augmented generation (RAG) to topic modeling and text classification.\\nCohere's Embed model, available on Amazon Bedrock, is a powerful text embeddings model that offers these capabilities. This model supports over 100 languages and is unique among text embedding models due to its emphasis on document quality for applications like semantic search.\\nFor example, a global network security company wanted to build an AI assistant to help its IT professionals respond to queries on network security setup and security best practices. Its security policy documentation repository includes tens of thousands of articles consisting of unstructured data, and its legacy search tools often produce inaccurate results.\"),\n",
       " Document(metadata={'title': 'Semantic Search Using Cohere Embed on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-search'}, page_content='They built the AI assistant with retrieval-augmented generation (RAG). They used Cohere Embed for semantic search to generate embeddings of the security policy documentation and Cohere Rerank to further improve retrieval accuracy by selecting the most relevant documents for answer generation. The team then fine-tuned Cohere Command to optimize how it answered user questions.\\nImplementing an AI assistant at a global network security company\\nIn this chapter, we\\'ll explore how to use Cohere\\'s Embed model on Amazon Bedrock. At the time of writing, users can choose between two models on Amazon Bedrock: Embed English and Embed Multilingual which covers 100+ languages. For the most updated list of models, visit the Amazon Bedrock website.\\nCode Walkthrough\\nUsing Cohere Embed in a financial services scenario, let’s build an application that can perform semantic search across financial news in different languages.\\nFinancial analysts need to digest a lot of content, such as financial publications and news media, to stay informed. A semantic search system helps analysts quickly search across numerous article titles in multiple languages for the most relevant articles, saving an enormous amount of time and effort. Unlike traditional keyword-based search, semantic search goes beyond matching exact terms and instead focuses on understanding the underlying concepts and relationships between words and phrases.\\nAn overview of what we\\'ll cover in the code walkthrough\\nSetup\\nFirst, let\\'s install and import the necessary libraries and set up our Cohere client using the cohere SDK. To use Bedrock, we create a BedrockClient by passing the necessary AWS credentials.\\n# ! pip install cohere pandas hnswlib -q \\nimport pandas as pd import hnswlib import re import cohere co = cohere.BedrockClient( aws_region=\"YOUR_AWS_REGION\", aws_access_key=\"YOUR_AWS_ACCESS_KEY_ID\", aws_secret_key=\"YOUR_AWS_SECRET_ACCESS_KEY\", aws_session_token=\"YOUR_AWS_SESSION_TOKEN\", ) \\nDownload Dataset'),\n",
       " Document(metadata={'title': 'Semantic Search Using Cohere Embed on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-search'}, page_content='Download Dataset\\nWe use a dataset (MultiFIN) containing a list of real-world article headlines covering 15 languages (English, Turkish, Danish, Spanish, Polish, Greek, Finnish, Hebrew, Japanese, Hungarian, Norwegian, Russian, Italian, Icelandic, and Swedish). This is an open-source dataset curated for financial natural language processing (NLP) and is available on a GitHub repository.\\nIn our case, we’ve created a CSV file with MultiFIN’s data, as well as a column with translations. We don’t use this column to feed the model; we use it to help us follow along when we print the results for those who don’t speak Danish or Spanish. We point to that CSV to create our dataframe.\\nurl = \"https://raw.githubusercontent.com/cohere-ai/cohere-aws/main/notebooks/bedrock/multiFIN_train.csv\" df = pd.read_csv(url) # Inspect dataset df.head(5) \\nPre-Process Dataset\\nMultiFIN has over 6,000 records in 15 different languages. For our example use case, we focus on three languages: English, Spanish, and Danish.\\nFor this, we’ll need to do some pre-processing steps. First, we remove the duplicates, remove the languages other than the three we need, and pick the top 80 articles for demonstration purposes.\\n# Ensure there is no duplicated text in the headers def remove_duplicates(text): return re.sub(r\\'((\\\\b\\\\w+\\\\b.{1,2}\\\\w+\\\\b)+).+\\\\1\\', r\\'\\\\1\\', text, flags=re.I) df [\\'text\\'] = df[\\'text\\'].apply(remove_duplicates) # Keep only selected languages languages = [\\'English\\', \\'Spanish\\', \\'Danish\\'] df = df.loc[df[\\'lang\\'].isin(languages)] # Pick the top 80 longest articles df[\\'text_length\\'] = df[\\'text\\'].str.len() df.sort_values(by=[\\'text_length\\'], ascending=False, inplace=True) top_80_df = df[:80] # Language distribution top_80_df[\\'lang\\'].value_counts() \\nEmbed and Index Documents'),\n",
       " Document(metadata={'title': 'Semantic Search Using Cohere Embed on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-search'}, page_content='Embed and Index Documents\\nNow, we want to embed our documents and store the embeddings. The embeddings are very large vectors that encapsulate the semantic meaning of our document. In particular, we use Cohere’s embed-multilingual-v3.0 model, which creates embeddings with 1,024 dimensions.\\nWith the v3.0 embeddings models, we need to specify the input_type parameter to indicate the nature of the document. In semantic search applications, this is either search_document, which is for the documents to search, or search_query, which is for the search query that we’ll define later.\\nWe also keep track of the language and translation of the document to enrich the display of the results.\\nNext, we create a search index using the hnsw vector library. This stores the embeddings in an index, which makes searching the documents more efficient.\\n# Embed documents docs = top_80_df[\\'text\\'].to_list() docs_lang = top_80_df[\\'lang\\'].to_list() translated_docs = top_80_df[\\'translation\\'].to_list() #for reference when returning non-English results doc_embs = co.embed(texts=docs, model=\"cohere.embed-multilingual-v3\", input_type=\\'search_document\\').embeddings # Create a search index index = hnswlib.Index(space=\\'ip\\', dim=1024) index.init_index(max_elements=len(doc_embs), ef_construction=512, M=64) index.add_items(doc_embs, list(range(len(doc_embs)))) \\nSend Query and Retrieve Documents\\nNext, we build a function that takes a query as input, embeds it, and finds the four headers more closely related to it.'),\n",
       " Document(metadata={'title': 'Semantic Search Using Cohere Embed on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-search'}, page_content='Send Query and Retrieve Documents\\nNext, we build a function that takes a query as input, embeds it, and finds the four headers more closely related to it.\\n# Retrieval of 4 closest docs to query def retrieval(query): # Embed query and retrieve results query_emb = co.embed(texts=[query], model=\"cohere.embed-multilingual-v3\", input_type=\"search_query\").embeddings doc_ids = index.knn_query(query_emb, k=3)[0][0] # we will retrieve 4 closest neighbors # Print and append results print(f\"QUERY: {query.upper()} \\\\n\") retrieved_docs, translated_retrieved_docs = [], [] for doc_id in doc_ids: # Append results retrieved_docs.append(docs[doc_id]) translated_retrieved_docs.append(translated_docs[doc_id]) # Print results print(f\"ORIGINAL ({docs_lang[doc_id]}): {docs[doc_id]}\") if docs_lang[doc_id] != \"English\": print(f\"TRANSLATION: {translated_docs[doc_id]} \\\\n----\") else: print(\"----\") print(\"END OF RESULTS \\\\n\\\\n\") return retrieved_docs, translated_retrieved_docs \\nLet’s now try to query the index with a couple of examples, one each in English and Danish.\\nqueries = [ \"Can data science help meet sustainability goals?\", # English example \"Hvor kan jeg finde den seneste danske boligplan?\" # Danish example - \"Where can I find the latest Danish property plan?\" ] for query in queries: retrieval(query) \\nHere’s the response for the English query, showing semantic search in action. Notice how the retrieval system was able to surface documents similar in meaning, i.e., data science vs. AI. Keyword-based search systems would not be able to capture this.'),\n",
       " Document(metadata={'title': 'Semantic Search Using Cohere Embed on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-search'}, page_content='QUERY: CAN DATA SCIENCE HELP MEET SUSTAINABILITY GOALS? ORIGINAL (English): Using AI to better manage the environment could reduce greenhouse gas emissions, boost global GDP by up to 38m jobs by 2030 ---- ORIGINAL (English): Quality of business reporting on the Sustainable Development Goals improves, but has a long way to go to meet and drive targets. ---- ORIGINAL (English): Only 10 years to achieve Sustainable Development Goals but businesses remain on starting blocks for integration and progress ---- END OF RESULTS \\nHere’s the response for the Danish query. This example highlights the ability to perform cross-lingual searches with the Embed Multilingual model. You can enter a query in one language and get relevant search results in other languages.\\nAnother observation here is that the English acronym “PP&E” stands for “property, plant, and equipment,” and the model was able to connect it to the query.\\nQUERY: HVOR KAN JEG FINDE DEN SENESTE DANSKE BOLIGPLAN? ORIGINAL (Danish): Nyt fra CFOdirect: Ny PP&E-guide, FAQs om den nye leasingstandard, podcast om udfordringerne ved implementering af leasingstandarden og meget mere TRANSLATION: New from CFOdirect: New PP&E guide, FAQs on the new leasing standard, podcast on the challenges of implementing the leasing standard and much more ---- ORIGINAL (Danish): Lovforslag fremlagt om rentefri lån, udskudt frist for lønsumsafgift, førtidig udbetaling af skattekredit og loft på indestående på skattekontoen TRANSLATION: Bills presented on interest -free loans, deferred deadline for payroll tax, early payment of tax credit and ceiling on the balance in the tax account ---- ORIGINAL (Danish): Nyt fra CFOdirect: Shareholder-spørgsmål til ledelsen, SEC cybersikkerhedsguide, den amerikanske skattereform og meget mere TRANSLATION: New from CFOdirect: Shareholder questions for management, the SEC cybersecurity guide, US tax reform and more ---- END OF RESULTS \\nConclusion'),\n",
       " Document(metadata={'title': 'Semantic Search Using Cohere Embed on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-search'}, page_content=\"Conclusion\\nSemantic search applications, enabled by text embeddings, offer a significantly more effective approach to retrieving and analyzing information. Cohere's Embed model can do this across over 100 languages. Its application in fields like financial analysis, as demonstrated in this chapter, shows how it can transform data retrieval and processing tasks, saving time and improving accuracy.\\nIn Chapter 5, we’ll switch to Amazon SageMaker and look at an example using the Cohere Rerank model on SageMaker.\"),\n",
       " Document(metadata={'title': 'What Are Word and Sentence Embeddings?', 'source': 'https://cohere.com/llmu/sentence-word-embeddings'}, page_content='What Are Word and Sentence Embeddings?What Are Word and Sentence Embeddings?\\nWord and sentence embeddings are the bread and butter of language models. This chapter shows a very simple introduction to what they are.\\nWhat Are Word and Sentence Embeddings? - YouTube\\nIn old futuristic movies, such as the 2001 Space Odyssey, the main computer (HAL) was able to talk to humans and understand what they would say with great ease. At the time, getting computers to understand and produce language seemed like an impossible task, but the latest large language models (LLM) are able to do this in a way that makes it almost impossible for a human to tell if they are talking to another human, or to a computer.\\nThe quintessential task of natural language processing (NLP) is to understand human language. However, there is a big disconnection there. Humans speak in words and sentences, but computers only understand and process numbers. How can we turn words and sentences into numbers in a coherent way? An assignment of words to numbers is called a word embedding. We can think of a word embedding as an assignment of scores to the words, with some nice properties (that we’ll learn soon).\\nWhat is a Word Embedding?\\nBefore we get into what is a word embedding, let me test your intuition. In the figure underneath (Quiz 1), I have located 12 words in the plane. The words are the following:\\nBanana\\nBasketball\\nBicycle\\nBuilding\\nCar\\nCastle\\nCherry\\nHouse\\nSoccer\\nStrawberry\\nTennis\\nTruck\\nNow, the question is, where would you locate the word “Apple” in this plane? There are many places it could go, but I’m allowing 3 possibilities labeled A, B, and C.'),\n",
       " Document(metadata={'title': 'What Are Word and Sentence Embeddings?', 'source': 'https://cohere.com/llmu/sentence-word-embeddings'}, page_content='Soccer\\nStrawberry\\nTennis\\nTruck\\nNow, the question is, where would you locate the word “Apple” in this plane? There are many places it could go, but I’m allowing 3 possibilities labeled A, B, and C.\\nWhat I would do, is locate it in point C, because it would make sense to have the word “Apple” close to the words “Banana”, “Strawberry”, and “Cherry”, and far from the other words such as “House”, “Car”, or “Tennis”. This is precisely a word embedding. And what are the numbers we are assigning to each word? Simply the horizontal and vertical coordinates of the location of the word. In this way, the word “Apple” is assigned to the numbers [5,5], and the word “Bicycle” to the coordinates [5,1].\\nFor the sake of redundancy, let’s enumerate some properties that a nice word embedding should have:\\nWords that are similar should correspond to points that are close by (or equivalently, to scores that are similar).\\nWords that are different should correspond to points that are far away (or equivalently, to scores that are significantly different).\\nWord Embeddings Capture Features of the Word\\nThe word embedding above satisfies properties 1 and 2. Is that it? Not yet. There is something more to these word embeddings, and it is that they don’t only capture word similarity, but they also capture other properties of the language. In language, words can be combined to get more complicated concepts. In mathematics, numbers can be added or subtracted to get other numbers. Could we build a word embedding that captures relations between words, as relations between numbers?\\nLet’s look at four words, “Puppy”, “Dog”, “Calf”, and “Cow”. These words are clearly correlated. Now to test your intuition again, I’m going to locate the words “Puppy”, “Dog”, and “Calf” in the plane, and I’ll ask you to add the word “Cow”. Where would you add it, in the spot labeled A, B, or C?'),\n",
       " Document(metadata={'title': 'What Are Word and Sentence Embeddings?', 'source': 'https://cohere.com/llmu/sentence-word-embeddings'}, page_content='While it would make sense to locate it in A, closer to “Calf” since they are both bovine, or in B, since it’s an adult animal, like “Dog”, the place where I would put this is in spot C, with coordinates [3,4]. Why? Because the rectangle formed by the four words captures some very important relationships between them. For instance, two analogies are captured here. The analogy “A puppy is to a dog like a calf is to a cow” can be translated into “The path from the word puppy to the word dog is the same as the path from the word calf to the word cow”. The analogy “A dog is to a cow like a puppy is to a calf” is also captured in this rectangle, as it’s shown in the figure below.\\nHowever, this is not even the tip of the iceberg. The main property of word embeddings that is in effect here is that the two axes (vertical and horizontal) represent different things. If you look carefully, moving towards the right turns the puppy into a dog, and the calf into a cow, which is an increase in age. Likewise, moving upwards turns a puppy into a calf and a dog into a cow, which is an increase in the size of the animal. It seems that this embedding is understanding that the words in it have two main properties, or features: age and size. Furthermore, it seems that the embedding is locating age in the horizontal axis and size in the vertical axis. In that case, where would you imagine that the word “whale” goes? Probably somewhere above the word “cow”. And if there was a word for “really old dog”? That word would go somewhere to the right of the word “dog”.'),\n",
       " Document(metadata={'title': 'What Are Word and Sentence Embeddings?', 'source': 'https://cohere.com/llmu/sentence-word-embeddings'}, page_content='A good word embedding would be able to capture not only age and size, but also many more features of the words. Since each feature is one new axis, or coordinate, then a good embedding must have many more than two coordinates assigned to every word. One of the Cohere embeddings, for example, has 1024 coordinates associated with each word. These rows of 1024 (or however many) coordinates are called vectors, so we often talk about the vector corresponding to a word, and to each of the numbers inside a vector as a coordinate. Some of these coordinates may represent important properties of the word, such as age, gender, size. Some may represent combinations of properties. But some others may represent obscure properties that a human may not be able to understand. But all in all, a word embedding can be seen as a good way to translate human language (words) into computer language (numbers), so that we can start training machine learning models with these numbers.\\nSentence embeddings\\nSo word embeddings seem to be pretty useful, but in reality, human language is much more complicated than simply a bunch of words put together. Human language has structure, sentences, etc. How would one be able to represent, for instance, a sentence? Well, here’s an idea. How about the sums of scores of all the words? For example, say we have a word embedding that assigns the following scores to these words:\\nNo: [1,0,0,0]\\nI: [0,2,0,0]\\nAm: [-1,0,1,0]\\nGood: [0,0,1,3]\\nThen the sentence “No, I am good!” corresponds to the vector [0,2,2,3]. However, the sentence “I am no good” will also correspond to the vector [0,2,2,3]. This is not a good thing, since the computer understand these two sentences in the exact same way, yet they are quite different, almost opposite sentences! Therefore, we need better embeddings that take into account the order of the words, the semantics of the language, and the actual meaning of the sentence.'),\n",
       " Document(metadata={'title': 'What Are Word and Sentence Embeddings?', 'source': 'https://cohere.com/llmu/sentence-word-embeddings'}, page_content=\"This is where sentence embeddings come into play. A sentence embedding is just like a word embedding, except it associates every sentence with a vector full of numbers, in a coherent way. By coherent, I mean that it satisfies similar properties as a word embedding. For instance, similar sentences are assigned to similar vectors, different sentences are assigned to different vectors, and most importantly, each of the coordinates of the vector identifies some (whether clear or obscure) property of the sentence.\\nThe Cohere embedding does just this. Using transformers, attention mechanisms, and other cutting edge algorithms, this embedding sends every sentence to a vector formed by 4096 numbers, and this embedding works really well. As a small example, here is a heatmap of an embedding containing 10 entries for each sentence, for several sentences (writing the entire 4096 entries will take too much space, so we compressed it using a dimensionality reduction algorithm called Principal Component Analysis.\\nNotice that these sentences are all very similar. In particular, the three highlighted sentences pretty much have the same meaning. If you look at their corresponding vectors, these are also really similar. That is exactly what an embedding should do.\\nHow to Use These Embeddings?\\nNow that you’ve learned how useful these embeddings are, it’s time to start playing with them and finding good practical uses for them! The Cohere dashboard provides a very friendly interface to use them. Here is a small example, with the following phrases:\\nI like my dog\\nI love my dog\\nI adore my dog\\nHello, how are you?\\nHey, how's it going?\\nHi, what's up?\\nI love watching soccer\\nI enjoyed watching the world cup\\nI like watching soccer matches\\nTo see the results of the sentence embedding, go to the “Embed” tab in the Cohere dashboard, and type the sentences (click here for an embed demo you can play with).\"),\n",
       " Document(metadata={'title': 'What Are Word and Sentence Embeddings?', 'source': 'https://cohere.com/llmu/sentence-word-embeddings'}, page_content='I like watching soccer matches\\nTo see the results of the sentence embedding, go to the “Embed” tab in the Cohere dashboard, and type the sentences (click here for an embed demo you can play with).\\nThe results come out as vectors with 4096 entries for each sentence. These are obviously hard to visualize, but there is a way to bring them down to 2 entries per sentence in order to be easily visualized. This visualization is shown in the plot below.\\nNotice that the embedding seemed to capture the essence of the sentences, and there are 3 clear clusters of sentences. In the top left corner you find the sentences that greet a person, in the middle, those that talk about a person’s dog, and in the bottom right corner, those that talk about soccer. Notice that sentences such as “Hey what’s up” and “Hello, how are you?” have no words in common, yet the model can tell that they have the same meaning.\\nMultilingual Sentence Embeddings\\nMost word and sentence embeddings are dependent on the language that the model is trained on. If you were to try to fit the French sentence “Bonjour, comment ça va?” (meaning: hello, how are you?) in the embedding from the previous section, it will struggle to understand that it should be close to the sentence “Hello, how are you?” in English. For the purpose of unifying many languages into one, and being able to understand text in all these languages, Cohere has trained a large multilingual model, that has showed wonderful results with more than 100 languages. Here is a small example, with the following sentences in English, French, and Spanish.\\nThe bear lives in the woods\\nEl oso vive en el bosque\\nL’ours vit dans la foret\\nThe world cup is in Qatar\\nEl mundial es en Qatar\\nLa coupe du monde est au Qatar\\nAn apple is a fruit\\nUna manzana es una fruta\\nUne pomme est un fruit\\nEl cielo es azul\\nThe sky is blue\\nLe ciel est bleu\\nThe model returned the following embedding.'),\n",
       " Document(metadata={'title': 'What Are Word and Sentence Embeddings?', 'source': 'https://cohere.com/llmu/sentence-word-embeddings'}, page_content='La coupe du monde est au Qatar\\nAn apple is a fruit\\nUna manzana es una fruta\\nUne pomme est un fruit\\nEl cielo es azul\\nThe sky is blue\\nLe ciel est bleu\\nThe model returned the following embedding.\\nNotice that the model managed to identify the sentences about the bear, soccer, an apple, and the sky, even if they are in different languages.\\nConclusion\\nWord and sentence embeddings are the bread and butter of LLMs. They are the basic building block of most language models, since they translate human speak (words) into computer speak (numbers) in a way that captures many relations between words, semantics, and nuances of the language, into equations regarding the corresponding numbers.\\nSentence embeddings can be extended to language embeddings, in which the numbers attached to each sentence are language-agnostic. These models are very useful for translation and for searching and understanding text in different languages.'),\n",
       " Document(metadata={'title': 'Reranking Using Cohere Rerank on Amazon SageMaker', 'source': 'https://cohere.com/llmu/co-aws-rerank'}, page_content=\"Reranking Using Cohere Rerank on Amazon SageMakerQualified\\nContents\\nHow Cohere Rerank Works\\nCode Walkthrough \\nSetup\\nCreate Endpoint\\nRetrieve Documents\\nRerank Documents\\nView Results\\nConclusion\\nWe’ll use Cohere’s Python SDKs (cohere and cohere-aws) for the code examples. Follow along in this notebook.\\nIntroduction\\nReranking is an essential technique in information retrieval systems, especially in large-scale search applications. It is a process of reordering a set of initially retrieved documents based on their relevance to a user's query.\\nWith the vast amount of information available, reranking helps refine and improve the quality of search results, ensuring that the most relevant and useful documents are presented to the user. Cohere’s reranking models offer a powerful solution to enhance the search experience.\\nWhat’s great about reranking is that while it gives a huge boost to search results, implementing Cohere’s Rerank models requires adding only one line of code to any existing search system, whether it’s a semantic search system or a traditional search system that uses keyword-based approaches.\\nFor example, a major provider of digital workforce collaboration and productivity tools found that their existing search tools did not perform well across customer-generated knowledge bases, and the large volume of new customer-generated content meant embedding was not viable.\\nCohere Rerank was integrated with the customer’s search systems. Rerank used output from existing search tools and reordered the results for use by the company’s Q&A app to provide relevant and more accurate answers.\\nImplementing a reranking system at a provider of digital workforce collaboration and productivity tools\\nHow Cohere Rerank Works\"),\n",
       " Document(metadata={'title': 'Reranking Using Cohere Rerank on Amazon SageMaker', 'source': 'https://cohere.com/llmu/co-aws-rerank'}, page_content='Implementing a reranking system at a provider of digital workforce collaboration and productivity tools\\nHow Cohere Rerank Works\\nThe Cohere Rerank endpoint takes a query and a response and outputs a relevance score between them. This enables the retrieval of the most relevant documents to a particular query. For example, when searching for \"Regulatory approval\", the initial results may include documents that only tangentially mention the topic. However, by applying Rerank, the top results are those with the highest relevance scores, directly answering the query.\\nRerank improves the ordering of documents based on relevance to a query\\nRerank 3 is Cohere’s latest reranking model, which provides state-of-the-art capabilities for enterprise search, including:\\n4k context length to significantly improve search quality for longer documents\\nAbility to search over multi-aspect and semi-structured data, such as emails, invoices, JSON documents, code, and tables\\nMultilingual coverage of 100+ languages\\nImproved latency and lower total cost of ownership (TCO)\\nIn this chapter, we\\'ll explore how to use the Cohere Rerank endpoint on Amazon SageMaker. To see the most updated list of available models on SageMaker, visit Cohere’s listing on the AWS Marketplace.\\nCode Walkthrough\\nLet’s look at an example of a multi-aspect search on semi-structured data, and walk through how to perform reranking on email data that contains multiple fields: “title” and “content.”\\nAn overview of what we\\'ll cover in the code walkthrough\\nSetup\\nTo get set up with SageMaker, follow the steps outlined in the Cohere documentation, which takes you through the prerequisites and getting access to Cohere models.\\nLet’s now install and import the necessary libraries and set up our Cohere client.\\nWe’ll need to create a SageMaker endpoint that exposes access to a Cohere model (Rerank v3 in our case). For this, we’ll use the cohere_aws SDK which makes it easy to set up the endpoint, together with AWS’s boto3 library.'),\n",
       " Document(metadata={'title': 'Reranking Using Cohere Rerank on Amazon SageMaker', 'source': 'https://cohere.com/llmu/co-aws-rerank'}, page_content='# ! pip install cohere cohere-aws boto3\\nimport os import boto3 import cohere import cohere_aws from cohere_aws import Client \\nOnce the endpoint is created (as we’ll walk through later), we can access it using the Cohere SDK. To do this, let’s create a SagemakerClient by passing the necessary AWS credentials.\\nimport cohere # Create SageMaker client via the native Cohere SDK # Contact your AWS administrator for the credentials co = cohere.SagemakerClient( aws_region=\"YOUR_AWS_REGION\", aws_access_key=\"YOUR_AWS_ACCESS_KEY_ID\", aws_secret_key=\"YOUR_AWS_SECRET_ACCESS_KEY\", aws_session_token=\"YOUR_AWS_SESSION_TOKEN\", ) # For creating an endpoint, you need to use the cohere_aws client: Set environment variables with the AWS credentials os.environ[\\'AWS_ACCESS_KEY_ID\\'] = \"YOUR_AWS_ACCESS_KEY_ID\" os.environ[\\'AWS_SECRET_ACCESS_KEY\\'] = \"YOUR_AWS_SECRET_ACCESS_KEY\" os.environ[\\'AWS_SESSION_TOKEN\\'] = \"YOUR_AWS_SESSION_TOKEN\" \\nThe next step is to define the model package Amazon Resource Names (ARN) for the Rerank model. The ARN is an identifying string for a SageMaker resource, and it varies between the regions where a resource is available.\\nHere, we define the Cohere package for the Rerank model and map the model package against each region, which gives the complete ARN for each region.'),\n",
       " Document(metadata={'title': 'Reranking Using Cohere Rerank on Amazon SageMaker', 'source': 'https://cohere.com/llmu/co-aws-rerank'}, page_content='Here, we define the Cohere package for the Rerank model and map the model package against each region, which gives the complete ARN for each region.\\n# Create SageMaker endpoint via the cohere_aws SDK cohere_package = \"cohere-rerank-english-v3-01-d3687e0d2e3a366bb904275616424807\" model_package_map = { \"us-east-1\": f\"arn:aws:sagemaker:us-east-1:865070037744:model-package/{cohere_package}\", \"us-east-2\": f\"arn:aws:sagemaker:us-east-2:057799348421:model-package/{cohere_package}\", \"us-west-1\": f\"arn:aws:sagemaker:us-west-1:382657785993:model-package/{cohere_package}\", \"us-west-2\": f\"arn:aws:sagemaker:us-west-2:594846645681:model-package/{cohere_package}\", \"ca-central-1\": f\"arn:aws:sagemaker:ca-central-1:470592106596:model-package/{cohere_package}\", \"eu-central-1\": f\"arn:aws:sagemaker:eu-central-1:446921602837:model-package/{cohere_package}\", \"eu-west-1\": f\"arn:aws:sagemaker:eu-west-1:985815980388:model-package/{cohere_package}\", \"eu-west-2\": f\"arn:aws:sagemaker:eu-west-2:856760150666:model-package/{cohere_package}\", \"eu-west-3\": f\"arn:aws:sagemaker:eu-west-3:843114510376:model-package/{cohere_package}\", \"eu-north-1\": f\"arn:aws:sagemaker:eu-north-1:136758871317:model-package/{cohere_package}\", \"ap-southeast-1\": f\"arn:aws:sagemaker:ap-southeast-1:192199979996:model-package/{cohere_package}\", \"ap-southeast-2\": f\"arn:aws:sagemaker:ap-southeast-2:666831318237:model-package/{cohere_package}\", \"ap-northeast-2\": f\"arn:aws:sagemaker:ap-northeast-2:745090734665:model-package/{cohere_package}\", \"ap-northeast-1\": f\"arn:aws:sagemaker:ap-northeast-1:977537786026:model-package/{cohere_package}\", \"ap-south-1\": f\"arn:aws:sagemaker:ap-south-1:077584701553:model-package/{cohere_package}\", \"sa-east-1\": f\"arn:aws:sagemaker:sa-east-1:270155090741:model-package/{cohere_package}\", } region = boto3.Session().region_name if region not in model_package_map.keys(): raise Exception(\"UNSUPPORTED REGION\") model_package_arn = model_package_map[region]'),\n",
       " Document(metadata={'title': 'Reranking Using Cohere Rerank on Amazon SageMaker', 'source': 'https://cohere.com/llmu/co-aws-rerank'}, page_content='The ARN for each model is available in the Marketplace once you subscribe to the model and proceed to configure it. The screenshot below shows the Rerank 3 English listing as an example.\\nRerank 3 English model listing on the AWS Marketplace\\nCreate Endpoint\\nWith SageMaker, we’ll need to create an endpoint via an AWS instance. The marketplace listing provides more details, including pricing, on the recommended instance type for a particular model.\\nTo create the endpoint, we define:\\narn: The model package ARN we defined in the previous step\\nendpoint_name: A name we can give as an identifier\\ninstance_type: The instance type to be used\\nn_instances: The number of instances\\nWe pass the arguments to the create_endpoint method from the cohere_aws library.\\nco_aws = Client(region_name=region) co_aws.create_endpoint(arn=model_package_arn, endpoint_name=\"my-rerank-v3\", instance_type=\"ml.g5.xlarge\", n_instances=1) \\nThe following output indicates that the process of creating an endpoint is complete.\\nRetrieve Documents\\nLet’s assume that the first stage of retrieval has already been performed, whether it’s through a semantic, keyword, or any other type of search system.\\nHere we have a list of nine documents that represent the search results of that first stage. Each document has two fields, Title and Content, corresponding to the contents of an email. Each email is a dictionary containing these fields that preserve its semi-structured format, which the Rerank endpoint can use.'),\n",
       " Document(metadata={'title': 'Reranking Using Cohere Rerank on Amazon SageMaker', 'source': 'https://cohere.com/llmu/co-aws-rerank'}, page_content='documents = [ {\"Title\":\"Incorrect Password\",\"Content\":\"Hello, I have been trying to access my account for the past hour and it keeps saying my password is incorrect. Can you please help me?\"}, {\"Title\":\"Confirmation Email Missed\",\"Content\":\"Hi, I recently purchased a product from your website but I never received a confirmation email. Can you please look into this for me?\"}, {\"Title\":\"Questions about Return Policy\",\"Content\":\"Hello, I have a question about the return policy for this product. I purchased it a few weeks ago and it is defective.\"}, {\"Title\":\"Customer Support is Busy\",\"Content\":\"Good morning, I have been trying to reach your customer support team for the past week but I keep getting a busy signal. Can you please help me?\"}, {\"Title\":\"Received Wrong Item\",\"Content\":\"Hi, I have a question about my recent order. I received the wrong item and I need to return it.\"}, {\"Title\":\"Customer Service is Unavailable\",\"Content\":\"Hello, I have been trying to reach your customer support team for the past hour but I keep getting a busy signal. Can you please help me?\"}, {\"Title\":\"Return Policy for Defective Product\",\"Content\":\"Hi, I have a question about the return policy for this product. I purchased it a few weeks ago and it is defective.\"}, {\"Title\":\"Wrong Item Received\",\"Content\":\"Good morning, I have a question about my recent order. I received the wrong item and I need to return it.\"}, {\"Title\":\"Return Defective Product\",\"Content\":\"Hello, I have a question about the return policy for this product. I purchased it a few weeks ago and it is defective.\"} ]\\nRerank Documents\\nTo use the endpoint, we now use the cohere SDK. Adding a reranking component is simple with Cohere Rerank. It takes just one line of code to implement.\\nCalling the Rerank endpoint requires the following arguments:\\ndocuments: The list of documents, which we defined in the previous section\\nquery: The user query; we’ll use \\'What emails have been about refunds?\\' as an example'),\n",
       " Document(metadata={'title': 'Reranking Using Cohere Rerank on Amazon SageMaker', 'source': 'https://cohere.com/llmu/co-aws-rerank'}, page_content='documents: The list of documents, which we defined in the previous section\\nquery: The user query; we’ll use \\'What emails have been about refunds?\\' as an example\\nrank_fields: The list of fields that the data contains, which in our case are Title and Content\\ntop_n: The number of documents we want to be returned, sorted from the most to the least relevant document\\nmodel: The endpoint_name we defined earlier, which in our case is my-rerank-v3\\nquery = \\'What emails have been about refunds?\\' response = co.rerank(documents=documents, query=query, rank_fields=[\"Title\",\"Content\"], top_n=3, model=\"my-rerank-v3\")\\nView Results\\nSince we defined top_n=3, we’ll get the top three most relevant documents to the query. For each document, the response contains the index of its position in the original list and its relevance score against the query.\\nLet’s print the results and see the outcome.\\nprint(\"Documents\",\"\\\\n\") for idx,doc in enumerate(response.results): print(f\"#{idx+1}:\\\\n{documents[doc.index]}\\\\n\")\\nThe search query was looking for emails about refunds. But none of the documents mention the word “refunds” specifically.\\nHowever, the Rerank model was able to retrieve the right documents. Some of the documents mentioned the word “return.” The Rerank model can capture semantically similar meanings between two pieces of text, so it can return documents that mention return instead, which has a very similar meaning to return.\\nDocuments #1: {\\'Title\\': \\'Questions about Return Policy\\', \\'Content\\': \\'Hello, I have a question about the return policy for this product. I purchased it a few weeks ago and it is defective.\\'} #2: {\\'Title\\': \\'Return Policy for Defective Product\\', \\'Content\\': \\'Hi, I have a question about the return policy for this product. I purchased it a few weeks ago and it is defective.\\'} #3: {\\'Title\\': \\'Return Defective Product\\', \\'Content\\': \\'Hello, I have a question about the return policy for this product. I purchased it a few weeks ago and it is defective.\\'}'),\n",
       " Document(metadata={'title': 'Reranking Using Cohere Rerank on Amazon SageMaker', 'source': 'https://cohere.com/llmu/co-aws-rerank'}, page_content=\"Important Note: You will continue to incur charges as long as an endpoint is running, so remember to delete it when your usage ends.\\nco_aws.delete_endpoint() co_aws.close() \\nConclusion\\nReranking is a valuable technique used in information retrieval systems to enhance the relevance of search results. Cohere's Rerank endpoint, including its latest model, Rerank 3, offers improved capabilities for enterprise search.\\nBy incorporating reranking with a single line of code, as shown in our example in this chapter, the model successfully identified semantically similar documents, even when specific keywords were absent from the query. This example highlights the potential benefits of integrating reranking into existing search systems to enhance search accuracy and user satisfaction.\\nIn Chapter 6, we’ll explore how to build a retrieval-augmented generation (RAG) application using Chat and Embed on Amazon Bedrock and Rerank on Amazon SageMaker.\"),\n",
       " Document(metadata={'title': 'What is Similarity Between Sentences?', 'source': 'https://cohere.com/llmu/what-is-similarity-between-sentences'}, page_content='What is Similarity Between Sentences?What is Similarity Between Sentences?\\nWhat is Similarity Between Sentences? - YouTube\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nFor large language models, it is crucial to know when two words, or two sentences, are similar or different. This can be a hard problem, but luckily, word and sentence embeddings are very helpful for this task. In this chapter, we go over some different notions of similarity.\\nSimilarity between text\\nKnowing if two words are similar or different is a very important task for every large language model. An even harder problem is knowing if two different sentences are similar or different. Luckily, word and sentence embeddings are very useful for this task.\\nIn the previous chapter, I explained the concept of word embeddings. In a nutshell, a word embedding is an assignment of a list of numbers (vector) to every word, in a way that semantic properties of the word translate into mathematical properties of the numbers. What do we mean by this? For example, two similar words will have similar vectors, and two different words will have different vectors. But most importantly, each entry in the vector corresponding to a word keeps track of some property of the word. Some of these properties can be understandable to humans, such as age, size, gender, etc., but some others could potentially only be understood by the computer. Either way, we can benefit from these embeddings for many useful tasks.'),\n",
       " Document(metadata={'title': 'What is Similarity Between Sentences?', 'source': 'https://cohere.com/llmu/what-is-similarity-between-sentences'}, page_content='Sentence embeddings are even more powerful, as they assign a vector of numbers to each sentence, in a way that these numbers also carry important properties of the sentence. One of the Cohere embeddings assigns a vector of length 4096 (i.e., a list of 4096 numbers) to each sentence. Furthermore, multilingual embedding does this for sentences in more than 100 languages. In this way, the sentence “Hello, how are you?” and its corresponding French translation, “Bonjour, comment ça va?” will be assigned very similar numbers, as they have the same semantic meaning.\\nNow that we know embeddings quite well, let’s move on to using them to find similarities. There are two types of similarities we’ll define in this post: dot product similarity and cosine similarity. Both are very similar and very useful to determine if two words (or sentences) are similar.\\nDot Product Similarity\\nLet’s start with a small example of sentence embedding. For simplicity, let’s consider a dataset of 4 sentences, all movie titles, and an embedding of dimension 2, meaning that each sentence is assigned to two numbers. Let’s say that the embedding is the following:\\nYou’ve Got Mail: [0, 5]\\nRush Hour: [6, 5]\\nRush Hour 2: [7, 4]\\nTaken: [7, 0]\\nLet’s take a closer look at these scores. Would they mean anything? As mentioned before, these scores sometimes mean something that humans can understand, and other times they don’t. In this case, notice that the first score is 0 for You’ve Got Mail, but high for all the other movies. Is there a feature that these three movies have, and that You’ve Got Mail doesn’t? I can think of one: being an action movie. Similarly, the second score is high for You’ve Got Mail, Rush Hour, and Rush Hour 2, but low for Taken. What could this property be? Comedy seems to be one. Therefore, in our embedding, it could well be that the first score is the amount of action in the movie, and the second score is the amount of comedy. The following table represents the embedding.'),\n",
       " Document(metadata={'title': 'What is Similarity Between Sentences?', 'source': 'https://cohere.com/llmu/what-is-similarity-between-sentences'}, page_content='MovieScore 1 (Action)Score 2 (Comedy)\\nYou’ve got mail\\t0\\t5\\t\\nRush Hour\\t6\\t5\\t\\nRush Hour 2\\t7\\t4\\t\\nTaken\\t7\\t0\\t\\nNow, imagine that we want to find the similarities between these movies. In particular, how similar would you say Taken is from You’ve Got Mail? How similar is Rush Hour to Rush Hour 2? In my opinion, Taken and You’ve Got Mail are very different, and Rush Hour and Rush Hour 2 are very similar. We now need to create a similarity score that is low for the pair [You’ve Got Mail, Taken], and high for the pair [Rush Hour, Rush Hour 2].\\nHere is one way to create this similarity score. Notice that if two movies are similar, then they must have similar action scores and similar comedy scores. So if we multiply the two action scores, then multiply the two comedy scores, and add them, this number would be high if the scores match. On the other hand, if the scores don’t match very well, the similarity score would be lower. This operation is called the dot product. Let’s see how it works for the two pairs of movies.\\nDot product for the pair [You’ve got mail, Taken] = 0*7 + 5*0 = 0\\nDot product for the pair [Rush Hour, Rush Hour 2] = 6*7 + 5*4 = 62\\nThis matches our intuition since we were expecting a low similarity for the first pair, and a high similarity for the second pair.\\nCosine Similarity\\nAnother measure of similarity between sentences (and words) is to look at the angle between them. For example, let’s plot the movie embedding in the plane, where the horizontal axis represents the action score, and the vertical axis represents the comedy score. The embedding looks like this.'),\n",
       " Document(metadata={'title': 'What is Similarity Between Sentences?', 'source': 'https://cohere.com/llmu/what-is-similarity-between-sentences'}, page_content='Notice that You’ve Got Mail is quite far from Taken, which makes sense since they are very different movies. Furthermore, Rush Hour and Rush Hour 2 are very close, as they are similar movies. So Euclidean distance (the length of the line between the points) is a good measure for similarity. We need to tweak it a little bit, since we want a measure of similarity that is high for sentences that are close to each other, and low for sentences that are far away from each other. Distance does the exact opposite. So in order to tweak this metric, let’s look at the angle between the rays from the origin (the point with coordinates [0,0]), and each sentence. Notice that this angle is small if the points are close to each other, and large if the points are far away from each other. Now we need the help of another function, the cosine. The cosine of angles close to zero is close to 1, and as the angle grows, the cosine decreases. This is exactly what we need. Therefore, we define the cosine distance as the cosine of the angle formed by the two rays going from the origin, to the two sentences.\\nNotice that in the ongoing example, the angle between the movies You’ve Got Mail, and Taken, is 90 degrees, with a cosine of 0. Therefore, the similarity between them is 0. On the other hand, the angle between the movies Rush Hour and Rush Hour 2 is 11.31 degrees. Its cosine is 0.98, which is quite high. In fact, the similarity between a sentence and itself is always 1, as the angle is 0, with a cosine of 1.\\nReal-Life Example\\nOf course, this was a very small example. Let’s do a real-life example with the Cohere embedding.\\nTo set up, we first import several tools we\\'ll need.\\nimport numpy as np import seaborn as sns import altair as alt from sklearn.metrics.pairwise import cosine_similarity \\nWe also import the Cohere module and create a client.\\nimport cohere co = cohere.Client(\"COHERE_API_KEY\") # Your Cohere API key \\nConsider the following 3 sentences, stored in the Python list texts.'),\n",
       " Document(metadata={'title': 'What is Similarity Between Sentences?', 'source': 'https://cohere.com/llmu/what-is-similarity-between-sentences'}, page_content='We also import the Cohere module and create a client.\\nimport cohere co = cohere.Client(\"COHERE_API_KEY\") # Your Cohere API key \\nConsider the following 3 sentences, stored in the Python list texts.\\ntexts = [\"I like to be in my house\", \"I enjoy staying home\", \"the isotope 238u decays to 206pb\"] \\nOne would expect the two first sentences to have a high similarity score when compared to each other, and the third one to have a very low similarity score when compared to the other two.\\nTo get the corresponding sentence embeddings, we call the Embed endpoint with co.embed(). We supply three parameters:\\ntexts - our list of sentences\\nmodel - we use embed-english-v3.0, Cohere\\'s latest (at the time of writing) English-only embeddings model to generate the embeddings\\ninput_type - we use search_document to indicate that we intend to use the embeddings for search use-cases\\nYou\\'ll learn about these parameters in more detail in the LLMU Module on Text Representation.\\nresponse = co.embed( texts=texts, model=\\'embed-english-v3.0\\', input_type=\\'search_document\\' ) \\nThe embeddings are stored in the embeddings value of the response. After getting the embeddings, we separate them by sentence and print the values.\\nembeddings = response.embeddings [sentence1, sentence2, sentence3] = embeddings print(\"Embedding for sentence 1\", np.array(sentence1)) print(\"Embedding for sentence 2\", np.array(sentence2)) print(\"Embedding for sentence 3\", np.array(sentence3)) \\nThe results are as follows:\\nEmbedding for sentence 1 [ 0.04968262 0.03799438 -0.02963257 ... -0.0737915 -0.0079422 -0.01863098] Embedding for sentence 2 [ 0.043396 0.05401611 -0.02461243 ... -0.06216431 -0.0196228 -0.00948334] Embedding for sentence 3 [ 0.0243988 0.00712967 -0.04669189 ... -0.03903198 -0.02403259 0.01942444]'),\n",
       " Document(metadata={'title': 'What is Similarity Between Sentences?', 'source': 'https://cohere.com/llmu/what-is-similarity-between-sentences'}, page_content='Note that the embeddings are vectors (lists) of 1024 numbers, so they are truncated here (thus the dots in between). One would expect that the vectors corresponding to sentences 1 and 2 are similar to each other and that both are different from the vector corresponding to sentence 3. However, from inspection, this is not very clear. We need to calculate some similarities to see if this is the case.\\nDot Product Similarity\\nLet’s calculate the dot products between the three sentences. The following line of code will do it.\\nprint(\"Similarity between sentences 1 and 2:\", np.dot(sentence1, sentence2)) print(\"Similarity between sentences 1 and 3:\", np.dot(sentence1, sentence3)) print(\"Similarity between sentences 2 and 3:\", np.dot(sentence2, sentence3)) \\nAnd the results are:\\nSimilarity between sentences 1 and 2: 0.818827121924668 Similarity between sentences 1 and 3: 0.19770800712384107 Similarity between sentences 2 and 3: 0.19897217756830138 \\nThe similarity between sentences 1 and 2 (0.8188) is much larger than the similarities between the other pairs. This confirms our predictions.\\nJust for consistency, we also calculate the similarities between each sentence and itself, to confirm that a sentence and itself has the highest similarity score.\\nSimilarity between sentences 1 and 1: 0.9994656785851899 Similarity between sentences 2 and 2: 1.0006820582016114 Similarity between sentences 3 and 3: 1.0005095878377965 \\nThis checks out—the similarity between a sentence and itself is around 1, which is higher than all the other similarities.\\nCosine Similarity\\nNow let’s calculate the cosine similarities between them.\\nprint(\"Cosine similarity between sentences 1 and 2:\", cosine_similarity([sentence1], [sentence2])[0][0]) print(\"Cosine similarity between sentences 1 and 3:\", cosine_similarity([sentence1], [sentence3])[0][0]) print(\"Cosine similarity between sentences 2 and 3:\", cosine_similarity([sentence2], [sentence3])[0][0]) \\nThe results are the following:'),\n",
       " Document(metadata={'title': 'What is Similarity Between Sentences?', 'source': 'https://cohere.com/llmu/what-is-similarity-between-sentences'}, page_content='The results are the following:\\nCosine similarity between sentences 1 and 2: 0.818766792354783 Cosine similarity between sentences 1 and 3: 0.1977104790996451 Cosine similarity between sentences 2 and 3: 0.19885369669720415 \\nNext, we check the cosine similarity between each sentence and itself.\\nCosine similarity between sentences 1 and 1: 0.9999999999999998 Cosine similarity between sentences 2 and 2: 1.0000000000000004 Cosine similarity between sentences 3 and 3: 1.0000000000000004 \\nWe also plot the results in a grid.\\nThe similarity between each sentence and itself is 1 (the diagonal in the plot), which is consistent with our expectations. Furthermore, a sentence and itself represent the same point in space, which gives an angle of 0 with the origin, so it makes sense that the similarity is the cosine of 0, which is 1!\\nNotice that the dot product and cosine distance give nearly identical values. The reason for this is that the embedding is normalized (meaning each vector has norm equal to 1). When the embedding is not normalized, the dot product and cosine distance would give different values.\\nConclusion'),\n",
       " Document(metadata={'title': 'What is Similarity Between Sentences?', 'source': 'https://cohere.com/llmu/what-is-similarity-between-sentences'}, page_content='Conclusion\\nIn the previous chapter, we learned that sentence embeddings are the bread and butter of language models, as they associate each sentence with a particular list of numbers (a vector), in a way that similar sentences give similar vectors. We can think of embeddings as a way to locate each sentence in space (a high dimensional space, but a space nonetheless), in a way that similar sentences are located close by. Once we have each sentence somewhere in space, it’s natural to think of distances between them. And an even more intuitive way to think of distances is to think of similarities, i.e., a score assigned to each pair of sentences, which is high when these sentences are similar, and low when they are different. The similarity is a very useful concept in large language models, as it can be used for search, for translation, for summarization, and in many other applications. To learn more about these applications, stay tuned for the next article!'),\n",
       " Document(metadata={'title': 'What Is Attention in Language Models?', 'source': 'https://cohere.com/llmu/what-is-attention-in-language-models'}, page_content='What Is Attention in Language Models?What is Attention in Language Models? - YouTubeThe Attention Mechanism in Large Language Models - YouTubeThe math behind Attention: Keys, Queries, and Values matrices\\nWhat is Attention in Language Models? - YouTube\\nIn the previous chapters, you learned about word and sentence embeddings and similarity between words and sentences. In short, a word embedding is a way to associate words with lists of numbers (vectors) in such a way that similar words are associated with numbers that are close by, and dissimilar words with numbers that are far away from each other. A sentence embedding does the same thing, but associating a vector to every sentence. Similarity is a way to measure how similar two words (or sentences) are, by assigning large numbers to words (sentences) that are similar, and small numbers to those that are different.\\nHowever, word embeddings have a huge Achilles heel: words that have more than one definition. If a word embedding assigns a vector to, say, the word ‘bank’, it assigns the same vector to all the definitions of ‘bank’. What if you want to use this word in different contexts? Here is where attention comes into play. Self-attention was introduced in the seminal paper Attention is All you Need, written by several co-authors, including Cohere’s cofounder Aidan Gomez. Attention is a very clever way to tell words apart when they are used in different contexts (which turns word embeddings into contextualized word embeddings).\\nOne Word, Multiple Meanings\\nIn order to understand attention, let’s look at two sentences:\\nSentence 1: The bank of the river.\\nSentence 2: Money in the bank.'),\n",
       " Document(metadata={'title': 'What Is Attention in Language Models?', 'source': 'https://cohere.com/llmu/what-is-attention-in-language-models'}, page_content='One Word, Multiple Meanings\\nIn order to understand attention, let’s look at two sentences:\\nSentence 1: The bank of the river.\\nSentence 2: Money in the bank.\\nHow would a computer know that the word “bank” in the first sentence refers to a setting in nature, and in the second sentence to a financial setting? Well, let’s ask a simpler question: How would a human know this? How did you figure out these two settings? The way you and I did it was probably to look at the neighbouring words. In the first sentence, the word “river” was the one that hinted at the nature setting, and in the second sentence, the word “money” was key to the financial setting. So in short, we need a way to use the other words in the sentence to understand what context of the word “bank” we want to use.\\nHere is where word embeddings come into play. As you learned in a previous chapter, word embeddings are a way to assign vectors (lists of numbers) to each word. I like to imagine them geometrically. Imagine that the words “bank”, “river”, and “money” are all attached on a cork board. Furthermore, this cork board contains all the existing words, and in such a way that two words that are similar (such as “apple” and “pear”) are close by. Now, in this cork board, “bank”, “river”, and “money” are not exactly close by. However, what you do is take the word “bank”, and move it slightly towards the word “river”. Call this word “bank1”. Now, take another copy of the word “bank”, and move it slightly towards the word “money”. Call this one “bank2”. Now, consider the following two modified sentences.\\nModified sentence 1: The bank1 of the river.\\nModified sentence 2: Money in the bank2.'),\n",
       " Document(metadata={'title': 'What Is Attention in Language Models?', 'source': 'https://cohere.com/llmu/what-is-attention-in-language-models'}, page_content='Modified sentence 1: The bank1 of the river.\\nModified sentence 2: Money in the bank2.\\nIn these two sentences, the computer now knows a little more about the context of the word “bank”, as the word has been split into two distinct ones. One whose definition is closer to “river”, and another one whose definition is closer to “money”. That, in short, is how attention mechanisms work. However, there may be many questions lingering in your head. For example:\\nWhat do you mean by “moving a word closer to another one”?\\nWhy did you ignore the other words in the sentence? How did you know the words “river” and “money” were the ones dictating the context, instead of the words “the”, “in”, or “of”? As humans, we know which words provide context, but a computer wouldn’t have a clue.\\nAs computers only handle numbers, how do we attach numbers to all these methods?\\nAll these (and hopefully more!) questions will be answered next.\\nMoving Words on the Cork Board\\nFirst, let me tell you what I mean by “moving a word closer to another one”. The way I like to imagine this, is to average two words. For example, let’s say that I want to move the word “bank” 10% closer to the word river. I now think of the word “0.9_Bank + 0.1_River”. That is, “bank1” is 90% “bank”, and 10% “river”. Also, let’s say that “bank2” is 80% “bank” and 20% “money”. So let’s say these are the modified words:\\nBank1 = 0.9*Bank + 0.1*River\\nBank2 = 0.8*Bank + 0.2*Money'),\n",
       " Document(metadata={'title': 'What Is Attention in Language Models?', 'source': 'https://cohere.com/llmu/what-is-attention-in-language-models'}, page_content='Bank1 = 0.9*Bank + 0.1*River\\nBank2 = 0.8*Bank + 0.2*Money\\nHow did I come up with the numbers 0.9, 0.1, 0.8, and 0.2? That comes later (ok I have to spoil it, the answer is similarity, but I’ll elaborate later in this chapter). For now, you may be wondering what do I mean by 0.9*Bank + 0.1*River. Well, in the embeddings chapter, we learned that a word embedding consists of assigning a vector (list) of numbers to each word. The Cohere embedding associates each word with a vector of length 4096 (that is, a list of 4096 numbers per word). For simplicity, let’s imagine an embedding that associates a vector of two numbers to each word, and that the following are the numbers:\\nRiver: [0,5]\\nMoney: [8,0]\\nBank: [6,6]\\nThese two numbers can be interpreted as coordinates in the plane, where the first number is the horizontal coordinate, and the second one, the vertical coordinate. This gives a graphic like the one below.\\nSo in order to calculate the embeddings of Bank1 and Bank2, we simply do the math componentwise (that means, for each of the two components of the vector, separately). We get this:\\nBank1: 0.9*Bank + 0.1*River = 0.9*[6, 6] + 0.1*[0, 5]\\n= [5.4, 5.4] + [0, 0.5]\\n= [5.4, 5.9]\\nBank2: 0.8_Bank + 0.2_Money = 0.8*[6,6] + 0.2*[8,0]\\n= [4.8, 4.8] + [1.6, 0]\\n= [6.4, 4.8]\\nAs you can see, “bank1” is closer to “river”, and “bank2” is closer to “money”. As a matter of fact, “bank1” is on the line between “bank” and “river”, 10% along the way. Similarly, “bank2” is on the line between “bank” and “money”, 20% along the way.\\nThus, the attention mechanism managed to split the word “bank” into two words, and use each one in the corresponding sentence. You may still have some questions, however, such as the following one.\\nHow to Decide Which Words Determine Context?'),\n",
       " Document(metadata={'title': 'What Is Attention in Language Models?', 'source': 'https://cohere.com/llmu/what-is-attention-in-language-models'}, page_content=\"How to Decide Which Words Determine Context?\\nIn other words, why did I pick the words “river” and “money” instead of “the”, “of”, and “in”, in order to determine the context of “bank”. Obviously the answer is “because I’m human and I know the language”. But what can the computer do? It can rely on two mechanisms, one of them is a metric such as similarity, which you learned in the previous chapter. The second one is multi-head attention, which we'll talk about at the end of the chapter.\\nLet’s first discuss the similarity mechanism. What the computer is going to do is to consider all the words in the sentence as context, including irrelevant words such as “the”, “of”, and “in”. However, it’s going to consider them a certain amount, and that amount is precisely the similarity between the word, and “bank”. We trust that in a good embedding, the similarity between “bank” and a word such as “the” is almost zero, as they are unrelated. Therefore, the model will know to ignore these words, and focus on those that may have a higher similarity with the word “bank”.\\nBut let me add some numbers to this reasoning to make it more clear. Imagine that we calculate similarities for the words in each sentence, and we get the following:\\nThis similarity makes sense in the following ways:\\nThe similarity between each word and itself is 1.\\nThe similarity between any irrelevant word (“the”, “of”, etc.) and any other word is 0.\\nThe similarity between “bank” and “river” is 0.11.\\nThe similarity between “bank” and “money” is 0.25.\\nWhy is the similarity between “bank” and “money” higher than the similarity between “bank” and “river”. We can imagine that “bank” gets used more often in the same context as “money”, than as “river”, and that explains the difference. We are simplifying this model quite a bit, it could be that the similarity between “the” and “of” is not zero, but 0.001. However, to simplify our calculations, we’ll use these numbers.\"),\n",
       " Document(metadata={'title': 'What Is Attention in Language Models?', 'source': 'https://cohere.com/llmu/what-is-attention-in-language-models'}, page_content='Now, on to the next step. We are going to use the similarities to transform each of the words of this sentence. We’ll call these new words “the1”, “bank1”, “of1”, “river1” for the first sentence, and “money2”, “in2”, “the2”, “bank2” for the second sentence. Beware, we’ll be doing some math with words, but it won’t be very different from the one we did before. Let’s look at the first sentence, “The bank of the river”, and the word “the”. The similarities with the other words are as follows:\\nSimilarities with “the”:\\nthe: 1\\nbank: 0\\nof: 0\\nriver: 0\\nTherefore, we turn the word “the” into the new word “the1”, which corresponds to the sum 1*“the” + 0*”bank” + 0*”of” + 0*”river”. This is equal to the word “the”. That means, the word “the” doesn’t change, and it’s equal to “the1”.\\nNow, let’s follow the same procedure with the word “bank”, and see what we get. The similarities of the word “bank” with the other words of the sentence “The bank of the river” are the following:\\nthe: 0\\nbank: 1\\nof: 0\\nriver: 0.11\\nTherefore, the word “bank” turns into the word “bank1”, given by the equation 1_”bank” + 0.11_”river”. We are almost done. We want the coefficients of the word to add to 1, so we can divide everything by their sum, which is 1.11. When we do that, we get that the word “bank” gets transformed into the word 0.9*”bank” + 0.1”river”.\\nLet’s do one more for consistency. The word “money” in the second sentence turns into “money2”, given by the equation 1*”money” + 0.25*”bank”. When we normalize, we divide everything by 1.25, to get the equation 0.8*”money” + 0.2*”bank”. All these equations are summarized in the table below.\\nThis is the way we obtained the modified words that we use in the attention mechanism above. The modified sentences then become the following:\\nIs There More to Attention?'),\n",
       " Document(metadata={'title': 'What Is Attention in Language Models?', 'source': 'https://cohere.com/llmu/what-is-attention-in-language-models'}, page_content=\"This is the way we obtained the modified words that we use in the attention mechanism above. The modified sentences then become the following:\\nIs There More to Attention?\\nWhat you learned in this chapter is simple self-attention. However, we can do much better than that. There is a method called multi-head attention, in which one doesn't only consider one embedding, but several different ones. These are all obtained from the original by transforming it in different ways. Multi-head attention has been very successful at the task of adding context to text. If you'd like to learn more about the self and multi-head attention, you can check out the following two videos:\\nThe Attention Mechanism in Large Language Models - YouTube\\nThe math behind Attention: Keys, Queries, and Values matrices - YouTube\\nConclusion\\nIn this post, you learned what attention mechanisms are. They are a very useful method that helps give words the context coming from the sentence where they belong. In this way, the model has less chance of getting confused by words taken out of context. LLMs make great use of attention mechanisms in order to understand text.\"),\n",
       " Document(metadata={'title': 'What Are Transformer Models and How Do They Work?', 'source': 'https://cohere.com/llmu/what-are-transformer-models'}, page_content='What Are Transformer Models and How Do They Work?What are Transformer Models and How do they Work? - YouTubeThe Narrated Transformer Language Model - YouTubeWhat are Transformer Models and how do they work?\\nWhat are Transformer Models and How do they Work? - YouTube\\nTransformer models are one of the most exciting new developments in machine learning. They were introduced in the paper Attention is All You Need. Transformers can be used to write stories, essays, poems, answer questions, translate between languages, chat with humans, and they can even pass exams that are hard for humans! But what are they? You’ll be happy to know that the architecture of transformer models is not that complex, it simply is a concatenation of some very useful components, each of which has its own function. In this chapter, you will learn all of these components.\\nIn a nutshell, what does a transformer do? Imagine that you’re writing a text message on your phone. After each word, you may get three words suggested to you. For example, if you type “Hello, how are”, the phone may suggest words such as “you”, or “your” as the next word. Of course, if you continue selecting the suggested word in your phone, you’ll quickly find that the message formed by these words makes no sense. If you look at each set of 3 or 4 consecutive words, it may make sense, but these words don’t concatenate to anything with a meaning. This is because the model used in the phone doesn’t carry the overall context of the message, it simply predicts which word is more likely to come up after the last few. Transformers, on the other hand, keep track of the context of what is being written, and this is why the text that they write makes sense.\\nThe phone can suggest the next word to use in a text message, but does not have the power to generate coherent text.\\nThe phone can suggest the next word to use in a text message, but does not have the power to generate coherent text.'),\n",
       " Document(metadata={'title': 'What Are Transformer Models and How Do They Work?', 'source': 'https://cohere.com/llmu/what-are-transformer-models'}, page_content='The phone can suggest the next word to use in a text message, but does not have the power to generate coherent text.\\nI have to be honest with you, the first time I found out that transformers build text one word at a time, I couldn’t believe it. First of all, this is not how humans form sentences and thoughts. We first form a basic thought, and then start refining it and adding words to it. This is also not how ML models do other things. For example, images are not built this way. Most neural network based graphical models form a rough version of the image, and slowly refine it or add detail until it is perfect. So why would a transformer model build text word by word? One answer is, because that works really well. A more satisfying one is that because transformers are so incredibly good at keeping track of the context, that the next word they pick is exactly what it needs to keep going with an idea.\\nAnd how are transformers trained? With a lot of data, all the data on the internet, in fact. So when you input the sentence “Hello, how are” into the transformer, it simply knows that, based on all the text in the internet, the best next word is “you”. If you were to give it a more complicated command, say, “Write a story.”, it may figure out that a good next word to use is “Once”. Then it adds this word to the command, and figures out that a good next word is “upon”, and so on. And word by word, it will continue until it writes a story.\\nCommand: Write a story.\\nResponse: Once\\nNext command: Write a story. Once\\nResponse: upon\\nNext command: Write a story. Once upon\\nResponse: a\\n**Next command: Write a story. Once upon a\\n**Response:** time\\n**Next command: Write a story. Once upon a time\\n**Response:** there\\netc.'),\n",
       " Document(metadata={'title': 'What Are Transformer Models and How Do They Work?', 'source': 'https://cohere.com/llmu/what-are-transformer-models'}, page_content='Next command: Write a story. Once upon\\nResponse: a\\n**Next command: Write a story. Once upon a\\n**Response:** time\\n**Next command: Write a story. Once upon a time\\n**Response:** there\\netc.\\nNow that we know what transformers do, let’s get to their architecture. If you’ve seen the architecture of a transformer model, you may have jumped in awe like I did the first time I saw it, it looks quite complicated! However, when you break it down into its most important parts, it’s not so bad. The transformer has 4 main parts:\\nTokenization\\nEmbedding\\nPositional encoding\\nTransformer block (several of these)\\nSoftmax\\nThe fourth one, the transformer block, is the most complex of all. Many of these can be concatenated, and each one contains two main parts: The attention and the feedforward components.\\nThe architecture of a transformer model\\nLet’s study these parts one by one.\\nTokenization\\nTokenization is the most basic step. It consists of a large dataset of tokens, including all the words, punctuation signs, etc. The tokenization step takes every word, prefix, suffix, and punctuation signs, and sends them to a known token from the library.\\nTokenization: Turning words into tokens\\nFor example, if the sentence is “Write a story”, then the 4 corresponding tokens will be <Write>, <a>, <story>, and \\\\<.>.\\nEmbedding\\nOnce the input has been tokenized, it’s time to turn words into numbers. For this, we use an embedding. In a previous chapter, you learned about how text embeddings send every piece of text to a vector (a list) of numbers. If two pieces of text are similar, then the numbers in their corresponding vectors are similar to each other (componentwise, meaning each pair of numbers in the same position is similar). Otherwise, if two pieces of text are different, then the numbers in their corresponding vectors are different.'),\n",
       " Document(metadata={'title': 'What Are Transformer Models and How Do They Work?', 'source': 'https://cohere.com/llmu/what-are-transformer-models'}, page_content='For example, if the sentence we are considering is “Write a story.” and the tokens are <Write>, <a>, <story>, and \\\\<.>, then each one of these will be sent to a long vector, and we’ll have four vectors.\\nIn general embeddings send every word (token) to a long list of numbers.\\nPositional encoding\\nOnce we have the vectors corresponding to each of the tokens in the sentence, the next step is to turn all these into one vector to process. The most common way to turn a bunch of vectors into one vector is to add them, componentwise. That means, we add each coordinate separately. For example, if the vectors (of length 2) are [1,2], and [3,4], their corresponding sum is [1+3, 2+4], which equals [4, 6]. This can work, but there’s a small caveat. Addition is commutative, meaning that if you add the same numbers in a different order, you get the same result. In that case, the sentence “I’m not sad, I’m happy” and the sentence “I’m not happy, I’m sad”, will result in the same vector, given that they have the same words, except in different order. This is not good. Therefore, we must come up with some method that will give us a different vector for the two sentences. Several methods work, and we’ll go with one of them: positional encoding. Positional encoding consists of adding a sequence of predefined vectors to the embedding vectors of the words. This ensures we get a unique vector for every sentence, and sentences with the same words in different order will be assigned different vectors. In the example below, the vectors corresponding to the words “Write”, “a”, “story”, and “.” become the modified vectors that carry information about their position, labeled “Write (1)”, “a (2)”, “story (3)”, and “. (4)”.\\nPositional encoding adds a positional vector to each word, in order to keep track of the positions of the words.'),\n",
       " Document(metadata={'title': 'What Are Transformer Models and How Do They Work?', 'source': 'https://cohere.com/llmu/what-are-transformer-models'}, page_content='Positional encoding adds a positional vector to each word, in order to keep track of the positions of the words.\\nNow that we know we have a unique vector corresponding to the sentence, and that this vector carries the information on all the words in the sentence and their order, we can move to the next step.\\nTransformer block\\nLet’s recap what we have so far. The words come in and get turned into tokens (tokenization), tokenized words are turned into numbers (embeddings), then order gets taken into account (positional encoding). This gives us a vector for every token that we input to the model. Now, the next step is to predict the next word in this sentence. This is done with a really really large neural network, which is trained precisely with that goal, to predict the next word in a sentence.\\nWe can train such a large network, but we can vastly improve it by adding a key step: the attention component. Introduced in the seminal paper Attention is All you Need, it is one of the key ingredients in transformer models, and one of the reasons they work so well. Attention is explained in the previous section, but for now, imagine it as a way to add context to each word in the text.\\nThe attention component is added at every block of the feedforward network. Therefore, if you imagine a large feedforward neural network whose goal is to predict the next word, formed by several blocks of smaller neural networks, an attention component is added to each one of these blocks. Each component of the transformer, called a transformer block, is then formed by two main components:\\nThe attention component.\\nThe feedforward component.\\nThe transformer is a concatenation of many transformer blocks.\\nThe transformer is a concatenation of many transformer blocks. Each one of these is composed by an attention component followed by a feedforward component (a neural network).\\nAttention'),\n",
       " Document(metadata={'title': 'What Are Transformer Models and How Do They Work?', 'source': 'https://cohere.com/llmu/what-are-transformer-models'}, page_content='The transformer is a concatenation of many transformer blocks. Each one of these is composed by an attention component followed by a feedforward component (a neural network).\\nAttention\\nThe next step is attention. As you learned in the previous chapter, the attention mechanism deals with a very important problem: the problem of context. Sometimes, as you know, the same word can be used with different meanings. This tends to confuse language models, since an embedding simply sends words to vectors, without knowing which definition of the word they’re using.\\nAttention is a very useful technique that helps language models understand the context. In order to understand how attention works, consider the following two sentences:\\nSentence 1: The bank of the river.\\nSentence 2: Money in the bank.\\nAs you can see, the word ‘bank’ appears in both, but with different definitions. In sentence 1, we are referring to the land at the side of the river, and in the second one to the institution that holds money. The computer has no idea of this, so we need to somehow inject that knowledge into it. What can help us? Well, it seems that the other words in the sentence can come to our rescue. For the first sentence, the words ‘the’, and ‘of’ do us no good. But the word ‘river’ is the one that is letting us know that we’re talking about the land at the side of the river. Similarly, in sentence 2, the word ‘money’ is the one that is helping us understand that the word ‘bank’ is now referring to the institution that holds money.\\nAttention helps give context to each word, based on the other words in the sentence (or text).'),\n",
       " Document(metadata={'title': 'What Are Transformer Models and How Do They Work?', 'source': 'https://cohere.com/llmu/what-are-transformer-models'}, page_content='Attention helps give context to each word, based on the other words in the sentence (or text).\\nIn short, what attention does is it moves the words in a sentence (or piece of text) closer in the word embedding. In that way, the word “bank” in the sentence “Money in the bank” will be moved closer to the word “money”. Equivalently, in the sentence “The bank of the river”, the word “bank” will be moved closer to the word “river”. That way, the modified word “bank” in each of the two sentences will carry some of the information of the neighboring words, adding context to it.\\nThe attention step used in transformer models is actually much more powerful, and it’s called multi-head attention. In multi-head attention, several different embeddings are used to modify the vectors and add context to them. Multi-head attention has helped language models reach much higher levels of efficacy when processing and generating text.\\nThe Softmax Layer\\nNow that you know that a transformer is formed by many layers of transformer blocks, each containing attention and a feedforward layer, you can think of it as a large neural network that predicts the next word in a sentence. The transformer outputs scores for all the words, where the highest scores are given to the words that are most likely to be next in the sentence.\\nThe last step of a transformer is a softmax layer, which turns these scores into probabilities (that add to 1), where the highest scores correspond to the highest probabilities. Then, we can sample out of these probabilities for the next word. In the example below, the transformer gives the highest probability of 0.5 to “Once”, and probabilities of 0.3 and 0.2 to “Somewhere” and “There”. Once we sample, the word “once” is selected, and that’s the output of the transformer.\\nThe softmax layer turns the scores into probabilities, and these are used to pick the next word in the text.'),\n",
       " Document(metadata={'title': 'What Are Transformer Models and How Do They Work?', 'source': 'https://cohere.com/llmu/what-are-transformer-models'}, page_content='The softmax layer turns the scores into probabilities, and these are used to pick the next word in the text.\\nNow what? Well, we repeat the step. We now input the text “Write a story. Once” into the model, and most likely, the output will be “upon”. Repeating this step again and again, the transformer will end up writing a story, such as “Once upon a time, there was a …”.\\nPost Training\\nNow that you know how transformers work, we still have a bit of work to do. Imagine the following: You ask the transformer “What is the capital of Algeria?”. We would love for it to answer “Algiers”, and move on. However, the transformer is trained on the entire internet. The internet is a big place, and it’s not necessarily the best question/answer repository. Many pages, for example, would have long lists of questions without answers. In this case, the next sentence after “What is the capital of Algeria?” could be another question, such as “What is the population of Algeria?”, or “What is the capital of Burkina Faso?”. The transformer is not a human who thinks about their responses, it simply mimics what it sees on the internet (or any dataset that has been provided). So how do we get the transformer to answer questions?\\nThe answer is post-training. In the same way that you would teach a person to do certain tasks, you can get a transformer to perform tasks. Once a transformer is trained on the entire internet, then it is trained again on a large dataset which corresponds to lots of questions and their respective answers. Transformers (like humans), have a bias towards the last things they’ve learned, so post-training has proven a very useful step to help transformers succeed at the tasks they are asked to.\\nPost-training also helps with many other tasks. For example, one can post-train a transformer with large datasets of conversations, in order to help it perform well as a chatbot, or to help us write stories, poems, or even code.\\nMore'),\n",
       " Document(metadata={'title': 'What Are Transformer Models and How Do They Work?', 'source': 'https://cohere.com/llmu/what-are-transformer-models'}, page_content=\"More\\nAs mentioned above, this is a conceptual introduction to give you an idea of how transformers generate text. If you'd like to open the hood and get a more detailed intuition of the mathematics behind a transformer, we invite you to check out the following articles and video by our course instructors, Jay Alammar, and Luis Serrano.\\nThe Illustrated Transformer\\nHow GPT3 Works\\nThe Narrated Transformer Language Model - YouTube\\nWhat are Transformer Models and how do they work? - YouTube\\nConclusion\\nIn this chapter you’ve learned how transformers work. They are formed by several blocks, each one with its own function, working together to understand the text and generate the next word. These blocks are the following:\\nTokenizer: Turns words into tokens.\\nEmbedding: Turns tokens into numbers (vectors)\\nPositional encoding: Adds order to the words in the text.\\nTransformer block: Guesses the next word. It is formed by an attention block and a feedforward block.\\nAttention: Adds context to the text.\\nFeedforward: Is a block in the transformer neural network, which guesses the next word.\\nSoftmax: Turns the scores into probabilities in order to sample the next word.\\nThe repetition of these steps is what writes the amazing text you’ve seen transformers create. The main reason they work so well is because they have a huge amount of parameters that can capture many aspects of the context. We’re excited to see what you can build using transformer models!\"),\n",
       " Document(metadata={'title': 'Retrieval-Augmented Generation (RAG) Using Cohere on AWS', 'source': 'https://cohere.com/llmu/co-aws-rag'}, page_content=\"Retrieval-Augmented Generation (RAG) Using Cohere on AWSQualified\\nContents\\nIntroduction\\nSet Up Amazon Bedrock and SageMaker \\nSetup Bedrock\\nSetup SageMaker\\nCreate SageMaker Endpoint\\nQuick Example\\nCode Walkthrough \\nSetup\\nDefine Documents\\nCreate Vectorstore\\nProcess Documents\\nRun Chatbot\\nConclusion\\nWe’ll use Cohere’s Python SDKs (cohere and cohere-aws) for the code examples. Follow along in this notebook.\\nIntroduction\\nLarge Language Models (LLMs) have proven effective at performing text generation tasks and maintaining the context of a conversation in a chat setting. However, we can sometimes encounter a scenario where an LLM hallucinates and provides factually inaccurate responses to a given question. This is especially true in business settings, where companies have proprietary data that an LLM would not have seen during its training phase.\\nRetrieval-augmented generation (RAG) bridges the gap by allowing an LLM to integrate external data sources and use them in its response generation. This significantly minimizes the hallucination issue, making the model's responses more accurate and reliable.\\nWith RAG, an LLM can use external data sources in its response generation\\nFor example, a major provider of CRM and ERP software wanted to provide a better support Q&A experience that could report on transactional data.\\nThe customer used Cohere Command and Rerank with retrieval-augmented generation (RAG) to build a conversational support app. Users could ask detailed technical questions and get relevant responses, along with citations.\\nImplementing a RAG system at a major provider of CRM and ERP software\\nIn previous chapters, we have seen how to use Cohere’s Chat, Embed, and Rerank endpoints individually. In this chapter, we’ll explore how to build a RAG application using all three endpoints: Chat and Embed on Amazon Bedrock and Rerank on Amazon SageMaker (at the time of writing, Rerank is not available on Bedrock).\\nAn overview of what we'll cover in the code walkthrough\"),\n",
       " Document(metadata={'title': 'Retrieval-Augmented Generation (RAG) Using Cohere on AWS', 'source': 'https://cohere.com/llmu/co-aws-rag'}, page_content='An overview of what we\\'ll cover in the code walkthrough\\nWith Cohere Chat, in particular, you get the complete suite of tools needed to build a high-quality RAG application in the shortest time possible. Some of the highlights (see a more comprehensive list) of Cohere’s RAG capabilities include:\\nQuery generation: With Cohere’s RAG solution, you get an LLM that’s trained for query generation. It takes a user message and transforms it into queries that are more relevant and optimized for the retrieval process.\\nRetrieval models: Cohere Embed helps you build a high-quality semantic search system that retrieves the most relevant documents using embeddings. Cohere Rerank, on the other hand, helps you boost the results further by reranking the search results based on relevance.\\nFine-grained citation: Each grounded response includes fine-grained citations linking back to the source documents. This makes the response easily verifiable and builds trust with the user.\\nCohere Chat provides a complete set of tools needed to build a high-quality RAG application\\nSet Up Amazon Bedrock and SageMaker\\nFirst, we set up the clients for Bedrock (to be used for Chat and Embed) and SageMaker (to be used for Rerank) using the same steps as in the previous chapters. Here we name the clients co_br for Bedrock and co_sm for SageMaker.\\n# ! pip install cohere cohere-aws boto3 hnswlib unstructured -q\\nimport os import cohere import boto3 import cohere_aws from cohere_aws import Client\\nSetup Bedrock\\nTo use Bedrock, we create a BedrockClient by passing the necessary AWS credentials.\\n# Create Bedrock client via the native Cohere SDK # Contact your AWS administrator for the credentials co_br = cohere.BedrockClient( aws_region=\"YOUR_AWS_REGION\", aws_access_key=\"YOUR_AWS_ACCESS_KEY_ID\", aws_secret_key=\"YOUR_AWS_SECRET_ACCESS_KEY\", aws_session_token=\"YOUR_AWS_SESSION_TOKEN\", ) \\nSetup SageMaker'),\n",
       " Document(metadata={'title': 'Retrieval-Augmented Generation (RAG) Using Cohere on AWS', 'source': 'https://cohere.com/llmu/co-aws-rag'}, page_content='Setup SageMaker\\nLater, we’ll need to create a SageMaker endpoint that exposes access to a Cohere model (Rerank v3 in our case). For this, we’ll use the cohere_aws SDK which makes it easy to set up the endpoint, together with AWS’s boto3 library.\\nOnce the endpoint is created (as we’ll walk through later), we can access it using the Cohere SDK. To do this, let’s create a SagemakerClient by passing the necessary AWS credentials.\\n# Create SageMaker client via the native Cohere SDK # Contact your AWS administrator for the credentials co_sm = cohere.SagemakerClient( aws_region=\"YOUR_AWS_REGION\", aws_access_key=\"YOUR_AWS_ACCESS_KEY_ID\", aws_secret_key=\"YOUR_AWS_SECRET_ACCESS_KEY\", aws_session_token=\"YOUR_AWS_SESSION_TOKEN\", ) # For creating an endpoint, you need to use the cohere_aws client: Set environment variables with the AWS credentials os.environ[\\'AWS_ACCESS_KEY_ID\\'] = \"YOUR_AWS_ACCESS_KEY_ID\" os.environ[\\'AWS_SECRET_ACCESS_KEY\\'] = \"YOUR_AWS_SECRET_ACCESS_KEY\" os.environ[\\'AWS_SESSION_TOKEN\\'] = \"YOUR_AWS_SESSION_TOKEN\"\\nCreate SageMaker Endpoint\\nThe next step is to create a Rerank SageMaker endpoint by defining the model package Amazon Resource Names (ARN) for the Rerank model. The ARN is an identifying string for a SageMaker resource, and it varies between the regions where a resource is available.\\nHere, we define the Cohere package for the Rerank model and map the model package against each region, which gives the complete ARN for each region.'),\n",
       " Document(metadata={'title': 'Retrieval-Augmented Generation (RAG) Using Cohere on AWS', 'source': 'https://cohere.com/llmu/co-aws-rag'}, page_content='# Create SageMaker endpoint via the cohere_aws SDK cohere_package = \"cohere-rerank-english-v3-01-d3687e0d2e3a366bb904275616424807\" model_package_map = { \"us-east-1\": f\"arn:aws:sagemaker:us-east-1:865070037744:model-package/{cohere_package}\", \"us-east-2\": f\"arn:aws:sagemaker:us-east-2:057799348421:model-package/{cohere_package}\", \"us-west-1\": f\"arn:aws:sagemaker:us-west-1:382657785993:model-package/{cohere_package}\", \"us-west-2\": f\"arn:aws:sagemaker:us-west-2:594846645681:model-package/{cohere_package}\", \"ca-central-1\": f\"arn:aws:sagemaker:ca-central-1:470592106596:model-package/{cohere_package}\", \"eu-central-1\": f\"arn:aws:sagemaker:eu-central-1:446921602837:model-package/{cohere_package}\", \"eu-west-1\": f\"arn:aws:sagemaker:eu-west-1:985815980388:model-package/{cohere_package}\", \"eu-west-2\": f\"arn:aws:sagemaker:eu-west-2:856760150666:model-package/{cohere_package}\", \"eu-west-3\": f\"arn:aws:sagemaker:eu-west-3:843114510376:model-package/{cohere_package}\", \"eu-north-1\": f\"arn:aws:sagemaker:eu-north-1:136758871317:model-package/{cohere_package}\", \"ap-southeast-1\": f\"arn:aws:sagemaker:ap-southeast-1:192199979996:model-package/{cohere_package}\", \"ap-southeast-2\": f\"arn:aws:sagemaker:ap-southeast-2:666831318237:model-package/{cohere_package}\", \"ap-northeast-2\": f\"arn:aws:sagemaker:ap-northeast-2:745090734665:model-package/{cohere_package}\", \"ap-northeast-1\": f\"arn:aws:sagemaker:ap-northeast-1:977537786026:model-package/{cohere_package}\", \"ap-south-1\": f\"arn:aws:sagemaker:ap-south-1:077584701553:model-package/{cohere_package}\", \"sa-east-1\": f\"arn:aws:sagemaker:sa-east-1:270155090741:model-package/{cohere_package}\", } region = boto3.Session().region_name if region not in model_package_map.keys(): raise Exception(\"UNSUPPORTED REGION\") model_package_arn = model_package_map[region] co_aws = Client(region_name=region) co_aws.create_endpoint(arn=model_package_arn, endpoint_name=\"my-rerank-v3\", instance_type=\"ml.g5.xlarge\", n_instances=1)'),\n",
       " Document(metadata={'title': 'Retrieval-Augmented Generation (RAG) Using Cohere on AWS', 'source': 'https://cohere.com/llmu/co-aws-rag'}, page_content='The following output indicates that the process of creating an endpoint is complete.\\nQuick Example\\nWe’ll start with a quick example to understand the key aspects of RAG.\\nWith RAG, the first step is to define the documents that an LLM will have access to. Here, we have a short list of simple documents. Typically, there is a retrieval process to retrieve the most relevant documents based on a user query, which we’ll cover in the longer walkthrough next. But at this point, let’s assume that these are the only documents and we’ll pass all of them to the LLM.\\ndocuments = [ { \"title\": \"Tall penguins\", \"text\": \"Emperor penguins are the tallest.\"}, { \"title\": \"Penguin habitats\", \"text\": \"Emperor penguins only live in Antarctica.\"}, { \"title\": \"What are animals?\", \"text\": \"Animals are different from plants.\"} ] \\nWe have seen how to use the Chat endpoint in the text generation chapter. To use the RAG feature, we simply need to add one additional parameter, documents, to the endpoint call. These are the documents we defined earlier, which are now available for the model to consider utilizing in its response.\\nLet’s now see how the model responds when given the user message, What are the tallest living penguins?\\nmessage = \"What are the tallest living penguins?\" response = co_br.chat(message=message, documents=documents, model=\"cohere.command-r-plus-v1:0\") print(\"\\\\nRESPONSE:\\\\n\") print(response.text) if response.citations: print(\"\\\\nCITATIONS:\\\\n\") for citation in response.citations: print(citation)\\nAnd here’s the response. The model used the documents to inform its answer to the question. For example, the tallest living penguins are the Emperor penguins part of its response was cited from doc_0, which is the first document in the list containing the text Emperor penguins are the tallest.'),\n",
       " Document(metadata={'title': 'Retrieval-Augmented Generation (RAG) Using Cohere on AWS', 'source': 'https://cohere.com/llmu/co-aws-rag'}, page_content='RESPONSE: The tallest living penguins are the Emperor penguins. These penguins only live in Antarctica. CITATIONS: start=4 end=53 text=\\'tallest living penguins are the Emperor penguins.\\' document_ids=[\\'doc_0\\'] start=69 end=93 text=\\'only live in Antarctica.\\' document_ids=[\\'doc_1\\']\\nCode Walkthrough\\nNow that we’ve covered the basics, let’s look at a more comprehensive example of RAG that includes:\\nBuilding a retrieval system that includes turning documents into text embeddings and storing them in an index\\nBuilding a query generation system that turns user messages into optimized queries for retrieval\\nWrapping a user interaction with an LLM in a chat interface\\nBuilding a response generation system that’s able to answer different types of queries, such as those that require and don’t require RAG\\nSetup\\nFirst, let’s import the necessary libraries for this project. This includes hnswlib for the vector library and unstructured for chunking the documents (more details on these later).\\nimport uuid import hnswlib from typing import List, Dict from unstructured.partition.html import partition_html from unstructured.chunking.title import chunk_by_title \\nDefine Documents\\nNext, we’ll define the documents we’ll use for RAG. We’ll use a few pages from the Cohere documentation that discuss prompt engineering, each in the Python list raw_documents below. Each entry is identified by its title and URL.\\nraw_documents = [ { \"title\": \"Crafting Effective Prompts\", \"url\": \"https://docs.cohere.com/docs/crafting-effective-prompts\"}, { \"title\": \"Advanced Prompt Engineering Techniques\", \"url\": \"https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\"}, { \"title\": \"Prompt Truncation\", \"url\": \"https://docs.cohere.com/docs/prompt-truncation\"}, { \"title\": \"Preambles\", \"url\": \"https://docs.cohere.com/docs/preambles\"} ] \\nCreate Vectorstore\\nThe Vectorstore class handles the ingestion of documents into embeddings (or vectors) and the retrieval of relevant documents given a query.'),\n",
       " Document(metadata={'title': 'Retrieval-Augmented Generation (RAG) Using Cohere on AWS', 'source': 'https://cohere.com/llmu/co-aws-rag'}, page_content='Create Vectorstore\\nThe Vectorstore class handles the ingestion of documents into embeddings (or vectors) and the retrieval of relevant documents given a query.\\nThe Vectorstore component for handling document ingestion and retrieval\\nIt includes a few methods:\\nload_and_chunk: Loads the raw documents from the URL and breaks them into smaller chunks. We’ll utilize the partition_html method from the unstructured library to perform the chunking.\\nembed: Generates embeddings of the chunked documents. We use the Embed endpoint available on Bedrock, which uses the cohere.embed-english-v3 model.\\nindex: Indexes the document chunk embeddings to ensure efficient similarity search during retrieval. For this, we’ll use the hnswlib vector library.\\nretrieve: Uses semantic search to retrieve relevant document chunks from the index, given a query. It involves two steps: first, dense retrieval from the index via the Embed endpoint, and second, a reranking via the Rerank endpoint to boost the search results further.'),\n",
       " Document(metadata={'title': 'Retrieval-Augmented Generation (RAG) Using Cohere on AWS', 'source': 'https://cohere.com/llmu/co-aws-rag'}, page_content='class Vectorstore: def __init__(self, raw_documents: List[Dict[str, str]]): self.raw_documents = raw_documents self.docs = [] self.docs_embs = [] self.retrieve_top_k = 10 self.rerank_top_k = 3 self.load_and_chunk() self.embed() self.index() def load_and_chunk(self) -> None: \"\"\" Loads the text from the sources and chunks the HTML content. \"\"\" print(\"Loading documents...\") for raw_document in self.raw_documents: elements = partition_html(url=raw_document[\"url\"]) chunks = chunk_by_title(elements) for chunk in chunks: self.docs.append( { \"title\": raw_document[\"title\"], \"text\": str(chunk), \"url\": raw_document[\"url\"], } ) def embed(self) -> None: \"\"\" Embeds the document chunks using the Cohere API. \"\"\" print(\"Embedding document chunks...\") batch_size = 90 self.docs_len = len(self.docs) for i in range(0, self.docs_len, batch_size): batch = self.docs[i : min(i + batch_size, self.docs_len)] texts = [item[\"text\"] for item in batch] docs_embs_batch = co_br.embed( texts=texts, model=\"cohere.embed-english-v3\", input_type=\"search_document\" ).embeddings self.docs_embs.extend(docs_embs_batch) def index(self) -> None: \"\"\" Indexes the document chunks for efficient retrieval. \"\"\" print(\"Indexing document chunks...\") self.idx = hnswlib.Index(space=\"ip\", dim=1024) self.idx.init_index(max_elements=self.docs_len, ef_construction=512, M=64) self.idx.add_items(self.docs_embs, list(range(len(self.docs_embs)))) print(f\"Indexing complete with {self.idx.get_current_count()} document chunks.\") def retrieve(self, query: str) -> List[Dict[str, str]]: \"\"\" Retrieves document chunks based on the given query. Parameters: query (str): The query to retrieve document chunks for. Returns: List[Dict[str, str]]: A list of dictionaries representing the retrieved document chunks, with \\'title\\', \\'text\\', and \\'url\\' keys. \"\"\" # Dense retrieval query_emb = co_br.embed( texts=[query], model=\"cohere.embed-english-v3\", input_type=\"search_query\" ).embeddings doc_ids = self.idx.knn_query(query_emb,'),\n",
       " Document(metadata={'title': 'Retrieval-Augmented Generation (RAG) Using Cohere on AWS', 'source': 'https://cohere.com/llmu/co-aws-rag'}, page_content='\\'text\\', and \\'url\\' keys. \"\"\" # Dense retrieval query_emb = co_br.embed( texts=[query], model=\"cohere.embed-english-v3\", input_type=\"search_query\" ).embeddings doc_ids = self.idx.knn_query(query_emb, k=self.retrieve_top_k)[0][0] # Reranking rank_fields = [\"title\", \"text\"] # We\\'ll use the title and text fields for reranking docs_to_rerank = [self.docs[doc_id] for doc_id in doc_ids] rerank_results = co_sm.rerank( query=query, documents=docs_to_rerank, top_n=self.rerank_top_k, rank_fields=rank_fields, model=\"my-rerank-v3\") doc_ids_reranked = [doc_ids[result.index] for result in rerank_results.results] docs_retrieved = [] for doc_id in doc_ids_reranked: docs_retrieved.append( { \"title\": self.docs[doc_id][\"title\"], \"text\": self.docs[doc_id][\"text\"], \"url\": self.docs[doc_id][\"url\"], } ) return docs_retrieved'),\n",
       " Document(metadata={'title': 'Retrieval-Augmented Generation (RAG) Using Cohere on AWS', 'source': 'https://cohere.com/llmu/co-aws-rag'}, page_content='Process Documents\\nNow that the Vectorstore component is set up, we can process the documents, which will involve chunking, embedding, and indexing. We do this by creating an instance of the Vectorstore and passing the raw documents we defined earlier.\\n# Create an instance of the Vectorstore class with the given sources vectorstore = Vectorstore(raw_documents) \\nThe result is 44 chunks from the original four web pages.\\nLoading documents... Embedding document chunks... Indexing document chunks... Indexing complete with 44 document chunks. \\nWe can test if the retrieval is working by entering a search query.\\nvectorstore.retrieve(\"Prompting by giving examples\") \\nThis returns the following results, which indeed are returning relevant chunks to the query.'),\n",
       " Document(metadata={'title': 'Retrieval-Augmented Generation (RAG) Using Cohere on AWS', 'source': 'https://cohere.com/llmu/co-aws-rag'}, page_content='vectorstore.retrieve(\"Prompting by giving examples\") \\nThis returns the following results, which indeed are returning relevant chunks to the query.\\n[{\\'title\\': \\'Advanced Prompt Engineering Techniques\\', \\'text\\': \\'Few-shot Prompting\\\\n\\\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.\\', \\'url\\': \\'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\\'}, {\\'title\\': \\'Crafting Effective Prompts\\', \\'text\\': \\'Incorporating Example Outputs\\\\n\\\\nLLMs respond well when they have specific examples to work from. For example, instead of asking for the salient points of the text and using bullet points “where appropriate”, give an example of what the output should look like.\\', \\'url\\': \\'https://docs.cohere.com/docs/crafting-effective-prompts\\'}, {\\'title\\': \\'Advanced Prompt Engineering Techniques\\', \\'text\\': \\'In addition to giving correct examples, including negative examples with a clear indication of why they are wrong can help the LLM learn to distinguish between correct and incorrect responses. Ordering the examples can also be important; if there are patterns that could be picked up on that are not relevant to the correctness of the question, the model may incorrectly pick up on those instead of the semantics of the question itself.\\', \\'url\\': \\'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\\'}] \\nRun Chatbot\\nWe can now run the chatbot. For this, we create a run_chatbot function, which is an expanded version of the same function we created in Chapter 3.\\nThis time, it includes the RAG components:'),\n",
       " Document(metadata={'title': 'Retrieval-Augmented Generation (RAG) Using Cohere on AWS', 'source': 'https://cohere.com/llmu/co-aws-rag'}, page_content='We can now run the chatbot. For this, we create a run_chatbot function, which is an expanded version of the same function we created in Chapter 3.\\nThis time, it includes the RAG components:\\nFor each user message, we use the endpoint’s search query generation feature to turn the message into one or more queries that are optimized for retrieval. The endpoint can even return no query, meaning a user message can be responded to directly without retrieval. This is done by calling the Chat endpoint with the search_queries_only parameter and setting it as True.\\nIf no search query is generated, we call the Chat endpoint to generate a response directly. If there is at least one, we call the retrieve method from the Vectorstore instance to retrieve the most relevant documents to each query.\\nFinally, all the results from all queries are appended to a list and passed to the Chat endpoint for response generation.\\nWe print the response, together with the citations and the list of document chunks cited, for easy reference.'),\n",
       " Document(metadata={'title': 'Retrieval-Augmented Generation (RAG) Using Cohere on AWS', 'source': 'https://cohere.com/llmu/co-aws-rag'}, page_content='We print the response, together with the citations and the list of document chunks cited, for easy reference.\\ndef run_chatbot(message, chat_history=None): if chat_history is None: chat_history = [] # Generate search queries, if any response = co_br.chat(message=message, search_queries_only=True, model=\"cohere.command-r-plus-v1:0\", chat_history=chat_history) search_queries = [] for query in response.search_queries: search_queries.append(query.text) # If there are search queries, retrieve the documents if search_queries: print(\"Retrieving information...\", end=\"\") # Retrieve document chunks for each query documents = [] for query in search_queries: documents.extend(vectorstore.retrieve(query)) # Use document chunks to respond response = co_br.chat( message=message, model=\"cohere.command-r-plus-v1:0\", documents=documents, chat_history=chat_history) else: response = co_br.chat( message=message, model=\"cohere.command-r-plus-v1:0\", chat_history=chat_history) # Print the chatbot response, citations, and documents print(\"\\\\nRESPONSE:\\\\n\") print(response.text) if response.citations: print(\"\\\\nCITATIONS:\\\\n\") for citation in response.citations: print(citation) print(\"\\\\nDOCUMENTS:\\\\n\") for document in response.documents: print(document) chat_history = response.chat_history return chat_history \\nHere is a sample conversation consisting of a few turns.\\nTurn #1:\\nchat_history = run_chatbot(\"Hello, I have a question\") \\nRESPONSE: Of course! I am here to help. Please go ahead and ask your question, and I will do my best to provide a helpful response.\\nTurn #2:\\nchat_history = run_chatbot(\"What\\'s the difference between zero-shot and few-shot prompting\", chat_history)'),\n",
       " Document(metadata={'title': 'Retrieval-Augmented Generation (RAG) Using Cohere on AWS', 'source': 'https://cohere.com/llmu/co-aws-rag'}, page_content='Turn #2:\\nchat_history = run_chatbot(\"What\\'s the difference between zero-shot and few-shot prompting\", chat_history) \\nRetrieving information... RESPONSE: Zero-shot prompting is when no examples of the task are provided to the model. On the other hand, few-shot prompting is a technique where a model is given a few examples of the task being performed before asking the specific question to be answered. CITATIONS: start=0 end=19 text=\\'Zero-shot prompting\\' document_ids=[\\'doc_0\\'] start=28 end=78 text=\\'no examples of the task are provided to the model.\\' document_ids=[\\'doc_0\\'] start=98 end=116 text=\\'few-shot prompting\\' document_ids=[\\'doc_0\\'] start=140 end=197 text=\\'model is given a few examples of the task being performed\\' document_ids=[\\'doc_0\\'] start=205 end=249 text=\\'asking the specific question to be answered.\\' document_ids=[\\'doc_0\\'] DOCUMENTS: {\\'id\\': \\'doc_0\\', \\'text\\': \\'Few-shot Prompting\\\\n\\\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.\\', \\'title\\': \\'Advanced Prompt Engineering Techniques\\', \\'url\\': \\'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\\'}\\nTurn #3:\\nchat_history = run_chatbot(\"How would the latter help?\", chat_history)'),\n",
       " Document(metadata={'title': 'Retrieval-Augmented Generation (RAG) Using Cohere on AWS', 'source': 'https://cohere.com/llmu/co-aws-rag'}, page_content='Turn #3:\\nchat_history = run_chatbot(\"How would the latter help?\", chat_history) \\nRetrieving information... RESPONSE: Few-shot prompting can vastly improve the quality of the model\\'s completions. By providing a few relevant and diverse examples, the model can be steered toward a high-quality solution. These examples condition the model to the expected response type and style. CITATIONS: start=23 end=77 text=\"vastly improve the quality of the model\\'s completions.\" document_ids=[\\'doc_2\\'] start=97 end=126 text=\\'relevant and diverse examples\\' document_ids=[\\'doc_0\\'] start=145 end=184 text=\\'steered toward a high-quality solution.\\' document_ids=[\\'doc_0\\'] start=200 end=260 text=\\'condition the model to the expected response type and style.\\' document_ids=[\\'doc_0\\'] DOCUMENTS: {\\'id\\': \\'doc_2\\', \\'text\\': \\'Advanced Prompt Engineering Techniques\\\\n\\\\nSuggest Edits\\\\n\\\\nThe previous chapter discussed general rules and heuristics to follow for successfully prompting the Command family of models. Here, we will discuss specific advanced prompt engineering techniques that can in many cases vastly improve the quality of the model’s completions. These include how to give clear and unambiguous instructions, few-shot prompting, chain-of-thought (CoT) techniques, and prompt chaining.\\', \\'title\\': \\'Advanced Prompt Engineering Techniques\\', \\'url\\': \\'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\\'} {\\'id\\': \\'doc_0\\', \\'text\\': \\'Few-shot Prompting\\\\n\\\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.\\', \\'title\\': \\'Advanced Prompt Engineering Techniques\\', \\'url\\': \\'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\\'}\\nTurn #4:'),\n",
       " Document(metadata={'title': 'Retrieval-Augmented Generation (RAG) Using Cohere on AWS', 'source': 'https://cohere.com/llmu/co-aws-rag'}, page_content='Turn #4:\\nchat_history = run_chatbot(\"What do you know about 5G networks?\", chat_history) \\nRetrieving information... RESPONSE: Sorry, I don\\'t have any information about 5G networks. Is there anything else you would like to ask?\\nThere are a few observations worth pointing out:\\nDirect response: For user messages that don’t require retrieval (“Hello, I have a question”), the chatbot responds directly without requiring retrieval.\\nCitation generation: For responses that do require retrieval (\"What\\'s the difference between zero-shot and few-shot prompting\"), the endpoint returns the response together with the citations. These are fine-grained citations, which means they refer to specific spans of the generated text.\\nState management: The endpoint maintains the state of the conversation via the chat_history parameter, for example, by correctly responding to a vague user message, such as \"How would the latter help?\"\\nResponse synthesis: The model can decide if none of the retrieved documents provide the necessary information to answer a user message. For example, when asked the question, “What do you know about 5G networks”, the chatbot retrieves external information from the index. However, it doesn’t use any of the information in its response as none of it is relevant to the question.\\nConclusion\\nThis chapter demonstrated how to create a RAG application using Cohere Chat and Embed on Amazon Bedrock and Cohere Rerank on Amazon SageMaker. RAG enhances LLMs by enabling them to integrate external data sources and reduce hallucination, resulting in more accurate and reliable responses.\\nBy following the steps outlined in this chapter, you can leverage Cohere\\'s RAG capabilities to build high-quality applications that utilize LLMs and external data effectively, making it a powerful tool for creating robust and trustworthy AI solutions.'),\n",
       " Document(metadata={'title': 'Retrieval-Augmented Generation (RAG) Using Cohere on AWS', 'source': 'https://cohere.com/llmu/co-aws-rag'}, page_content='But we can extend the concept of RAG and make it more powerful using a tool use approach. This makes it possible to build applications that can not only answer questions, but also automate tasks.\\nIn Chapter 7, we’ll learn how to use Command R+ on Amazon Bedrock to implement tool use.'),\n",
       " Document(metadata={'title': 'Introduction to Text Embeddings', 'source': 'https://cohere.com/llmu/text-embeddings'}, page_content='Introduction to Text EmbeddingsQualified\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nWhen you hear about large language models (LLMs), probably the first thing that comes to mind is the text generation capability, such as writing an essay or creating marketing copy.\\nAnother thing you can get is text representation: a set of numbers that represent what the text means and capture the semantics of the text. These numbers are called text embeddings.\\nText generation outputs text, while text representation outputs embeddings\\nText embeddings give you the ability to turn unstructured text data into a structured form. With embeddings, you can compare two or more pieces of text, be it single words, sentences, paragraphs, or even longer documents. And since these are sets of numbers, the ways you can process and extract insights from them are limited only by your imagination.\\nWhat does this bring? It opens up many possible use cases that apply in the real world today. Embeddings power applications we interact with on a daily basis, such as modern search engines, eCommerce product recommendations, social media content moderation, email spam filtering, customer support conversational agents, and many more.\\nIn this chapter, we take a visual approach to understand the intuition behind text embeddings.\\nStep-by-Step Guide\\nTo set up, we first import several tools. We\\'ll use the same notebook for the next several chapters, and we\\'ll import everything we need here.\\nimport pandas as pd import numpy as np import altair as alt from sklearn.decomposition import PCA from sklearn.metrics.pairwise import cosine_similarity from sklearn.cluster import KMeans \\nWe also import the Cohere module and create a client.\\nimport cohere co = cohere.Client(\"COHERE_API_KEY\") # Your Cohere API key \\nStep 1: Prepare the Dataset'),\n",
       " Document(metadata={'title': 'Introduction to Text Embeddings', 'source': 'https://cohere.com/llmu/text-embeddings'}, page_content='We also import the Cohere module and create a client.\\nimport cohere co = cohere.Client(\"COHERE_API_KEY\") # Your Cohere API key \\nStep 1: Prepare the Dataset\\nWe\\'ll work a subset of the Airline Travel Information System (ATIS) intent classification dataset [Source]. The following code loads the dataset into a pandas Dataframe df with a single column \"queries\" containing 91 inquiries coming to airline travel inquiry systems.\\n# Load the dataset to a dataframe df_orig = pd.read_csv(\\'https://raw.githubusercontent.com/cohere-ai/notebooks/main/notebooks/data/atis_intents_train.csv\\', names=[\\'intent\\',\\'query\\']) # Take a small sample for illustration purposes sample_classes = [\\'atis_airfare\\', \\'atis_airline\\', \\'atis_ground_service\\'] df = df_orig.sample(frac=0.1, random_state=30) df = df[df.intent.isin(sample_classes)] df_orig = df_orig.drop(df.index) df.reset_index(drop=True,inplace=True) # Remove unnecessary column intents = df[\\'intent\\'] #save for a later need df.drop(columns=[\\'intent\\'], inplace=True) \\nHere are a few example data points:\\n- which airlines fly from boston to washington dc via other cities - show me the airlines that fly between toronto and denver - show me round trip first class tickets from new york to miami - i\\'d like the lowest fare from denver to pittsburgh - show me a list of ground transportation at boston airport - show me boston ground transportation - of all airlines which airline has the most arrivals in atlanta - what ground transportation is available in boston - i would like your rates between atlanta and boston on september third - which airlines fly between boston and pittsburgh \\nStep 2: Turn Text into Embeddings\\nNext, we embed each inquiry by calling Cohere’s Embed endpoint with co.embed(). It takes in texts as input and returns embeddings as output. We supply three parameters:\\ntexts: The list of texts you want to embed\\nmodel: The model to use to generate the embedding. At the time of writing, there are four models available:'),\n",
       " Document(metadata={'title': 'Introduction to Text Embeddings', 'source': 'https://cohere.com/llmu/text-embeddings'}, page_content='texts: The list of texts you want to embed\\nmodel: The model to use to generate the embedding. At the time of writing, there are four models available:\\nembed-english-v3.0 (English)\\nembed-english-light-v3.0 (English)\\nembed-multilingual-v3.0 (Multilingual: 100+ languages)\\nembed-multilingual-light-v3.0 (Multilingual: 100+ languages)\\ninput_type — Specifies the type of document to be embedded. At the time of writing, there are four options:\\nsearch_document: For documents against which search is performed\\nsearch_query: For query documents\\nclassification: For when the embeddings will be used as an input to a text classifier\\nclustering: For when you want to cluster the embeddings\\ndef get_embeddings(texts, model=\\'embed-english-v3.0\\', input_type=\"search_document\"): output = co.embed( model=model, input_type=input_type, texts=texts) return output.embeddings df[\\'query_embeds\\'] = get_embeddings(df[\\'query\\'].tolist()) \\nFor every piece of text passed to the Embed endpoint, a sequence of 1024 numbers will be generated. Each number represents a piece of information about the meaning contained in that piece of text. Here are the first few dimensions given by the embed-english-v3.0 model for \"show me a list of ground transportation at boston airport\":\\n[0.03793335, -0.008010864, -0.002319336, -0.0110321045, -0.019882202, -0.023864746, 0.011428833, -0.030349731, -0.044830322, 0.028289795, -0.02810669, -0.0032749176, -0.04208374, -0.0077705383, -0.0033798218, -0.06335449, ... ] \\nStep 3: Visualize Embeddings with a Heatmap\\nLet’s get some visual intuition about this by plotting these numbers in a heatmap. What we can do is compress the dimension to a much lower number, say 10.\\nThe get_pc() function below does this via a technique called Principal Component Analysis (PCA), which reduces the number of dimensions in an embedding while retaining as much information as possible. We set embeds_pc to the ten-dimensional version of the document embeddings.'),\n",
       " Document(metadata={'title': 'Introduction to Text Embeddings', 'source': 'https://cohere.com/llmu/text-embeddings'}, page_content=\"# Function to return the principal components def get_pc(arr, n): pca = PCA(n_components=n) embeds_transform = pca.fit_transform(arr) return embeds_transform # Reduce embeddings to 10 principal components to aid visualization embeds = np.array(df['query_embeds'].tolist()) embeds_pc = get_pc(embeds, 10) \\nWe’ll use the 9 data point above as examples and display their compressed embeddings on a heatmap. We have each data point on the y-axis and its corresponding set of 10 embedding values on the x-axis, which looks like this:\\nA heatmap showing 10-dimensional embeddings of 9 data points\\nThere are some patterns emerging. To see this, let’s look at a smaller number of examples.\\nTake these three for example. They are all inquiries about ground transportation in Boston. And by visual inspection, we can see that their embedding patterns are very similar.\\nThe 10-dimensional embeddings of 3 inquiries, all about ground transportation in Boston\\nNow, compare them to the other kinds of inquiries, such as those related to airline information (see two examples below). Notice that while the embeddings about ground transportation inquiries look very similar to each other, they are distinctive from the rest.\\nThe 10-dimensional embeddings of 2 inquiries about other matters\\nHere, the model was able to capture the context and meaning of each piece of text, and it then represents them as embeddings. Each dimension of an embedding, called a feature, represents a certain universal characteristic of text according to how the model understands it.\\nHow is this possible? A large language model has been pre-trained with a vast amount of text data, where the training objective is set up in such a way as to encourage the model to extract contextual information about a piece of text and store it as embeddings.\\nStep 4: Visualize Embeddings on a 2D Plot\"),\n",
       " Document(metadata={'title': 'Introduction to Text Embeddings', 'source': 'https://cohere.com/llmu/text-embeddings'}, page_content=\"Step 4: Visualize Embeddings on a 2D Plot\\nWe can investigate this further by compressing the embeddings to two dimensions and plotting them on a scatter plot. What we would expect is that texts of similar meaning would be closer to each other, and vice versa.\\nDo note that as we compress the embeddings to lower dimensions, the information retained becomes lesser. However, humans can only visualize in 2D or 3D, and it turns out this is still a good enough approximation to help us gain intuition about the data.\\nA plot showing 2D embeddings of 9 data points\\nBy visual inspection, we can see that texts of similar meaning are indeed located close together. We see inquiries about tickets on the left, inquiries about airlines somewhere around the middle, and inquiries about ground transportation on the top right.\\nThese kinds of insights enable various downstream analyses and applications, such as topic modeling, by clustering documents into groups. In other words, text embeddings allow us to take a huge corpus of unstructured text and turn it into a structured form, making it possible to objectively compare, dissect, and derive insights from all that text.\\nIn the coming chapters, we'll dive deeper into these topics.\\nConclusion\\nIn this chapter you learned about the Embed endpoint. Text embeddings make possible a wide array of downstream applications such as semantic search, clustering, and classification. You'll learn more about those in the subsequent chapters.\"),\n",
       " Document(metadata={'title': 'Tool Use and Agents on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-tooluse'}, page_content=\"Tool Use and Agents on Amazon BedrockQualified\\nContents\\nIntroduction\\nCode Walkthrough\\nSetup \\nSet Up the Tools\\nRun the Tool Use Workflow\\nSingle-Step Scenario\\nMulti-Step Scenario\\nConclusion\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nIntroduction\\nTool use extends the capabilities of retrieval-augmented generation (RAG) by making it possible to build applications that can not only answer questions but also automate tasks.\\nWith a tool use approach, external systems are used to guide an LLM’s response, but they do so by leveraging a much bigger set of tools than what’s possible with RAG. The concept of tool use expands on LLMs' useful feature of being able to act as a reasoning and decision-making engine. Tool use also enables developers to build agentic applications that can take actions; that is, they are capable of doing both read and write operations on an external system.\\nFor example, a global financial consultancy wanted to build an executive AI assistant for a national telecom company that could support executive decision-making. To meet the client’s requirements, the solution needed to extract information from internal document stores and real-time data sources.\\nUsing Cohere’s tool use solution, along with RAG, the firm was able to leverage Command’s multi-step tool use capabilities and external agents (e.g., calculators and stock price sources) to retrieve and manipulate structured data.\\nImplementing a tool use workflow at a global financial consultancy\\nIn this chapter, we'll explore how to build an agentic application that leverages tool using Cohere's Command R+ model on Amazon Bedrock. For the most updated list of Cohere models available on Amazon Bedrock, visit the Amazon Bedrock website.\\nCode Walkthrough\"),\n",
       " Document(metadata={'title': 'Tool Use and Agents on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-tooluse'}, page_content='Code Walkthrough\\nLet’s learn about tool use by building an onboarding assistant that helps new hires of a fictitious company called Co1t get set up in their first week. The assistant can help answer user questions about the company, search for information from emails, and create meeting appointments.\\nAn overview of what we\\'ll cover in the code walkthrough\\nSetup\\nFirst, let\\'s install and import the necessary libraries and set up our Cohere client using the cohere SDK. To use Bedrock, we create a BedrockClient by passing the necessary AWS credentials.\\nimport cohere # Create Bedrock client via the native Cohere SDK # Contact your AWS administrator for the credentials co = cohere.BedrockClient( aws_region=\"YOUR_AWS_REGION\", aws_access_key=\"YOUR_AWS_ACCESS_KEY_ID\", aws_secret_key=\"YOUR_AWS_SECRET_ACCESS_KEY\", aws_session_token=\"YOUR_AWS_SESSION_TOKEN\", )\\nThe pre-requisite, before we can run a tool use workflow, is to set up the tools. Let\\'s create three tools:\\nsearch_faqs: A tool for searching the FAQs. For simplicity, we\\'ll not implement any retrieval logic, but we\\'ll simply pass a list of three predefined documents. In practice, we would set up a retrieval system as we did in Chapters 4, 5, and 6.\\nsearch_emails: A tool for searching the emails. Same as above, we\\'ll simply pass a list of predefined emails.\\ncreate_calendar_event: A tool for creating new calendar events. Again, for simplicity, we\\'ll only return mock successful event creations without actual implementation. In practice, we can connect to a calendar service API and implement all the necessary logic here.\\nHere, we are defining a Python function for each tool, but more broadly, the tool can be any function or service that can receive and send objects.'),\n",
       " Document(metadata={'title': 'Tool Use and Agents on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-tooluse'}, page_content='Here, we are defining a Python function for each tool, but more broadly, the tool can be any function or service that can receive and send objects.\\ndef search_faqs(query): faqs = [ {\"text\" : \"Submitting Travel Expenses:\\\\nSubmit your expenses through our user-friendly finance tool.\"}, {\"text\" : \"Side Projects Policy:\\\\nWe encourage you to explore your passions! Just ensure there\\'s no conflict of interest with our business.\"}, {\"text\" : \"Wellness Benefits:\\\\nTo promote a healthy lifestyle, we provide gym memberships, on-site yoga classes, and health insurance.\"} ] return {\"faqs\" : faqs} def search_emails(query): emails = [ {\"from\": \"hr@co1t.com\", \"to\": \"david@co1t.com\", \"date\": \"2024-06-24\", \"subject\": \"A Warm Welcome to Co1t, David!\", \"text\": \"We are delighted to have you on board. Please find attached your first week\\'s agenda.\"}, {\"from\": \"it@co1t.com\", \"to\": \"david@co1t.com\", \"date\": \"2024-06-24\", \"subject\": \"Instructions for IT Setup\", \"text\": \"Welcome, David! To get you started, please follow the attached guide to set up your work accounts.\"}, {\"from\": \"john@co1t.com\", \"to\": \"david@co1t.com\", \"date\": \"2024-06-24\", \"subject\": \"First Week Check-In\", \"text\": \"Hi David, let\\'s chat briefly tomorrow to discuss your first week. Also, come join us for lunch this Thursday at noon to meet everyone!\"}, ] return {\"emails\" : emails} def create_calendar_event(date: str, time: str, duration: int): # You can implement any logic here return {\"is_success\": True, \"message\": f\"Created a {duration} hour long event at {time} on {date}\"} functions_map = { \"search_faqs\": search_faqs, \"search_emails\": search_emails, \"create_calendar_event\": create_calendar_event }\\nThe next step is to define the tool schemas in a format that can be passed to the Chat endpoint. The schema must contain the following fields: name, description, and parameter_definitions in the format shown below.'),\n",
       " Document(metadata={'title': 'Tool Use and Agents on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-tooluse'}, page_content='This schema informs the LLM about what the tool does, and the LLM decides whether to use a particular tool based on it. Therefore, the more descriptive and specific the schema, the more likely the LLM will make the right tool call decisions.\\ntools = [ { \"name\": \"search_faqs\", \"description\": \"Given a user query, searches a company\\'s frequently asked questions (FAQs) list and returns the most relevant matches to the query.\", \"parameter_definitions\": { \"query\": { \"description\": \"The query from the user\", \"type\": \"str\", \"required\": True } } }, { \"name\": \"search_emails\", \"description\": \"Given a user query, searches a person\\'s emails and returns the most relevant matches to the query.\", \"parameter_definitions\": { \"query\": { \"description\": \"The query from the user\", \"type\": \"str\", \"required\": True } } }, { \"name\": \"create_calendar_event\", \"description\": \"Creates a new calendar event of the specified duration at the specified time and date. A new event cannot be created on the same time as an existing event.\", \"parameter_definitions\": { \"date\": { \"description\": \"the date on which the event starts, formatted as mm/dd/yy\", \"type\": \"str\", \"required\": True }, \"time\": { \"description\": \"the time of the event, formatted using 24h military time formatting\", \"type\": \"str\", \"required\": True }, \"duration\": { \"description\": \"the number of hours the event lasts for\", \"type\": \"float\", \"required\": True } } } ]\\nThe tool is now ready to use. We can think of a tool use system as consisting of four components:\\nThe user\\nThe application\\nThe LLM\\nThe tools\\nAt its most basic, these four components interact in a workflow through four steps:\\nStep 1: Get user message. The LLM gets the user message (via the application).\\nStep 2: Generate tool calls. The LLM makes a decision on the tools to call (if any) and generates the tool calls.\\nStep 3: Get tool results. The application executes the tools and sends the tool results to the LLM.'),\n",
       " Document(metadata={'title': 'Tool Use and Agents on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-tooluse'}, page_content=\"Step 3: Get tool results. The application executes the tools and sends the tool results to the LLM.\\nStep 4: Generate response and citations. The LLM generates the response and citations and sends them back to the user.\\nLet's create a function called run_assistant to implement these steps and print out the key events and messages along the way. This function also optionally accepts the chat history as an argument to keep the state in a multi-turn conversation.\"),\n",
       " Document(metadata={'title': 'Tool Use and Agents on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-tooluse'}, page_content='model = \"cohere.command-r-plus-v1:0\" preamble=\"\"\"## Task and Context You are an assistant who assist new employees of Co1t with their first week. You respond to their questions and assist them with their needs. Today is Monday, June 24, 2024\"\"\" def run_assistant(message, chat_history=None): if chat_history is None: chat_history = [] # Step 1: get user message print(f\"Question:\\\\n{message}\") print(\"=\"*50) # Step 2: Generate tool calls (if any) response = co.chat( message=message, model=model, preamble=preamble, tools=tools, chat_history=chat_history ) while response.tool_calls: tool_calls = response.tool_calls if response.text: print(\"Tool plan:\") print(response.text,\"\\\\n\") print(\"Tool calls:\") for call in tool_calls: print(f\"Tool name: {call.name} | Parameters: {call.parameters}\") print(\"=\"*50) # Step 3: Get tool results tool_results = [] for tc in tool_calls: tool_call = {\"name\": tc.name, \"parameters\": tc.parameters} tool_output = functions_map[tc.name](**tc.parameters) tool_results.append({\"call\": tool_call, \"outputs\": [tool_output]}) # Step 4: Generate response and citations response = co.chat( message=\"\", model=model, preamble=preamble, tools=tools, tool_results=tool_results, chat_history=response.chat_history ) chat_history = response.chat_history # Print final response print(\"RESPONSE:\\\\n\") print(response.text) print(\"=\"*50) # Print citations (if any) if response.citations: print(\"\\\\nCITATIONS:\\\\n\") for citation in response.citations: print(citation) print(\"\\\\nCITED REFERENCES:\\\\n\") for document in response.documents: print(document) return chat_history\\nSingle-Step Scenario\\nCohere supports running tool use in single-step and multi-step modes. In a single-step scenario, the model will make only one round of tool calling. Having said that, within this one round, the model can decide to call multiple tools in parallel. This can be calling the same tool multiple times, calling different tools, or a combination of both.'),\n",
       " Document(metadata={'title': 'Tool Use and Agents on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-tooluse'}, page_content='Let’s look at an example of a new hire asking about IT access and the travel expense process.\\nchat_history = run_assistant(\"Any emails about setting up IT access? Also, how do I submit travel expenses?\") \\nGiven three tools to choose from, the model is able to pick the right tools (in this case, search_faqs and search_emails) based on what the user is asking for.\\nAlso, notice that the model first generates a plan about what it should do (\"I will do ...\") before actually generating the tool call(s).\\nAdditionally, the model also generates fine-grained citations in tool use mode based on the tool results it receives, the same way we saw with RAG.'),\n",
       " Document(metadata={'title': 'Tool Use and Agents on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-tooluse'}, page_content='Question: Any emails about setting up IT access? Also, how do I submit travel expenses? ================================================== Tool plan: I will search for emails about setting up IT access and travel expenses. Tool calls: Tool name: search_emails | Parameters: {\\'query\\': \\'IT access\\'} Tool name: search_emails | Parameters: {\\'query\\': \\'travel expenses\\'} ================================================== RESPONSE: You have an email from *it@co1t.com* with the subject \"Instructions for IT Setup\". It contains an attached guide to set up your work accounts. To submit travel expenses, you can send an email to *expenses@co1t.com*. ================================================== CITATIONS: start=23 end=35 text=\\'*it@co1t.com\\' document_ids=[\\'search_emails:0:2:0\\', \\'search_emails:1:2:0\\'] start=54 end=80 text=\\'\"Instructions for IT Setup\\' document_ids=[\\'search_emails:0:2:0\\', \\'search_emails:1:2:0\\'] start=98 end=141 text=\\'attached guide to set up your work accounts\\' document_ids=[\\'search_emails:0:2:0\\', \\'search_emails:1:2:0\\'] CITED REFERENCES: {\\'emails\\': \\'[{\"date\":\"2024-06-24\",\"from\":\"hr@co1t.com\",\"subject\":\"A Warm Welcome to Co1t, David!\",\"text\":\"We are delighted to have you on board. Please find attached your first week\\\\\\'s agenda.\",\"to\":\"david@co1t.com\"},{\"date\":\"2024-06-24\",\"from\":\"it@co1t.com\",\"subject\":\"Instructions for IT Setup\",\"text\":\"Welcome, David! To get you started, please follow the attached guide to set up your work accounts.\",\"to\":\"david@co1t.com\"},{\"date\":\"2024-06-24\",\"from\":\"john@co1t.com\",\"subject\":\"First Week Check-In\",\"text\":\"Hi David, let\\\\\\'s chat briefly tomorrow to discuss your first week. Also, come join us for lunch this Thursday at noon to meet everyone!\",\"to\":\"david@co1t.com\"}]\\', \\'id\\': \\'search_emails:0:2:0\\', \\'tool_name\\': \\'search_emails\\'} {\\'emails\\': \\'[{\"date\":\"2024-06-24\",\"from\":\"hr@co1t.com\",\"subject\":\"A Warm Welcome to Co1t, David!\",\"text\":\"We are delighted to have you on board. Please find attached your first week\\\\\\'s'),\n",
       " Document(metadata={'title': 'Tool Use and Agents on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-tooluse'}, page_content='{\\'emails\\': \\'[{\"date\":\"2024-06-24\",\"from\":\"hr@co1t.com\",\"subject\":\"A Warm Welcome to Co1t, David!\",\"text\":\"We are delighted to have you on board. Please find attached your first week\\\\\\'s agenda.\",\"to\":\"david@co1t.com\"},{\"date\":\"2024-06-24\",\"from\":\"it@co1t.com\",\"subject\":\"Instructions for IT Setup\",\"text\":\"Welcome, David! To get you started, please follow the attached guide to set up your work accounts.\",\"to\":\"david@co1t.com\"},{\"date\":\"2024-06-24\",\"from\":\"john@co1t.com\",\"subject\":\"First Week Check-In\",\"text\":\"Hi David, let\\\\\\'s chat briefly tomorrow to discuss your first week. Also, come join us for lunch this Thursday at noon to meet everyone!\",\"to\":\"david@co1t.com\"}]\\', \\'id\\': \\'search_emails:1:2:0\\', \\'tool_name\\': \\'search_emails\\'}'),\n",
       " Document(metadata={'title': 'Tool Use and Agents on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-tooluse'}, page_content='Multi-Step Scenario\\nCohere supports running more complex tasks in tool use — tasks that require tool calls to happen in a sequence. This is referred to as \"multi-step\" tool use.\\nTo illustrate this, let\\'s ask the assistant to block time for any lunch invites received in the email.\\nchat_history = run_assistant(\"Can you check if there are any lunch invites, and for those days, block an hour on my calendar from 12-1PM.\")\\nHere, we see the assistant running these steps:\\nFirst, it calls the search_emails tool to find any lunch invites, and it finds one.\\nNext, it calls the create_calendar_event tool to create an event to block the person\\'s calendar on the day mentioned in the email.\\nThis is also an example of tool use enabling a write operation instead of just a read operation that we saw with RAG.'),\n",
       " Document(metadata={'title': 'Tool Use and Agents on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-tooluse'}, page_content='Question: Can you check if there are any lunch invites, and for those days, block an hour on my calendar from 12-1PM. ================================================== Tool plan: I will search the user\\'s emails for lunch invites, and then create calendar events for the dates of the invites. Tool calls: Tool name: search_emails | Parameters: {\\'query\\': \\'lunch invite\\'} ================================================== Tool plan: I have found an email from John inviting David for lunch on Thursday at noon. I will now create a calendar event for this lunch. Tool calls: Tool name: create_calendar_event | Parameters: {\\'date\\': \\'06/27/24\\', \\'duration\\': 1, \\'time\\': \\'12:00\\'} ================================================== RESPONSE: I have found an email from John inviting you for lunch on Thursday at noon. I have created a calendar event for this lunch. ================================================== CITATIONS: start=16 end=31 text=\\'email from John\\' document_ids=[\\'search_emails:0:2:0\\'] start=49 end=74 text=\\'lunch on Thursday at noon\\' document_ids=[\\'search_emails:0:2:0\\'] start=93 end=107 text=\\'calendar event\\' document_ids=[\\'create_calendar_event:0:4:0\\'] CITED REFERENCES: {\\'emails\\': \\'[{\"date\":\"2024-06-24\",\"from\":\"hr@co1t.com\",\"subject\":\"A Warm Welcome to Co1t, David!\",\"text\":\"We are delighted to have you on board. Please find attached your first week\\\\\\'s agenda.\",\"to\":\"david@co1t.com\"},{\"date\":\"2024-06-24\",\"from\":\"it@co1t.com\",\"subject\":\"Instructions for IT Setup\",\"text\":\"Welcome, David! To get you started, please follow the attached guide to set up your work accounts.\",\"to\":\"david@co1t.com\"},{\"date\":\"2024-06-24\",\"from\":\"john@co1t.com\",\"subject\":\"First Week Check-In\",\"text\":\"Hi David, let\\\\\\'s chat briefly tomorrow to discuss your first week. Also, come join us for lunch this Thursday at noon to meet everyone!\",\"to\":\"david@co1t.com\"}]\\', \\'id\\': \\'search_emails:0:2:0\\', \\'tool_name\\': \\'search_emails\\'} {\\'id\\': \\'create_calendar_event:0:4:0\\', \\'is_success\\': \\'true\\', \\'message\\': \\'Created a'),\n",
       " Document(metadata={'title': 'Tool Use and Agents on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-tooluse'}, page_content='at noon to meet everyone!\",\"to\":\"david@co1t.com\"}]\\', \\'id\\': \\'search_emails:0:2:0\\', \\'tool_name\\': \\'search_emails\\'} {\\'id\\': \\'create_calendar_event:0:4:0\\', \\'is_success\\': \\'true\\', \\'message\\': \\'Created a 1 hour long event at 12:00 on 06/27/24\\', \\'tool_name\\': \\'create_calendar_event\\'}'),\n",
       " Document(metadata={'title': 'Tool Use and Agents on Amazon Bedrock', 'source': 'https://cohere.com/llmu/co-aws-tooluse'}, page_content='Conclusion\\nIn this chapter, we explored how to use Command R+ on Amazon Bedrock in an application that implements tool use. We covered the steps to create and define the tools and looked at how tool use works in single-step and multi-step scenarios.\\nIf you want to learn more LLM use cases, visit our LLMU chapter on use case patterns.\\nIn Chapter 8, we’ll look at an example of fine-tuning the Command R model using SageMaker.'),\n",
       " Document(metadata={'title': 'Fine-Tuning Cohere Command R on Amazon SageMaker', 'source': 'https://cohere.com/llmu/co-aws-finetuning'}, page_content=\"Fine-Tuning Cohere Command R on Amazon SageMakerQualified\\nContents\\nIntroduction\\nCode Walkthrough \\nFine-Tuning\\nInference\\nConclusion\\nWe’ll use Cohere’s Python SDK (cohere-aws) for the code examples. Follow along in this notebook.\\nIntroduction\\nFine-tuning an LLM involves taking a default model and tailoring it for a specific purpose or domain. This process enhances the model's performance and adaptability by training it on a smaller, more focused dataset relevant to the desired task. This additional training refines the model's capabilities and makes it more attuned to specific needs.\\nFine-tuning is recommended when you want to teach the model a new task or leverage your company's unique knowledge base. Fine-tuning models is also helpful for generating a specific writing style or format.\\nFor example, a global provider of video conferencing services needed to add call summarization and action item extraction features to its video conferencing offering in response to strong customer demand.\\nThey built a solution by fine-tuning Cohere’s Command model on the nuances of transcript summarization. It accurately summarized live meetings in bullet points, including action items for call participants, and delivered it on the company’s proprietary conferencing platform.\\nImplementing an automated summarization system at a global provider of video conferencing services\\nFine-tuning of Cohere models is available in both Amazon Bedrock and Amazon SageMaker. Sample notebooks on how to fine-tune a model on both platforms are available on the cohere-aws repository.\\nIn this chapter, we’ll review one example using SageMaker to fine-tune the Command R model. At the time of writing, Command R fine-tuning is not yet available on Bedrock.\\nCode Walkthrough\\nWe’ll go through the fine-tuning steps, followed by using a model once fine-tuning is complete.\\nAn overview of what we'll cover in the code walkthrough\\nFine-Tuning\"),\n",
       " Document(metadata={'title': 'Fine-Tuning Cohere Command R on Amazon SageMaker', 'source': 'https://cohere.com/llmu/co-aws-finetuning'}, page_content='Code Walkthrough\\nWe’ll go through the fine-tuning steps, followed by using a model once fine-tuning is complete.\\nAn overview of what we\\'ll cover in the code walkthrough\\nFine-Tuning\\nLet\\'s install and import the necessary libraries and set up our Cohere client. Here we are using the cohere_aws SDK, which makes it easy to use Cohere\\'s models on AWS, together with AWS’s boto3 and sagemaker libraries.\\n# ! pip install --upgrade cohere-aws \\nimport os from cohere_aws import Client import boto3 import sagemaker as sage from sagemaker.s3 import S3Uploader ## Set environment variables with the AWS credentials os.environ[\\'AWS_ACCESS_KEY_ID\\'] = \"YOUR_AWS_ACCESS_KEY_ID\" os.environ[\\'AWS_SECRET_ACCESS_KEY\\'] = \"YOUR_AWS_SECRET_ACCESS_KEY\" os.environ[\\'AWS_SESSION_TOKEN\\'] = \"YOUR_AWS_SESSION_TOKEN\" \\nThe next step is to define the model package Amazon Resource Names (ARN) for the Command R model. The ARN is an identifying string for a SageMaker resource, and it varies between the regions where a resource is available.\\nHere, we define the Cohere package for the Command R model and map the model package against each region, which gives the complete ARN for each region.'),\n",
       " Document(metadata={'title': 'Fine-Tuning Cohere Command R on Amazon SageMaker', 'source': 'https://cohere.com/llmu/co-aws-finetuning'}, page_content='Here, we define the Cohere package for the Command R model and map the model package against each region, which gives the complete ARN for each region.\\nregion = boto3.Session().region_name cohere_package = \"cohere-command-r-ft-v-0-1-2-bae2282f0f4a30bca8bc6fea9efeb7ca\" # Mapping for algorithms algorithm_map = { \"us-east-1\": f\"arn:aws:sagemaker:us-east-1:865070037744:algorithm/{cohere_package}\", \"us-east-2\": f\"arn:aws:sagemaker:us-east-2:057799348421:algorithm/{cohere_package}\", \"us-west-2\": f\"arn:aws:sagemaker:us-west-2:594846645681:algorithm/{cohere_package}\", \"eu-central-1\": f\"arn:aws:sagemaker:eu-central-1:446921602837:algorithm/{cohere_package}\", \"ap-southeast-1\": f\"arn:aws:sagemaker:ap-southeast-1:192199979996:algorithm/{cohere_package}\", \"ap-southeast-2\": f\"arn:aws:sagemaker:ap-southeast-2:666831318237:algorithm/{cohere_package}\", \"ap-northeast-1\": f\"arn:aws:sagemaker:ap-northeast-1:977537786026:algorithm/{cohere_package}\", \"ap-south-1\": f\"arn:aws:sagemaker:ap-south-1:077584701553:algorithm/{cohere_package}\", } if region not in algorithm_map.keys(): raise Exception(f\"Current boto3 session region {region} is not supported.\") arn = algorithm_map[region]\\nNext, we need to upload the dataset required for fine-tuning. The expected format for fine-tuning the Command R model via the Chat endpoint is a jsonl file, where each JSON object is a conversation containing a series of messages.\\nA System message in the beginning that acts as the preamble that guides the whole conversation\\nMultiple pairs of User and Chatbot messages representing the conversation that takes place between a human user and a chatbot\\nHere’s an example of what a data point looks like:'),\n",
       " Document(metadata={'title': 'Fine-Tuning Cohere Command R on Amazon SageMaker', 'source': 'https://cohere.com/llmu/co-aws-finetuning'}, page_content='Multiple pairs of User and Chatbot messages representing the conversation that takes place between a human user and a chatbot\\nHere’s an example of what a data point looks like:\\n{\\'messages\\': [{\\'role\\': \\'System\\', \\'content\\': \\'You are a chatbot trained to answer to my every question.\\' }, {\\'role\\': \\'User\\', \\'content\\': \\'Hello\\' }, {\\'role\\': \\'Chatbot\\', \\'content\\': \\'Greetings! How can I help you?\\' }, {\\'role\\': \\'User\\', \\'content\\': \\'What makes a good running route?\\' }, {\\'role\\': \\'Chatbot\\', \\'content\\': \\'A sidewalk-lined road is ideal so that you’re up and off the road away from vehicular traffic.\\' } ] } \\nWe’ll use the ScienceQA dataset for this demonstration, with the pre-formatted versions of the training and evaluation sets available. Here are two example data points from the dataset:\\n{\"messages\": [{\"role\": \"User\", \"content\": \"Context: Fish live underwater. They have fins, not limbs. A tiger shark is an example of a fish.\\\\nQuestion: Select the fish below.\\\\nOptions:(A) black howler (B) cardinalfish\\\\nAnswer:\"}, {\"role\": \"Chatbot\", \"content\": \"B\"}]} \\n{\"messages\": [{\"role\": \"User\", \"content\": \"Context: Select the best estimate.\\\\nQuestion: How long is a guitar?\\\\nOptions:(A) 1 centimeter (B) 1 meter (C) 1 kilometer (D) 1 millimeter\\\\nAnswer:\"}, {\"role\": \"Chatbot\", \"content\": \"B\"}]} \\nA few points to note:\\nThe evaluation dataset is optional. If no evaluation dataset is provided, the training dataset will be auto-split into training and evaluation datasets at an 80:20 ratio.\\nThe dataset must contain at least one example. If an evaluation dataset is absent, the training dataset must contain at least two examples.\\nHowever, we recommend using a dataset that contains at least 100 examples. Larger datasets are more likely to yield high-quality fine-tunes.\\nWe can now upload the training and evaluation datasets to an S3 bucket using the S3Uploader.upload method.\\ns3_data_dir = f\"s3://YOUR_S3_DATA_PATH\" # DO NOT add a trailing slash at the end'),\n",
       " Document(metadata={'title': 'Fine-Tuning Cohere Command R on Amazon SageMaker', 'source': 'https://cohere.com/llmu/co-aws-finetuning'}, page_content='We can now upload the training and evaluation datasets to an S3 bucket using the S3Uploader.upload method.\\ns3_data_dir = f\"s3://YOUR_S3_DATA_PATH\" # DO NOT add a trailing slash at the end \\nsess = sage.Session() train_dataset = S3Uploader.upload(\"./sample_finetune_scienceQA_train.jsonl\", s3_data_dir, sagemaker_session=sess) eval_dataset = S3Uploader.upload(\"./sample_finetune_scienceQA_eval.jsonl\", s3_data_dir, sagemaker_session=sess)\\nWe also need to specify a directory on S3 where fine-tuned models should be stored.\\ns3_models_dir = f\"s3://YOUR_S3_MODEL_PATH\" # DO NOT add a trailing slash at the end \\nNext, we create a Cohere client and define the set of hyperparameters to be used during fine-tuning. Hyperparameters are optional, but they provide more flexibility to developers who want to further optimize the fine-tuning process.\\nDevelopers can define the following hyperparameters:\\ntrain_epochs: The maximum number of training steps\\nlearning_rate: The initial learning rate to be used during training\\ntrain_batch_size: The batch size during training\\nearly_stopping_enabled: Enables early stopping\\nearly_stopping_patience: Stops training if the loss metric does not improve beyond the early_stopping_threshold for this many rounds of evaluation\\nearly_stopping_threshold: How much the loss must improve to prevent early stopping\\nco = Client(region_name=region) # Example of how to pass hyperparameters to the fine-tuning job train_parameters = { \"train_epochs\": 1, \"early_stopping_patience\": 2, \"early_stopping_threshold\": 0.001, \"learning_rate\": 0.01, \"train_batch_size\": 16, } \\nWe are now ready to start the fine-tuning process. To do this, we use the create_finetune method and specify the required arguments, as follows.\\nfinetune_name = \"test-scienceQA\" co.create_finetune(arn=arn, name=finetune_name, train_data=train_dataset, eval_data=eval_dataset, s3_models_dir=s3_models_dir, instance_type=\"ml.p4de.24xlarge\", training_parameters=train_parameters )'),\n",
       " Document(metadata={'title': 'Fine-Tuning Cohere Command R on Amazon SageMaker', 'source': 'https://cohere.com/llmu/co-aws-finetuning'}, page_content='A successful fine-tuning process will return a response like this:'),\n",
       " Document(metadata={'title': 'Fine-Tuning Cohere Command R on Amazon SageMaker', 'source': 'https://cohere.com/llmu/co-aws-finetuning'}, page_content='INFO:sagemaker:Creating training-job with name: cohere-command-R-ft-v-0-1-1-2024-05-07-04-45-52-979 2024-05-07 04:45:53 Starting - Starting the training job... 2024-05-07 04:46:01 Pending - Training job waiting for capacity... 2024-05-07 04:46:39 Pending - Preparing the instances for training........................ 2024-05-07 04:50:49 Downloading - Downloading input data........................................................................................................................................................................ 2024-05-07 05:18:40 Training - Training image download completed. Training in progress.........INFO:root:Loading weights from /opt/ml/additonals3data INFO:root:[Rank 7]Finished loading all variables in 89.23 s INFO:root:[Rank 6]Finished loading all variables in 89.34 s INFO:root:[Rank 2]Finished loading all variables in 90.56 s INFO:root:[Rank 5]Finished loading all variables in 89.63 s INFO:root:[Rank 1]Finished loading all variables in 91.93 s INFO:root:[Rank 0]Finished loading all variables in 89.76 s INFO:root:[Rank 4]Finished loading all variables in 90.22 s INFO:root:[Rank 3]Finished loading all variables in 94.28 s INFO:root:Finished assigning 646 variables from checkpoint INFO:root:Trainable params: 194560, Non trainable params: 4636421376 INFO:root:Weights loaded from /opt/ml/additonals3data INFO:root:After removing empty, NA and duplicate documents: 5262 INFO:root:After removing empty, NA and duplicate documents: 1901 INFO:root:Finished step 1, train loss: 0.714844, step time: 10.336927s INFO:root:Finished step 2, train loss: 0.204102, step time: 6.781313s INFO:root:Finished step 3, train loss: 0.208984, step time: 6.751096s ... (truncated for brevity) INFO:root:Finished step 159, train loss: 0.072754, step time: 6.774719s INFO:root:Finished step 160, train loss: 0.032715, step time: 6.775016s INFO:root:Exporting finished. INFO:root:Model weights exported at /opt/ml/model/finetune/export_test-scienceQA in 135.482641s'),\n",
       " Document(metadata={'title': 'Fine-Tuning Cohere Command R on Amazon SageMaker', 'source': 'https://cohere.com/llmu/co-aws-finetuning'}, page_content='INFO:root:Finished step 160, train loss: 0.032715, step time: 6.775016s INFO:root:Exporting finished. INFO:root:Model weights exported at /opt/ml/model/finetune/export_test-scienceQA in 135.482641s 2024-05-07 05:57:48 Uploading - Uploading generated training model 2024-05-07 05:59:24 Completed - Training job completed Training seconds: 4114 Billable seconds: 4114'),\n",
       " Document(metadata={'title': 'Fine-Tuning Cohere Command R on Amazon SageMaker', 'source': 'https://cohere.com/llmu/co-aws-finetuning'}, page_content='Inference\\nWe can now use the fine-tuned model for inference. With SageMaker, we’ll need to create an endpoint via an AWS instance. The marketplace listing provides more details on the recommended instance type for a particular model, including pricing.\\nThe steps are the same as the SageMaker example in an earlier chapter, with a few additional arguments. In particular, s3_models_dir defines the location where the fine-tuned model is stored.\\nendpoint_name=\"command-finetune-test-scienceQA\" co.create_endpoint(arn=arn, endpoint_name=endpoint_name, s3_models_dir=s3_models_dir, recreate=True, instance_type=\"ml.p4de.24xlarge\") \\nThis returns the following response:\\nFound one fine-tuned model: s3://YOUR_S3_MODEL_PATH/test-scienceQA.tar.gz ......... --------------! \\nWe can then perform inference on the endpoint. For this, we call the Chat endpoint with the user message and get a response generated by the model.\\nmessage = \"Select the best estimate.\\\\nQuestion: How long is a guitar?\\\\nOptions:(A) 32 feet (B) 32 yards (C) 32 inches (D) 32 miles\\\\nAnswer:\" result = co.chat(message=message, return_prompt=True) print(result)\\ncohere.Chat { response_id: ba6be7d5-0509-4038-adfb-7de695c264b0 generation_id: 3b509b4f-b6b3-4eee-ae5d-ccb5e46f8822 text: C chat_history: [{\\'role\\': \\'USER\\', \\'message\\': \\'Select the best estimate.\\\\nQuestion: How long is a guitar?\\\\nOptions:(A) 32 feet (B) 32 yards (C) 32 inches (D) 32 miles\\\\nAnswer:\\'}, {\\'role\\': \\'CHATBOT\\', \\'message\\': \\'C\\'}] preamble: None finish_reason: COMPLETE token_count: None tool_calls: None citations: None documents: None search_results: None search_queries: None is_search_required: None } \\nWe can test the fine-tuning performance on the evaluation dataset, as follows:'),\n",
       " Document(metadata={'title': 'Fine-Tuning Cohere Command R on Amazon SageMaker', 'source': 'https://cohere.com/llmu/co-aws-finetuning'}, page_content='We can test the fine-tuning performance on the evaluation dataset, as follows:\\nimport json from tqdm import tqdm total = 0 correct = 0 for line in tqdm(open(\\'./sample_finetune_scienceQA_eval.jsonl\\').readlines()): total += 1 question_answer_json = json.loads(line) question = question_answer_json[\"messages\"][0][\"content\"] answer = question_answer_json[\"messages\"][1][\"content\"] model_ans = co.chat(message=question, temperature=0, k=1).text if model_ans == answer: correct +=1 \\nprint(f\"Accuracy of finetuned model is %.4f\" % (correct / total)) \\nIn our case, we get 83.10% accuracy.\\n100%|██████████| 2095/2095 [01:35<00:00, 21.83it/s] Accuracy of finetuned model is 0.8310 \\nImportant Note: You will continue to incur charges as long as an endpoint is running, so remember to delete it when your usage ends.\\nco.delete_endpoint() co.close() \\nConclusion\\nFine-tuning an LLM is a powerful technique for customizing and enhancing its performance for specific tasks and domains. Businesses can use fine-tuning to improve accuracy and address unique scenarios specific to their domain. In this chapter, we explored how to fine-tune Cohere Command on Amazon SageMaker.\\nThis chapter concludes our module, Cohere on AWS, and we hope you’ve found it useful and informative. We encourage you to continue your learning by exploring more modules at Cohere’s LLM University.'),\n",
       " Document(metadata={'title': 'Semantic Search', 'source': 'https://cohere.com/llmu/introduction-semantic-search'}, page_content='Semantic SearchQualified\\nThis chapter uses the same notebook as the previous chapter.\\nNote: This chapter covers the basics of semantic search. If you want to explore this topic further, we have a dedicated LLMU module on semantic search.\\nWe deal with unstructured text data on a regular basis, and one of the common needs is to search for information from a vast repository. A common approach is keyword-matching, but the problem with this is that the results are limited to the exact query entered.\\nThis is where we can utilize text embeddings. As you learned in the previous chapter, embeddings can capture the meaning of a piece of text beyond keyword-matching.\\nIn this chapter, you\\'ll learn how to use embeddings to build a search capability that surfaces relevant information based on the semantic meaning of a query.\\nStep-by-Step Guide\\nLet’s use the same 9 data points from the previous chapter and pretend that those make up a list of Frequently Asked Questions (FAQ). Whenever a new query comes in, we want to match that query to the closest FAQ so we can provide the most relevant answer. Here is the list again:\\n- which airlines fly from boston to washington dc via other cities - show me the airlines that fly between toronto and denver - show me round trip first class tickets from new york to miami - i\\'d like the lowest fare from denver to pittsburgh - show me a list of ground transportation at boston airport - show me boston ground transportation - of all airlines which airline has the most arrivals in atlanta - what ground transportation is available in boston - i would like your rates between atlanta and boston on september third - which airlines fly between boston and pittsburgh \\nLet’s say a person enters the query “How can I find a taxi or a bus when the plane lands?”. Note that the “taxi” and \"bus\" keywords don\\'t exist anywhere in our FAQ, so let’s see what results we get with semantic search.'),\n",
       " Document(metadata={'title': 'Semantic Search', 'source': 'https://cohere.com/llmu/introduction-semantic-search'}, page_content='Implementation-wise, there are many ways we can approach this. And in our case, we use cosine similarity to compare the embeddings of the search query with those from the FAQ and find the most similar ones.\\nStep 1: Embed the Documents\\nThe first step is to turn the documents into embeddings. We embed each inquiry by calling Cohere’s Embed endpoint with co.embed(). It takes in texts as input and returns embeddings as output. We supply three parameters:\\ntexts: The list of texts you want to embed\\nmodel: The model to use to generate the embedding. At the time of writing, there are four models available\\ninput_type — Specifies the type of document to be embedded. At the time of writing, there are four options:\\nsearch_document: For documents against which search is performed\\nsearch_query: For query documents\\nclassification: For when the embeddings will be used as an input to a text classifier\\nclustering: For when you want to cluster the embeddings\\ndef get_embeddings(texts, model=\\'embed-english-v3.0\\', input_type=\"search_document\"): output = co.embed( model=model, input_type=input_type, texts=texts) return output.embeddings df[\\'query_embeds\\'] = get_embeddings(df[\\'query\\'].tolist()) \\nStep 2: Embed the Search Query\\nNext, we embed the query using the same get_embeddings() function. But now we set search_query as the input_type because we\\'re now embedding the search query.\\n# Define new query new_query = \"How can I find a taxi or a bus when the plane lands?\" # Get embeddings of the new query new_query_embeds = get_embeddings([new_query], input_type=\"search_query\")[0] \\nStep 3: Perform Search\\nNext, we create a function get_similarity() that uses cosine similarity to determine how similar each of the documents is to the query.'),\n",
       " Document(metadata={'title': 'Semantic Search', 'source': 'https://cohere.com/llmu/introduction-semantic-search'}, page_content='Step 3: Perform Search\\nNext, we create a function get_similarity() that uses cosine similarity to determine how similar each of the documents is to the query.\\n# Calculate cosine similarity between the search query and existing queries def get_similarity(target, candidates): # Turn list into array candidates = np.array(candidates) target = np.expand_dims(np.array(target),axis=0) # Calculate cosine similarity sim = cosine_similarity(target, candidates) sim = np.squeeze(sim).tolist() sort_index = np.argsort(sim)[::-1] sort_score = [sim[i] for i in sort_index] similarity_scores = zip(sort_index,sort_score) # Return similarity scores return similarity_scores # Get the similarity between the search query and existing queries similarity = get_similarity(new_query_embeds, embeds[:sample]) \\nWe\\'ll then view the documents in decreasing order of similarity.\\n# View the top 5 articles print(\\'Query:\\') print(new_query,\\'\\\\n\\') print(\\'Most Similar Documents:\\') for idx, sim in similarity: print(f\\'Similarity: {sim:.2f};\\', df.iloc[idx][\\'query\\']) \\nBelow are the results, showing the FAQs with their similarity score (ranging from 0 to 1; higher scores are better). The top-3 ranked FAQs we get are inquiries about ground transportation, which are very relevant considering the other options. Notice that they don\\'t contain the keywords “taxi” or \"bus\", and the search query doesn\\'t contain the keywords “ground transportation”. But they are the most similar in meaning compared to the rest, and their embeddings capture this.'),\n",
       " Document(metadata={'title': 'Semantic Search', 'source': 'https://cohere.com/llmu/introduction-semantic-search'}, page_content='Query: How can I find a taxi or a bus when the plane lands? Most Similar Documents: Similarity: 0.37; show me a list of ground transportation at boston airport Similarity: 0.36; what ground transportation is available in boston Similarity: 0.33; show me boston ground transportation Similarity: 0.27; show me the airlines that fly between toronto and denver Similarity: 0.25; which airlines fly from boston to washington dc via other cities Similarity: 0.24; of all airlines which airline has the most arrivals in atlanta Similarity: 0.18; i\\'d like the lowest fare from denver to pittsburgh Similarity: 0.17; show me round trip first class tickets from new york to miami Similarity: 0.17; i would like your rates between atlanta and boston on september third \\nStep 4: Visualize the Results in a 2D Plot\\nWe can also plot this new query on a 2D plot as we did earlier. And we see that the query is located closest to the FAQs about ground transportation.\\nThe query about \"a taxi or a bus\" is located closest to documents about ground transportation\\nConclusion\\nIn this chapter you learned how to use embedding and similarity to build a semantic search model. There are many more applications of embeddings, which you\\'ll learn in the following chapters!'),\n",
       " Document(metadata={'title': 'Text Clustering', 'source': 'https://cohere.com/llmu/clustering-with-embeddings'}, page_content='Text ClusteringQualified\\nThis chapter uses the same notebook as the previous chapter.\\nAs the amount of unstructured text data increases, organizations will want to be able to derive an understanding of its contents. One example would be to discover underlying topics in a collection of documents so we can explore trends and insights. Another could be for businesses to segment customers based on preferences and activity.\\nThese kinds of tasks fall under a category called clustering. In machine learning, clustering is a process of grouping similar documents into clusters. It is used to organize a large number of documents into a smaller number of groups. And it lets us discover emerging patterns in a collection of documents without us having to specify much information beyond supplying the data.\\nIn this chapter, you will learn how to use embeddings to partition a text dataset into distinct clusters of semantically related sentences.\\nStep-by-Step Guide\\nLet’s look at an example using the same 9 data points.\\n- which airlines fly from boston to washington dc via other cities - show me the airlines that fly between toronto and denver - show me round trip first class tickets from new york to miami - i\\'d like the lowest fare from denver to pittsburgh - show me a list of ground transportation at boston airport - show me boston ground transportation - of all airlines which airline has the most arrivals in atlanta - what ground transportation is available in boston - i would like your rates between atlanta and boston on september third - which airlines fly between boston and pittsburgh \\nStep 1: Embed the Text for Clustesring\\nWe embed the documents using the same get_embeddings() function as before, but now we set input_type=\"clustering\" because we\\'ll use the embeddings for clustering.\\n# Embed the text for clustering df[\\'clustering_embeds\\'] = get_embeddings(df[\\'query\\'].tolist(), input_type=\"clustering\") embeds = np.array(df[\\'clustering_embeds\\'].tolist())'),\n",
       " Document(metadata={'title': 'Text Clustering', 'source': 'https://cohere.com/llmu/clustering-with-embeddings'}, page_content='# Embed the text for clustering df[\\'clustering_embeds\\'] = get_embeddings(df[\\'query\\'].tolist(), input_type=\"clustering\") embeds = np.array(df[\\'clustering_embeds\\'].tolist()) \\nStep 2: Cluster the Embeddings\\nImplementation-wise, we use the K-means algorithms to cluster these data points (if you\\'d like to learn more about it, please check out this video about the K-means algorithm).\\nOther than providing the embeddings, the only other key information we need to provide for the algorithm is the number of clusters we want to find. This is normally larger in actual applications, but since our dataset is small, we’ll set the number of clusters to 2.\\n# Pick the number of clusters n_clusters = 2 # Cluster the embeddings kmeans_model = KMeans(n_clusters=n_clusters, random_state=0) classes = kmeans_model.fit_predict(embeds).tolist() # Store the cluster assignments df_clust = df_pc2.copy() df_clust[\\'cluster\\'] = (list(map(str,classes))) \\nStep 3: Visualize the Results in a 2D Plot\\nThe plot below shows the clusters that the algorithm returned. It looks to be spot on, where we have one cluster related to airline information and one cluster related to ground service information.\\nClustering results with 2 clusters\\nConclusion\\nIn this chapter, you learned how to cluster a dataset of sentences, and you observed that each cluster corresponds to a particular topic.'),\n",
       " Document(metadata={'title': 'Text Classification', 'source': 'https://cohere.com/llmu/text-classification'}, page_content='Text ClassificationQualified\\nIn previous chapters, we saw how to use embeddings to enable use cases such as semantic search, clustering, and topic modeling. All of these are \"unsupervised learning algorithms\", where we don’t know the number of groups and what they are.\\nBut what if we already know the groups (or classes) that we want to group our documents into? In this chapter, you\\'ll learn about classification, which is a \"supervised learning algorithm\" where we know the specific classes we want to use.\\nLike all machine learning models, classification models need to be evaluated in order to improve and measure their performance. In this chapter, you\\'ll also learn about four different ways to evaluate classification models.\\nTypes of Text Classification\\nA classification task falls under one of these two categories:\\nBinary classification, where the number of classes is two. Here are some examples:\\nA spam classifier could assign emails one one of two classes: \"Spam\" or \"Not spam\".\\nAn online forum could use a toxicity classifier to assist with content moderation by classifying posts as \"Neutral\" or \"Toxic\".\\nMulti-class classification, where the number of classes is more than two. Here are some examples:\\nA restaurant could use a sentiment classifier to categorize customer reviews as \"Positive\", \"Neutral\", or \"Negative\".\\nFor businesses that deal with a large volume of inbound communication, a topic classifier could assign the message to the relevant department (one of \"Customer Service\", \"Returns\", or \"Shipping\") for resolution. We could assign just one label to each message, or permit multiple labels. A classifier that allows multiple labels is a multi-label classifier.'),\n",
       " Document(metadata={'title': 'Text Classification', 'source': 'https://cohere.com/llmu/text-classification'}, page_content='A company could build a chatbot intent classifier to detect the intent behind a user\\'s message (one of \"Greeting\", \"Shipping and handling policy\", \"Start return or exchange\", \"Track order\", or \"Bye\"). Intent classifiers are broader than topic classifiers, as they include intents like “Greeting” or \"Bye\" that enable them to respond appropriately when a user says “hi” or “bye”, for example.\\nExamples of binary and multi-class classification\\nEvaluation Metrics\\nTo understand how to evaluate classification models, we\\'ll work with an email classification example. Our task is to classify a list of emails into one of two classes: Spam or Not Spam. We’ll represent Spam with the integer 1 (or Positive) and Not Spam with 0 (or Negative).\\nThe dataset contains 20 email titles. We put each data point through a binary classifier to get the predicted class and then compare it with its actual class.\\nThe classifier returns the following outcome:\\nThe email binary classification dataset with the actual and predicted classes\\nWe\\'ll cover 4 of the most important ways to evaluate classification models: Accuracy, Precision, Recall, and F1. In each case, we\\'ll calculate the performance of this classifier as an example.\\nAccuracy\\nThe most straightforward way to measure a classifier’s performance is using the Accuracy metric. Here, we compare the actual and predicted class of each data point, and each match counts for one correct prediction.\\nAccuracy is then given as the number of correct predictions divided by the total number of predictions. From the spam classifier output above, we have 15 correct predictions and 5 incorrect predictions, which gives us an Accuracy of 75%.\\nAccuracy calculation\\nAccuracy is often used as the measure of classification performance because it is simple to compute and easy to interpret. However, it can turn out to be misleading in some cases.'),\n",
       " Document(metadata={'title': 'Text Classification', 'source': 'https://cohere.com/llmu/text-classification'}, page_content=\"Accuracy is often used as the measure of classification performance because it is simple to compute and easy to interpret. However, it can turn out to be misleading in some cases.\\nThis is especially true when dealing with imbalanced data, a scenario when certain classes contain way more data points than the others.\\nLet's go back to our dataset to understand this. Notice that if the classifier had not been learning anything and was simply classifying all the outputs to be 0 (Not Spam), we would get 17 out of 20 correct classifications, which translates to a very high Accuracy of 85%! Clearly, something isn’t right.\\nIf you haven’t noticed yet, our dataset is indeed imbalanced. We have way more emails that are not spam than emails that are spam.\\nThe issue of imbalanced datasets is common in the real wo\\xad\\xadrld.\\xad For this, there must be a better way to measure a classifier’s performance than using Accuracy alone.\\nConfusion Matrix\\nThe other three metrics can provide a more balanced view of a classifier’s true performance. But before we can see them in action, we need to first understand the Confusion Matrix.\\nThe Confusion Matrix takes the classification results and groups them into four categories:\\nTrue Positive (TP): when both the actual and predicted values are 1.\\nTrue Negative (TN): when both the actual and predicted values are 0.\\nFalse Positive (FP): when the actual value is 0 but the predicted value is 1.\\nFalse Negative (FN): when the actual value is 1 but the predicted value is 0.\\nRecall that in our case, we refer to the event we want to capture (1 - Spam) as Positive and non-event (0 - Not Spam) as Negative.\\nThe Confusion Matrix for binary classification is a 2-by-2 matrix, where each column represents one class, as follows:\\nThe Confusion Matrix\\nApplied to our dataset, we get the following values:\\nTrue Positive (TP): 1\\nTrue Negative (TN): 14\\nFalse Positive (FP): 3\\nFalse Negative (FN): 2\\nWe can populate these values in the Confusion Matrix, as follows:\"),\n",
       " Document(metadata={'title': 'Text Classification', 'source': 'https://cohere.com/llmu/text-classification'}, page_content='True Positive (TP): 1\\nTrue Negative (TN): 14\\nFalse Positive (FP): 3\\nFalse Negative (FN): 2\\nWe can populate these values in the Confusion Matrix, as follows:\\nPopulating the classification outcome in the Confusion Matrix\\nWe can also map the Confusion Matrix to the Accuracy formula that we saw earlier, as follows:\\nAccuracy calculation via the Confusion Matrix\\nWe can now see via this matrix why Accuracy can sometimes hide the nuance of imbalanced datasets. The reason is in these kinds of datasets, the True Negative category dominates, diluting the effect of the rest.\\nSo even if the classifier were to perform poorly in the other three categories, its Accuracy will still look good, masking its deficiencies.\\nThe True Negatives dominate the Accuracy calculation\\nPrecision\\nLet’s now see how the other three metrics can provide a more balanced view of a classifier’s performance. Let’s start with Precision.\\nPrecision is calculated as follows:\\nPrecision calculation\\nNotice what just happened? Now, the True Negatives are not even part of the calculation. Precision focuses on the True Positives and False Positives, therefore providing a representation that may be missed via Accuracy. Whereas Accuracy looked impressive at 75% earlier, we now see that Precision is quite far off at 25%.\\nRecall\\nRecall uses the same principle as Precision, except the focus is now on the False Negatives instead of the False Positives. Again, the True Negatives are not part of the consideration.\\nRecall is calculated as follows:\\nRecall calculation\\nBetween Precision and Recall though, there is a tradeoff. It is hard to optimize for both simultaneously as optimizing for the False Positives (thereby improving Precision) comes at the expense of the False Negatives (thereby deteriorating Recall), and vice versa.\\nWhich then brings the question: which metric should you prioritize—Precision or Recall?\\nThe answer is that it depends on the nature of your task. Let’s see why.'),\n",
       " Document(metadata={'title': 'Text Classification', 'source': 'https://cohere.com/llmu/text-classification'}, page_content='Which then brings the question: which metric should you prioritize—Precision or Recall?\\nThe answer is that it depends on the nature of your task. Let’s see why.\\nSuppose the spam classifier achieved high Precision and low Recall (Scenario A). This would result in fewer non-spam emails flagged as spam (False Positive). But this would also mean more of the actual spam emails went undetected (False Negative).\\nConversely, if the classifier achieved high Recall and low Precision (Scenario B), there would be fewer undetected spam emails (False Negative), but it comes at the expense of more non-spam emails being flagged as spam (False Positive).\\nFor a spam classification task, it’s probably more desirable to avoid important emails being moved into the spam folder than to have the occasional spam emails going into the inbox. So for this task, we will want to prioritize Precision over Recall.\\nTwo example scenarios showing the Precision-Recall tradeoff\\nF1 Score\\nWhat if both Precision and Recall are important to you and you need the classifier to do well in both? The answer is, to use the final metric of the four—F1.\\nF1 takes into consideration both Precision and Recall. It is calculated as follows:\\nF1 score calculation\\nF1 provides the balance between Precision and Recall. Now, there are different versions of the ‘F-score’ family if you want to go for it, for example assigning bigger weight to either Precision or Recall, but F1 is a good enough option to get started.\\nF1 provides a balance between Precision and Recall\\nMulti-Class Classification\\nSo we have seen how the four metrics work in a binary classification case. But how do we compute them when the number of classes is more than two (i.e., multi-class classification)? They follow the same principle with some slight tweaks.'),\n",
       " Document(metadata={'title': 'Text Classification', 'source': 'https://cohere.com/llmu/text-classification'}, page_content='Let’s say we have the task of classifying a list of eCommerce customer emails into one of three classes: Shipping, Returns, and Tracking. We’ll represent each class with integer values of 0, 1, and 2 respectively.\\nThe dataset contains 15 email titles. We put each data point through a multi-class classifier to get the predicted class and then compare it with its actual class.\\nThe classifier returns the following outcome:\\nThe email multi-class classification dataset with the actual and predicted classes\\nFirst, Accuracy. The calculation is the same as we did with binary classification— the number of correct predictions divided by the total number of predictions. For our dataset, there are 10 correct predictions and 5 incorrect predictions, which give us an Accuracy of 67%.\\nNext, to compute Precision, Recall, and F1, we’ll build the Confusion Matrix.\\nSince we have three classes, the matrix now becomes a 3-by-3 matrix, each column representing one class. Applied to our dataset, we get the following matrix:\\nPopulating the classification outcome in the Confusion Matrix\\nPrecision and Recall are calculated in the same way as we did with binary classification. The only difference now is each class will have its own set of Precision and Recall values.\\nLet’s take class 0 (Shipping) as an example. Here, we use Positive to refer to the Shipping class and Negative to refer to all the other classes (Not Shipping).\\nPrecision for class 0 (Shipping) is then calculated as follows:\\nPrecision calculation for class 0 - Shipping\\nAnd Recall for class 0 (Shipping) is calculated as follows:\\nRecall calculation for class 0 - Shipping\\nEach class will have its own F1 too. F1 for class 0 (Shipping) is calculated as follows:\\nF1 calculation for class 0 - Shipping\\nGoing through all classes in our dataset, this is what Recall, Precision, and F1 look like:\\nPrecision, Recall, and F1 for all three classes'),\n",
       " Document(metadata={'title': 'Text Classification', 'source': 'https://cohere.com/llmu/text-classification'}, page_content='F1 calculation for class 0 - Shipping\\nGoing through all classes in our dataset, this is what Recall, Precision, and F1 look like:\\nPrecision, Recall, and F1 for all three classes\\nNow, what if the number of classes started to get really big? This table would become huge and it will be harder to get a snapshot of the performance. So we may want to still have a single value for each of these metrics.\\nThere are a few approaches we can take, and one common option is to take the average of all the classes. This is also called the Macro-Average, and we apply it to each of Precision, Recall, and F1 as follows:\\nMacro-averaged calculation of Precision, Recall, and F1\\nConclusion\\nIn this chapter, you learned what classification models are, and how they can be used for numerous applications. You also took a deep dive into the metrics used for evaluating classification models, which are accuracy, precision, recall, and F1-score. These will be necessary to successfully build classification models in the future.'),\n",
       " Document(metadata={'title': 'Few-Shot Classification', 'source': 'https://cohere.com/llmu/few-shot-classification'}, page_content='Few-Shot ClassificationQualifiedIntercom Live Chat\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nIn the text classification space, a trend is emerging where developers and teams are leveraging large language models (LLMs) when building an AI-based classifier system. This is opposed to building a system from scratch on their own, which first, requires the team to have the know-how in machine learning and engineering, and second, requires a huge amount of labeled training data to build a working solution.\\nWith LLMs, instead of having to prepare thousands of training data points, you can get up and running with just a handful of examples per class, called few-shot classification. Think about the sudden drop in the associated costs, time, and effort to collect and build a training dataset. This means that more teams can now think about deploying their own text classification systems, whereas they would not have considered it before.\\nLet\\'s see how to do this with an example. We\\'ll classify the sentiment of text into a number of classes, say, positive, negative, or neutral. This is useful for applications like analyzing social media content or categorizing product feedback.\\nFor example, a human can easily tell you that “Hello, World! What a beautiful day” conveys a positive sentiment, but let’s see if our models can do that too. And while we’re at it, let’s try classifying other phrases that you might find on social media.\\nIn this chapter, you\\'ll learn to classify text based on sentiment using Cohere\\'s Classify endpoint.\\nStep-by-Step Guide\\nTo set up, we first import several tools.\\nimport cohere from cohere import ClassifyExample \\nWe also create a Cohere client.\\nco = cohere.Client(\"COHERE_API_KEY\") # Your Cohere API key \\nStep 1: Prepare Examples and Input'),\n",
       " Document(metadata={'title': 'Few-Shot Classification', 'source': 'https://cohere.com/llmu/few-shot-classification'}, page_content='import cohere from cohere import ClassifyExample \\nWe also create a Cohere client.\\nco = cohere.Client(\"COHERE_API_KEY\") # Your Cohere API key \\nStep 1: Prepare Examples and Input\\nA typical machine learning model requires many training examples to perform text classification, but with the Classify endpoint, you can get started with as few as two examples per class. You need to prepare the following:\\nExamples\\nThese are the training examples we give the model to show the output we want it to generate.\\nEach example contains the text itself and the corresponding label, or class.\\nThe minimum number of examples required is two per class.\\nYou can have as many classes as possible. If you are classifying text into two classes, that means you need a minimum of four examples, and if you have three, that means you need six examples, and so on.\\nInputs\\nThese are the list of text pieces you’d like to classify.\\nThe examples and inputs to a classifier\\nOur sentiment analysis classifier has three classes with five examples each: “Positive” for a positive sentiment, “Negative” for a negative sentiment, and “Neutral” for a neutral sentiment. The code looks as follows.\\nThe examples:'),\n",
       " Document(metadata={'title': 'Few-Shot Classification', 'source': 'https://cohere.com/llmu/few-shot-classification'}, page_content='The examples:\\nexamples = [ClassifyExample(text=\"I’m so proud of you\", label=\"positive\"), ClassifyExample(text=\"What a great time to be alive\", label=\"positive\"), ClassifyExample(text=\"That’s awesome work\", label=\"positive\"), ClassifyExample(text=\"The service was amazing\", label=\"positive\"), ClassifyExample(text=\"I love my family\", label=\"positive\"), ClassifyExample(text=\"They don\\'t care about me\", label=\"negative\"), ClassifyExample(text=\"I hate this place\", label=\"negative\"), ClassifyExample(text=\"The most ridiculous thing I\\'ve ever heard\", label=\"negative\"), ClassifyExample(text=\"I am really frustrated\", label=\"negative\"), ClassifyExample(text=\"This is so unfair\", label=\"negative\"), ClassifyExample(text=\"This made me think\", label=\"neutral\"), ClassifyExample(text=\"The good old days\", label=\"neutral\"), ClassifyExample(text=\"What\\'s the difference\", label=\"neutral\"), ClassifyExample(text=\"You can\\'t ignore this\", label=\"neutral\"), ClassifyExample(text=\"That\\'s how I see it\", label=\"neutral\")] \\nThe inputs (we have twelve in this example):\\ninputs = [\"Hello, world! What a beautiful day\", \"It was a great time with great people\", \"Great place to work\", \"That was a wonderful evening\", \"Maybe this is why\", \"Let\\'s start again\", \"That\\'s how I see it\", \"These are all facts\", \"This is the worst thing\", \"I cannot stand this any longer\", \"This is really annoying\", \"I am just plain fed up\"] \\nStep 2: Generate Predictions\\nWith the Classify endpoint, setting up the model is quite straightforward. The main thing to do is to define the model type. For the Classify endpoint, we need to use an embedding model, and we\\'ll useembed-english-v3.0.\\nPutting everything together with the Classify endpoint looks like the following:'),\n",
       " Document(metadata={'title': 'Few-Shot Classification', 'source': 'https://cohere.com/llmu/few-shot-classification'}, page_content='Putting everything together with the Classify endpoint looks like the following:\\ndef classify_text(inputs, examples): \"\"\" Classifies a list of input texts given the examples Arguments: model (str): identifier of the model inputs (list[str]): a list of input texts to be classified examples (list[Example]): a list of example texts and class labels Returns: classifications (list): each result contains the text, labels, and conf values \"\"\" # Classify text by calling the Classify endpoint response = co.classify( model=\\'embed-english-v3.0\\', inputs=inputs, examples=examples) classifications = response.classifications return classifications # Classify the inputs predictions = classify_text(inputs, examples) \\nTogether with the predicted class, the endpoint also returns the confidence value of the prediction (between 0 and 1). These confidence values are split among the classes, in this case three, in which the values add up to a total of 1. The classifier then selects the class with the highest confidence value as the “predicted class.” A high confidence value for the predicted class therefore indicates that the model is very confident of its prediction, and vice versa.\\nHere’s a sample output returned:'),\n",
       " Document(metadata={'title': 'Few-Shot Classification', 'source': 'https://cohere.com/llmu/few-shot-classification'}, page_content=\"Here’s a sample output returned:\\nInput: Hello, world! What a beautiful day Prediction: positive Confidence: 0.84 ---------- Input: It was a great time with great people Prediction: positive Confidence: 0.99 ---------- Input: Great place to work Prediction: positive Confidence: 0.91 ---------- Input: That was a wonderful evening Prediction: positive Confidence: 0.96 ---------- Input: Maybe this is why Prediction: neutral Confidence: 0.70 ---------- Input: Let's start again Prediction: neutral Confidence: 0.83 ---------- Input: That's how I see it Prediction: neutral Confidence: 1.00 ---------- Input: These are all facts Prediction: neutral Confidence: 0.78 ---------- Input: This is the worst thing Prediction: negative Confidence: 0.93 ---------- Input: I cannot stand this any longer Prediction: negative Confidence: 0.93 ---------- Input: This is really annoying Prediction: negative Confidence: 0.99 ---------- Input: I am just plain fed up Prediction: negative Confidence: 1.00 ---------- \\nThe model returned a Positive sentiment for “Hello, world! What a beautiful day,” which is what we would expect! And the predictions for all the rest look spot on too.\\nConclusion\\nIn this chapter, you used Cohere's Classify endpoint for sentiment analysis with a simple dataset. This is just one example, but you can classify any kind of text into any number of possible classes according to your needs.\"),\n",
       " Document(metadata={'title': 'Fine-Tuning for Classification', 'source': 'https://cohere.com/llmu/fine-tuning-for-classification'}, page_content='Fine-Tuning for ClassificationQualified\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nIn the previous chapter, you used few-shot classification for a sentiment analysis task and got decent results with a small dataset.\\nLet’s say you actually have a decent amount of training dataset at your disposal. Then, you can leverage these additional data points to build a potentially better-performing classifier through a process known as fine-tuning.\\nFine-tuning is a powerful concept, and here’s the reason. With fine-tuning, you can actually alter the model itself and customize it according to your task. This means that what you get is a custom model that’s fine-tuned to excel at a specific task, and potentially can outperform the few-shot approach from the previous chapter.\\nIn this chapter, you\\'ll learn how to fine-tune a model for classification.\\nStep-by-Step Guide\\nTo set up, we first import several tools.\\nimport os import json import numpy as np import pandas as pd from cohere import ClassifyExample from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.metrics import f1_score \\nWe also import the Cohere module and create a client.\\nimport cohere co = cohere.Client(\"COHERE_API_KEY\") # Your Cohere API key \\nStep 1: Prepare the Dataset\\nWe\\'ll use the Airline Travel Information System (ATIS) intent classification dataset [source]. For demonstration purposes, we’ll take just a small portion of the dataset: 1,000 data points in total.\\n# Load the dataset to a dataframe df = pd.read_csv(\\'https://raw.githubusercontent.com/cohere-ai/notebooks/main/notebooks/data/atis_subset.csv\\', names=[\\'query\\',\\'intent\\']) \\nThe first thing we need is to create a training dataset, to be used for building the classifier, and a test dataset, to be used for testing the classifier performance. We will use 800 and 200 data points for these datasets, respectively.'),\n",
       " Document(metadata={'title': 'Fine-Tuning for Classification', 'source': 'https://cohere.com/llmu/fine-tuning-for-classification'}, page_content='# Split the dataset into training and test portions df_train, df_test = train_test_split(df, test_size=200, random_state=21) \\nOur goal is to train the classifier so it can predict the class of a new customer inquiry out of eight classes, as follows:\\n[\\'atis_flight\\', \\'atis_airfare\\', \\'atis_ground_service\\', \\'atis_flight_time\\', \\'atis_airline\\', \\'atis_quantity\\', \\'atis_abbreviation\\', \\'atis_aircraft\\'] \\nWe transform the data to JSONL format to match the style expected by the Classification endpoint (documentation).\\ndef create_classification_data(text, label): formatted_data = { \"text\": text, \"label\": label } return formatted_data if not os.path.isfile(\"data.jsonl\"): print(\"Creating jsonl file ...\") with open(\"data.jsonl\", \\'w+\\') as file: for row in df_train.itertuples(): formatted_data = create_classification_data(row.query, row.intent) file.write(json.dumps(formatted_data) + \\'\\\\n\\') file.close() print(\"Done\") else: print(\"data.jsonl file already exists\") \\nThe JSONL file has a row for each example. Each example has \"text\" and \"label\" fields.\\n{\"text\": \" what flights go from newark to boston after 5 pm\", \"label\": \"atis_flight\"} {\"text\": \" can you show me economy class one way fares for flights from oakland to dallas on december sixteenth\", \"label\": \"atis_airfare\"} {\"text\": \" show me the latest flight from salt lake city to phoenix\", \"label\": \"atis_flight\"} {\"text\": \" pittsburgh to denver\", \"label\": \"atis_flight\"} {\"text\": \" show me all flights from san francisco to atlanta\", \"label\": \"atis_flight\"} ... \\nStep 2: Fine-Tune the Model\\nWe kick off a fine-tuning job by navigating to the fine-tuning tab of the Dashboard. Under \"Classify\", click on \"Create a Classify model\".\\nNext, upload the .jsonl file you just created as the training set by clicking on the \"TRAINING SET\" button. When ready, click on \"Review data\" to proceed to the next step.'),\n",
       " Document(metadata={'title': 'Fine-Tuning for Classification', 'source': 'https://cohere.com/llmu/fine-tuning-for-classification'}, page_content='Next, upload the .jsonl file you just created as the training set by clicking on the \"TRAINING SET\" button. When ready, click on \"Review data\" to proceed to the next step.\\nThen, you\\'ll see a preview of how the model will ingest your data. If anything is wrong with the data, the page will also provide suggested changes to fix the training file. Otherwise, if everything looks good, you can proceed to the next step.\\nFinally, you\\'ll see a page where you\\'ll provide a nickname to your model. We used atis-classify-ft as the nickname for our model. Under \"BASE MODEL\", ensure \"english\" is selected.\\nThen click on \"Start training\" to kick off the fine-tuning process. This will navigate you to a page where you can monitor the status of the model. A model that has finished fine-tuning will show the status as READY.\\nStep 3: Use/Evaluate the Fine-Tuned Model\\nOnce the model has finished fine-tuning, it’s time to evaluate its performance. Navigate to the API tab of the fine-tuned model. There, you\\'ll see the model ID that you should use when calling co.classify().\\nWe fill in the model ID to generate test predictions.\\n# Generate classification predictions on the test dataset using the finetuned model # Classification function def classify_text_finetune(texts): classifications = co.classify( model=\\'b2c94ac3-7a74-4de7-a11a-9808a3b8ef59-ft\\', inputs=texts, examples=None ).classifications return [c.predictions[0] for c in classifications] # Create batches of texts and classify them BATCH_SIZE = 90 # The API accepts a maximum of 96 inputs y_pred = [] for i in range(0, len(df_test), BATCH_SIZE): batch_texts = df_test[\"query\"][i:i+BATCH_SIZE].tolist() y_pred.extend(classify_text_finetune(batch_texts)) \\nNext, we calculate the model\\'s test accuracy and F1 score.\\n# Compute metrics on the test dataset accuracy = accuracy_score(df_test[\"intent\"], y_pred) f1 = f1_score(df_test[\"intent\"], y_pred, average=\\'weighted\\') print(f\\'Accuracy: {100*accuracy:.2f}\\') print(f\\'F1-score: {100*f1:.2f}\\')'),\n",
       " Document(metadata={'title': 'Fine-Tuning for Classification', 'source': 'https://cohere.com/llmu/fine-tuning-for-classification'}, page_content=\"Accuracy: 98.00 F1-score: 98.05 \\nNot bad!\\nConclusion\\nIn this chapter, you saw that fine-tuning is an alternative to few-shot classification that allows you to have more control over the model's training. It is generally preferred if you have a larger dataset and need superior performance.\"),\n",
       " Document(metadata={'title': 'Multilingual Sentiment Analysis', 'source': 'https://cohere.com/llmu/multilingual-sentiment-analysis'}, page_content='Multilingual Sentiment AnalysisQualified\\nEmotional expression is subjective. It’s shaped by cultural background, emotional intelligence, individual personality traits and characteristics, and more. While we can communicate emotions clearly in verbal communication due to nuances of tone and inflection, it’s harder to determine how a person feels and what emotions they’re expressing through words alone.\\nFortunately we can use sentiment analysis to determine the tone—and by extension, the emotion—that text conveys. In this demonstration, we’re creating an app that uses sentiment analysis to determine what emotions a person is experiencing based on the text input.\\nThe steps for building the Sentiment Analysis application are:\\nStep 1: Gather Emotion Data\\nStep 2: Train the Emotion Classifier\\nStep 3: Get User Input\\nStep 4: Embed the Input\\nStep 5: Classify Sentiment\\nStep 6: Display Results\\nStep 7: Put It All together\\nThe repository for this project is here, and we encourage you to follow the code along with this tutorial.\\nStep-by-Step Guide\\nWe’ll use the embed-multilingual-v3.0 model via the Cohere API—specifically the Embed endpoint—to generate embeddings for user-provided text in real time. The embed-multilingual-v3.0 model generates embeddings for text data in over 100 languages. Then, we’ll use these embeddings to train a classifier to predict the emotions the user expressed.\\nStep 1: Gather Emotion Data\\nWe’ll use the XED dataset, consisting of 8 emotion categories, to train the emotion classifier. For this app, the embeddings for this dataset have already been created and stored as xed_with_embeddings.json, along with the labels for each data point.\\nStep 2: Train the Emotion Classifier\\nThe bulk of the code that drives this application is stored in the sentiment.py file. Let’s open it and examine the train_and_save function line by line.'),\n",
       " Document(metadata={'title': 'Multilingual Sentiment Analysis', 'source': 'https://cohere.com/llmu/multilingual-sentiment-analysis'}, page_content='Step 2: Train the Emotion Classifier\\nThe bulk of the code that drives this application is stored in the sentiment.py file. Let’s open it and examine the train_and_save function line by line.\\nFirst, the function reads xed_with_embeddings.json into a data frame. In this data frame, column df.embeddings contains the embeddings for each sample and column df.labels_text contains the emotion label for each sample. The function transforms the embeddings for each sample into a list, forming data matrix X.\\nThe function uses MultiLabelBinarizer from scikit-learn to one-hot encode df.labels_text and form y, which is a one-hot encoded label matrix. Next, the function executes the train-test split with the test set size configured at 1 percent of the total set. Afterwards, it instantiates a chain of classifiers with logistic regression as the base classifier.\\nA classifier chain is a technique for multi-label classification that involves training a chain of binary classifiers, one for each label. The output of each classifier serves as the input to the next classifier in the chain. Consequently, the final output is a vector of binary labels indicating the presence or absence of each emotion. This demo uses a chain of classifiers instantiated with logistic regression as the base classifier via ClassifierChain and LogisticRegression from scikit-learn. This allows us to predict the probability of each emotion class for each sample with chain_model.predict_proba in Step 5.\\nSubsequently, the train_and_save function fits the classifier on the train set and evaluates it with chain.score. This calculates the mean accuracy relative to the test set. In the last line, the function saves the trained classifier for determining emotion in a pickle file emotion_chain.pkl. A pickle file is a way of serializing and saving Python objects, such as trained machine learning models. By saving a trained model as a pickle file, we can reuse the model later without retraining it from scratch.'),\n",
       " Document(metadata={'title': 'Multilingual Sentiment Analysis', 'source': 'https://cohere.com/llmu/multilingual-sentiment-analysis'}, page_content='Then, we save the trained classifier for determining emotion into a pickle file named emotion_chain.pkl. To load the saved model later, we can simply use the pickle.load function in Python.\\ndef train_and_save(): full_df = pd.read_json(\"./data/xed_with_embeddings.json\", orient=\\'index\\') df = full_df mlb = MultiLabelBinarizer() X = np.array(df.embeddings.tolist()) y = mlb.fit_transform(df.labels_text) classes = mlb.classes_ classes_mapping = {index: emotion for index, emotion in enumerate(mlb.classes_)} X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=42) base_lr = LogisticRegression(solver=\\'lbfgs\\', random_state=0) chain = ClassifierChain(base_lr, order=\\'random\\', random_state=0) chain.fit(X_train, y_train) print(chain.score(X_test, y_test)) pickle.dump(chain, open(\"./data/models/emotion_chain.pkl\", \\'wb\\')) \\nStep 3: Get User Input\\nStreamlit functions facilitate user input. Calling st.text_input creates a page object that asks the user “How are you feeling?” and captures the user’s text response. It then presents the user a slider via st.slider, which they use to select the number of top emotions they’d like to be presented (k).\\nfeeling_text = st.text_input(\"How are you feeling?\", \"\") top_k = st.slider(\"Top Emotions\", min_value=1, max_value=len(classes_mapping), value=1, step=1) \\nStep 4: Embed the Input\\nThe beginning of function score_sentence embeds the user input text into embeddings and converts it into a torch-based tensor.\\nembeddings = torch.as_tensor(get_embeddings(co=co, model_name=model_name, texts=[text]), dtype=torch.float32) \\nThe function imports get_embeddings from the utils.py file. Then, the get_embeddings function calculates the embeddings by calling the embed-multilingual-v3.0model.'),\n",
       " Document(metadata={'title': 'Multilingual Sentiment Analysis', 'source': 'https://cohere.com/llmu/multilingual-sentiment-analysis'}, page_content='The function imports get_embeddings from the utils.py file. Then, the get_embeddings function calculates the embeddings by calling the embed-multilingual-v3.0model.\\ndef get_embeddings(co: cohere.Client, texts: List[str], model_name: str = \\'embed-multilingual-v3.0\\', input_type = \"search_document\", truncate: str = \\'RIGHT\\', batch_size: int = 2048) -> List[float]: @limiter.ratelimit(\"blobheart\", delay=True) def get_embeddings_api(texts_batch: List[str]): for i in range(N_MAX_RETRIES): try: output = co.embed(model=model_name, input_type=input_type, texts=texts_batch, truncate=truncate) break except Exception as e: if i == (N_MAX_RETRIES - 1): print(f\"Exceeded max retries with error {e}\") raise f\"Error {e}\" return output.embeddings st_pbar = tqdm(range(0, len(texts), batch_size)) for index in st_pbar: texts_batch = texts[index:index + batch_size] embeddings_batch = get_embeddings_api(texts_batch) #list(pool.imap(get_embeddings_api, [texts_batch])) embeddings.append(embeddings_batch) return np.concatenate(embeddings, axis=0).tolist() \\nStep 5: Classify Sentiment\\nIn the function setup within the sentiment.py tile, you can access the trained classifier model from emotion_chain.pkl where we saved it in Step 1.\\ndef setup(): … chain_model = pickle.load(open(model_path, \\'rb\\')) \\nWith the model stored as chain_model, we access it in score_sentence after acquiring the embeddings. The function then executes the model on the float tensor of embeddings to predict probabilities of emotion class(es), determined for each sample with chain_model.predict_proba. Then, torch.sort sorts the probability outputs for the emotion(s) associated with the user text input in ascending order.'),\n",
       " Document(metadata={'title': 'Multilingual Sentiment Analysis', 'source': 'https://cohere.com/llmu/multilingual-sentiment-analysis'}, page_content='In the last step, we convert the tensor of probabilities and tensor of associated indices from GPU-based to CPU-based, allowing for each tensor to be configured into a NumPy array. This then reverses both NumPy arrays such that the emotion with the highest probability is first. This sets up the loop we’ll run to display the images associated with the determined top k emotions experienced by the user.\\noutputs = torch.as_tensor(chain_model.predict_proba(embeddings), dtype=torch.float32) probas, indices = torch.sort(outputs) probas = probas.cpu().numpy()[0][::-1] indices = indices.cpu().numpy()[0][::-1] \\nSection 6: Display Results\\nLet’s first head back to the setup function we discussed in step 5. The first part of this function initiates emotions2image_mapping as a dictionary and each of the eight emotion labels are mapped with file paths corresponding to respective emotion GIFs. Then, emotions2image_mapping iteratively filled with the emotion gifs themselves utilizing the file paths.\\nemotions2image_mapping = { \\'Anger\\': \\'./data/emotions/anger.gif\\', \\'Anticipation\\': \\'./data/emotions/anticipation.gif\\', \\'Disgust\\': \\'./data/emotions/disgust.gif\\', \\'Fear\\': \\'./data/emotions/fear.gif\\', \\'Joy\\': \\'./data/emotions/joy.gif\\', \\'Sadness\\': \\'./data/emotions/sadness.gif\\', \\'Surprise\\': \\'./data/emotions/surprise.gif\\', \\'Trust\\': \\'./data/emotions/trust.gif\\', } for key, value in emotions2image_mapping.items(): with open(value, \"rb\") as f: emotions2image_mapping[key] = f.read() \\nNow circling back to the score_sentence function, the call returns emotions2image_mapping to setup and is afterwards accessed in the loop inside score_sentence to display the top k emotions to the user as GIFs.\\nThe loop iterates through each of the top k emotions, with the index and probability associated with each emotion is extracted as tuple (index, p) at each iteration of the loop. Before the start of the loop, we use Streamlit to create k columns that will display the top k emotion(s) to the user.'),\n",
       " Document(metadata={'title': 'Multilingual Sentiment Analysis', 'source': 'https://cohere.com/llmu/multilingual-sentiment-analysis'}, page_content='In the first iteration, a row of three columns is created via st.columns. Once three columns are filled with GIFs corresponding to ranked emotions, a new row of three columns is created in the following iteration. Structuring the GIFs to be displayed in rows of up to three emotions facilitates user readability.\\ncols = st.columns(top_k, gap=\"large\") for i, (index, p) in enumerate(zip(indices[:top_k], probas[:top_k])): if i % 3 == 0: cols = st.columns(3, gap=\"large\") emotion = classes_mapping[index] i = i % 3 image_file = emotions2image_mapping.get(emotion, None) if image_file: image_gif = base64.b64encode(image_file).decode(\"utf-8\") cols[i].markdown( f\\'<img src=\"data:image/gif;base64,{image_gif}\" style=\"width:250px;height:250px;border-radius: 25%;\">\\', unsafe_allow_html=True, ) cols[i].markdown(\"---\") cols[i].markdown(f\"**{emotion}**: {p * 100:.2f}%\") print(f\"Predicted emotion: {emotion}, with probability: {p}\") \\nConclusion\\nIn this chapter, we learned how to use embeddings to build a text classifier that can determine emotions. It is important to note the layers of this process. We first generated the embeddings by transforming the text via the embed-multilingual-v3.0 embedding model, and then we built the multi-label classifier. When we later predicted and displayed emotions to the user, we followed the same layered process of calculating embeddings and then applied the classifier to produce probabilities of detected emotions.'),\n",
       " Document(metadata={'title': 'Building a Chatbot', 'source': 'https://cohere.com/llmu/building-a-chatbot'}, page_content='Building a ChatbotQualified\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nUnderstanding text generation is the first step to creating your own chatbot, but the real learning takes place when you start building one from scratch. In this chapter, you’ll learn how to use Cohere’s Chat endpoint to build a simple chatbot that can respond to user messages and maintain the context of the conversation.\\nAdditionally, the API reference page contains a detailed description of the Chat endpoint’s input parameters and response objects.\\nStep-by-Step Guide\\nStep 1: Quickstart\\nTo set up, we first import the Cohere module and create a client.\\nimport cohere co = cohere.Client(\"COHERE_API_KEY\") # Your Cohere API key \\nAt its most basic, we only need to pass to the Chat endpoint the user message using the message parameter – the only required parameter for the endpoint.\\nAt its most basic, the endpoint takes a user message and generates a response\\nHere’s an example. We call the endpoint with \"Hello\" as the user message. In our case, we also use the model parameter to specify the underlying chat model we want to use. For this, we pick Command R+, Cohere\\'s newest large language model.\\nRight now, we’re interested in the main content of the response, which is stored in the text value of the response.\\nresponse = co.chat(message=\"Hello\", model=\"command-r-plus\") print(response.text) \\n# RESPONSE Hello! How can I help you today? \\nStep 2: Defining a Preamble\\nA conversation starts with a system message, or a preamble, to help steer a chatbot’s response toward certain characteristics. For example, if we want the chatbot to adopt a formal style, the preamble can be used to encourage the generation of more business-like and professional responses.\\nThe preamble and the current user message form the prompt to the chatbot'),\n",
       " Document(metadata={'title': 'Building a Chatbot', 'source': 'https://cohere.com/llmu/building-a-chatbot'}, page_content='The preamble and the current user message form the prompt to the chatbot\\nIn the quickstart example, we didn’t have to define a preamble because a default one was used. We can, however, define our own preamble using the preamble parameter.\\nHere’s an example. We added a preamble telling the chatbot to assume the persona of an expert public speaking coach. As a result, we get a response that adopts that persona.\\nresponse = co.chat(message=\"Hello\", model=\"command-r-plus\", preamble=\"You are an expert public speaking coach. Don\\'t use any greetings.\") print(response.text) \\n# RESPONSE How can I help you with your public speaking today? \\nStep 3: Streaming the Chatbot Response\\nOur examples so far generate responses in a non-streamed manner. This means that the endpoint would return a response object only after the model has generated the text in full. The longer the text is, the longer it takes to get back the response. If you are building an application, this directly impacts the user’s perception of the application’s latency.\\nThe Chat endpoint solves this problem by supporting streamed responses. In a streamed response, the endpoint would return a response object for each token as it is being generated. This means you can display the text incrementally without having to wait for the full completion.\\nTo activate it, use co.chat_stream() instead of co.chat().\\nIn streaming mode, the endpoint will generate a series of objects. To get the actual text contents, we take objects whose event_type is text-generation.\\nIf you have not already, make your own copy of the Google Colaboratory notebook and run the code in this section to see the same example with streamed responses activated.\\nstream = co.chat_stream(message=\"Hello. I\\'d like to learn about techniques for effective audience engagement\", model=\"command-r-plus\", preamble=\"You are an expert public speaking coach\") for event in stream: if event.event_type == \"text-generation\": print(event.text, end=\\'\\')'),\n",
       " Document(metadata={'title': 'Building a Chatbot', 'source': 'https://cohere.com/llmu/building-a-chatbot'}, page_content='# RESPONSE (Streamed) Sure! Here are some techniques for effective audience engagement: - **Make eye contact:** Look at different individuals in the audience and try to maintain eye contact for a few seconds at a time. This helps create a sense of connection and makes your speech more intimate. - **Use gestures:** Incorporate hand gestures and body language to emphasize your points and add variety to your delivery. This helps keep your audience engaged and can also help you appear more confident and passionate about your topic. - **Vary your tone and volume:** Avoid monotone delivery by varying your pitch, pace, and volume. Emphasize important points by raising your volume or slowing down your pace. This helps to create emphasis and keeps your audience interested. - **Ask questions:** Pose rhetorical questions to your audience to get them thinking about your topic. You can also directly ask for their opinions or input, either by a show of hands or by inviting them to share their thoughts. This creates a dialogue and makes your speech more interactive. - **Tell stories:** Use anecdotes and personal stories to illustrate your points and create a connection with your audience. Stories are memorable and help your audience relate to you and your message. - **Use humor:** Appropriate humor can help to lighten the mood, engage your audience, and make your speech more enjoyable. However, be careful to avoid offensive or inappropriate jokes, and ensure your humor is relevant to your topic. - **Incorporate multimedia:** Use slides, videos, or props to enhance your presentation. Visual aids can help to break up your speech, provide additional information, and keep your audience focused. - **Encourage interaction:** If appropriate, include activities that involve your audience. This could be a group exercise, a poll, or a Q&A session. This helps to create a more engaging and interactive experience for your audience. - **Be passionate:** Show your enthusiasm for your topic.'),\n",
       " Document(metadata={'title': 'Building a Chatbot', 'source': 'https://cohere.com/llmu/building-a-chatbot'}, page_content='This could be a group exercise, a poll, or a Q&A session. This helps to create a more engaging and interactive experience for your audience. - **Be passionate:** Show your enthusiasm for your topic. Audiences are more likely to engage if they sense your passion and authenticity. Let your personality shine through and speak from the heart. - **Practice and adapt:** Rehearse your speech and pay attention to your delivery. Practice in front of a mirror, record yourself, or seek feedback from others. This will help you refine your technique and improve your audience engagement skills. Remember, effective audience engagement is about creating a connection and making your speech memorable. By using these techniques and adapting them to your own style and personality, you can deliver a compelling presentation that resonates with your audience.'),\n",
       " Document(metadata={'title': 'Building a Chatbot', 'source': 'https://cohere.com/llmu/building-a-chatbot'}, page_content='Step 4: Building the Chat History\\nAt the core of a conversation is a multi-turn dialog between the user and the chatbot. This requires the chatbot to have a “memory” of all the previous turns to maintain the state of the conversation.\\nOption 1: Using the Conversation History Persistence Feature\\nThe Chat endpoint supports state management by persisting the conversation history. As a conversation progresses, the endpoint continuously updates the conversation history. This means developers don’t have to deal with the complexity and inconvenience of managing conversation history in their application.\\nState management is handled by the Chat endpoint\\nTo use this feature, use the conversation_id parameter, which is a unique string you assign to a conversation. We\\'ll use the uuid library to do this.\\nPutting everything together, let’s now build a simple chat interface that takes in a user message, generates the chatbot response, automatically updates the conversation history, and repeats these steps until the user quits the conversation.\\nAs described before, in streaming mode, the Chat endpoint generates a series of objects. To get the conversation history, we take the object with event_type of \"stream-end\" and save it as a new variable chat_history.\\n# Create a conversation ID conversation_id = str(uuid.uuid4()) # Define the preamble preamble = \"You are an expert public speaking coach\" print(\\'Starting the chat. Type \"quit\" to end.\\\\n\\') while True: # User message message = input(\"User: \") # Typing \"quit\" ends the conversation if message.lower() == \\'quit\\': print(\"Ending chat.\") break # Chatbot response stream = co.chat_stream(message=message, model=\"command-r-plus\", preamble=preamble, conversation_id=conversation_id) print(\"Chatbot: \", end=\\'\\') for event in stream: if event.event_type == \"text-generation\": print(event.text, end=\\'\\') if event.event_type == \"stream-end\": chat_history = event.response.chat_history print(f\"\\\\n{\\'-\\'*100}\\\\n\")'),\n",
       " Document(metadata={'title': 'Building a Chatbot', 'source': 'https://cohere.com/llmu/building-a-chatbot'}, page_content='# RESPONSE (Streamed) Starting the chat. Type \"quit\" to end. User: Hello Chatbot: Hello! How can I help you today? ---------------------------------------------------------------------------------------------------- User: I\\'d like to learn about techniques for effective audience engagement Chatbot: Sure! Here are some techniques for effective audience engagement: - Make eye contact with your audience and use hand gestures to emphasize your points. - Use a clear and concise language that your audience can easily understand. - Vary your tone and volume to keep your audience interested. - Use visual aids such as slides, props, or demonstrations to help illustrate your points. - Tell stories or anecdotes to help your audience connect with your message on a personal level. - Ask questions to involve your audience and encourage them to think about your topic. - Use humor appropriately to keep your audience entertained and engaged. - Be enthusiastic and passionate about your topic to show your audience that you care about what you\\'re talking about. - Connect with your audience by showing that you understand their interests and concerns. - End your speech with a call to action that encourages your audience to take action on your topic. Remember, effective audience engagement is about creating a connection with your audience and making your speech memorable and enjoyable for them. ---------------------------------------------------------------------------------------------------- User: Could you elaborate on the third point? Chatbot: Sure! Varying your tone and volume is an important technique for keeping your audience engaged. Here are some tips: - Use a varied tone of voice to express different emotions and emphasize certain words or phrases. For example, you might use a serious tone to convey the importance of a particular issue, or an excited tone to show your enthusiasm for a new idea. - Change your volume to highlight important points or to create a sense of drama.'),\n",
       " Document(metadata={'title': 'Building a Chatbot', 'source': 'https://cohere.com/llmu/building-a-chatbot'}, page_content='tone to convey the importance of a particular issue, or an excited tone to show your enthusiasm for a new idea. - Change your volume to highlight important points or to create a sense of drama. Speaking loudly can help to emphasize a particular word or phrase, while speaking softly can create a sense of intimacy or seriousness. - Avoid monotony by varying your pitch and pace. A monotone delivery can be boring and make it difficult for your audience to stay focused. Try to vary your pitch and pace to keep your audience engaged. - Practice your speech or presentation beforehand to get a feel for how your tone and volume can impact your delivery. Experiment with different tones and volumes to see what works best for your style and your message. By varying your tone and volume, you can add interest and emphasis to your speech, and keep your audience engaged and responsive. ---------------------------------------------------------------------------------------------------- User: quit Ending chat.'),\n",
       " Document(metadata={'title': 'Building a Chatbot', 'source': 'https://cohere.com/llmu/building-a-chatbot'}, page_content='Next, we print the full conversation history.\\nfor chat in chat_history: print(chat)'),\n",
       " Document(metadata={'title': 'Building a Chatbot', 'source': 'https://cohere.com/llmu/building-a-chatbot'}, page_content='role=\\'USER\\' message=\\'Hello\\' generation_id=\\'b4146b59-0884-4aa3-a573-bb6bd4948511\\' response_id=\\'b72e9f29-ac13-41e2-8892-b6529f9547f2\\' role=\\'CHATBOT\\' message=\\'Hello! How can I help you today?\\' generation_id=\\'3a002415-0213-49b9-8d4b-b7c24eba95f8\\' response_id=\\'b72e9f29-ac13-41e2-8892-b6529f9547f2\\' role=\\'USER\\' message=\"I\\'d like to learn about techniques for effective audience engagement\" generation_id=\\'fee7fbf4-fd93-4477-b987-bd2765694962\\' response_id=\\'520e2329-979b-4e4b-9cdc-b2e4b49c3612\\' role=\\'CHATBOT\\' message=\"Sure! Here are some techniques for effective audience engagement:\\\\n\\\\n- Make eye contact with your audience and use hand gestures to emphasize your points.\\\\n- Use a clear and concise language that your audience can easily understand.\\\\n- Vary your tone and volume to keep your audience interested.\\\\n- Use visual aids such as slides, props, or demonstrations to help illustrate your points.\\\\n- Tell stories or anecdotes to help your audience connect with your message on a personal level.\\\\n- Ask questions to involve your audience and encourage them to think about your topic.\\\\n- Use humor appropriately to keep your audience entertained and engaged.\\\\n- Be enthusiastic and passionate about your topic to show your audience that you care about what you\\'re talking about.\\\\n- Connect with your audience by showing that you understand their interests and concerns.\\\\n- End your speech with a call to action that encourages your audience to take action on your topic.\\\\n\\\\nRemember, effective audience engagement is about creating a connection with your audience and making your speech memorable and enjoyable for them.\" generation_id=\\'2cc3d664-587f-4f1c-8c45-4ac658bb1668\\' response_id=\\'520e2329-979b-4e4b-9cdc-b2e4b49c3612\\' role=\\'USER\\' message=\\'Could you elaborate on the third point?\\' role=\\'CHATBOT\\' message=\\'Sure! Varying your tone and volume is an important technique for keeping your audience engaged. Here are some tips:\\\\n\\\\n- Use a varied tone of voice to express different emotions and'),\n",
       " Document(metadata={'title': 'Building a Chatbot', 'source': 'https://cohere.com/llmu/building-a-chatbot'}, page_content=\"message='Sure! Varying your tone and volume is an important technique for keeping your audience engaged. Here are some tips:\\\\n\\\\n- Use a varied tone of voice to express different emotions and emphasize certain words or phrases. For example, you might use a serious tone to convey the importance of a particular issue, or an excited tone to show your enthusiasm for a new idea.\\\\n- Change your volume to highlight important points or to create a sense of drama. Speaking loudly can help to emphasize a particular word or phrase, while speaking softly can create a sense of intimacy or seriousness.\\\\n- Avoid monotony by varying your pitch and pace. A monotone delivery can be boring and make it difficult for your audience to stay focused. Try to vary your pitch and pace to keep your audience engaged.\\\\n- Practice your speech or presentation beforehand to get a feel for how your tone and volume can impact your delivery. Experiment with different tones and volumes to see what works best for your style and your message.\\\\n\\\\nBy varying your tone and volume, you can add interest and emphasis to your speech, and keep your audience engaged and responsive.'\"),\n",
       " Document(metadata={'title': 'Building a Chatbot', 'source': 'https://cohere.com/llmu/building-a-chatbot'}, page_content='Option 2: Managing the Conversation History Yourself\\nIf you opt not to use the endpoint’s conversation history persistence feature, you can use the chat_history parameter to manage the conversation history yourself.\\nThe chat history is a list of multiple turns of messages from the user and the chatbot. Each item is a cohere.ChatMessage object containing the role, which can be either ”USER” or ”CHATBOT”, and the message containing the message string. The following is an example of a chat history.\\nfrom cohere import ChatMessage chat_history = [ ChatMessage(role=\"USER\", message=\"What is 2 + 2\"), ChatMessage(role=\"CHATBOT\", message=\"The answer is 4\"), ChatMessage(role=\"USER\", message=\"Add 5 to that number\"), ChatMessage(role=\"CHATBOT\", message=\"Sure. The answer is 9\"), ... ] \\nThe following modifies the previous implementation by using chat_history instead of conversation_id for managing the conversation history.\\n# Initialize the chat history chat_history = [] # Define the preamble preamble = \"You are an expert public speaking coach\" print(\\'Starting the chat. Type \"quit\" to end.\\\\n\\') while True: # User message message = input(\"User: \") # Typing \"quit\" ends the conversation if message.lower() == \\'quit\\': print(\"Ending chat.\") break # Chatbot response stream = co.chat_stream(message=message, model=\"command-r-plus\", preamble=preamble, chat_history=chat_history) chatbot_response = \"\" print(\"Chatbot: \", end=\\'\\') for event in stream: if event.event_type == \"text-generation\": print(event.text, end=\\'\\') chatbot_response += event.text print(\"\\\\n\") # Add to chat history chat_history.extend( [ChatMessage(role=\"USER\", message=message), ChatMessage(role=\"CHATBOT\", message=chatbot_response)] )'),\n",
       " Document(metadata={'title': 'Building a Chatbot', 'source': 'https://cohere.com/llmu/building-a-chatbot'}, page_content='# RESPONSE (Streamed) Starting the chat. Type \"quit\" to end. User: Hello Chatbot: Hello! How can I help you today? User: I\\'d like to learn about techniques for effective audience engagement Chatbot: Sure! Here are some techniques for effective audience engagement: - Make eye contact with your audience and use hand gestures to emphasize your points. - Use a clear and concise language that your audience can easily understand. - Vary your tone and volume to keep your audience interested. - Use stories, examples, and analogies to illustrate your points and make your message more memorable. - Ask questions to involve your audience and encourage them to think about your topic. - Use humor appropriately to lighten the mood and engage your audience. - Use visuals, such as slides, photos, or props, to break up your speech and add interest. - Move around the stage or room to change your position and keep your audience focused. - Use pauses to emphasize important points and give your audience time to absorb your message. - Finally, be passionate and enthusiastic about your topic. Your enthusiasm will be contagious and will help to engage your audience. Remember, effective audience engagement is about connecting with your audience and making your message memorable. By using these techniques, you can deliver a speech that is both informative and engaging. User: Could you elaborate on the fourth point? Chatbot: Certainly! Using stories, examples, and analogies is a powerful technique to engage your audience and help them understand and remember your message. Here\\'s how: - Stories: Sharing a relevant story or anecdote can make your speech more personal and engaging. Stories have the power to evoke emotions and create a deeper connection with your audience. For example, if you\\'re speaking about the importance of perseverance, you could share a story about a time when you faced a challenge and how you overcame it. - Examples: Providing concrete examples helps to illustrate your'),\n",
       " Document(metadata={'title': 'Building a Chatbot', 'source': 'https://cohere.com/llmu/building-a-chatbot'}, page_content=\"about the importance of perseverance, you could share a story about a time when you faced a challenge and how you overcame it. - Examples: Providing concrete examples helps to illustrate your points and makes abstract concepts more tangible. For instance, if you're explaining a complex technical process, you could use an example to simplify and break down the information for your audience. - Analogies: Using analogies is a way to explain something unfamiliar by comparing it to something familiar. Analogies can help your audience understand and remember your message by creating a mental link between two concepts. For example, if you're describing a new technology, you could compare it to something your audience is already familiar with. When using stories, examples, and analogies, make sure they are relevant to your topic and concise. You don't want to lose your audience's attention by rambling or veering off-topic. Also, try to use vivid details and sensory information to create a more engaging and memorable narrative. By incorporating these techniques, you can make your speech more interesting, relatable, and easier for your audience to understand and retain. User: quit Ending chat.\"),\n",
       " Document(metadata={'title': 'Building a Chatbot', 'source': 'https://cohere.com/llmu/building-a-chatbot'}, page_content='And with that, we have built a simple chatbot that can respond to user messages and maintain the context of the conversation.\\nConclusion\\nThis chapter showed how to build a simple chatbot using the Chat endpoint and how to configure the chatbot, such as overriding the preamble, building the chat history, streaming the response, and modifying the parameters.'),\n",
       " Document(metadata={'title': 'Introduction to Text Generation', 'source': 'https://cohere.com/llmu/introduction-to-text-generation'}, page_content=\"Introduction to Text GenerationQualified\\nChatbots brought large language models (LLMs) into the mainstream. LLMs have been around for a few years, but their adoption was largely limited to the AI community. The launch of AI-powered consumer chatbots has made LLMs accessible to the everyday user, and now they're a hot topic in tech and enterprise circles alike.\\nThis text generation module teaches you how to build LLM chatbots using Cohere’s Chat endpoint.\\nCommand Model\\nCommand is Cohere’s flagship LLM. It generates a response given a prompt or message from a user. It is trained to follow user commands and to be instantly useful in practical business applications, like summarization, copywriting, extraction, and question answering.\\nCommand has been trained with a large volume of multi-turn conversations to ensure that it excels at the various nuances associated with conversational language. It ranks at the top of the Holistic Evaluation of Language Models (HELM) benchmark, an evaluation leaderboard comparing large language models on a wide number of tasks (March ‘23 results).\\nCommand R and Command R+ Models\\nCommand R and Command R+ are designed to be the market leading family of models in the ‘scalable’ category that balance high efficiency with strong accuracy to enable enterprises to move from proof of concept into production-grade AI.\\nHere are some key features of Command R:\\nHigh-performance RAG: Retrieval-augmented generation (RAG) enables enterprises to give the model access to private knowledge that it otherwise would not have.\\nAccess to tools: Tool use enables enterprise developers to turn Command R into an engine for powering the automation of tasks and workflows that require using internal infrastructure like databases and software tools, as well as external tools like CRMs, search engines, and more. Command R+ supports Multi-Step Tool Use which allows the model to combine multiple tools over multiple steps to accomplish difficult tasks.\"),\n",
       " Document(metadata={'title': 'Introduction to Text Generation', 'source': 'https://cohere.com/llmu/introduction-to-text-generation'}, page_content='Low latency and high throughput: Command R targets the “scalable” category of models that balance high performance with strong accuracy, enabling companies to move beyond proof of concept and into production.\\n128k context length and lower pricing: Command R features a longer context length, supporting up to 128k tokens in its initial release.\\nStrong capabilities across 10 key languages: The model excels at 10 major languages for global business: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, and Chinese.\\nModel weights available for research and evaluation: Cohere For AI is releasing the weights for this version of Command R publicly, so that it can be used for research purposes.\\nHow an LLM Chatbot Works\\nTo understand how LLM chatbots work, it’s important to develop an understanding of their building blocks. This section focuses on how to build the generative part of a chatbot by looking at how to use a foundational model and added layers of context to generate answers in a conversation style.\\nThe Foundation of an LLM Chatbot\\nThe foundation of an LLM chatbot is an LLM that has been fine-tuned to follow instructions. It can generate a response given a prompt or message from a user. This type of model is tuned to follow instructions and questions, such as “Write a headline for my homemade jewelry product” or “What is the capital of Canada?”.\\nA message or prompt returning a model response\\nHowever, the LLM’s context is limited to only the last message it receives, and it does not consider any previous messages and responses.\\nYet, chatbots are characterized by their ability to maintain a conversation with a user, which takes place over multiple interactions.\\nA baseline LLM’s context is limited to only the last message it receives'),\n",
       " Document(metadata={'title': 'Introduction to Text Generation', 'source': 'https://cohere.com/llmu/introduction-to-text-generation'}, page_content='A baseline LLM’s context is limited to only the last message it receives\\nThe goal of a chatbot is to solve this problem by linking a sequence of interactions into a single instance, allowing the chatbot to hold an ongoing conversation. In doing so, the model’s response can keep a memory of all the previous interactions instead of having to start from scratch every time.\\nHow to Build a Chatbot\\'s Memory\\nWorking off of the baseline generation model above, we can layer together multiple interactions into a single prompt and create a memory of the entire conversation.\\nFirst, we add a system-level prompt called a preamble. A preamble contains instructions to help steer a chatbot’s response toward specific characteristics, such as a persona, style, or format. For example, if we want the chatbot to adopt a formal style, the preamble can be used to encourage the generation of more business-like and professional responses. The preamble could be something like \"You are a helpful chatbot, trained to assist human users by providing responses in a formal and professional tone.\"\\nThen, we append the current user message to the preamble, which becomes the prompt for the chatbot’s response. Next, we append the chatbot response and the following user message to the prompt.\\nWe can repeat this step for any number of interactions until we reach the model’s maximum context length. Context length is the total number of tokens taken up by the prompt and response, and each model has a maximum context length that it can support.\\nBuilding a conversation by stitching multiple prompt-response pairs together\\nThis multi-turn framework is what gives chatbots the ability to hold the full context of conversation from start to finish.\\nMulti-turn conversations can happen when the full context is available\\nHowever, building on top of a baseline LLM alone is not sufficient.'),\n",
       " Document(metadata={'title': 'Introduction to Text Generation', 'source': 'https://cohere.com/llmu/introduction-to-text-generation'}, page_content=\"Multi-turn conversations can happen when the full context is available\\nHowever, building on top of a baseline LLM alone is not sufficient.\\nChatbots need to perform well in a wide range of scenarios. To create a robust chatbot that consistently generates high-quality and reliable output, the baseline LLM needs to be adapted specifically to conversations. This means taking the baseline model and fine-tuning it further with a large volume of conversational data.\\nThis is what forms the foundation of Cohere’s Chat endpoint — let’s take a closer look.\\nCohere's Chat Endpoint\\nImproving LLM chatbot performance starts with how the baseline LLM is trained. The model powering the Chat endpoint is Cohere’s Command model, trained with a large volume of multi-turn conversations. This ensures that the model will excel at the various nuances associated with conversational language and perform well across different use cases.\\nBeyond training, fine-tuning a baseline LLM for conversations requires adding a standardized interface on top of the prompt formatting system. The Chat endpoint provides a consistent, simplified, and structured way of handling the prompt formatting that defines how the prompt inputs should be organized, making it easier for developers to build chatbot applications. This added layer includes a fixed abstraction and schema, providing more stability to scale and build applications on top of the foundation model.\\nThe Chat endpoint includes all the elements required for an LLM chatbot (as discussed in the previous sections), exposing a simple interface for developers. It consists of the following key components:\\nPreamble management: Developers can opt to use the endpoint’s default preamble or override it with their own preambles.\\nMulti-turn conversations: The Chat endpoint builds upon the Command model by enabling multi-turn conversations.\"),\n",
       " Document(metadata={'title': 'Introduction to Text Generation', 'source': 'https://cohere.com/llmu/introduction-to-text-generation'}, page_content=\"Multi-turn conversations: The Chat endpoint builds upon the Command model by enabling multi-turn conversations.\\nState management: State management preserves the conversation memory. Developers can either leverage the endpoint’s conversation history persistence feature or manage the conversation history themselves.\\nFully-managed conversation: The abstraction layer of the Chat endpoint means there’s only one item to send to the API: the user message. Everything else is managed automatically. At the same time, developers who want greater control over a chatbot’s configuration can still do so.\\nThe Chat endpoint takes care of the underlying logic, exposing a simple interface for developers\\nConclusion\\nCohere's Command model and Chat endpoint offer powerful tools for developers looking to harness the potential of LLMs in their applications. Command's capabilities, coupled with the Chat endpoint's developer-friendly interface, allow developers to build text generation applications across diverse business applications.\"),\n",
       " Document(metadata={'title': 'Parameters for Controlling Outputs', 'source': 'https://cohere.com/llmu/parameters-for-controlling-outputs'}, page_content='Parameters for Controlling OutputsQualified\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nThe Chat endpoint is a versatile tool that empowers developers with an extensive array of options and parameters.\\nAs you’ll learn, the Command model has many variations to select from, where each has been carefully crafted to suit different needs. Additionally, you will see how to use parameters to control the creativity of model responses.\\nSetup\\nTo set up, we first import the Cohere module and create a client.\\nimport cohere co = cohere.Client(\"COHERE_API_KEY\") # Your Cohere API key \\nModel Type\\nWith the Chat endpoint , you can choose from several variations of the Command model . Different models may produce different output profiles, so you may want to experiment with different models to get the best output for your use case.\\nSee the documentation for the most updated list of available Cohere models. At the time of writing, the models are as follows:\\ncommand: The default model used in a Chat endpoint call. If you don’t define the model parameter, this model will be used.\\ncommand-light: A smaller, faster version of command. Almost as capable, but a lot faster.\\ncommand-r: Performs language tasks at a higher quality, more reliably, and with a longer context than previous models.\\ncommand-nightly and command-light-nightly: The latest, most experimental, and (possibly) unstable versions of their default counterparts. Not recommended for production use.\\nUse the model parameter to select a variation that suits your requirements. In the code cell, we select command-r.\\nresponse = co.chat(message=\"Hello\", model=\"command-r-plus\") print(response.text) \\n# RESPONSE Hello! How can I help you today? \\nRandomness\\nOften, you’ll need to control the level of randomness of the model. There are a number of factors to keep in mind when tuning model randomness, including:'),\n",
       " Document(metadata={'title': 'Parameters for Controlling Outputs', 'source': 'https://cohere.com/llmu/parameters-for-controlling-outputs'}, page_content='Randomness\\nOften, you’ll need to control the level of randomness of the model. There are a number of factors to keep in mind when tuning model randomness, including:\\nTask type: You’ll likely want to decrease the randomness of the model when performing structured tasks that have a correct answer, like for question answering or summarization, or for generating technical text. In these cases, we want the model to yield a safe and predictable response. On the other hand, if you’re generating poetry or brainstorming ideas, you might want to increase the randomness to produce more diverse and creative responses.\\nModel behavior: We likely need to increase the randomness of the model if it gets stuck in a loop and starts repeating itself, or if it is producing overly generic phrases. Increasing the randomness will expand the set of words that the model can use when generating responses.\\nControlling style and tone: If generating text that needs to have a specific tone or style, like for a company blog post or customer support responses, a low level of randomness might be desirable, to keep the model from generating unusual words.\\nModifying the temperature parameter changes the extent to which the model considers incorporating unlikely tokens (can be words, parts of words, or punctuation) in its response, which can make the output more random and creative.\\nTo understand this, we’ll look at an example. The model would likely predict that the token cookies has a much higher likelihood than chair for appearing after the phrase I like to bake.\\nThe model assigns a likelihood number to each of all possible next tokens\\nBefore these likelihoods can be used to select the next token, they first need to be converted to probabilities. The temperature parameter controls how this conversion is done.\\nAt low temperature, low likelihood tokens are assigned very low probabilities, and high likelihood tokens are assigned very high probabilities.'),\n",
       " Document(metadata={'title': 'Parameters for Controlling Outputs', 'source': 'https://cohere.com/llmu/parameters-for-controlling-outputs'}, page_content='At low temperature, low likelihood tokens are assigned very low probabilities, and high likelihood tokens are assigned very high probabilities.\\nAt high temperature, the probabilities will look roughly similar for each token, with high likelihood tokens assigned only slightly higher probability.\\nAdjusting the temperature setting\\nBuilding off the example above,\\nAt low temperature, there’s a probability that chair is selected, but the probability is significantly lower than cookies.\\nAt high temperature, the probability that chair is selected is only slightly lower than cookies.\\nThe temperature parameter is a value between 0 and 1. As you increase the temperature, the model gets more creative and random. Temperature can be tuned for different problems, and most people will find that the default temperature of 0.3 is a good starting point.\\nLet’s look at a code example, where we suggest that the model generate alternative names for a blog post. Prompting the endpoint five times when the temperature is set to 0 yields the same output each time.\\nmessage = \"\"\"Suggest a more exciting title for a blog post titled: Intro to Retrieval-Augmented Generation. \\\\ Respond in a single line.\"\"\" for _ in range(5): response = co.chat(message=message, temperature=0, model=\"command-r-plus\") print(response.text) \\n# RESPONSE The Future of AI: Unlocking the Power of Retrieval-Augmented Generation The Future of AI: Unlocking the Power of Retrieval-Augmented Generation The Future of AI: Unlocking the Power of Retrieval-Augmented Generation The Future of AI: Unlocking the Power of Retrieval-Augmented Generation The Future of AI: Unlocking the Power of Retrieval-Augmented Generation \\nHowever, if we increase the temperature to the maximum value of 1, the model gives different proposals.'),\n",
       " Document(metadata={'title': 'Parameters for Controlling Outputs', 'source': 'https://cohere.com/llmu/parameters-for-controlling-outputs'}, page_content='However, if we increase the temperature to the maximum value of 1, the model gives different proposals.\\nmessage = \"\"\"Suggest a more exciting title for a blog post titled: Intro to Retrieval-Augmented Generation. \\\\ Respond in a single line.\"\"\" for _ in range(5): response = co.chat(message=message, temperature=1, model=\"command-r-plus\") print(response.text) \\n# RESPONSE Unleashing the Power of Retrieval-Augmented Generation: A Comprehensive Guide The Exciting Future of AI: How Retrieval-Augmented Generation Will Transform the Way We Interact With Machines The Magic of AI: Unlocking the Power of Retrieval-Augmented Generation The Future of AI: Unlocking the Power of Retrieval-Augmented Generation \"Unleashing the Power of AI: The Rise of Retrieval-Augmented Generation.\" \\nConclusion\\nIn this chapter, you learned how to call different variations of the Command model when using the Chat endpoint. You worked with a code example using the temperature parameter to control the Command model’s level of randomness.'),\n",
       " Document(metadata={'title': 'Prompt Engineering Basics', 'source': 'https://cohere.com/llmu/prompt-engineering-basics'}, page_content='Prompt Engineering BasicsQualified\\nWe’ll use Cohere’s Python SDK for the code examples. Follow along in this notebook.\\nNote: This chapter covers the basics of prompt engineering. If you want to explore this topic further, we have a dedicated LLMU module on prompt engineering as well as further documentation on prompt engineering.\\nPrompting is at the heart of working with LLMs. The prompt provides context for the text that we want the model to generate. The prompts we create can be anything from simple instructions to more complex pieces of text, and they are used to encourage the model to produce a specific type of output.\\nComing up with a good prompt is a bit of both science and art. On the one hand, we know the broad patterns that enable us to construct a prompt that will generate the output that we want. But on the other hand, there is so much room for creativity and imagination, as you’ll see in the examples in this section.\\nSetup\\nTo set up, we first import the Cohere module and create a client.\\nimport cohere co = cohere.Client(\"COHERE_API_KEY\") # Your Cohere API key \\nLet\\'s also define a function generate_text() to take a user message, call the Chat endpoint, and stream the response.\\ndef generate_text(message): stream = co.chat_stream(message=message, model=\"command-r-plus\") for event in stream: if event.event_type == \"text-generation\": print(event.text, end=\\'\\') \\nWriting a Basic Prompt\\nThe best way to design prompts for a model like Command is to give a command or an instruction. One way to do this is by using imperative verbs, for example: generate, write, list, provide, and other variations.\\nFor instance, let’s say that we are creating the product description copy for a wireless earbuds product. We can write the prompt as follows.\\ngenerate_text(\"Generate a concise product description for the product: wireless earbuds.\")'),\n",
       " Document(metadata={'title': 'Prompt Engineering Basics', 'source': 'https://cohere.com/llmu/prompt-engineering-basics'}, page_content='generate_text(\"Generate a concise product description for the product: wireless earbuds.\") \\n# RESPONSE Sure! Here is a concise product description for wireless earbuds: \"Experience unparalleled sound with our wireless earbuds. Immerse yourself in rich, high-quality audio and enjoy the freedom of movement without tangles or hassles. Our earbuds offer a secure and comfortable fit, ensuring they stay put during any activity. With a sleek and portable design, these earbuds are the perfect companion for your active lifestyle.\" \\nThat’s not bad. With a simple, one-line prompt, we already have a piece of product description that can make a digital marketer proud!\\nLayering Additional Instructions\\nBut perhaps we want to be more specific regarding what we want the output to look like. For this, we can layer additional instructions onto the model in the prompt.\\nLet’s say we want the model to write the product description in a particular format with specific information. In this case, we can append this specific instruction in the prompt.\\ngenerate_text(\"\"\" Generate a concise product description for the product: wireless earbuds. Use the following format: Hook, Solution, Features and Benefits, Call to Action. \"\"\") \\n# RESPONSE Here is a concise product description for wireless earbuds: **Hook:** Tired of tangled cords and restricted movement? **Solution:** Go wireless with our latest earbuds! **Features and Benefits:** - Sleek and ergonomic design ensures a secure and comfortable fit. - Advanced Bluetooth technology for seamless connectivity and uninterrupted audio. - Immersive sound quality with powerful bass and crystal-clear highs. - Built-in microphone for hands-free calls and voice assistant access. - Convenient charging case provides on-the-go power. **Call to Action:** Experience true wireless freedom and elevate your audio experience with our cutting-edge earbuds. Order now and enjoy uninterrupted music, calls, and more!'),\n",
       " Document(metadata={'title': 'Prompt Engineering Basics', 'source': 'https://cohere.com/llmu/prompt-engineering-basics'}, page_content='The model returns an output following the format that we wanted.\\nAdding Context to a Prompt\\nThe prompt can also be constructed as a combination of an instruction and some context. Let’s see this in action with another example: emails. We can create a prompt to summarize an email, which is included in the prompt for context.\\ngenerate_text(\"\"\" Summarize this email in one sentence. Dear [Team Members], I am writing to thank you for your hard work and dedication in organizing our recent community meetup. The event was a great success and it would not have been possible without your efforts. I am especially grateful for the time and energy you have invested in making this event a reality. Your commitment to ensuring that everything ran smoothly and that our guests had a great time is greatly appreciated. I am also thankful for the support and guidance you have provided to me throughout the planning process. Your insights and ideas have been invaluable in ensuring that the event was a success. I am confident that our community will benefit greatly from this event and I am excited to see the positive impact it will have. Thank you again for your hard work and dedication. I am looking forward to working with you on future events. Sincerely, [Your Name] \"\"\") \\n# RESPONSE The email expresses gratitude to the team members for their hard work, dedication, and guidance in organizing a successful community meetup, and looks forward to future collaborations. \\nThis instruction–context prompt format is extremely useful as it means that we can supply additional information as context to help ground the model\\'s output. One such example is a question-answering system for, let\\'s say, a company\\'s knowledge base. Given a question (the instruction), the model will only be able to provide accurate answers if provided with the knowledge base (the context).'),\n",
       " Document(metadata={'title': 'Prompt Engineering Basics', 'source': 'https://cohere.com/llmu/prompt-engineering-basics'}, page_content='Let\\'s move to another example — an extraction task, which a generative model can do very well. Given context, which in this case is a description of a movie, we want the model to extract the movie title.\\ngenerate_text(\"\"\" Extract the movie title from the text below. Deadpool 2 | Official HD Deadpool\\'s \"Wet on Wet\" Teaser | 2018 \"\"\") \\nRewriting Text\\nThe model is also effective at tasks that involve taking a piece of text and rewriting it into another format that we need.\\nHere\\'s an example. We have a one-line instruction followed by the context, which in this case is a blog excerpt. The instruction is to generate a list of frequently asked questions (FAQ) based on the passage, which involves a mixture of several tasks, such as extraction and rewriting.'),\n",
       " Document(metadata={'title': 'Prompt Engineering Basics', 'source': 'https://cohere.com/llmu/prompt-engineering-basics'}, page_content='generate_text(\"\"\" Given the following text, write down a list of potential frequently asked questions (FAQ), together with the answers. The Cohere Platform provides an API for developers and organizations to access cutting-edge LLMs without needing machine learning know-how. The platform handles all the complexities of curating massive amounts of text data, model development, distributed training, model serving, and more. This means that developers can focus on creating value on the applied side rather than spending time and effort on the capability-building side. There are two key types of language processing capabilities that the Cohere Platform provides — text generation and text embedding — and each is served by a different type of model. With text generation, we enter a piece of text, or prompt, and get back a stream of text as a completion to the prompt. One example is asking the model to write a haiku (the prompt) and getting an originally written haiku in return (the completion). With text embedding, we enter a piece of text and get back a list of numbers that represents its semantic meaning (we’ll see what “semantic” means in a section below). This is useful for use cases that involve “measuring” what a passage of text represents, for example, in analyzing its sentiment. \"\"\")'),\n",
       " Document(metadata={'title': 'Prompt Engineering Basics', 'source': 'https://cohere.com/llmu/prompt-engineering-basics'}, page_content=\"# RESPONSE Here is a list of potential FAQs based on the provided text: - **Q: What does the Cohere Platform offer to developers and organizations?** A: The Cohere Platform offers an API that provides access to advanced Large Language Models (LLMs) without requiring machine learning expertise. It simplifies the process by handling data curation, model development, training, and serving. - **Q: What are the key language processing capabilities of the Cohere Platform?** A: The platform offers two main capabilities: text generation and text embedding. Text generation involves completing a prompt with a stream of generated text, like writing a haiku. Text embedding returns a numerical representation of the semantic meaning of a text input, useful for sentiment analysis and other measurements. - **Q: How does the Cohere Platform benefit developers?** A: Developers can focus on creating valuable applications without getting bogged down by the complexities of building language processing capabilities from scratch. The platform handles the heavy lifting, allowing developers to save time and effort. - **Q: What is the difference between text generation and text embedding?** A: Text generation is about creating new text based on a prompt, like generating a haiku or continuing a story. Text embedding, on the other hand, translates text into a series of numbers that represent its semantic meaning, enabling quantitative analysis and understanding of the text's context. - **Q: Can I use the Cohere Platform for sentiment analysis?** A: Yes, the text embedding capability of the Cohere Platform is particularly useful for sentiment analysis. By converting text into numerical representations, you can quantitatively analyze and understand the sentiment or emotional tone expressed in a given piece of text. - **Q: Does the Cohere Platform require machine learning expertise to use?** A: No, the platform is designed to abstract away the complexities of machine learning. Developers can\"),\n",
       " Document(metadata={'title': 'Prompt Engineering Basics', 'source': 'https://cohere.com/llmu/prompt-engineering-basics'}, page_content='given piece of text. - **Q: Does the Cohere Platform require machine learning expertise to use?** A: No, the platform is designed to abstract away the complexities of machine learning. Developers can utilize the API without needing to understand the intricate details of machine learning or natural language processing. These FAQs aim to address common inquiries that users might have about the Cohere Platform and its language processing capabilities.'),\n",
       " Document(metadata={'title': 'Prompt Engineering Basics', 'source': 'https://cohere.com/llmu/prompt-engineering-basics'}, page_content='By now, we can see how versatile our model is at performing various forms of tasks — not just freeform text generation, but also following instructions, working with contextual information, summarizing long passages, extracting information, rewriting text into different formats, and more.\\nThis is just a taste of what kinds of prompts you can design. You can keep layering your instructions to be as specific as you want, and see the output generated by the model. And there is really no right or wrong way to design a prompt. It’s really about applying an idea and continuing to iterate the prompt until you get the outcome you are looking for.\\nAfter completing this module, we encourage you to take a look at LLMU’s Prompt Engineering module to go deeper into prompt engineering techniques and apply them to Cohere’s Command model.\\nConclusion\\nIn this chapter, you learned how to prompt a model — probably the most important and definitely the most fun part of working with large language models.'),\n",
       " Document(metadata={'title': 'Fine-Tuning for Chat', 'source': 'https://cohere.com/llmu/fine-tuning-for-chat'}, page_content=\"Fine-Tuning for ChatQualified\\nWe’ll use Cohere’s Dashboard for the code examples. Follow along in this notebook.\\nCohere’s large language models (LLMs) have been trained to be useful in practical business applications. Using the Chat endpoint, you can leverage a pre-trained LLM to build a chatbot that performs tasks ranging from summarization to copywriting to question answering.\\nDepending on the use case you have in mind, you might want to amend how the chatbot generates its output. For instance, if your task uses highly technical data or you want to change the chatbot’s output format, you often need to perform another round of training on additional data to ensure the best performance. This extra training is referred to as fine-tuning.\\nFine-tuning is also recommended when you want to incorporate your company's unique knowledge base. For example, if you are aiming to use a model to draft responses to customer-support inquiries, fine-tuning on old conversations with customers can improve the quality of the output.\\nStep-by-Step Guide\\nStep 1: Prepare and Validate the Dataset\\nWe will work with the CoEdIT dataset of text editing examples (Raheja, et al). In each example, the user asks a writing assistant to rewrite text to suit a specific task (editing fluency, coherence, clarity, or style) and receives a response. Below, you can see some examples from the raw dataset.\"),\n",
       " Document(metadata={'title': 'Fine-Tuning for Chat', 'source': 'https://cohere.com/llmu/fine-tuning-for-chat'}, page_content='{ \"_id\": \"57241\", \"task\": \"coherence\", \"src\": \"Make the text more coherent: It lasted for 60 minutes. It featured the three men taking questions from a studio audience.\", \"tgt\": \"Lasting for 60 minutes, it featured the three men taking questions from a studio audience.\" } { \"_id\": \"69028\", \"task\": \"clarity\", \"src\": \"Make the sentence clearer: URLe Lilanga (1934 27 June 2005) was a Tanzanian painter and sculptor, active from the late 1970s and until the early years of the 21st century.\", \"tgt\": \"URLe Lilanga (1934 27 June 2005) was a Tanzanian painter and sculptor, active from the late 1970s and until the early 21st century.\" } \\nWe will use the src and tgt fields from each example, which correspond to the user’s prompt and the writing assistant’s response, respectively. Instead of using the full dataset, we will use a subset focused on making text coherent: 927 total conversations.\\nTo format the dataset for the Python SDK, we create a .jsonl where each JSON object is a conversation containing a series of messages.\\nA System message in the beginning, acting as the preamble that guides the whole conversation\\nMultiple pairs of User and Chatbot messages, representing the conversation that takes place between a human user and a chatbot\\nFor more detail on best practices for formatting your dataset, check out the documentation . Here is a preview of the prepared dataset:\\n{\\'messages\\': [{\\'role\\': \\'System\\', \\'content\\': \\'You are a writing assistant that helps the user write coherent text.\\' }, {\\'role\\': \\'User\\', \\'content\\': \\'Make the text more coherent: It lasted for 60 minutes. It featured the three men taking questions from a studio audience.\\' }, {\\'role\\': \\'Chatbot\\', \\'content\\': \\'Lasting for 60 minutes, it featured the three men taking questions from a studio audience.\\' } ] } \\nStep 2: Fine-Tune the Model\\nWe kick off a fine-tuning job by navigating to the fine-tuning tab of the Dashboard. Under \"Chat\", click on \"Create a Chat model\".\\nCohere Dashboard'),\n",
       " Document(metadata={'title': 'Fine-Tuning for Chat', 'source': 'https://cohere.com/llmu/fine-tuning-for-chat'}, page_content='Step 2: Fine-Tune the Model\\nWe kick off a fine-tuning job by navigating to the fine-tuning tab of the Dashboard. Under \"Chat\", click on \"Create a Chat model\".\\nCohere Dashboard\\nNext, upload the .jsonl file you just created as the training set by clicking on the \"TRAINING SET\" button. When ready, click on \"Review data\" to proceed to the next step.\\nUploading the training set\\nThen, you\\'ll see a preview of how the model will ingest your data. It shows the total number of conversations, the total number of turns, and the average number of turns per conversation. If anything is wrong with the data, the page will also provide suggested changes to fix the data file. Otherwise, if everything looks good, you can proceed to the next step.\\nReviewing your fine-tuning data\\nNext, you\\'ll see an estimated cost of fine-tuning, followed by a page where you\\'ll provide a nickname to your model. We used coedit-coherence as the nickname for our model. This page also allows you to provide custom values for the hyperparameters used during training, but we\\'ll keep them at the default values for now.\\nStarting a training job\\nOnce you have filled in a name, click on \"Start training\" to kick off the fine-tuning process. This will navigate you to a page where you can monitor the status of the model. A model that has finished fine-tuning will show the status as READY.\\nA model has finished fine-tuning when its status is READY\\nStep 3: Evaluate the Fine-Tuned Model\\nOnce the model has completed the fine-tuning process, it’s time to evaluate its performance.\\nWith Test Data\\nWhen you\\'re ready to use the fine-tuned model, navigate to the API tab. There, you\\'ll see the model ID that you should use when callingco.chat().\\nGet the model ID from the API tab\\nIn the following code, we supply a message from the test dataset to both the pre-trained and fine-tuned models for comparison.'),\n",
       " Document(metadata={'title': 'Fine-Tuning for Chat', 'source': 'https://cohere.com/llmu/fine-tuning-for-chat'}, page_content='Get the model ID from the API tab\\nIn the following code, we supply a message from the test dataset to both the pre-trained and fine-tuned models for comparison.\\nuser_message = \"Make the text coherent: Pimelodella kronei is a species of three-barbeled catfish endemic to Brazil. Discovered by the German naturalist Sigismund Ernst Richard Krone, Pimelodella kronei was the first troglobitic species described in Brazil, but several others have been described later.\" # Desired response: Pimelodella kronei is a species of three-barbeled catfish endemic to Brazil. Discovered by the German naturalist Sigismund Ernst Richard Krone, it was the first troglobitic fish described in Brazil, but several others have been described later. preamble = \"You are a writing assistant that helps the user write coherent text.\" # Get default model response response_pretrained=co.chat( message=user_message, preamble=preamble, ) # Get fine-tuned model response response_finetuned = co.chat( message=user_message, model=\\'acb944bb-fb49-4c29-a15b-e6a245a7bdf9-ft\\', preamble=preamble, ) print(f\"Default response: {response_pretrained.text}\",\"\\\\n-----\") print(f\"Fine-tuned response: {response_finetuned.text}\") \\nFor this example, the output appears as follows:\\nDefault response: The three-barbeled catfish, Pimelodella kronei, is exclusive to Brazil. It was Sigismund Ernst Richard Krone, a German naturalist, who first discovered this remarkable species. Notably, P. kronei was also the initial troglobitic species to be identified in Brazil, though other such species have since been uncovered. Would you like to know more about the discovery of this species or its natural habitat? ----- Fine-tuned response: Pimelodella kronei, a species of three-barbeled catfish endemic to Brazil, was discovered by the German naturalist Sigismund Ernst Richard Krone. It was the first troglobitic species described in Brazil, but several others have been described later.'),\n",
       " Document(metadata={'title': 'Fine-Tuning for Chat', 'source': 'https://cohere.com/llmu/fine-tuning-for-chat'}, page_content='In this example, both models provide reasonable answers that are an improvement over the user’s original text. However, the fine-tuned model’s response better matches the style of the fine-tuning data, because it is more succinct: both the fine-tuned model’s response and the desired response have 36 words, whereas the response from the pre-trained model is about 25% (10 words) longer.\\nThis is just one example, but you can compare responses to more prompts in the notebook.\\nIn the Chat Context\\nWe have demonstrated that the fine-tuned model can provide good answers to individual questions. But it is also a competent participant in longer, multi-turn conversations.\\nTo see this, we will borrow from the code in the Building a Chatbot chapter to build a simple chat interface. The only change we need to make is to supply the model nickname when using co.chat_stream() to ensure that we are chatting with the model we just fine-tuned.\\n# Create a conversation ID import uuid conversation_id = str(uuid.uuid4()) print(\\'Starting the chat. Type \"quit\" to end.\\\\n\\') while True: # User message message = input(\"User: \") # Typing \"quit\" ends the conversation if message.lower() == \\'quit\\': print(\"Ending chat.\") break # Chatbot response stream = co.chat_stream(message=message, model=\\'acb944bb-fb49-4c29-a15b-e6a245a7bdf9-ft\\', preamble=preamble, conversation_id=conversation_id) print(\"Chatbot: \", end=\\'\\') for event in stream: if event.event_type == \"text-generation\": print(event.text, end=\\'\\') print(\"\\\\n\",\"-\"*50) \\nWe can use the code above to have a conversation with the fine-tuned model.'),\n",
       " Document(metadata={'title': 'Fine-Tuning for Chat', 'source': 'https://cohere.com/llmu/fine-tuning-for-chat'}, page_content='Starting the chat. Type \"quit\" to end. User: Hello Chatbot: Hello, how can I help you today? -------------------------------------------------- User: I\\'m fine. Can I ask you for help with some tasks? Chatbot: I am happy to help you with any tasks that you need help with. I can also provide information about any topic that you would like to know more about. -------------------------------------------------- User: Make this more coherent: Manuel now has to decide-will he let his best friend be happy with her Prince Charming. Or will he fight for the love that has kept him alive for the last 16 years? Chatbot: Manuel now has to decide-will he let his best friend be happy with her Prince Charming, or will he fight for the love that has kept him alive for the last 16 years? -------------------------------------------------- User: Help me with this one - She left Benaras. Conditions back home were bad. Chatbot: She left Benaras because conditions back home were bad. -------------------------------------------------- User: What\\'s a good time to visit London Chatbot: A good time to visit London is in the spring or fall, when the weather is mild and the city is not too crowded. The best time to visit London depends on what you want to see and do. If you want to see the sights and do some shopping, a good time to visit London is in the spring, when the weather is mild and the city is not too crowded. If you want to see the sights and do some sightseeing, a good time to visit London is in the fall, when the weather is mild and the city is not too crowded. -------------------------------------------------- User: Could you help with this please: Make the text coherent: Critically the album has not been as well received as other Browne recordings. It remains his only album to date to reach number 1 on the Billboard chart. Chatbot: Critically the album has not been as well received as other Browne recordings, but it remains his only album to date to reach number 1 on the'),\n",
       " Document(metadata={'title': 'Fine-Tuning for Chat', 'source': 'https://cohere.com/llmu/fine-tuning-for-chat'}, page_content='date to reach number 1 on the Billboard chart. Chatbot: Critically the album has not been as well received as other Browne recordings, but it remains his only album to date to reach number 1 on the Billboard chart. -------------------------------------------------- User: quit Ending chat.'),\n",
       " Document(metadata={'title': 'Fine-Tuning for Chat', 'source': 'https://cohere.com/llmu/fine-tuning-for-chat'}, page_content='Note the fine-tuned model is still able to respond to prompts like “Hello”, “I’m fine. Can I ask you for help with some tasks?”, and “What’s a good time to visit London” instead of strictly following the fine-tuning objective of editing text.\\nThe model also did a good job with context switching; it can hold a conversation when the user switches from friendly greetings, to a request for writing help, to travel planning, and finally back to writing assistance. It can also infer when the user is asking for help with making a text coherent, even if it is not explicitly stated (e.g., “Help me with this one”) or if the request is buried slightly (e.g., with “Could you help me with this please”).\\nIf you’re interested in further improving model performance, explore refining data quality or iterating on hyperparameters, as described in the documentation.\\nConclusion\\nIn this chapter, you learned how to fine-tune a model for the Chat endpoint on a custom dataset. You saw how fine-tuning allows you to tailor a chatbot to a specific use case and give it a particular style. As described in the documentation, you learned how to prepare the fine-tuning data, start a fine-tuning job, and understand the results.'),\n",
       " Document(metadata={'title': 'Introduction to RAG', 'source': 'https://cohere.com/llmu/introduction-to-rag'}, page_content='Introduction to RAGQualified\\nNote: This chapter covers an introduction to retrieval-augmented generation (RAG). If you want to explore this topic further and learn how to implement RAG with the Cohere API, we have a dedicated module on RAG.\\nIn this module on text generation, we have so far explored LLM chatbots that only have access to the data they have been trained on, or their internal knowledge. In many applications, particularly for enterprise use, a chatbot needs to also be able to access external knowledge for it to be useful.\\nCohere’s Chat endpoint comes with a retrieval-augmented generation (RAG) feature that makes it possible to connect to external knowledge bases and deliver more accurate responses.\\nWhat is RAG?\\nTo understand RAG, we will work with the example of a company that wants to deploy a chatbot as an intelligent knowledge assistant. For the chatbot to be useful, it will need to be connected to the company’s knowledge base.\\nConnecting the Chat endpoint with external knowledge\\nThis allows the chatbot to have the correct context when responding to requests, such as summarizing a meeting transcript, extracting information from the company wiki, and assisting a customer support agent in responding to a customer inquiry. Without access to the company’s knowledge base, the chatbot will not be able to perform these types of tasks successfully.\\nRAG solves the lack of specific knowledge problem\\nThe company will also likely need the chatbot to respond to time-sensitive prompts and provide up-to-date answers. For example, suppose an employee asks the chatbot about a recent public event. A baseline LLM is trained with data that is current up to a certain cut-off time. Without accessing external data, the model relies on the most recent information it has been trained on (assuming the specific information is available in the training data). In this situation, the lack of recency in the training data would produce an inadequate answer.'),\n",
       " Document(metadata={'title': 'Introduction to RAG', 'source': 'https://cohere.com/llmu/introduction-to-rag'}, page_content='RAG solves the lack of recency problem\\nRAG consists of two parts: a retrieval system and an augmented generation system. Let’s take a look at how they work.\\nRetrieval\\nThe first part of RAG is to retrieve the right information needed to respond to a user query. Given a user message (1), the Chat endpoint queries an external knowledge base with the relevant queries (2), and finally retrieves the query results (3).\\nThe retrieval part of RAG: Given a user message, the endpoint retrieves information from an external knowledge base\\nRetrieval applies to any system that can fetch relevant documents based on a user message. Cohere offers two ways to build RAG systems with the Chat endpoint: connectors mode and documents mode.\\nConnectors Mode\\nAs the name implies, connectors are ways of connecting to data sources. These data sources could be internal documents, document databases, the broader internet, or any other data source which can inform the model’s replies. When the Chat endpoint is called in connectors mode, it connects to the associated data sources and automatically handles document retrieval.\\nDevelopers can leverage pre-built connectors to various data sources or even build their own. Also, there is a web search connector managed by Cohere which runs searches against a browser in safe mode. Developers can use it immediately without any additional configuration or deployment.\\nFor example, suppose we are working with a RAG system that uses Cohere’s managed web search connector, and say you input this query: “Who was the keynote speaker at the AI conference last week?”. Given that the response requires a fact from a recent event, the chatbot triggers a retrieval of this information using the web search API. It gets back the information it requires, such as a few website snippets containing the details about the conference.\\nAn example of retrieving information via web search'),\n",
       " Document(metadata={'title': 'Introduction to RAG', 'source': 'https://cohere.com/llmu/introduction-to-rag'}, page_content=\"An example of retrieving information via web search\\nIn an enterprise setting, data is distributed across multiple platforms and datastores. With connectors mode, you can use multiple connectors at the same time to get the system to retrieve and synthesize information from various data sources.\\nConnectors mode allows you to use multiple connectors to connect to multiple datastores\\nDocuments Mode\\nWith documents mode, a developer directly provides the model with source documents that it can use to ground its replies. In this case, these documents first need to be ingested. Typically, this involves chunking large documents into smaller chunks, turning these chunks into text embeddings (also called vectors), and storing these embeddings in a vector store.\\nDocuments a inregested before retrieval\\nConnectors mode automatically handles document ingestion and is excellent for those who prefer a more hands-off approach. However, documents mode allows developers to customize the RAG stack's components to fit specific needs.\\nAugmented Generation\\nThe second part of RAG is augmented generation. Here, the prompt is augmented with the information retrieved from the retrieval step. The prompt is now grounded with the best information to provide the user with an accurate and helpful response.\\nThe chatbot responds to the user query, now having the augmented prompt as its context.\\nThe augmented generation part of RAG: The Chat endpoint uses the retrieved information to provide a grounded response\\nCohere’s Chat endpoint also provides citations to indicate the parts of the retrieved documents on which the response was grounded.\\nCitations provide verifiable references for the generated content\\nCitations provide a critical benefit by delivering the generated content with verifiable references, enhancing the credibility and trustworthiness of the presented information, and allowing users to further explore responses for a deeper understanding.\\nConclusion\"),\n",
       " Document(metadata={'title': 'Introduction to RAG', 'source': 'https://cohere.com/llmu/introduction-to-rag'}, page_content='Conclusion\\nIn this chapter, we looked at enhancing LLM chatbots with RAG, a key component of the Chat endpoint that makes it possible to connect the API to external data for augmented generation. To continue your learning, we encourage you to take a look at LLMU’s Retrieval-Augmented Generation (RAG) module to learn how to build your own RAG systems.'),\n",
       " Document(metadata={'title': 'Deploying with Streamlit', 'source': 'https://cohere.com/llmu/deploy-streamlit'}, page_content=\"Deploying with StreamlitQualified\\nWe’ll show only selected code snippets in this article, but you can find the complete code in this repository.\\nIn this module, we will explore various options for deploying LLM-powered applications. And we'll begin with one of the fastest and simplest options to get started – Streamlit.\\nWeb development is a multi-stage process, and front-end development usually comes back towards the end. While it may seem like front-end development should be one of the quickest parts of the process, it often turns out to be quite a lengthy endeavor.\\nEnter Streamlit. It provides a very fast way to build front-end web applications with very little hassle. In this article, we’ll see how we can quickly prototype our Startup Idea Generator idea, deploy it as a web app and share it with the world!\\nBut let’s go back to the start and see how we might approach this. We can break the plan into three key steps, as follows:\\nBuild the back-end with Cohere\\nBuild the front-end with Streamlit\\nDeploy with Streamlit Cloud\\nStep 1: Build the back-end with Cohere\\nBuild the startup idea generator\\nThe first thing we need to do is build our main feature—the startup idea generator. For this, we use the Chat endpoint , which generates text given an input called a prompt.\\nCreating prompts, or prompt engineering, does require some experimentation. This is also good news - it means there’s huge room for you to innovate and apply your own creativity in designing prompts that get the best out of the endpoint. Read this documentation if you are interested in learning more.\\nIn our case, we create a prompt containing an instruction and a few examples of an industry and its corresponding startup idea.\\nWe then build a function that leverages Cohere’s Python SDK to take in user input and return the generated text, and the code looks as follows.\"),\n",
       " Document(metadata={'title': 'Deploying with Streamlit', 'source': 'https://cohere.com/llmu/deploy-streamlit'}, page_content='We then build a function that leverages Cohere’s Python SDK to take in user input and return the generated text, and the code looks as follows.\\ndef generate_idea(industry): prompt = f\"\"\" Generate a startup idea given the industry. # Examples Industry: Workplace Startup Idea: A platform that generates slide deck contents automatically based on a given outline Industry: Home Decor Startup Idea: An app that calculates the best position of your indoor plants for your apartment Industry: Healthcare Startup Idea: A hearing aid for the elderly that automatically adjusts its levels and with a battery lasting a whole week Industry: Education Startup Idea: An online primary school that lets students mix and match their own curriculum based on their interests and goals Industry: {industry}\"\"\" # Call the Cohere Chat endpoint response = co.chat( model=\\'command-r\\', message=prompt, preamble=\"\") return response.text.replace(\"Startup Idea: \", \"\") \\nDoing a quick test, entering “Public Transportation” returns the following startup idea: “A small electronic device that notifies users when they have to get off the train or bus”. Looking good so far.\\nBuild the startup name generator\\nGenerating startup ideas is great, but it would make the app much more exciting if we could also suggest a startup name for each idea. Using a similar approach as before, we create a new prompt containing an instruction and a few examples of a startup idea description and its corresponding startup name.\\nWe create another function that takes in a startup idea as the input and returns the generated startup name. The code looks as follows.'),\n",
       " Document(metadata={'title': 'Deploying with Streamlit', 'source': 'https://cohere.com/llmu/deploy-streamlit'}, page_content='We create another function that takes in a startup idea as the input and returns the generated startup name. The code looks as follows.\\ndef generate_name(idea): prompt= f\"\"\" Generate a startup name and name given the startup idea. # Examples Startup Idea: A platform that generates slide deck contents automatically based on a given outline Startup Name: Deckerize Startup Idea: An app that calculates the best position of your indoor plants for your apartment Startup Name: Planteasy Startup Idea: A hearing aid for the elderly that automatically adjusts its levels and with a battery lasting a whole week Startup Name: Hearspan Startup Idea: An online primary school that lets students mix and match their own curriculum based on their interests and goals Startup Name: Prime Age Startup Idea: {idea}\"\"\" # Call the Cohere Chat endpoint response = co.chat( model=\\'command-r\\', message=prompt, preamble=\"\") return response.text.replace(\"Startup Name: \", \"\") \\nDoing a quick test, entering the public transportation startup idea we got earlier returns the following startup name - “Beepro”. Not bad.\\nStep 2: Build the front-end with Streamlit\\nBuild the basic components\\nNow that the text generation part is working, let’s create the front end with Streamlit. Streamlit is an open-source Python library that makes it easy to create and share custom web apps. Instead of hours or even days, you can get a basic front end up and running and deploy it in minutes.\\nThe following is our front-end code using Streamlit, which gets us a basic working app in just a few lines.\\nst.title(\"🚀 Startup Idea Generator\") form = st.form(key=\"user_settings\") with form: industry_input = st.text_input(\"Industry\", key = \"industry_input\") generate_button = form.form_submit_button(\"Generate Idea\") if generate_button: startup_idea = generate_idea(industry_input) startup_name = generate_name(startup_idea) st.markdown(\"##### \" + startup_name) st.write(startup_idea)'),\n",
       " Document(metadata={'title': 'Deploying with Streamlit', 'source': 'https://cohere.com/llmu/deploy-streamlit'}, page_content='It uses a number of features from the Streamlit API, as follows:\\nA form via the st.form() control flow\\nst.title() for the header title\\nst.text_input() for the user text input\\nA submit button that comes with st.form()\\nst.markdown() and st.write() to display the generated text.\\nWe can deploy this Streamlit app by running the following command at the terminal (in this example, our code above is stored in app.py).\\nRunning the app locally renders the following.\\nA basic version of the app\\nAdd some interactivity\\nNow that we have a basic version of the app running, let’s see how we can enhance it. One way is to give users greater control when generating ideas. Let’s add a couple of options to add interactivity to the app.\\nThe first one is straightforward. We want to let users generate more than one idea in one go. For this, we’ll use the st.slider() widget to let users input the number of generations to make. We’ll set the maximum number of generations to 10.\\nThe second is a bit more interesting. With the Chat endpoint, we can use the temperature parameter to control the randomness of the model output. The value can range between 0 and 1. Lower values tend to generate more “predictable” output, while higher values tend to generate more “creative” output. The sweet spot is typically between 0 and 1, and for our app, we’ll just add a small buffer and set the range to be between 0.1 and 0.9. We’ll name this user setting Creativity.\\nFor this, we’ll also use st.slider() to let the users control the temperature value. We’ll also need to modify the generate_idea() and generate_name() functions to accept the temperature argument, to be passed to the Chat API call. The following is an example with generate_idea().\\ndef generate_idea(industry, temperature): ... response = co.chat( model=\\'command-r\\', message=prompt, temperature=temperature, preamble=\"\") ...'),\n",
       " Document(metadata={'title': 'Deploying with Streamlit', 'source': 'https://cohere.com/llmu/deploy-streamlit'}, page_content='def generate_idea(industry, temperature): ... response = co.chat( model=\\'command-r\\', message=prompt, temperature=temperature, preamble=\"\") ... \\nLet’s also add a couple more things. First, we’ll use st.progress() to show a progress bar as the idea generation takes place. Second, we’ll add a check to the Industry user input to make it a required field.\\nAltogether, the completed front-end Streamlit code looks like the following, about 20 lines.\\nst.title(\"🚀 Startup Idea Generator\") form = st.form(key=\"user_settings\") with form: # User input - Industry name industry_input = st.text_input(\"Industry\", key = \"industry_input\") # Create a two-column view col1, col2 = st.columns(2) with col1: # User input - The number of ideas to generate num_input = st.slider(\"Number of ideas\", value = 3, key = \"num_input\", min_value=1, max_value=10) with col2: # User input - The \\'temperature\\' value representing the level of creativity creativity_input = st.slider(\"Creativity\", value = 0.5, key = \"creativity_input\", min_value=0.1, max_value=0.9) # Submit button to start generating ideas generate_button = form.form_submit_button(\"Generate Idea\") if generate_button: if industry_input == \"\": st.error(\"Industry field cannot be blank\") else: my_bar = st.progress(0.05) st.subheader(\"Startup Ideas\") for i in range(num_input): st.markdown(\"\"\"---\"\"\") startup_idea = generate_idea(industry_input,creativity_input) startup_name = generate_name(startup_idea,creativity_input) st.markdown(\"##### \" + startup_name) st.write(startup_idea) my_bar.progress((i+1)/num_input) \\nAnd with that, the final front-end now looks like the following.\\nThe final version of the app\\nStep 3: Deploy with Streamlit Cloud\\nNow that our app is ready, let’s deploy it to the web so everyone can start interacting with it. You can deploy the app via your own hosting option, but there’s also a quick and hassle-free way to do it — via Streamlit Cloud.'),\n",
       " Document(metadata={'title': 'Deploying with Streamlit', 'source': 'https://cohere.com/llmu/deploy-streamlit'}, page_content='Streamlit Cloud launches apps directly from a GitHub repository. So, before you can deploy your app, do the following steps.\\nPush your code to a GitHub repository\\nSign up for Streamlit Cloud\\nConnect your Streamlit Cloud to the GitHub repository\\nOnce you have completed the steps, on your Streamlit Cloud page, click on New app.\\nYou’ll be brought to the Deploy an app page, where you can select the repository, the branch and the main file path i.e. the code we wrote earlier. You will also need to add your Cohere API key in the Advanced settings… link (more information here).\\nAnd once that is done, click Deploy!. Wait a few minutes for Streamlit to deploy it, and the app is live. Now literally anyone on the web can access it!\\nLet’s now take the app for a spin and, say, we want to generate three startup ideas in the productivity industry.\\nLet’s first try with a low Creativity setting, of say 0.2, which should give us ideas that are more predictable and possibly more proven. The outcome? It generates ideas revolving around the usual suspects — to-do, time management, and scheduling.\\nNext, we try with a high Creativity setting, of say 0.8, which should result in ideas that are more creative but are probably bordering on being too ambitious. This time, the ideas are a lot more diverse — travel planning, diagram drawing, and personal finance.\\nHere are the screenshots of these generated ideas.\\nExample outputs generated with different Creativity settings, 0.2 (left) and 0.8 (right)\\nAnd that concludes our startup idea generator prototype, powered by Cohere and deployed with Streamlit.\\nConclusion\\nThis example demonstrated how using Cohere and Streamlit, you can quickly turn your language AI product idea into a basic prototype. It’s an amazing feeling to be able to deploy a web app without spending long, frustrating hours building a front-end.'),\n",
       " Document(metadata={'title': 'Deploying with FastAPI', 'source': 'https://cohere.com/llmu/deploy-fastapi'}, page_content='Deploying with FastAPIQualified\\nThe source code used for this example is available in our GitHub repository.\\nFastAPI is a web framework for building APIs with Python. It is designed to be fast, simple, and easy to use. It is also designed to be flexible for a wide variety of use cases.\\nA web framework is a set of tools and libraries that simplify the process of building web applications. It provides a high-level interface that allows developers to build web applications without having to worry about low-level details such as network protocols and data storage.\\nDeveloping with FastAPI has many benefits, but the major one is as advertised in its name – it is fast. It\\'s both fast in performance (on par with NodeJS and Go) and in development time (increasing the speed to develop features by about 200% to 300%). You can read more about these in its documentation.\\nIn this chapter, we\\'ll build a simple sentiment analysis API with FastAPI and Cohere\\'s Classify endpoint.\\nIt involves the following steps.\\nStep 1: Setup\\nStep 2: Create a sentiment classifier\\nStep 3: Create a FastAPI endpoint\\nStep 4: Call the endpoint\\nStep 1: Setup\\nFirst, we create a Python file. Let\\'s name it main.py.\\nNext, we import FastAPI and Cohere, as well as Pydantic, for structuring inputs to the API.\\npip install cohere fastapi \"uvicorn[standard]\" \\nfrom fastapi import FastAPI from pydantic import BaseModel, conlist import cohere from cohere import ClassifyExample # Setup the Cohere client co = cohere.Client(\"COHERE_API_KEY\") \\nUvicorn serves as the default server for FastAPI, providing an asynchronous interface, while Pydantic provides a way to define data schemas using plain Python classes with type hints.\\nStep 2: Create a Sentiment Classifier\\nTo create a sentiment classifier, we need to provide the Classify endpoint with a list of labeled examples of the texts to classify and the expected classes.'),\n",
       " Document(metadata={'title': 'Deploying with FastAPI', 'source': 'https://cohere.com/llmu/deploy-fastapi'}, page_content='Step 2: Create a Sentiment Classifier\\nTo create a sentiment classifier, we need to provide the Classify endpoint with a list of labeled examples of the texts to classify and the expected classes.\\nThe Classify endpoint requires a minimum of two examples per class, but the more examples, the better it performs. In the example below, we have three classes to classify text into – positive, negative, and neutral – and five examples per class, making it a total of fifteen examples.\\nWe feed these examples to the co.classify() method via the examples argument. We also provide the actual list of texts we want to classify via the input argument. The other thing we need to define is the type of model to be used via the model argument, which needs to be one of the embedding models. The code is shown below.\\nIf you need more information about the endpoint, visit the Classify endpoint documentation.'),\n",
       " Document(metadata={'title': 'Deploying with FastAPI', 'source': 'https://cohere.com/llmu/deploy-fastapi'}, page_content='If you need more information about the endpoint, visit the Classify endpoint documentation.\\nexamples=[ClassifyExample(text=\"The order came 5 days early\", label=\"positive\"), ClassifyExample(text=\"The item exceeded my expectations\", label=\"positive\"), ClassifyExample(text=\"I ordered more for my friends\", label=\"positive\"), ClassifyExample(text=\"I would buy this again\", label=\"positive\"), ClassifyExample(text=\"I would recommend this to others\", label=\"positive\"), ClassifyExample(text=\"The package was damaged\", label=\"negative\"), ClassifyExample(text=\"The order is 5 days late\", label=\"negative\"), ClassifyExample(text=\"The order was incorrect\", label=\"negative\"), ClassifyExample(text=\"I want to return my item\", label=\"negative\"), ClassifyExample(text=\"The item\\'s material feels low quality\", label=\"negative\"), ClassifyExample(text=\"The item was nothing special\", label=\"neutral\"), ClassifyExample(text=\"I would not buy this again but it wasn\\'t a waste of money\", label=\"neutral\"), ClassifyExample(text=\"The item was neither amazing or terrible\", label=\"neutral\"), ClassifyExample(text=\"The item was okay\", label=\"neutral\"), ClassifyExample(text=\"I have no emotions towards this item\", label=\"neutral\")] response = co.classify( inputs=product_reviews.reviews, examples=examples) \\nStep 3: Create a FastAPI Endpoint\\nNow let\\'s create a FastAPI wrapper around that code to extend the model as an API endpoint.\\nFirst, we create a ProductReviews class, which is a Pydantic model that defines the structure of the request body expected by the prediction endpoint (which we\\'ll create after this). Specifically, it specifies that the request body must contain a field named reviews, which is a list of strings (conlist(str, min_length=1)). Each string in the list represents a product review. The min_length=1 constraint ensures that the list contains at least one review.'),\n",
       " Document(metadata={'title': 'Deploying with FastAPI', 'source': 'https://cohere.com/llmu/deploy-fastapi'}, page_content='Next, we create an endpoint that we call predictiontogether with a function that callspredict_sentiment. This endpoint will receive the user’s inputted list of strings and invoke the function.\\nWe take the code to call the Classify endpoint from the previous step and put it inside the function.\\napp = FastAPI() class ProductReviews(BaseModel): reviews: conlist(str, min_length=1) @app.post(\"/prediction\") def predict_sentiment(product_reviews: ProductReviews): examples=[ClassifyExample(text=\"The order came 5 days early\", label=\"positive\"), ClassifyExample(text=\"The item exceeded my expectations\", label=\"positive\"), ClassifyExample(text=\"I ordered more for my friends\", label=\"positive\"), ClassifyExample(text=\"I would buy this again\", label=\"positive\"), ClassifyExample(text=\"I would recommend this to others\", label=\"positive\"), ClassifyExample(text=\"The package was damaged\", label=\"negative\"), ClassifyExample(text=\"The order is 5 days late\", label=\"negative\"), ClassifyExample(text=\"The order was incorrect\", label=\"negative\"), ClassifyExample(text=\"I want to return my item\", label=\"negative\"), ClassifyExample(text=\"The item\\'s material feels low quality\", label=\"negative\"), ClassifyExample(text=\"The item was nothing special\", label=\"neutral\"), ClassifyExample(text=\"I would not buy this again but it wasn\\'t a waste of money\", label=\"neutral\"), ClassifyExample(text=\"The item was neither amazing or terrible\", label=\"neutral\"), ClassifyExample(text=\"The item was okay\", label=\"neutral\"), ClassifyExample(text=\"I have no emotions towards this item\", label=\"neutral\")] response = co.classify( inputs=product_reviews.reviews, examples=examples) return response.classifications \\nStep 4: Call the Endpoint\\nWe can now test the endpoint locally. Switch your terminal working directory to the location of your saved Python file, then input the following shell command. This brings up a server on your localhost.\\nLet\\'s test with these two text inputs and get the predicted classes from the model.'),\n",
       " Document(metadata={'title': 'Deploying with FastAPI', 'source': 'https://cohere.com/llmu/deploy-fastapi'}, page_content='Let\\'s test with these two text inputs and get the predicted classes from the model.\\nThe product was faulty Customer support was very responsive \\nThere are a couple of options to call the endpoint. One way is to run a cURL command on your terminal.\\ncurl -X \\'POST\\' \\\\ \\'http://127.0.0.1:8000/prediction\\' \\\\ -H \\'accept: application/json\\' \\\\ -H \\'Content-Type: application/json\\' \\\\ -d \\'{ \"reviews\": [ \"The product was faulty\", \"Customer support was very responsive\" ] }\\' \\nAnother way is to use FastAPI\\'s built-in documentation feature. It offers easy documentation based on OpenAPI standards showcasing API endpoints, sample values, responses, and their types.\\nFor this, go to the URL provided when you run the shell command earlier to bring up the server and add docs as the path (For example, 127.0.0.1:8000/docs).\\nClick the Try it out button and add comma-separated text to the Request body section\\nClick Execute. A sample curl command of your request is shown, along with the API call results. Here, the response contains the predicted class for each input text. It also provides its confidence scores against each possible class, with the predicted class having the highest confidence.\\nAs an example, in the screenshot below, we can see that The product was faulty is classified as having a negative sentiment with a 0.985 confidence score, while Customer support was very responsiveis classified as having a positive sentiment with a 0.972 confidence score.\\nIn this chapter, we built a simple sentiment analysis API with FastAPI and Cohere\\'s Classify endpoint.'),\n",
       " Document(metadata={'title': 'Deploying on Google Sheets with Google Apps Script', 'source': 'https://cohere.com/llmu/deploy-google-sheets'}, page_content=\"Deploying on Google Sheets with Google Apps ScriptQualified\\nThe source code used for this example is available in our GitHub repository.\\nThis chapter demonstrates how to use Cohere and Google Apps Script, a scripting language based on JavaScript, to build two custom functions that add large language model (LLM) capabilities to Google Sheets documents. We'll be classifying the sentiment and generating summaries of movie reviews.\\nIt involves the following steps.\\nStep 1: Add training examples required for text classification.\\nStep 2: Create a custom Apps Script function for text classification.\\nStep 3: Create a custom Apps Script function for text summarization.\\nStep 4: Run text classification and summarization on a new input.\\nSetup\\nWe’ve already created a file to run the demo of this project. Open the file in Google Sheets and follow the instructions described in the Instructions worksheet.\\nOnce you have completed the steps, you will be able to run the demo directly using the script provided in the repository. Let's understand how it is created.\\nStep 1: Add Training Examples Required for Text Classification\\nThe first step is to create a set of examples needed by the Classify endpoint to be able to perform a classification task.\\nThe Classify endpoint requires a minimum of two training examples per class, though more training examples are generally better. You can find the examples we'll use for running movie reviews sentiment analysis in the Classifier Examples worksheet. It contains five examples for each class – positive and negative.\\nStep 2: Create a Custom Apps Script Function for Text Classification\\nNow we are going to create the script for the first task: sentiment analysis. In Google Sheets, go to Extensions and then go to Apps Script, which will take us to the Apps Script environment.\"),\n",
       " Document(metadata={'title': 'Deploying on Google Sheets with Google Apps Script', 'source': 'https://cohere.com/llmu/deploy-google-sheets'}, page_content='Now we are going to create the script for the first task: sentiment analysis. In Google Sheets, go to Extensions and then go to Apps Script, which will take us to the Apps Script environment.\\nNext, we’ll use the Classify endpoint to build our text classification function, starting with the code below. It is a basic function definition taking in two parameters. The first one (s_args) is the range of cells for the training examples we defined in the previous section. The second one (s_input) is the new text we want to label or classify.\\nfunction classify(s_args, s_input) { } \\nLet’s now add the logic between the curly braces. First, we extract the training examples from s_args and package them into something that the Classify endpoint can ingest.\\nNext, we call the Classify endpoint to classify an input text, given the package of training examples. We pass the data package (such as authentication headers and other metadata) to the endpoint. Then, we store the response in a named response object.\\nNext, we parse the response and extract the classification and confidence level.\\nFinally, we output the parsed response back into the cell where we call the function.'),\n",
       " Document(metadata={'title': 'Deploying on Google Sheets with Google Apps Script', 'source': 'https://cohere.com/llmu/deploy-google-sheets'}, page_content='Next, we parse the response and extract the classification and confidence level.\\nFinally, we output the parsed response back into the cell where we call the function.\\nfunction classify(s_args, s_input) { // we start by extracting the training data from the specified range in s_args and packaging it into a prompt let inputs = [s_input]; let examples = []; for (let i = 0; i< s_args.length; i++) { //this is assuming label is always the 2nd parameter examples.push({\"text\" : s_args[i][0] , \"label\" : s_args[i][1]}) } const payload_for_classification = { examples, inputs, \"model\" : \"embed-english-v2.0\", }; const options = { \\'headers\\' : { \\'Authorization\\' : \\'Bearer \\' + TOKEN }, \\'method\\' : \\'post\\', \\'contentType\\': \\'application/json\\', \\'payload\\' : JSON.stringify(payload_for_classification) }; const response = UrlFetchApp.fetch(\\'https://api.cohere.ai/v1/classify\\', options); const responseContent = response.getContentText(); const responseContentText = JSON.parse(response.getContentText()); const classificationContent = responseContentText.classifications[0]; const ourPrediction = classificationContent.prediction; const confidence = (classificationContent.confidence * 100).toFixed(2) + \"%\"; let displayString = \"Prediction: \" + ourPrediction.toUpperCase() + \" review\\\\n\" + \"Confidence Level: \" + confidence + \"\\\\n\"; return displayString; } \\nStep 3: Create a Custom Apps Script Function for Text Summarization\\nThe methodology for text summarization is similar to the one used for text classification in the previous section, except we’ll be using the Chat endpoint instead.\\nWe create a simple prompt asking the endpoint to provide a summary in about ten words. We are defining three parameters for the endpoint call.\\nmessage - the prompt for the LLM\\ntemperature - we keep the value closer to 0 to make the output more predictable (0.2 in this example)\\nmodel- we\\'ll use command-r\\nFinally, we parse the response and output the summary.'),\n",
       " Document(metadata={'title': 'Deploying on Google Sheets with Google Apps Script', 'source': 'https://cohere.com/llmu/deploy-google-sheets'}, page_content='temperature - we keep the value closer to 0 to make the output more predictable (0.2 in this example)\\nmodel- we\\'ll use command-r\\nFinally, we parse the response and output the summary.\\nfunction summarize(cell_value) { let raw = JSON.stringify({ \"message\": \"Summarize this in about 10 words: \" + cell_value, \"temperature\": 0.2, \"model\": \"command-r\", }); let requestOptions = { \\'method\\': \\'post\\', \\'muteHttpExceptions\\': true, \\'contentType\\': \\'application/json\\', \\'headers\\': { \\'Authorization\\': \\'Bearer \\' + TOKEN }, \\'payload\\': raw, redirect: \\'follow\\' }; let response = UrlFetchApp.fetch(\"https://api.cohere.ai/v1/chat\", requestOptions) let responseContentTxt = JSON.parse(response.getContentText()); let summarizedTxt = \"SUMMARY: \" + responseContentTxt.text + \"\\\\n\"; return summarizedTxt; } \\nFinally, we save the code we have added for text classification and summarization.\\nStep 4: Run Text Classification and Summarization on a New Input\\nNow, we get to the part we’ve been waiting for – classifying and summarizing text on Google Sheets.\\nFor this, we’ll move back to the Google Sheets view and go to the Demo worksheet. It has three columns, as shown below.\\nThe first column is the input text, the movie review that we want to classify its sentiment and summarize. Here, we have a review of The Batman.\\nIn the second column, we call our text classification function as follows.\\n=classify(\\'Classifier Examples\\'!$A$3:$B$12,A3)\\nRecall that this function takes in two parameters. The first one is the list of training examples. Our training examples are provided in the Classifier Examples tab between cells A3 and B12.\\nThe second parameter is A3, the review of The Batman. When we hit enter, it predicts the class, as shown in the screenshot above.\\nThe third column calls the summarize function.\\n=summarize(A3)\\nThis takes cell the movie review in A3 and outputs the summary.\\nConclusion'),\n",
       " Document(metadata={'title': 'Deploying on Google Sheets with Google Apps Script', 'source': 'https://cohere.com/llmu/deploy-google-sheets'}, page_content='The third column calls the summarize function.\\n=summarize(A3)\\nThis takes cell the movie review in A3 and outputs the summary.\\nConclusion\\nDealing with numbers in a spreadsheet has always been easy, but the same couldn’t be said of text — until now. With Cohere’s natural language processing capabilities, you can supercharge your spreadsheets, saving time and effort.'),\n",
       " Document(metadata={'title': 'Deploying as a Chrome Extension', 'source': 'https://cohere.com/llmu/deploy-chrome-extension'}, page_content='Deploying as a Chrome ExtensionQualified\\nThis chapter is contributed by Leila Chan Currie. The source code used for this example is available in our GitHub repository.\\nThis tutorial demonstrates how to create Condense, a Google Chrome extension that summarizes the text content of a web page.\\nIt involves the following steps:\\nStep 1: Create a function for text display\\nStep 2: Create a function for text summarization\\nStep 3: Create a function for text capturing\\nStep 4: Create a function for Cohere API key entry\\nStep 5: Create a Chrome extension manifest file\\nStep 6: Load the extension to Google Chrome\\nStep 7: Run the extension on Google Chrome\\nThe source code used for this example is available in our GitHub repository. This tutorial uses Javascript.\\nStep 1: Create a Function for Text Display\\nThe first step is to create a folder for this extension, which we’ll call condense. Then create another folder called scripts and inside it, a file called content.js. This file will contain the main logic of this extension.\\nFirst, we’ll create a function for text display, which will display the generated summary of a web page at the top of the page. This function, display(text), creates the elements and styling for the header and the summary. Finally, it inserts the header immediately before the HTML body.\\n//Display the text at the top of the page function display(text) { //Create a purple header header = document.createElement(\"div\"); header.style.backgroundColor = \"#d18ee2\"; header.style.padding = \"5px\"; //Write the text with a bit of styling and add it to the header tldr = document.createElement(\"p\"); tldr.textContent = text; tldr.style.margin = \"10px 100px\"; tldr.style.fontSize = \"medium\"; tldr.style.color = \"white\"; tldr.style.textAlign = \"center\"; tldr.style.fontFamily = \"Verdana, Geneva, sans-serif\"; header.appendChild(tldr); //Insert the header immediately before the HTML body document.body.parentNode.insertBefore(header, document.body); }'),\n",
       " Document(metadata={'title': 'Deploying as a Chrome Extension', 'source': 'https://cohere.com/llmu/deploy-chrome-extension'}, page_content='Step 2: Create a Function for Text Summarization\\nNext, we’ll create a function, summarize(text), to perform the text summarization, leveraging Cohere’s Chat endpoint. This function calls the Chat endpoint by defining several parameters, such as message (used to direct the model to summarize the contents of the webpage), preamble (set to an empty string to make the model give a brief response), and temperature (we set it to a low value to make the outputs less random). Refer to the API reference to learn more about the parameters of the Chat endpoint.\\nThis function then takes the endpoint response, response.text, and sends it to the display(text) function we created in the previous section.\\n//Fetch the summary for the given text and display it function summarize(text) { // Use the stored API of user chrome.storage.sync.get(\\'apiKey\\', key => { // Set up the request to send to the endpoint options = { \"method\": \"POST\", \"headers\": { \"accept\": \"application/json\", \"content-type\": \"application/json\", \"authorization\": \"Bearer \" + key.apiKey }, /* These are the chat endpt paramters. Try playing around with them and reloading the extension to see how they affect the summarization behaviour. Reference: https://docs.cohere.com/reference/chat */ \"body\": JSON.stringify({ \"message\": \"Generate a summary of this webpage: \" + text, \"preamble\": \"\", \"temperature\": 0.1 }) }; fetch(\\'https://api.cohere.ai/v1/chat\\', options) .then((response) => response.json()) .then((response) => { if (response.text === undefined) { /* If there is no summary in the endpoint response, display whatever error message it returned */ display(\"There was an error: \" + response.message); } else { // Otherwise, display the summary display(\"tl;dr: \" + response.text); } }); }); } \\nStep 3: Create a Function for Text Capturing'),\n",
       " Document(metadata={'title': 'Deploying as a Chrome Extension', 'source': 'https://cohere.com/llmu/deploy-chrome-extension'}, page_content=\"Step 3: Create a Function for Text Capturing\\nNow we create a function that feeds the text needed by the summarize(text) function. Here, getVisibleText() returns only the visible text from the page. It uses jQuery selectors (requires jQuery v3.7.0 to be added to the scripts folder) to try to find the page's main body of content, often in a content or main element. It also defines a fallback to using the whole body content.\\n// Returns true if the given element is not visible on the page function isHidden(el) { var style = window.getComputedStyle(el); return ((style.display === 'none') || (style.visibility === 'hidden')) } // Returns only the visible text from the page function getVisibleText() { /* Using jQuery selectors, try to find the main body of content of the page, often in a content or main element. Fall back to using the whole body which is ~universal. */ var body = document.querySelector('body') if (document.querySelector('#content')) { body = document.querySelector('#content'); } if (document.main) { body = document.querySelector('main'); } var allTags = body.getElementsByTagName('*'); let visibleText = []; var nChars = 0; // Select all visible text in the body, up to charLimit for (var i = 0, max = allTags.length; i < max; i++) { var elem = allTags[i]; if (!isHidden(elem)) { var text = $(elem).contents().filter(function() { return this.nodeType == Node.TEXT_NODE; }); if (text === undefined || text.length == 0) { continue; } text = text[0].nodeValue nChars += text.length + 1; // for newline if (nChars < charLimit) { visibleText.push(text); } else { break } } } // Separate all the text elements with a newline return visibleText.join('\\\\n'); } \\nThe following code block stitches together the three steps we created to capture visible text from a web page, summarize it, and display it at the top.\"),\n",
       " Document(metadata={'title': 'Deploying as a Chrome Extension', 'source': 'https://cohere.com/llmu/deploy-chrome-extension'}, page_content='The following code block stitches together the three steps we created to capture visible text from a web page, summarize it, and display it at the top.\\n// This code block runs when pages are loaded. chrome.storage.sync.get(\\'apiKey\\', key => { if (key.apiKey === undefined) { // If there is no saved API key, tell the user how to add one display(\"Please set an API key in co:ndense > Options\"); } else { // If there is a key, we can use it to summarize the page const truncatedVisibleText = getVisibleText(); /* During the dev process, it is helpful to be able to see exactly what text is being summarized */ console.log(truncatedVisibleText); summarize(truncatedVisibleText); } }); \\nStep 4: Create a Function for Cohere API Key Entry\\nWe also need to create a user interface for users to add a Cohere API key before they can run the extension. For this, we create a new folder called options and create two files, options.html and options.js. For brevity, the code blocks are not included here, but you can refer to the repository for the full code.\\nStep 5: Create a Chrome Extension Manifest File\\nThe manifest file is required by a Chrome extension, describing how the extension is configured. It is a JSON file placed in the extension\\'s root directory. It also contains metadata about the extension, such as its name, version, and author.\\nSo let’s create a manifest.json file at the root of our folder. The contents of the file are shown below. It contains the metadata, permissions, description of the UI for adding the API key, and the scripts to run.'),\n",
       " Document(metadata={'title': 'Deploying as a Chrome Extension', 'source': 'https://cohere.com/llmu/deploy-chrome-extension'}, page_content='// This file describes how the extension is configured. { // Metadata \"name\": \"co:ndense\", \"version\": \"1.0\", \"description\": \"Summarizes web pages\", \"manifest_version\": 3, // - activeTab gives us access to the currently active tab of user // - scripting gives us permission to run our code // - storage allows us to store the user API key \"permissions\": [\"activeTab\", \"scripting\", \"storage\"], // This describes the UI for user options \"options_ui\": { // This page contains the UI \"page\": \"options/options.html\", // The options will open as a pop-up, not a new tab \"open_in_tab\": false }, \"content_scripts\": [{ // scripts/content.js contains the main logic of extension. // It relies on jQuery so we need to configure the path to that too. \"js\": [\"scripts/content.js\", \"scripts/jquery-3.7.0.min.js\"], // This extension will run on all webpages by default. \"matches\": [ \"<all_urls>\" ], // Do not run the extension on these specific sites \"exclude_globs\": [ // The header breaks cursor alignment in Google Docs \"*://docs.google.com/*\" ] }] } \\nStep 6: Load the Extension to Google Chrome\\nThe code package is now complete, and we can load the extension to Chrome. For this, take the following steps:\\nGo to chrome://extensions/.\\nAt the top right, turn on Developer mode.\\nClick Load unpacked at the top left.\\nFind and select the folder where the code package is located. The extension will now appear in the list of extensions. Make sure it is enabled using the button on the bottom right.\\nSelect Details on the extension. Click Extension Options and then add an API key\\nStep 7: Run the Extension on Google Chrome\\nWe are now ready to run the extension. Open a new tab in Chrome and then click the Extension icon at the top right (look for the jigsaw piece icon).\\nThen go to any web page. After a short while, a summary of the page will be displayed at the top! (look for the purple banner)\\nConclusion'),\n",
       " Document(metadata={'title': 'Deploying as a Chrome Extension', 'source': 'https://cohere.com/llmu/deploy-chrome-extension'}, page_content='Then go to any web page. After a short while, a summary of the page will be displayed at the top! (look for the purple banner)\\nConclusion\\nThe Chrome extension provides developers with a platform to build applications that enrich the web browsing experience. And the ease of access to large language models like Cohere’s unlocks new types of applications that enable more efficient and natural language-based interactions with the browser.'),\n",
       " Document(metadata={'title': 'Deploying with Databutton', 'source': 'https://cohere.com/llmu/deploy-databutton'}, page_content='Deploying with DatabuttonQualified\\nThis chapter is contributed by Cohere’s Community Champion, Elle Neal.\\nDatabutton is an all-in-one online workspace designed to streamline the process of creating, deploying, and managing data apps. It supports Streamlit-based applications, but it also comes with additional features to make the process of developing and deploying applications simple. For example, Pages allows you to create multipage UIs for your users, Jobs enables scheduling of Python code, Libraries provides a place to write reusable code across your app, and Data Storage offers a simple put/get data store for various types of data.\\nThis tutorial demonstrates creating a topic modeling application using Databutton.\\nIt involves the following steps:\\nStep 1: Load and embed data\\nStep 2: Create a user interface with Streamlit code\\nStep 3: Cluster data to identify groups\\nStep 4: Auto-generate cluster label names\\nStep 5: Deploy and share application\\nSetup\\nCreate a free account with Databutton.\\nCreate a new app by clicking on ‘New Blank App`on the home screen.\\nSet the app’s configurations in theConfiguresection.\\nInSecrets, add the Cohere API key.\\nIn Installed packages, add the following packages: cohere, scikit-learn, hdbscan, umap-learn, setuptools, plotly, matplotlib, and datasets.\\nStep 1: Load and Embed Data\\nAs a default, this app will use Amazon’s MASSIVE dataset, which contains a list of commands that people give to their AI-based personal assistant (e.g., Alexa).\\nWe’ll build a few helper functions:\\nTo generate embeddings via the Embed endpoint\\nTo reduce the dimensions of these embeddings to two for plotting purposes\\nTo save the embeddings into a JSON file in Databutton’s data storage\\nThe user needs to generate the embeddings of the dataset only once. This provides two outputs that are saved to Databutton’s data storage: the embeddings as a JSON file and the embeddings’ reduced dimensions that are appended to the dataset’s DataFrame.'),\n",
       " Document(metadata={'title': 'Deploying with Databutton', 'source': 'https://cohere.com/llmu/deploy-databutton'}, page_content='As the file is now saved to storage, the user can access the data throughout the application without having to perform this task again. There is no need to worry about caching the dataset for the user; with one line of code, you can return the data to the user anywhere within the application.\\nThe following are the helper functions mentioned above.\\n# Function to generate Cohere embeddings def embed_text(texts): embeddings = \\\\[] for i in range(0, len(texts), 90): batch = texts[i:i+90] output = co.embed( model=\"embed-english-v3.0\", input_type=\"search_document\", texts=batch) embeddings.extend(output.embeddings) time.sleep(60) # accounting for Cohere trial API key rate limit return embeddings \\n# Function to reduce dimensionality of embeddings using umap def reduce_dimensionality(embeddings): reducer = umap.UMAP() umap_embeddings = reducer.fit_transform(embeddings) return umap_embeddings[:, 0], umap_embeddings[:, 1] \\n# Function to save embeddings into a json file in Databutton data storage def save_embeddings_to_json(df): # Create a dictionary where each key is the index of the DataFrame and each value is the corresponding embedding embeddings_dict = df[\\'embedding\\'].to_dict() \\nStep 2: Create a User Interface with Streamlit\\nWe’ll build the user interface using the Streamlit API, which goes through the following steps when a user interacts with the application.\\nUser uploads a CSV file. Or, they can opt for the sample dataset (Amazon’s MASSIVE dataset).\\nText embeddings are generated using the Embed endpoint, and the embeddings will be reduced to two dimensions using UMAP (Uniform Manifold Approximation and Projection).\\nThe embeddings will be saved to Databutton storage as a JSON file.\\nThe reduced embeddings will be saved to Databutton’s data storage as a new CSV file.\\nThe following is what the user interface will look like at this point.'),\n",
       " Document(metadata={'title': 'Deploying with Databutton', 'source': 'https://cohere.com/llmu/deploy-databutton'}, page_content='The reduced embeddings will be saved to Databutton’s data storage as a new CSV file.\\nThe following is what the user interface will look like at this point.\\nThe reduced embeddings are now saved as a new file. We can copy the code snippet to import and use the DataFrame anywhere in our application. Here is an example of how you call the data as a DataFrame:\\n# Call the embeddings data as a DataFrame df = db.storage.dataframes.get(key=”reduced.csv”) \\nStep 3: Cluster Data to Identify Groups\\nThis next stage in the workflow uses machine learning to perform text clustering — finding clusters in the data that share similar properties.\\nWe will create a user interface that provides interactivity at a few crucial points along the process of text clustering. This allows users to make decisions based on the results of initial analyses. Here are the step-by-step instructions:\\nLoad the DataFrame: The script will load the DataFrame previously saved in Databutton’s storage.\\nExtract UMAP coordinates: The script will extract the 2D UMAP coordinates that were previously computed. These coordinates are reduced representations of your data.\\nDetermine the optimal number of clusters: The script will compute the sum of squared errors (SSE) for a range of potential numbers of clusters. This is part of the process to use the k-means algorithm, which requires specifying the number of clusters beforehand. The script will plot an elbow plot, which can be used to select the optimal number of clusters — look for the “elbow” in the curve where adding more clusters doesn’t significantly decrease SSE.\\nCreate the elbow plot: The script will then visualize the SSE for different numbers of clusters as an elbow plot. This visualization will help you in choosing the optimal number of clusters.\\nUser selection of clusters: You will select the number of clusters based on the elbow plot. The selection is done using a slider in the Streamlit app.'),\n",
       " Document(metadata={'title': 'Deploying with Databutton', 'source': 'https://cohere.com/llmu/deploy-databutton'}, page_content='User selection of clusters: You will select the number of clusters based on the elbow plot. The selection is done using a slider in the Streamlit app.\\nRun k-means algorithm: With the chosen number of clusters, the script will run the k-means algorithm, which assigns each point in your data to one of the clusters.\\nPlot the clusters: The script will create a scatter plot of your data points, colored by their assigned cluster. This visualization gives you a spatial representation of how the algorithm has classified your data.\\nCalculate cluster centers and distances from centroids: The script calculates the centroid (or geometric center) of each cluster. It then computes how far each point in your data is from its cluster’s centroid. We will need this in the next step of the process where we label our clusters.\\nDisplay and save the results: The script will display the DataFrame that now includes cluster labels and distances from centroids. If you’re satisfied with the results, you can save this labeled data back into Databutton’s storage for later use.'),\n",
       " Document(metadata={'title': 'Deploying with Databutton', 'source': 'https://cohere.com/llmu/deploy-databutton'}, page_content='import databutton as db import streamlit as st from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score import matplotlib.pyplot as plt import numpy as np import plotly.express as px from scipy.spatial import distance # 1: Load DataFrame df = db.storage.dataframes.get(key=\"reduced.csv\") # 2: Extract UMAP coordinates umapx = df[\\'umap_x\\'] umapy = df[\\'umap_y\\'] umap_coords = np.column_stack((umapx, umapy)) # 3: Define a range of potential clusters and compute SSE clusters = range(2, 10) # You may want to modify this range sse = \\\\[] for k in clusters: kmeans = KMeans(n_clusters=k, random_state=0) kmeans.fit(umap_coords) sse.append(kmeans.inertia_) # 4: Plot the elbow plot fig, ax = plt.subplots(figsize=(10, 5)) plt.plot(clusters, sse, \\'bx-\\') plt.xlabel(\\'k (number of clusters)\\') plt.ylabel(\\'Sum of Squared Errors (SSE)\\') plt.title(\\'Elbow Plot For Optimal Number of Clusters\\') st.pyplot(fig) # 5: User selects number of clusters based on elbow plot n_clusters = st.slider(\\'Number of Clusters\\', min_value=2, max_value=10, value=2) # 6: Run KMeans with optimal number of clusters kmeans_model = KMeans(n_clusters=n_clusters, random_state=0) df[\\'cluster_labels\\'] = kmeans_model.fit_predict(umap_coords) # 7: Plotting the clusters fig = px.scatter(df, x=\\'umap_x\\', y=\\'umap_y\\', color=\\'cluster_labels\\', hover_data=[\\'utt\\']) st.plotly_chart(fig) # 8: Calculate cluster centers and distances from centroids centroids = df.groupby(\\'cluster_labels\\')\\\\[[\\'umap_x\\', \\'umap_y\\']].mean().reset_index() def calc_distance(row): centroid = centroids\\\\[centroids[\\'cluster_labels\\'] == row[\\'cluster_labels\\']] centroid_coords = (centroid[\\'umap_x\\'].values[0], centroid[\\'umap_y\\'].values[0]) row_coords = (row[\\'umap_x\\'], row[\\'umap_y\\']) return distance.euclidean(row_coords, centroid_coords) df[\\'distance_from_centroid\\'] = df.apply(calc_distance, axis=1) # 9: Display and save the results selected_cluster = st.selectbox(\\'Select a cluster label\\', df[\\'cluster_labels\\'].unique()) temp_df = df\\\\[[\\'utt\\','),\n",
       " Document(metadata={'title': 'Deploying with Databutton', 'source': 'https://cohere.com/llmu/deploy-databutton'}, page_content='= df.apply(calc_distance, axis=1) # 9: Display and save the results selected_cluster = st.selectbox(\\'Select a cluster label\\', df[\\'cluster_labels\\'].unique()) temp_df = df\\\\[[\\'utt\\', \\'cluster_labels\\', \\'distance_from_centroid\\']] st.write(temp_df\\\\[temp_df[\\'cluster_labels\\'] == selected_cluster]) if st.button(\"Save Labelled Data\"): db.storage.dataframes.put(key=\"cluster.csv\", df=df)'),\n",
       " Document(metadata={'title': 'Deploying with Databutton', 'source': 'https://cohere.com/llmu/deploy-databutton'}, page_content='Step 4: Auto-Generate Cluster Names\\nFinally we’ll use the Chat endpoint to suggest a name for each cluster based on examples sampled from the clustered data. We’ll also generate the keywords for each cluster using the TF-IDF algorithm.\\nWe use the utterance_prompt as the prompt to the Chat endpoint to generate descriptive labels for the data clusters.\\nutterance_prompt = \"\"\" These are clusters of commands given to an AI-based personal assistant. Each cluster represents a specific type of task or query that users often ask their personal assistant to perform. A list of keywords summarizing the collection is included, along with the name of the cluster. The name of each cluster should be a brief, precise description of the common theme within the utterances. --- Cluster #0 Sample utterances from this cluster: - status for the pizza delivery from pizza hut - find and order rasgulla of janta sweet home pvt ltd - i will be at pizza hut in ten minutes and will stay there for next forty minutes arrange an uber for me that can drop me home Keywords for utterances in this cluster: pizza, delivery, uber, order Cluster name: Food Delivery --- Cluster #1 Sample utterances from this cluster: - show me where i can find a train - can you show me the directions to go museum of flight in seattle - please book train ticket to new york Keywords for utterances in this cluster: train, directions, museum, book, ticket Cluster name: Travel and Directions --- Cluster #2 Sample utterances from this cluster: - get route for los angles from here - nearest restaurants available at this time - i want you to book a train ticket for me Keywords for utterances in this cluster: route, los angeles, restaurants, time, book, train, ticket Cluster name: Route Navigation and Reservations --- Cluster #3 Sample utterances from this cluster: \"\"\"'),\n",
       " Document(metadata={'title': 'Deploying with Databutton', 'source': 'https://cohere.com/llmu/deploy-databutton'}, page_content='We create several helper functions for processing the DataFrame, generating keywords, creating labels, and displaying information to the user. The extract_top_n_words function generates the most relevant keywords for each cluster. The generate_label function uses an AI model to generate a descriptive label for each cluster. The generate_keywords_and_label function wraps up these processes for each cluster and updates the DataFrame accordingly. The present_cluster_data function is used to present the information about each cluster to the user.\\n# Function to generate a name for each cluster def generate_label(customer_service_prompt, text_series): # Initialize Cohere model COHERE_API_KEY = db.secrets.get(name=\"COHERE_API_KEY\") co = cohere.Client(COHERE_API_KEY) text_list = text_series.tolist() formatted_text_list = \"\" for text in text_list: formatted_text_list += \"- \" + text + \"\\\\\\\\n\" prompt=customer_service_prompt + formatted_text_list response = co.chat( model=\\'command-r\\', message=prompt, max_tokens=800, temperature=0.2, k=0) return response.text, prompt \\n# Function to generate keywords for each cluster def extract_top_n_words(vectorizer, tfidf_matrix, n=10): \"\"\" Given a TfidfVectorizer and a TF-IDF matrix, return the `n` words with the highest TF-IDF scores. \"\"\" # Get the names of the words #feature_names = vectorizer.get_feature_names_out() # Sum tfidf frequency of each term through documents summed_tfidf = np.sum(tfidf_matrix, axis=0) # Connecting term to its sums frequency words_freq = [(word, summed_tfidf[0, idx]) for word, idx in vectorizer.vocabulary_.items()] words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True) # Return the n words with highest tfidf return [word[0] for word in words_freq[:n]]'),\n",
       " Document(metadata={'title': 'Deploying with Databutton', 'source': 'https://cohere.com/llmu/deploy-databutton'}, page_content='# Helper function to generate the cluster name and keywords @st.cache_resource def generate_keywords_and_label(df, cluster, utterance_prompt): # Filter the DataFrame for each cluster df_cluster = df\\\\[df[\\'cluster_labels\\'] == cluster] # Generate the TF-IDF matrix vectorizer = TfidfVectorizer(stop_words=\\'english\\') tfidf_matrix = vectorizer.fit_transform(df_cluster[\\'utt\\']) # Extract the top N keywords from each cluster keywords = extract_top_n_words(vectorizer, tfidf_matrix, n=10) # Generate a summary label using the AI model prompt = utterance_prompt + \"\\\\nKeywords for messages in this cluster: \" + \\', \\'.join(keywords) + \"\\\\n\" summary, prompt = generate_label(prompt, df_cluster[\\'utt\\'].sample(n=5)) # Extract cluster name from AI generated label start = summary.find(\"Cluster name:\") + len(\"Cluster name:\") end = summary.find(\"\\\\n\", start) cluster_name = summary[start:end].strip() # Update original DataFrame with generated label and keywords df.loc[df[\\'cluster_labels\\'] == cluster, \\'label\\'] = cluster_name df.loc[df[\\'cluster_labels\\'] == cluster, \\'keywords\\'] = \\', \\'.join(keywords) return df, keywords, cluster_name # Helper function to present cluster data def present_cluster_data(df, cluster, keywords, label): df_cluster = df\\\\[df[\\'cluster_labels\\'] == cluster] st.markdown(f\"**Cluster {cluster}**\") st.markdown(f\"**Generated Keywords:** {\\', \\'.join(keywords)}\") st.markdown(f\"**AI Proposed Label:** {label}\") st.DataFrame(df_cluster[[\\'utt\\', \\'distance_from_centroid\\']]) \\nAnd finally, putting the steps together for cluster names and keyword generation, as well as adding the user interaction steps, we have the following:\\nData loading: The DataFrame is loaded from a Databutton storage with a key of \"cluster.csv\".'),\n",
       " Document(metadata={'title': 'Deploying with Databutton', 'source': 'https://cohere.com/llmu/deploy-databutton'}, page_content='Data loading: The DataFrame is loaded from a Databutton storage with a key of \"cluster.csv\".\\nCluster processing: For each unique cluster in the DataFrame, the generate_keywords_and_label function is called to generate relevant keywords and an AI-generated label. These are added to the DataFrame. Then, the present_cluster_data function is used to display this information to the user.\\nUser interactions: The user is given the option to rename the AI-generated label for each cluster. If the user enters a new label, the DataFrame is updated with this new label.\\nSaving changes: Finally, the user can click a button to save their changes to the DataFrame. When the “Save changes” button is clicked, the updated DataFrame is saved back to the Databutton storage with a new key of \"labeled_cluster.csv\".\\nThese are reflected in the corresponding code block.'),\n",
       " Document(metadata={'title': 'Deploying with Databutton', 'source': 'https://cohere.com/llmu/deploy-databutton'}, page_content='from sklearn.feature_extraction.text import TfidfVectorizer import databutton as db import streamlit as st import pandas as pd import cohere import numpy as np import time # helper functions... # Load Data df = db.storage.dataframes.get(key=\"cluster.csv\") # Initialize an empty dictionary to hold cluster labels cluster_labels = {} # Define the TF-IDF vectorizer vectorizer = TfidfVectorizer(stop_words=\"english\") clusters = df[\"cluster_labels\"].unique() if ( st.button(\"Generate AI Labels\", key=\"labelling\", type=\"primary\") or st.session_state.load_state ): st.session_state.load_state = True for cluster in clusters: df, keywords, label = generate_keywords_and_label(df, cluster, utterance_prompt) present_cluster_data(df, cluster, keywords, label) # Add user interaction to rename the label state_key = f\"user_label_{cluster}\" new_label = st.text_input( f\"Enter a new label for cluster {cluster} (leave empty to keep the AI proposed label)\", value=st.session_state.get(state_key, label), key=state_key, ) if new_label != label: df.loc\\\\[df[\"cluster_labels\"] == cluster, \"label\"] = new_label # For each cluster, find the utterance that is closest to the centroid for cluster in df[\"cluster_labels\"].unique(): min_distance_idx = df[df[\"cluster_labels\"] == cluster][ \"distance_from_centroid\" ].idxmin() df.loc[min_distance_idx, \"closest_centroid_utt\"] = df.loc[ min_distance_idx, \"utt\" ] # Create the scatter plot fig = px.scatter( df, x=\"umap_x\", y=\"umap_y\", color=\"cluster_labels\", hover_data=[\"utt\", \"label\"] ) # Add labels to the points that are closest to the centroid in each cluster for i in range(len(df)): if df.iloc[i][\"utt\"] == df.iloc[i][\"closest_centroid_utt\"]: fig.add_annotation( x=df.iloc[i][\"umap_x\"], y=df.iloc[i][\"umap_y\"], text=df.iloc[i][\"label\"] ) # Display the plot st.plotly_chart(fig) save = st.button(\"Save changes\", type=\"primary\") if save: # Reset the input session states after save. Also reset the button state st.session_state.load_state = False for key in'),\n",
       " Document(metadata={'title': 'Deploying with Databutton', 'source': 'https://cohere.com/llmu/deploy-databutton'}, page_content='st.plotly_chart(fig) save = st.button(\"Save changes\", type=\"primary\") if save: # Reset the input session states after save. Also reset the button state st.session_state.load_state = False for key in st.session_state.keys(): if key.startswith(\\'user_label_\\'): del st.session_state[key] db.storage.dataframes.put(key=\"labeled_cluster.csv\", df=df) st.write(\"Labelled Data Saved\")'),\n",
       " Document(metadata={'title': 'Deploying with Databutton', 'source': 'https://cohere.com/llmu/deploy-databutton'}, page_content='Now your app is ready to deploy and share with a few clicks.\\nConclusion\\nDeploying with Databutton enables the usage of Streamlit functionality with additional backend features, making it a more robust solution for larger or more complex projects. Its all-in-one, online workspace simplifies the process of application development, deployment, and management.'),\n",
       " Document(metadata={'title': 'What is Semantic Search?', 'source': 'https://cohere.com/llmu/what-is-semantic-search'}, page_content=\"What is Semantic Search?What is Semantic Search?\\nThis chapter comes with a notebook where you get to build a simple semantic search model to answer queries from a small dataset. And if you'd like a more advanced semantic search Colab, check this one here!\\nWhat is Semantic Search? - YouTube\\nWhat is Not Semantic Search?\\nBefore we learn semantic search, let’s see what is not semantic search. Before semantic search, the most popular way of searching was keyword search. Imagine that you have a list of many sentences, which are the responses. When you ask a question (query), keyword search looks for the sentence (response) with the largest number of words in common with the query. For example, consider the following query and set of responses:\\nQuery: Where is the world cup?\\nResponses:\\nThe world cup is in Qatar.\\nThe sky is blue.\\nThe bear lives in the woods.\\nAn apple is a fruit.\\nWith keyword search, you can notice that the responses have the following number of words in common with the query:\\nResponses:\\nThe world cup is in Qatar. (4 words in common)\\nThe sky is blue. (2 words in common)\\nThe bear lives in the woods. (2 words in common)\\nAn apple is a fruit. (1 word in common)\\nIn this case, the winning response is number 1, “The world cup is in Qatar”. This is the correct response, luckily. However, this won’t always be the case. Imagine if there was another response:\\nWhere in the world is my cup of coffee?\\nThis response has 5 words in common with the query, so it would win if it was in the list of responses. This is unfortunate, since this is not the correct response.\"),\n",
       " Document(metadata={'title': 'What is Semantic Search?', 'source': 'https://cohere.com/llmu/what-is-semantic-search'}, page_content='This response has 5 words in common with the query, so it would win if it was in the list of responses. This is unfortunate, since this is not the correct response.\\nWhat can we do? We can improve keyword search, by removing stop words such as “the”, “and”, “is”, etc. We can also use methods like TF-IDF in order to tell apart relevant from non-relevant words. However, as you may imagine, there will always be cases in which, due to the ambiguity of the language, synonyms, and other roadblocks, keyword search will fail to find the best response. So we move on to the next algorithm, one that has performed very well: Semantic search.\\nIn short, semantic search works as follows:\\nIt uses a text embedding to turn words into vectors (lists of numbers).\\nUses similarity to find the vector among the responses which is the most similar to the vector corresponding to the query.\\nOutputs the response corresponding to this most similar vector.\\nIn this post, we’ll learn all these steps in detail. First, let’s look at text embeddings. If you need to brush up on these, check out this article.\\n💡\\nIf you enjoy this content, be sure to check out more in LLM University!\\nHow to Search Using Text Embeddings?\\nAn embedding is a way to assign to each sentence (or more generally, to each text fragment, which can be as short as a word or as long as a full article), a vector, which is a list of numbers. The Cohere embedding model used in the codelab for this post returns a vector of length 4096. This is a list of 4096 numbers (other Cohere embeddings, such as the multilingual one, return smaller vectors, for example, of length 768). A very important property of embeddings is that similar pieces of text get assigned to similar lists of numbers. For example, the sentence “Hello, how are you?” and the sentence “Hi, what’s up?” will be assigned lists of similar numbers, whereas the sentence “Tomorrow is Friday” will be assigned a list of numbers that are quite different from the two previous ones.'),\n",
       " Document(metadata={'title': 'What is Semantic Search?', 'source': 'https://cohere.com/llmu/what-is-semantic-search'}, page_content='In the next image, there is an example of an embedding. For visual simplicity, this embedding assigns to each sentence, a vector of length 2 (a list of two numbers). These numbers are plotted in the graph in the right, as coordinates. For example, the sentence “The world cup is in Qatar” gets assigned to the vector (4, 2), so it gets plotted in the point with coordinates 4 (horizontal) and 2 (vertical).\\nIn this image, all the sentences are located as points in the plane. Visually, you can identify that the query (represented by the trophy) is closest to the response “The world cup is in Qatar”, represented by the soccer ball. The other queries (represented by a cloud, a bear, and an apple) are much farther. Thus, semantic search would return the response “The world cup is in Qatar”, which is the correct response.\\nBut before we get any further, let’s actually use a real-life text embedding to search in a small dataset. The following dataset has four queries, and their four corresponding responses.\\nDataset:\\nQueries:\\nWhere does the bear live?\\nWhere is the world cup?\\nWhat color is the sky?\\nWhat is an apple?\\nResponses\\nThe bear lives in the woods\\nThe world cup is in Qatar\\nThe sky is blue\\nAn apple is a fruit\\nWe can use the Cohere text embedding to encode these 8 sentences. That would give us 8 vectors of length 4096, but we can use some dimensionality reduction algorithms to bring those down to length 2. Just like before, this means we can plot the sentences in the plane with 2 coordinates. The plot is below.\\nNotice that each query is closest to its corresponding response. That means that if we used semantic search to search for the response to each of these 4 queries, we would get the correct response.'),\n",
       " Document(metadata={'title': 'What is Semantic Search?', 'source': 'https://cohere.com/llmu/what-is-semantic-search'}, page_content='Notice that each query is closest to its corresponding response. That means that if we used semantic search to search for the response to each of these 4 queries, we would get the correct response.\\nHowever, here’s a caveat. In the above example, we used Euclidean distance, which is simply distance in the plane. This can be generalized to vectors of 4096 entries as well (using the Pythagorean theorem). However, that’s not the ideal way to compare pieces of text. The way that is most commonly used and that has given the best results is similarity, which we study in the next section.\\nUsing Similarity to Find the Best Document\\nSimilarity is a way to tell if two pieces of text are similar or different. This uses text embeddings. If you’d like to brush up on similarity, check out this article. In this article, two types of similarity used in semantic search are described:\\nDot product similarity\\nCosine similarity\\nFor now, let’s join them into one notion, and let’s say that similarity is a number assigned to each pair of documents, with the following properties:\\nThe similarity between a piece of text and itself is a very high number.\\nThe similarity between two very similar pieces of text is a high number.\\nThe similarity between two different pieces of text is a small number.\\nFor this article, we’ll use cosine similarity, which has the extra property that the values it returns are between 0 and 1. The similarity between a piece of text and itself is always 1, and the lowest value that a similarity can take is 0 (when two pieces of text are really dissimilar).\\nNow, in order to perform semantic search, all you have to do is calculate the similarity between the query and every pair of sentences, and return the sentence with the highest similarity. Let’s do an example. Below is a plot of the cosine similarities between the 8 sentences in the above dataset.\\nIn this plot, the scale is given at the right. Notice the following properties:'),\n",
       " Document(metadata={'title': 'What is Semantic Search?', 'source': 'https://cohere.com/llmu/what-is-semantic-search'}, page_content='In this plot, the scale is given at the right. Notice the following properties:\\nThe diagonal is full of 1’s (as the similarity between each sentence and itself is 1).\\nThe similarities between each sentence and its corresponding response are around 0.7.\\nThe similarities between any other pair of sentences are lower values.\\nThis means, if you were to search for the answer to, for example, the query “What is an apple?”, semantic search would look at the penultimate row on the table, and notice that the closest sentences are “What is an apple?” (with similarity 1), and “An apple is a fruit” (with similarity around 0.7). The system would eliminate the same query from the list, as it wouldn’t want to respond with the same question that is given. Thus, the winning response would be “An apple is a fruit”, which is the correct response.\\nThere is a hidden algorithm here that we didn’t mention, but is very important: The nearest neighbors algorithm. In short, this algorithm finds the nearest neighbor of a point in a dataset. In this case, the algorithm found the nearest neighbor of the sentence “What is an apple?”, and the response was the sentence “An apple is a fruit”. In the next section, you’ll learn more about nearest neighbors.\\nNearest Neighbors - Pros and Cons, and How to Fix Them\\nNearest neighbors is a very simple and useful algorithm, normally used for classification. More generally, it’s called k-nearest neighbors (knn), where k is any number. If the task at hand is classification, k-nearest neighbors will simply look at the k closest neighbors or a particular data point, and assign the data point the most common label amongst the neighbors. For example, if the task at hand is to classify a sentence as happy or sad (sentiment analysis), what 3-nearest neighbors would do is to look at the 3 closest neighbors to the sentence (in some embedding), and see if their majority (2) are happy or sad. That’s the label it assigns to the sentence.'),\n",
       " Document(metadata={'title': 'What is Semantic Search?', 'source': 'https://cohere.com/llmu/what-is-semantic-search'}, page_content='As you can see, k-nearest-neighbors is exactly what we’ve been doing for semantic search in this article. Given a query, you look for the closest neighbor in the embedding, and that’s the response to the query. In the current examples, that method has worked well. However, k-nearest-neighbors is not the fastest algorithm. The reason for this is that in order to find the neighbors of one point, one needs to calculate the distances between that point and all the other points in the dataset, and then find the smallest one. As you can see in the image below, in order to find the closest neighbor to the sentence “Where is the world cup?”, we had to calculate 8 distances, one for each other data point.\\nWhen dealing with massive archives, however, we can optimize for performance by slightly adjusting the algorithm to become approximate-k-nearest-neighbors. In particular, in search, there are several improvements that can speed up this process quite a lot. Here are two of them:\\nInverted File Index (IVD): Consists of clustering similar documents, then searching in the clusters that are closest to the query.\\nHierarchical Navigable Small World (HNSW): Consists on starting with a few points, and searching there. Then adding more points at each iteration, and searching in each new space.\\nMultilingual Search\\nAs you may have noticed, the performance of semantic search depends on the strength of the embedding. Therefore, any superpowers of the embedding may translate into superpowers of semantic search. The new multilingual embedding, by Cohere, is an embedding that supports more than 100 languages. In short, the embedding will send any piece of text in any of these languages, to a vector (this time of length 768). Similar pieces of text will be sent to similar vectors. Therefore, one can search using a query in any language, and the model will search for answers in all the other languages.'),\n",
       " Document(metadata={'title': 'What is Semantic Search?', 'source': 'https://cohere.com/llmu/what-is-semantic-search'}, page_content='In the image below, you can see an example of multilingual embedding. The embedding sends each sentence to a vector of length 4096, but, just like in the previous example, a projection is used to send this vector to one of length 2. These 2 entries in the vector are used as coordinates in the plane for the plot.\\nIn this plot, we have 4 sentences in English, together with their direct translations in Spanish and French.\\nEnglish:\\nThe bear lives in the woods.\\nThe world cup is in Qatar.\\nAn apple is a fruit.\\nThe sky is blue.\\nSpanish:\\nEl oso vive en el bosque.\\nEl mundial es en Qatar.\\nUna manzana es una fruta.\\nEl cielo es azul.\\nFrench:\\nL’ours vit dans la forêt.\\nLa coupe du monde est au Qatar.\\nUne pomme est un fruit.\\nLe ciel est bleu.\\nAs you can see in the plot, the multilingual model has located each sentence and its two translations very close together. Here’s a video if you’d like to learn more about multilingual embeddings and search.\\nAre Embeddings and Similarity Enough? (No)\\nIn this article, you’ve seen how effective a search system can be when it consists of a solid embedding, plus a search based in similarity. But is this the end of the story? Unfortunately (or fortunately?) no. It turns out that only using these two tools can lead to some mishaps. Luckily, these are mishaps that we can fix. Here is an example. Let’s extend our initial dataset a bit, by adding some more responses to the world cup question. Consider the following sentences.\\nQuery: “Where is the world cup?”\\nResponses:\\nThe world cup is in Qatar\\nThe world cup is in the moon\\nThe previous world cup was in Russia'),\n",
       " Document(metadata={'title': 'What is Semantic Search?', 'source': 'https://cohere.com/llmu/what-is-semantic-search'}, page_content='Query: “Where is the world cup?”\\nResponses:\\nThe world cup is in Qatar\\nThe world cup is in the moon\\nThe previous world cup was in Russia\\nWhen we locate them in the embedding above, they are all close, as expected. However, the closest sentence to the query is not response 1 (the correct answer), but response 3. This response (“The previous world cup is in Russia”) is a correct statement, and semantically close to the question, but it’s not the answer to the question. Response 2 (“The world cup is in the moon”) is a completely wrong answer, but also semantically close to the query. As you can see in the embedding, it’s dangerously close to the query, meaning it is likely that bogus answers like these will be the top results using semantic search.\\nHow do we fix this? There are many ways to improve search performance so that the actual response the model returns is ideal, or at least close to ideal. One of them is multiple negative ranking loss: Having positive pairs (query, response) and several other negative pairs (query, wrong response). Training the model to reward positive pairs, and punish negative pairs.\\nIn the current example, we would take a positive (query, response) pair, such as this one:\\n(Where is the world cup?, The world cup is in Qatar.)\\nWe would also take several negative (query, response) pairs, such as:\\n(Where is the world cup?, The world cup is in the moon)\\n(Where is the world cup?, The previous world cup was in Russia)\\n(Where is the world cup?, The world cup is in 2022.)\\nBy training the model to respond negatively to bad (query, response) pairs, the model is more likely to give the correct answer to a query.\\nNow the question is, how do we train the model to do this? This is a topic for a future article. Other search topics we’ll be exploring in the future are the following, so stay tuned!\\nSemantic search, vector databases\\nMultilingual semantic search\\nSemantic search for long documents\\nRe-ranking endpoint\\nBillion-scale semantic search'),\n",
       " Document(metadata={'title': 'What is Semantic Search?', 'source': 'https://cohere.com/llmu/what-is-semantic-search'}, page_content='Semantic search, vector databases\\nMultilingual semantic search\\nSemantic search for long documents\\nRe-ranking endpoint\\nBillion-scale semantic search\\nSemantic search over semi-structured data\\nBulk encoding embeddings\\nConclusion\\nSemantic search is a very powerful way to search for an answer to a particular question. It is an improvement over keyword search, since it actually compares the semantic meaning of the query and the responses. In order to do this, it uses a text embedding, and it ranks the responses based on their similarity to the query.'),\n",
       " Document(metadata={'title': 'Keyword Search', 'source': 'https://cohere.com/llmu/keyword-search'}, page_content='Keyword SearchQualified\\nThis chapter comes with a notebook, and we encourage you to follow it along as you read the chapter.\\nIn the previous chapter, you learned about the difference between keyword search and dense retrieval. In this lab, you’ll learn to use keyword search to query a large dataset of Wikipedia articles. Later in this same module, you’ll be able to improve your results with dense retrieval and rerank, on the same wikipedia dataset, and even be able to combine the search results with a generative model in order to generate answers in the form of sentences!\\nUsing a Vector Database\\nWe\\'ll set up a vector database for use in this and the next chapter. In order to store the Wikipedia dataset query, we’ll use the Weaviate vector database, which will give us a range of benefits. In simple terms, a vector database is a place where one can store data objects and vector embeddings, and be able to access them and perform operations easily. For example, finding the nearest neighbors of a vector in a dataset is a lengthy process, which is sped up significantly by using a vector database. This is done with the following code.\\nimport weaviate import cohere # Add your Cohere API key here # You can obtain a key by signing up in https://dashboard.cohere.com/ or https://docs.cohere.com/reference/key cohere_api_key = \"COHERE_API_KEY\" co = cohere.Client(cohere_api_key) # Connect to the Weaviate demo databse containing 10M wikipedia vectors # This uses a public READ-ONLY Weaviate API key auth_config = weaviate.auth.AuthApiKey(api_key=\"76320a90-53d8-42bc-b41d-678647c6672e\") client = weaviate.Client( url=\"https://cohere-demo.weaviate.network/\", auth_client_secret=auth_config, additional_headers={ \"X-Cohere-Api-Key\": cohere_api_key, } ) \\nQuerying the Wikipedia Dataset Using Keyword Matching'),\n",
       " Document(metadata={'title': 'Keyword Search', 'source': 'https://cohere.com/llmu/keyword-search'}, page_content='Querying the Wikipedia Dataset Using Keyword Matching\\nTo use keyword matching, we’ll first define the following function for keyword search. In this function, we’ll tell the vector database what properties we want from each retrieved document. We’ll also filter them to the English language (using results_lang), but feel free to explore searching in other languages as well!\\ndef keyword_search(query, results_lang=\\'en\\', num_results=10): properties = [\"text\", \"title\", \"url\", \"views\", \"lang\", \"_additional {distance}\"] where_filter = { \"path\": [\"lang\"], \"operator\": \"Equal\", \"valueString\": results_lang } response = ( client.query.get(\"Articles\", properties) .with_bm25( query=query ) .with_where(where_filter) .with_limit(num_results) .do() ) result = response[\\'data\\'][\\'Get\\'][\\'Articles\\'] return result \\nWe’ll use two search queries, of varying difficulty.\\nSimple query: “Who discovered penicillin?”\\nHard query: “Who was the first person to win two Nobel prizes?”\\nThe responses for these queries are “Alexander Fleming”, and “Marie Curie”. Now let’s see how keyword search does. Here are the top three results for each query (some results are repeated, so let’s look at the three top distinct ones).\\nQuery 1: “Who discovered penicillin?”\\nResponses:\\nPenicillin\\nAntibiotic\\nAlexander Fleming\\nAs you can see, keyword search did quite well. All three articles contain the answer, and in particular, the third one is the correct response: Alexander Fleming.\\nNow let’s see how it did with the more complicated query.\\nQuery 2: “Who was the first person to win two Nobel prizes?”\\nResponses:\\nNeutrino\\nWestern culture\\nReality television'),\n",
       " Document(metadata={'title': 'Keyword Search', 'source': 'https://cohere.com/llmu/keyword-search'}, page_content='Now let’s see how it did with the more complicated query.\\nQuery 2: “Who was the first person to win two Nobel prizes?”\\nResponses:\\nNeutrino\\nWestern culture\\nReality television\\nThis time, keyword search was very far from finding the answer. If you explore the articles, you may notice that they contain several mentions of words such as “first”, “person”, “Nobel”, “prizes”, but none of them have any information on the first person to win two Nobel prizes. In fact, the neutrino article mentions a scientist who won two Nobel prizes, but this wasn’t the first person to achieve this feat.\\nConclusion\\nAs you can see, keyword search can be good for queries, like “Who discovered penicillin?”, in which you’d expect the answers to have a lot of words in common with the query. More specifically, if an article contains the words “discovered,” and “penicillin”, it’s also likely to contain the fact that Alexander Fleming discovered it.\\nWith harder queries like “Who was the first person to win two Nobel prizes?”, keyword search doesn’t do well. The reason is that the words in the query would appear in many instances without necessarily talking about something as specific as the first person who won two Nobel prizes. By matching words, we haven’t yet exploited the semantics of the sentence. A model that understands what we mean by “the first person to win two Nobel prizes” would be able to find the answer, which is exactly what dense retrieval does (see the next section).\\nAs you can see, keyword search can be good for some queries, like “Who discovered penicillin”, in which you’d expect the answers to have a lot of words in common with the query. More specifically, if an article contains the word “discovered” and \"penicillin\", it’s also likely to contain the fact that Alexander Fleming was the one who discovered it.'),\n",
       " Document(metadata={'title': 'Keyword Search', 'source': 'https://cohere.com/llmu/keyword-search'}, page_content='Keyword search can have a harder time with queries like “Who was the first person to win two Nobel prizes”, because there can be many articles which contain these words, yet not contain the answer. There can be articles that have words like \"first\", \"person\", \"win\", \"two\", and have nothing to do with the query. Moreover, there can be many articles about work that led to a Nobel prize which do not necessarily mention that Marie Curie was the first person to win two Nobel prizes.\\nLater in this module, you\\'ll learn two methods to improve keyword search. One is by creating a search system that actually understands the semantics of the queries and responses, and is able to match them based on their meaning and not based on the words contained. This is called semantic search, and you saw it in high level in the previous lesson (more specifically, the method you\\'ll learn is called dense retrieval, which is one type of semantic search). The other method is reranking, which is able to surface the pairs of queries and documents that are the most relevant to each other. In this way, you can use keyword search to retrieve, say, hundreds of articles with matching keywords, and then use reranking to surface the best ones.'),\n",
       " Document(metadata={'title': 'Dense Retrieval', 'source': 'https://cohere.com/llmu/dense-retrieval'}, page_content=\"Dense RetrievalQualified\\nThis chapter uses the same notebook as the previous chapter, and we encourage you to follow it along as you read the chapter.\\nIn the previous chapter, you used keyword search to query Wikipedia, using two different queries:\\nWho discovered penicillin?\\nWho was the first person to win two Nobel prizes?\\nKeyword search did well with the first query, but not with the second one. The reason is that the second one has very popular words, which may appear in many documents that don’t necessarily contain the answer to the question. A way to obtain better results is to have the model ‘understand’ what the question is actually asking. This is where semantic search comes into play. Semantic search is the ability to search by meaning – not just keyword matching. Language models enable two of the main ways of semantic search: Dense retrieval, and reranking. The concepts of dense retrievals were illustrated in a chapter earlier in this module, so feel free to go back and take a look if you’d like a refresher. In this chapter, you'll see dense retrieval in action, with the same Wikipedia dataset and the same queries that you used for keyword search.\\nQuerying the Dataset Using Dense Retrieval\\nDense retrieval uses a text embedding in order to search for documents that are similar to a query. If you’d like to learn more about embeddings, please take a look at the embeddings chapter. Embeddings assign a vector (long list of numbers) to each piece of text. One of the main properties in an embedding is that similar pieces of text go to similar vectors.\\nIn short, dense retrieval consists of the following:\\nFinding the embedding vector corresponding to the query\\nFinding the embedding vectors corresponding to each of the responses (in this case, Wikipedia articles)\\nRetrieving the response vectors that are closest to the query vector in the embedding\\nDense retrieval finds the closest documents to the query in the embedding\"),\n",
       " Document(metadata={'title': 'Dense Retrieval', 'source': 'https://cohere.com/llmu/dense-retrieval'}, page_content='Retrieving the response vectors that are closest to the query vector in the embedding\\nDense retrieval finds the closest documents to the query in the embedding\\nTo use dense retrieval, we’ll first define the following function which uses the Weaviate client we defined in the previous chapter. Just like with keyword search, we’ll tell the vector database what properties we want from each retrieved document, and filter them to the English language (using results_lang).\\ndef dense_retrieval(query, results_lang=\\'en\\', num_results=10): nearText = {\"concepts\": [query]} properties = [\"text\", \"title\", \"url\", \"views\", \"lang\", \"_additional {distance}\"] # To filter by language where_filter = { \"path\": [\"lang\"], \"operator\": \"Equal\", \"valueString\": results_lang } response = ( client.query .get(\"Articles\", properties) .with_near_text(nearText) .with_where(where_filter) .with_limit(num_results) .do() ) result = response[\\'data\\'][\\'Get\\'][\\'Articles\\'] return result \\nChunking the Articles\\nThis process of finding the closest documents to a query in an embedding will yield good results. However, articles may be very long and this could make things complicated. In order to have more granularity, we’ll split the articles by paragraph. This means that we’ll find the embedding vector corresponding to each paragraph of each article in the Wikipedia dataset. In that way, when the model retrieves the answer, it will actually output the paragraph that it found the most similar to the query, as well as the article in which this paragraph belongs.\\nWikipedia articles get chunked by paragraph, and each chunk gets assigned an embedding vector\\nBack to Querying the Dataset\\nLet’s review the two queries we used in the previous chapter.\\nSimple query: “Who discovered penicillin?”\\nHard query: “What was the first person to win two Nobel prizes?”'),\n",
       " Document(metadata={'title': 'Dense Retrieval', 'source': 'https://cohere.com/llmu/dense-retrieval'}, page_content='Back to Querying the Dataset\\nLet’s review the two queries we used in the previous chapter.\\nSimple query: “Who discovered penicillin?”\\nHard query: “What was the first person to win two Nobel prizes?”\\nNow, let’s look at the three top results for each query using dense retrieval. Recall that the responses here are at the paragraph level, so the model may sometimes retrieve the same article several times by outputting different paragraphs from the same article.\\nQuery 1: “Who discovered penicillin?”\\nResponses:\\nAlexander Fleming: “Sir Alexander Fleming (6 August 1881 - 11 March 1995) was a Scottish physician and microbiologist …”\\nPenicillin: “Penicillin was discovered in 1928 by Scottish scientist Alexander Fleming …”\\nPenicillin: “The term “penicillin” is defined as the natural product of “Penicillium” mould with antimicrobial activity. It was coined by Alexander Fleming ...”\\nAs you can see, dense retrieval did quite well by finding paragraphs that contain the exact answer. Now, let’s see how it did with the more complicated query.\\nQuery 2: “Who was the first person to win two Nobel prizes?”\\nResponses:\\nNobel prize in literature: “The Nobel prize in literature can be shared by two individuals …”\\nNobel prize: “Although posthumous nominations are not presently permitted, …”\\nNobel prize: “Few people have received two Nobel prizes. Marie Curie received the Physics prize …”\\nMarie Curie: “Marie Curie was the first woman to win a Nobel prize, the first person to win two Nobel prizes, …”\\nAs you can see, dense retrieval did much better than keyword search here. The second, third, and fourth results are in the correct documents (Nobel prize and Marie Curie), and in fact, the third and fourth results are in a paragraph which explicitly contains the answer. The reason for this is that the embedding captures the semantics of the text, and is able to see if two pieces of text have a similar meaning, even if they don’t necessarily share many words in common.\\nSearching in Other Languages'),\n",
       " Document(metadata={'title': 'Dense Retrieval', 'source': 'https://cohere.com/llmu/dense-retrieval'}, page_content='Searching in Other Languages\\nAs you may have noticed, the dense_retrieval function has a parameter called results_lang (see code lab). This parameter determines the language in which the search results are outputted. It is defaulted to English (\\'en\\') , but for this demo, it can also be set to German (\\'de\\'), French (\\'fr\\'), Spanish (\\'es\\'), Italian (\\'it\\'), Japanese (\\'ja\\'), Arabic (\\'ar\\'), (Simplified) Chinese (\\'zh\\'), Korean (\\'ko\\'), and Hindi (\\'hi\\'). However, the Cohere multilingual embedding handles over 100 languages.\\nFor the first example, let\\'s search for results to the English query \"Who was the first person to win two Nobel prizes\" in Arabic. The line of code is the following:\\narabic_results = dense_retrieval(hard_query, results_lang=\\'ar\\') \\nAs you can see in the lab, the top 3 are the following:\\nNobel Prize جائزة نوبل\\nList of Nobel Laureates قائمة الحاصلين على جائزة نوبل\\nWomen in Society امرأة\\nNow let\\'s search for the French results to a query in Spanish. The query is \"Quién descubrió la penicilina?\" (\"Who discovered penicillin?\").\\nspanish_query = \"Quien descubrio la penicilina?\" french_results = dense_retrieval(spanish_query, results_lang=\\'fr\\') \\nThe results are the following\\nPénicilline\\nPénicilline\\nAlexander Fleming\\nConclusion\\nDense retrieval is a search method that uses a text embedding to find the semantically closest answers to a query. In our Wikipedia example, dense retrieval did quite well in the three queries. In the next chapter you’ll learn how to improve search results even further, with the Reranking method.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb5486a-70b7-4a95-bc66-79df18f204cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1746ef28-82ef-42bd-a79c-9076aae5f21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.cohere import CohereEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6de1b0-fb3d-4d05-b234-95a7313b726f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `CohereEmbeddings` was deprecated in LangChain 0.0.30 and will be removed in 0.3.0. An updated version of the class exists in the langchain-cohere package and should be used instead. To use it run `pip install -U langchain-cohere` and import as `from langchain_cohere import CohereEmbeddings`.\n",
      "  warn_deprecated(\n",
      "d:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\pydantic\\_internal\\_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "* 'smart_union' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'user_agent'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\Projects\\LLM Git Repos\\Langchain-RAG\\scripts\\data_loader.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m embedding \u001b[39m=\u001b[39m CohereEmbeddings(model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39membed-english-v3.0\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:203\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    201\u001b[0m     warned \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    202\u001b[0m     emit_warning()\n\u001b[1;32m--> 203\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\pydantic\\v1\\main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[39m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[39m=\u001b[39m validate_model(__pydantic_self__\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m, data)\n\u001b[0;32m    340\u001b[0m \u001b[39mif\u001b[39;00m validation_error:\n\u001b[0;32m    341\u001b[0m     \u001b[39mraise\u001b[39;00m validation_error\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\pydantic\\v1\\main.py:1048\u001b[0m, in \u001b[0;36mvalidate_model\u001b[1;34m(model, input_data, cls)\u001b[0m\n\u001b[0;32m   1046\u001b[0m \u001b[39mfor\u001b[39;00m validator \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39m__pre_root_validators__:\n\u001b[0;32m   1047\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1048\u001b[0m         input_data \u001b[39m=\u001b[39m validator(cls_, input_data)\n\u001b[0;32m   1049\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m, \u001b[39mAssertionError\u001b[39;00m) \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m   1050\u001b[0m         \u001b[39mreturn\u001b[39;00m {}, \u001b[39mset\u001b[39m(), ValidationError([ErrorWrapper(exc, loc\u001b[39m=\u001b[39mROOT_KEY)], cls_)\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\Langchain-RAG\\lib\\site-packages\\langchain_community\\embeddings\\cohere.py:68\u001b[0m, in \u001b[0;36mCohereEmbeddings.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     66\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mcohere\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m     client_name \u001b[39m=\u001b[39m values[\u001b[39m\"\u001b[39;49m\u001b[39muser_agent\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[0;32m     69\u001b[0m     values[\u001b[39m\"\u001b[39m\u001b[39mclient\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m cohere\u001b[39m.\u001b[39mClient(\n\u001b[0;32m     70\u001b[0m         cohere_api_key,\n\u001b[0;32m     71\u001b[0m         timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m     72\u001b[0m         client_name\u001b[39m=\u001b[39mclient_name,\n\u001b[0;32m     73\u001b[0m     )\n\u001b[0;32m     74\u001b[0m     values[\u001b[39m\"\u001b[39m\u001b[39masync_client\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m cohere\u001b[39m.\u001b[39mAsyncClient(\n\u001b[0;32m     75\u001b[0m         cohere_api_key,\n\u001b[0;32m     76\u001b[0m         timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m     77\u001b[0m         client_name\u001b[39m=\u001b[39mclient_name,\n\u001b[0;32m     78\u001b[0m     )\n",
      "\u001b[1;31mKeyError\u001b[0m: 'user_agent'"
     ]
    }
   ],
   "source": [
    "embedding = CohereEmbeddings(model=\"embed-english-v3.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f43d2e7-af34-4023-982a-ccd69ba0fb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20752318-487a-4f74-81be-d51883f9f2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d8e09e-a36f-4a3f-8ad3-b02673d5e4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    }
   ],
   "source": [
    "DeepLake.force_delete_by_path(f\"hub://{org_id}/{db_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7175fcb-e197-41e3-9d34-b666bcb7f346",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = CohereEmbeddings(model=\"embed-english-v3.0\", user_agent=\"langchain\")  # 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74fc583-1d6a-4b4f-a7d2-5f4f4e117a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    }
   ],
   "source": [
    "# DeepLake Vector Store\n",
    "org_id = \"vijayv2807\"\n",
    "db_name = \"cohere-llmu-data\"\n",
    "dataset_path = f\"hub://{org_id}/{db_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df31466f-2bf0-42a8-b836-cb5d6f89638d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.deeplake.DeepLake at 0x23b4de59420>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57e1000-ad43-46a9-b8e2-8f75784180ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 396 embeddings in 1 batches of size 396:: 100%|██████████| 1/1 [01:37<00:00, 97.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://vijayv2807/cohere-llmu-data', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype       shape      dtype  compression\n",
      "  -------    -------     -------    -------  ------- \n",
      "   text       text      (396, 1)      str     None   \n",
      " metadata     json      (396, 1)      str     None   \n",
      " embedding  embedding  (396, 1024)  float32   None   \n",
      "    id        text      (396, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['94ab9cfd-5b83-11ef-8267-646c80368716',\n",
       " '94ab9cfe-5b83-11ef-9930-646c80368716',\n",
       " '94ab9cff-5b83-11ef-9713-646c80368716',\n",
       " '94ab9d00-5b83-11ef-b7f5-646c80368716',\n",
       " '94ab9d01-5b83-11ef-b9b3-646c80368716',\n",
       " '94ab9d02-5b83-11ef-bfaa-646c80368716',\n",
       " '94ab9d03-5b83-11ef-b8a5-646c80368716',\n",
       " '94ab9d04-5b83-11ef-a883-646c80368716',\n",
       " '94ab9d05-5b83-11ef-b2f6-646c80368716',\n",
       " '94ab9d06-5b83-11ef-99c5-646c80368716',\n",
       " '94ab9d07-5b83-11ef-9fad-646c80368716',\n",
       " '94ab9d08-5b83-11ef-aea1-646c80368716',\n",
       " '94ab9d09-5b83-11ef-855b-646c80368716',\n",
       " '94ab9d0a-5b83-11ef-a4ba-646c80368716',\n",
       " '94ab9d0b-5b83-11ef-81b3-646c80368716',\n",
       " '94ab9d0c-5b83-11ef-9be2-646c80368716',\n",
       " '94ab9d0d-5b83-11ef-bba8-646c80368716',\n",
       " '94ab9d0e-5b83-11ef-b0e0-646c80368716',\n",
       " '94ab9d0f-5b83-11ef-bfd2-646c80368716',\n",
       " '94ab9d10-5b83-11ef-a5be-646c80368716',\n",
       " '94ab9d11-5b83-11ef-b621-646c80368716',\n",
       " '94ab9d12-5b83-11ef-9599-646c80368716',\n",
       " '94ab9d13-5b83-11ef-86dd-646c80368716',\n",
       " '94ab9d14-5b83-11ef-a1e1-646c80368716',\n",
       " '94ab9d15-5b83-11ef-954d-646c80368716',\n",
       " '94ab9d16-5b83-11ef-bfe3-646c80368716',\n",
       " '94ab9d17-5b83-11ef-86f5-646c80368716',\n",
       " '94ab9d18-5b83-11ef-be1c-646c80368716',\n",
       " '94ab9d19-5b83-11ef-a4fd-646c80368716',\n",
       " '94ab9d1a-5b83-11ef-9da3-646c80368716',\n",
       " '94ab9d1b-5b83-11ef-8132-646c80368716',\n",
       " '94ab9d1c-5b83-11ef-86e2-646c80368716',\n",
       " '94ab9d1d-5b83-11ef-97b2-646c80368716',\n",
       " '94ab9d1e-5b83-11ef-9047-646c80368716',\n",
       " '94ab9d1f-5b83-11ef-9877-646c80368716',\n",
       " '94ab9d20-5b83-11ef-af03-646c80368716',\n",
       " '94ab9d21-5b83-11ef-aa9d-646c80368716',\n",
       " '94ab9d22-5b83-11ef-9090-646c80368716',\n",
       " '94ab9d23-5b83-11ef-9416-646c80368716',\n",
       " '94ab9d24-5b83-11ef-8b7d-646c80368716',\n",
       " '94ab9d25-5b83-11ef-ab45-646c80368716',\n",
       " '94ab9d26-5b83-11ef-b9f8-646c80368716',\n",
       " '94ab9d27-5b83-11ef-9f21-646c80368716',\n",
       " '94ab9d28-5b83-11ef-97fc-646c80368716',\n",
       " '94ab9d29-5b83-11ef-9e19-646c80368716',\n",
       " '94ab9d2a-5b83-11ef-8324-646c80368716',\n",
       " '94ab9d2b-5b83-11ef-aa2a-646c80368716',\n",
       " '94ab9d2c-5b83-11ef-b5a7-646c80368716',\n",
       " '94ab9d2d-5b83-11ef-853a-646c80368716',\n",
       " '94ab9d2e-5b83-11ef-82e0-646c80368716',\n",
       " '94ab9d2f-5b83-11ef-82e7-646c80368716',\n",
       " '94ab9d30-5b83-11ef-856e-646c80368716',\n",
       " '94ab9d31-5b83-11ef-92b3-646c80368716',\n",
       " '94ab9d32-5b83-11ef-af92-646c80368716',\n",
       " '94ab9d33-5b83-11ef-b937-646c80368716',\n",
       " '94ab9d34-5b83-11ef-95b3-646c80368716',\n",
       " '94ab9d35-5b83-11ef-989b-646c80368716',\n",
       " '94ab9d36-5b83-11ef-a21c-646c80368716',\n",
       " '94ab9d37-5b83-11ef-960c-646c80368716',\n",
       " '94ab9d38-5b83-11ef-b0df-646c80368716',\n",
       " '94ab9d39-5b83-11ef-8d4a-646c80368716',\n",
       " '94ab9d3a-5b83-11ef-8c24-646c80368716',\n",
       " '94ab9d3b-5b83-11ef-a0d4-646c80368716',\n",
       " '94ab9d3c-5b83-11ef-995b-646c80368716',\n",
       " '94ab9d3d-5b83-11ef-b6c9-646c80368716',\n",
       " '94ab9d3e-5b83-11ef-9517-646c80368716',\n",
       " '94ab9d3f-5b83-11ef-8ab3-646c80368716',\n",
       " '94ab9d40-5b83-11ef-9c32-646c80368716',\n",
       " '94ab9d41-5b83-11ef-9cdb-646c80368716',\n",
       " '94ab9d42-5b83-11ef-9cb3-646c80368716',\n",
       " '94ab9d43-5b83-11ef-8ac3-646c80368716',\n",
       " '94ab9d44-5b83-11ef-bb80-646c80368716',\n",
       " '94ab9d45-5b83-11ef-8f54-646c80368716',\n",
       " '94ab9d46-5b83-11ef-88ea-646c80368716',\n",
       " '94ab9d47-5b83-11ef-8ecf-646c80368716',\n",
       " '94ab9d48-5b83-11ef-811f-646c80368716',\n",
       " '94ab9d49-5b83-11ef-a29a-646c80368716',\n",
       " '94ab9d4a-5b83-11ef-bd68-646c80368716',\n",
       " '94ab9d4b-5b83-11ef-bdc8-646c80368716',\n",
       " '94ab9d4c-5b83-11ef-adb4-646c80368716',\n",
       " '94ab9d4d-5b83-11ef-883b-646c80368716',\n",
       " '94ab9d4e-5b83-11ef-8b0f-646c80368716',\n",
       " '94ab9d4f-5b83-11ef-8d3c-646c80368716',\n",
       " '94ab9d50-5b83-11ef-9c8d-646c80368716',\n",
       " '94ab9d51-5b83-11ef-b0ae-646c80368716',\n",
       " '94ab9d52-5b83-11ef-8b88-646c80368716',\n",
       " '94ab9d53-5b83-11ef-bd3a-646c80368716',\n",
       " '94ab9d54-5b83-11ef-8d85-646c80368716',\n",
       " '94ab9d55-5b83-11ef-9787-646c80368716',\n",
       " '94ab9d56-5b83-11ef-b160-646c80368716',\n",
       " '94ab9d57-5b83-11ef-80c7-646c80368716',\n",
       " '94ab9d58-5b83-11ef-9c9d-646c80368716',\n",
       " '94ab9d59-5b83-11ef-8493-646c80368716',\n",
       " '94ab9d5a-5b83-11ef-bd9d-646c80368716',\n",
       " '94ab9d5b-5b83-11ef-947a-646c80368716',\n",
       " '94ab9d5c-5b83-11ef-8465-646c80368716',\n",
       " '94ab9d5d-5b83-11ef-a63d-646c80368716',\n",
       " '94ab9d5e-5b83-11ef-ada0-646c80368716',\n",
       " '94ab9d5f-5b83-11ef-aa93-646c80368716',\n",
       " '94ab9d60-5b83-11ef-8656-646c80368716',\n",
       " '94ab9d61-5b83-11ef-ba1f-646c80368716',\n",
       " '94ab9d62-5b83-11ef-9f5a-646c80368716',\n",
       " '94ab9d63-5b83-11ef-8728-646c80368716',\n",
       " '94ab9d64-5b83-11ef-a3aa-646c80368716',\n",
       " '94ab9d65-5b83-11ef-8d60-646c80368716',\n",
       " '94ab9d66-5b83-11ef-81d5-646c80368716',\n",
       " '94ab9d67-5b83-11ef-a961-646c80368716',\n",
       " '94ab9d68-5b83-11ef-92ca-646c80368716',\n",
       " '94ab9d69-5b83-11ef-8c8b-646c80368716',\n",
       " '94ab9d6a-5b83-11ef-bdaa-646c80368716',\n",
       " '94ab9d6b-5b83-11ef-993a-646c80368716',\n",
       " '94ab9d6c-5b83-11ef-9ada-646c80368716',\n",
       " '94ab9d6d-5b83-11ef-b8b3-646c80368716',\n",
       " '94ab9d6e-5b83-11ef-b298-646c80368716',\n",
       " '94ab9d6f-5b83-11ef-9362-646c80368716',\n",
       " '94ab9d70-5b83-11ef-b0e5-646c80368716',\n",
       " '94ab9d71-5b83-11ef-ac4c-646c80368716',\n",
       " '94ab9d72-5b83-11ef-87d7-646c80368716',\n",
       " '94ab9d73-5b83-11ef-8e82-646c80368716',\n",
       " '94ab9d74-5b83-11ef-984b-646c80368716',\n",
       " '94ab9d75-5b83-11ef-9593-646c80368716',\n",
       " '94ab9d76-5b83-11ef-9278-646c80368716',\n",
       " '94ab9d77-5b83-11ef-b6b3-646c80368716',\n",
       " '94ab9d78-5b83-11ef-a25b-646c80368716',\n",
       " '94ab9d79-5b83-11ef-99a8-646c80368716',\n",
       " '94ab9d7a-5b83-11ef-887a-646c80368716',\n",
       " '94ab9d7b-5b83-11ef-8882-646c80368716',\n",
       " '94ab9d7c-5b83-11ef-bb9d-646c80368716',\n",
       " '94ab9d7d-5b83-11ef-9581-646c80368716',\n",
       " '94ab9d7e-5b83-11ef-8cb2-646c80368716',\n",
       " '94ab9d7f-5b83-11ef-8406-646c80368716',\n",
       " '94ab9d80-5b83-11ef-9744-646c80368716',\n",
       " '94ab9d81-5b83-11ef-a18f-646c80368716',\n",
       " '94ab9d82-5b83-11ef-b0d0-646c80368716',\n",
       " '94ab9d83-5b83-11ef-b236-646c80368716',\n",
       " '94ab9d84-5b83-11ef-954e-646c80368716',\n",
       " '94ab9d85-5b83-11ef-adc3-646c80368716',\n",
       " '94ab9d86-5b83-11ef-a501-646c80368716',\n",
       " '94ab9d87-5b83-11ef-8204-646c80368716',\n",
       " '94ab9d88-5b83-11ef-a742-646c80368716',\n",
       " '94ab9d89-5b83-11ef-ba0c-646c80368716',\n",
       " '94ab9d8a-5b83-11ef-9e0b-646c80368716',\n",
       " '94ab9d8b-5b83-11ef-9a10-646c80368716',\n",
       " '94ab9d8c-5b83-11ef-996c-646c80368716',\n",
       " '94ab9d8d-5b83-11ef-9a25-646c80368716',\n",
       " '94ab9d8e-5b83-11ef-bba1-646c80368716',\n",
       " '94ab9d8f-5b83-11ef-bb43-646c80368716',\n",
       " '94ab9d90-5b83-11ef-ac6c-646c80368716',\n",
       " '94ab9d91-5b83-11ef-90b7-646c80368716',\n",
       " '94ab9d92-5b83-11ef-b693-646c80368716',\n",
       " '94ab9d93-5b83-11ef-bf37-646c80368716',\n",
       " '94ab9d94-5b83-11ef-a88f-646c80368716',\n",
       " '94ab9d95-5b83-11ef-abdc-646c80368716',\n",
       " '94ab9d96-5b83-11ef-a068-646c80368716',\n",
       " '94ab9d97-5b83-11ef-91be-646c80368716',\n",
       " '94ab9d98-5b83-11ef-8fe6-646c80368716',\n",
       " '94ab9d99-5b83-11ef-9df6-646c80368716',\n",
       " '94ab9d9a-5b83-11ef-be25-646c80368716',\n",
       " '94ab9d9b-5b83-11ef-aa89-646c80368716',\n",
       " '94ab9d9c-5b83-11ef-8cc6-646c80368716',\n",
       " '94ab9d9d-5b83-11ef-a85d-646c80368716',\n",
       " '94ab9d9e-5b83-11ef-bbe5-646c80368716',\n",
       " '94ab9d9f-5b83-11ef-b78b-646c80368716',\n",
       " '94ab9da0-5b83-11ef-b680-646c80368716',\n",
       " '94ab9da1-5b83-11ef-8a81-646c80368716',\n",
       " '94ab9da2-5b83-11ef-bf1a-646c80368716',\n",
       " '94ab9da3-5b83-11ef-9105-646c80368716',\n",
       " '94ab9da4-5b83-11ef-bc11-646c80368716',\n",
       " '94ab9da5-5b83-11ef-b63b-646c80368716',\n",
       " '94ab9da6-5b83-11ef-9e20-646c80368716',\n",
       " '94ab9da7-5b83-11ef-8175-646c80368716',\n",
       " '94ab9da8-5b83-11ef-a08e-646c80368716',\n",
       " '94ab9da9-5b83-11ef-a620-646c80368716',\n",
       " '94ab9daa-5b83-11ef-a1e1-646c80368716',\n",
       " '94ab9dab-5b83-11ef-b6df-646c80368716',\n",
       " '94ab9dac-5b83-11ef-8deb-646c80368716',\n",
       " '94ab9dad-5b83-11ef-8be1-646c80368716',\n",
       " '94ab9dae-5b83-11ef-8803-646c80368716',\n",
       " '94ab9daf-5b83-11ef-86bd-646c80368716',\n",
       " '94ab9db0-5b83-11ef-a87b-646c80368716',\n",
       " '94ab9db1-5b83-11ef-8f4d-646c80368716',\n",
       " '94ab9db2-5b83-11ef-8866-646c80368716',\n",
       " '94ab9db3-5b83-11ef-8404-646c80368716',\n",
       " '94ab9db4-5b83-11ef-8694-646c80368716',\n",
       " '94ab9db5-5b83-11ef-a5bc-646c80368716',\n",
       " '94ab9db6-5b83-11ef-aaa6-646c80368716',\n",
       " '94ab9db7-5b83-11ef-8758-646c80368716',\n",
       " '94ab9db8-5b83-11ef-a39c-646c80368716',\n",
       " '94ab9db9-5b83-11ef-8d97-646c80368716',\n",
       " '94ab9dba-5b83-11ef-a323-646c80368716',\n",
       " '94ab9dbb-5b83-11ef-bc33-646c80368716',\n",
       " '94ab9dbc-5b83-11ef-afd6-646c80368716',\n",
       " '94ab9dbd-5b83-11ef-9d22-646c80368716',\n",
       " '94ab9dbe-5b83-11ef-a8c2-646c80368716',\n",
       " '94ab9dbf-5b83-11ef-bfea-646c80368716',\n",
       " '94ab9dc0-5b83-11ef-848f-646c80368716',\n",
       " '94ab9dc1-5b83-11ef-ad10-646c80368716',\n",
       " '94ab9dc2-5b83-11ef-b46b-646c80368716',\n",
       " '94ab9dc3-5b83-11ef-841b-646c80368716',\n",
       " '94ab9dc4-5b83-11ef-9cc9-646c80368716',\n",
       " '94ab9dc5-5b83-11ef-b6b7-646c80368716',\n",
       " '94ab9dc6-5b83-11ef-8595-646c80368716',\n",
       " '94ab9dc7-5b83-11ef-b44e-646c80368716',\n",
       " '94ab9dc8-5b83-11ef-a793-646c80368716',\n",
       " '94ab9dc9-5b83-11ef-ae14-646c80368716',\n",
       " '94ab9dca-5b83-11ef-86be-646c80368716',\n",
       " '94ab9dcb-5b83-11ef-908c-646c80368716',\n",
       " '94ab9dcc-5b83-11ef-a439-646c80368716',\n",
       " '94ab9dcd-5b83-11ef-988c-646c80368716',\n",
       " '94ab9dce-5b83-11ef-819c-646c80368716',\n",
       " '94ab9dcf-5b83-11ef-95a9-646c80368716',\n",
       " '94ab9dd0-5b83-11ef-8662-646c80368716',\n",
       " '94ab9dd1-5b83-11ef-a8b1-646c80368716',\n",
       " '94ab9dd2-5b83-11ef-876f-646c80368716',\n",
       " '94ab9dd3-5b83-11ef-8a5a-646c80368716',\n",
       " '94ab9dd4-5b83-11ef-b7b4-646c80368716',\n",
       " '94ab9dd5-5b83-11ef-b89b-646c80368716',\n",
       " '94ab9dd6-5b83-11ef-b680-646c80368716',\n",
       " '94ab9dd7-5b83-11ef-ac26-646c80368716',\n",
       " '94ab9dd8-5b83-11ef-9790-646c80368716',\n",
       " '94ab9dd9-5b83-11ef-8a49-646c80368716',\n",
       " '94ab9dda-5b83-11ef-b658-646c80368716',\n",
       " '94ab9ddb-5b83-11ef-944d-646c80368716',\n",
       " '94ab9ddc-5b83-11ef-8c4c-646c80368716',\n",
       " '94ab9ddd-5b83-11ef-be60-646c80368716',\n",
       " '94ab9dde-5b83-11ef-a2b5-646c80368716',\n",
       " '94ab9ddf-5b83-11ef-bab5-646c80368716',\n",
       " '94ab9de0-5b83-11ef-a3e6-646c80368716',\n",
       " '94ab9de1-5b83-11ef-8845-646c80368716',\n",
       " '94ab9de2-5b83-11ef-bf38-646c80368716',\n",
       " '94ab9de3-5b83-11ef-b214-646c80368716',\n",
       " '94ab9de4-5b83-11ef-90b6-646c80368716',\n",
       " '94ab9de5-5b83-11ef-851c-646c80368716',\n",
       " '94ab9de6-5b83-11ef-b556-646c80368716',\n",
       " '94ab9de7-5b83-11ef-98cd-646c80368716',\n",
       " '94ab9de8-5b83-11ef-9685-646c80368716',\n",
       " '94ab9de9-5b83-11ef-8de5-646c80368716',\n",
       " '94ab9dea-5b83-11ef-8b74-646c80368716',\n",
       " '94ab9deb-5b83-11ef-879d-646c80368716',\n",
       " '94ab9dec-5b83-11ef-a595-646c80368716',\n",
       " '94ab9ded-5b83-11ef-8e1c-646c80368716',\n",
       " '94ab9dee-5b83-11ef-ab6d-646c80368716',\n",
       " '94ab9def-5b83-11ef-8b88-646c80368716',\n",
       " '94ab9df0-5b83-11ef-a0b5-646c80368716',\n",
       " '94ab9df1-5b83-11ef-9891-646c80368716',\n",
       " '94ab9df2-5b83-11ef-829c-646c80368716',\n",
       " '94ab9df3-5b83-11ef-a5ab-646c80368716',\n",
       " '94ab9df4-5b83-11ef-bf3b-646c80368716',\n",
       " '94ab9df5-5b83-11ef-8698-646c80368716',\n",
       " '94ab9df6-5b83-11ef-b7ec-646c80368716',\n",
       " '94ab9df7-5b83-11ef-98ea-646c80368716',\n",
       " '94ab9df8-5b83-11ef-be89-646c80368716',\n",
       " '94ab9df9-5b83-11ef-bbe3-646c80368716',\n",
       " '94ab9dfa-5b83-11ef-8897-646c80368716',\n",
       " '94ab9dfb-5b83-11ef-ade8-646c80368716',\n",
       " '94ab9dfc-5b83-11ef-b43c-646c80368716',\n",
       " '94ab9dfd-5b83-11ef-a63c-646c80368716',\n",
       " '94ab9dfe-5b83-11ef-a8b0-646c80368716',\n",
       " '94ab9dff-5b83-11ef-ae1b-646c80368716',\n",
       " '94ab9e00-5b83-11ef-b8d7-646c80368716',\n",
       " '94ab9e01-5b83-11ef-ba1b-646c80368716',\n",
       " '94ab9e02-5b83-11ef-8061-646c80368716',\n",
       " '94ab9e03-5b83-11ef-9955-646c80368716',\n",
       " '94ab9e04-5b83-11ef-b395-646c80368716',\n",
       " '94ab9e05-5b83-11ef-8d51-646c80368716',\n",
       " '94ab9e06-5b83-11ef-8c34-646c80368716',\n",
       " '94ab9e07-5b83-11ef-bba6-646c80368716',\n",
       " '94ab9e08-5b83-11ef-8a9c-646c80368716',\n",
       " '94ab9e09-5b83-11ef-99a9-646c80368716',\n",
       " '94ab9e0a-5b83-11ef-a10c-646c80368716',\n",
       " '94ab9e0b-5b83-11ef-a310-646c80368716',\n",
       " '94ab9e0c-5b83-11ef-b945-646c80368716',\n",
       " '94ab9e0d-5b83-11ef-8c74-646c80368716',\n",
       " '94ab9e0e-5b83-11ef-814f-646c80368716',\n",
       " '94ab9e0f-5b83-11ef-a65f-646c80368716',\n",
       " '94ab9e10-5b83-11ef-b8ba-646c80368716',\n",
       " '94ab9e11-5b83-11ef-b6af-646c80368716',\n",
       " '94ab9e12-5b83-11ef-b73f-646c80368716',\n",
       " '94ab9e13-5b83-11ef-ba16-646c80368716',\n",
       " '94ab9e14-5b83-11ef-a8ea-646c80368716',\n",
       " '94ab9e15-5b83-11ef-b13c-646c80368716',\n",
       " '94ab9e16-5b83-11ef-8a8c-646c80368716',\n",
       " '94ab9e17-5b83-11ef-a64e-646c80368716',\n",
       " '94ab9e18-5b83-11ef-92ea-646c80368716',\n",
       " '94ab9e19-5b83-11ef-8289-646c80368716',\n",
       " '94ab9e1a-5b83-11ef-9530-646c80368716',\n",
       " '94ab9e1b-5b83-11ef-9f8c-646c80368716',\n",
       " '94ab9e1c-5b83-11ef-8043-646c80368716',\n",
       " '94ab9e1d-5b83-11ef-9bf8-646c80368716',\n",
       " '94ab9e1e-5b83-11ef-8fdf-646c80368716',\n",
       " '94ab9e1f-5b83-11ef-9e32-646c80368716',\n",
       " '94ab9e20-5b83-11ef-b9ec-646c80368716',\n",
       " '94ab9e21-5b83-11ef-be9d-646c80368716',\n",
       " '94ab9e22-5b83-11ef-a232-646c80368716',\n",
       " '94ab9e23-5b83-11ef-a26e-646c80368716',\n",
       " '94ab9e24-5b83-11ef-8549-646c80368716',\n",
       " '94ab9e25-5b83-11ef-98b6-646c80368716',\n",
       " '94ab9e26-5b83-11ef-9036-646c80368716',\n",
       " '94ab9e27-5b83-11ef-b391-646c80368716',\n",
       " '94ab9e28-5b83-11ef-8156-646c80368716',\n",
       " '94ab9e29-5b83-11ef-8789-646c80368716',\n",
       " '94ab9e2a-5b83-11ef-adbf-646c80368716',\n",
       " '94ab9e2b-5b83-11ef-b089-646c80368716',\n",
       " '94ab9e2c-5b83-11ef-976d-646c80368716',\n",
       " '94ab9e2d-5b83-11ef-995d-646c80368716',\n",
       " '94ab9e2e-5b83-11ef-9e2a-646c80368716',\n",
       " '94ab9e2f-5b83-11ef-9401-646c80368716',\n",
       " '94ab9e30-5b83-11ef-8261-646c80368716',\n",
       " '94ab9e31-5b83-11ef-81c7-646c80368716',\n",
       " '94ab9e32-5b83-11ef-8d43-646c80368716',\n",
       " '94ab9e33-5b83-11ef-b20a-646c80368716',\n",
       " '94ab9e34-5b83-11ef-8fa0-646c80368716',\n",
       " '94ab9e35-5b83-11ef-8413-646c80368716',\n",
       " '94ab9e36-5b83-11ef-8874-646c80368716',\n",
       " '94ab9e37-5b83-11ef-827e-646c80368716',\n",
       " '94ab9e38-5b83-11ef-a567-646c80368716',\n",
       " '94ab9e39-5b83-11ef-b6d4-646c80368716',\n",
       " '94ab9e3a-5b83-11ef-a0d8-646c80368716',\n",
       " '94ab9e3b-5b83-11ef-9ca5-646c80368716',\n",
       " '94ab9e3c-5b83-11ef-a2a3-646c80368716',\n",
       " '94ab9e3d-5b83-11ef-ab21-646c80368716',\n",
       " '94ab9e3e-5b83-11ef-8198-646c80368716',\n",
       " '94ab9e3f-5b83-11ef-8d32-646c80368716',\n",
       " '94ab9e40-5b83-11ef-9395-646c80368716',\n",
       " '94ab9e41-5b83-11ef-9ee0-646c80368716',\n",
       " '94ab9e42-5b83-11ef-8ecf-646c80368716',\n",
       " '94ab9e43-5b83-11ef-a03d-646c80368716',\n",
       " '94ab9e44-5b83-11ef-88c4-646c80368716',\n",
       " '94ab9e45-5b83-11ef-b0c1-646c80368716',\n",
       " '94ab9e46-5b83-11ef-bc3f-646c80368716',\n",
       " '94ab9e47-5b83-11ef-84fd-646c80368716',\n",
       " '94ab9e48-5b83-11ef-9737-646c80368716',\n",
       " '94ab9e49-5b83-11ef-877f-646c80368716',\n",
       " '94ab9e4a-5b83-11ef-9a69-646c80368716',\n",
       " '94ab9e4b-5b83-11ef-a768-646c80368716',\n",
       " '94ab9e4c-5b83-11ef-bb02-646c80368716',\n",
       " '94ab9e4d-5b83-11ef-9253-646c80368716',\n",
       " '94ab9e4e-5b83-11ef-84e2-646c80368716',\n",
       " '94ab9e4f-5b83-11ef-a5db-646c80368716',\n",
       " '94ab9e50-5b83-11ef-b74d-646c80368716',\n",
       " '94ab9e51-5b83-11ef-9616-646c80368716',\n",
       " '94ab9e52-5b83-11ef-8fca-646c80368716',\n",
       " '94ab9e53-5b83-11ef-83b4-646c80368716',\n",
       " '94ab9e54-5b83-11ef-8b4c-646c80368716',\n",
       " '94ab9e55-5b83-11ef-97eb-646c80368716',\n",
       " '94ab9e56-5b83-11ef-82be-646c80368716',\n",
       " '94ab9e57-5b83-11ef-8e13-646c80368716',\n",
       " '94ab9e58-5b83-11ef-bc4f-646c80368716',\n",
       " '94ab9e59-5b83-11ef-a447-646c80368716',\n",
       " '94ab9e5a-5b83-11ef-9cfd-646c80368716',\n",
       " '94ab9e5b-5b83-11ef-85cc-646c80368716',\n",
       " '94ab9e5c-5b83-11ef-8be0-646c80368716',\n",
       " '94ab9e5d-5b83-11ef-919d-646c80368716',\n",
       " '94ab9e5e-5b83-11ef-a722-646c80368716',\n",
       " '94ab9e5f-5b83-11ef-b9fe-646c80368716',\n",
       " '94ab9e60-5b83-11ef-a35d-646c80368716',\n",
       " '94ab9e61-5b83-11ef-971a-646c80368716',\n",
       " '94ab9e62-5b83-11ef-a580-646c80368716',\n",
       " '94ab9e63-5b83-11ef-9463-646c80368716',\n",
       " '94ab9e64-5b83-11ef-905f-646c80368716',\n",
       " '94ab9e65-5b83-11ef-8589-646c80368716',\n",
       " '94ab9e66-5b83-11ef-b137-646c80368716',\n",
       " '94ab9e67-5b83-11ef-89e6-646c80368716',\n",
       " '94ab9e68-5b83-11ef-8ce1-646c80368716',\n",
       " '94ab9e69-5b83-11ef-983c-646c80368716',\n",
       " '94ab9e6a-5b83-11ef-9a0a-646c80368716',\n",
       " '94ab9e6b-5b83-11ef-8c0e-646c80368716',\n",
       " '94ab9e6c-5b83-11ef-bc55-646c80368716',\n",
       " '94ab9e6d-5b83-11ef-9321-646c80368716',\n",
       " '94ab9e6e-5b83-11ef-852b-646c80368716',\n",
       " '94ab9e6f-5b83-11ef-892b-646c80368716',\n",
       " '94ab9e70-5b83-11ef-a0a1-646c80368716',\n",
       " '94ab9e71-5b83-11ef-bc0a-646c80368716',\n",
       " '94ab9e72-5b83-11ef-b48e-646c80368716',\n",
       " '94ab9e73-5b83-11ef-af20-646c80368716',\n",
       " '94ab9e74-5b83-11ef-a4cc-646c80368716',\n",
       " '94ab9e75-5b83-11ef-bdf5-646c80368716',\n",
       " '94ab9e76-5b83-11ef-8c95-646c80368716',\n",
       " '94ab9e77-5b83-11ef-ac08-646c80368716',\n",
       " '94ab9e78-5b83-11ef-8c96-646c80368716',\n",
       " '94ab9e79-5b83-11ef-8936-646c80368716',\n",
       " '94ab9e7a-5b83-11ef-97d2-646c80368716',\n",
       " '94ab9e7b-5b83-11ef-9ea6-646c80368716',\n",
       " '94ab9e7c-5b83-11ef-aa0a-646c80368716',\n",
       " '94ab9e7d-5b83-11ef-9ffe-646c80368716',\n",
       " '94ab9e7e-5b83-11ef-b26e-646c80368716',\n",
       " '94ab9e7f-5b83-11ef-bcbc-646c80368716',\n",
       " '94ab9e80-5b83-11ef-a15b-646c80368716',\n",
       " '94ab9e81-5b83-11ef-84d2-646c80368716',\n",
       " '94ab9e82-5b83-11ef-9cd6-646c80368716',\n",
       " '94ab9e83-5b83-11ef-b666-646c80368716',\n",
       " '94ab9e84-5b83-11ef-a3c4-646c80368716',\n",
       " '94ab9e85-5b83-11ef-af27-646c80368716',\n",
       " '94ab9e86-5b83-11ef-927c-646c80368716',\n",
       " '94ab9e87-5b83-11ef-9a40-646c80368716',\n",
       " '94ab9e88-5b83-11ef-9f56-646c80368716']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.add_documents(docs_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1825fc9f-7fdb-4830-b2b5-6b2b9ae701f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone Vector Store\n",
    "index_name = \"cohere-llmu-data\"\n",
    "pc = Pinecone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e0b7f6-ac88-4f3e-98f4-930f69ec0756",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77adbd64-7b61-4ecb-aa5c-9bad504977ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ai-assistant', 'lang-sample-2', 'langchain-demo']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1361822-b238-4d2d-b323-38f1e99038ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1024,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b677781-f757-4b82-a03b-4358ccc5d3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.data.index.Index at 0x23b4db84c70>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5197cc-73c6-4d6e-b8ea-4889d717be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PineconeVectorStore(index=index, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb0fd7b-3253-4c0d-95de-ce949b208da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a82f3981-af02-40a7-9eef-4665dba0f262',\n",
       " '7431654f-5db4-44a8-9b86-882f48d8786c',\n",
       " '9c6d9200-f171-4e81-9e2d-c8ac4f8e71f5',\n",
       " '1ed6ae39-ab3d-4990-ad2a-886be7ebd668',\n",
       " '48c88946-99ae-463a-9fd1-533b14699bd7',\n",
       " '7c77c8fa-ce78-4390-b074-2b07b0d63ee7',\n",
       " '20abf1ce-e808-4344-a7b8-f77d87f4982c',\n",
       " 'b8174db5-86de-4cb8-8c48-21d63e5d6342',\n",
       " '261ea28c-131b-4234-96e5-aef418468d0b',\n",
       " '06ce9803-ecea-473e-8421-cb44f4cce324',\n",
       " 'a505b81e-cc03-4423-9a8b-711ed63ea3b0',\n",
       " '50f71b5d-ed42-4e96-8853-ca79165860c8',\n",
       " '67842fdb-2d4a-4bf3-a3fb-ca8b17cb9d07',\n",
       " 'd30baf30-544d-4f05-be3d-63a18ecd5f20',\n",
       " '3155bac2-70b3-495e-94a9-3d0dff3cde13',\n",
       " '1edddef1-ff47-45a6-9b29-963c894fb83d',\n",
       " '63b1c7f1-21e5-45a3-bc71-96579f9ecc68',\n",
       " '8b824615-a457-421f-872b-d0ba0e0023e8',\n",
       " '888d46cc-70ef-439a-a534-df5be0f03193',\n",
       " 'f562eada-36fe-407c-9f82-90db8668590e',\n",
       " '125ec80d-c33c-4094-bcce-3d86dbc2f91d',\n",
       " '8b1c9922-6925-4956-92d8-bfec306e5dd1',\n",
       " '59c99c8b-ed22-4c92-afdf-84cfbd0550af',\n",
       " '91cddbe1-0a9a-4f51-af0c-5d245906f7d6',\n",
       " '44fb4fc8-d44f-494f-8a39-59e178011128',\n",
       " 'e2792596-cd91-4315-bdbd-6aa6adf919ec',\n",
       " 'a382d408-6824-4d8f-9672-19f03027301b',\n",
       " '7266828e-5c28-41e8-8a05-8856d2806af6',\n",
       " 'd772f8ee-bba3-4092-93a7-b33f80730686',\n",
       " 'dd210900-30ec-4df1-8191-670765eae95a',\n",
       " '75b7f3e8-1434-435e-b58c-f9ce752b6b3b',\n",
       " '3667f4ef-9063-4837-bf9b-6a76ccd97cf2',\n",
       " 'b4cb312f-66d5-4d6c-8ce7-a4cf8cec8985',\n",
       " 'fa887e3c-e77a-477d-9fec-b0c8c65f06a8',\n",
       " 'a4b4c2d8-abbe-4984-899a-70e99c0f1486',\n",
       " '864a775c-1f4b-45b1-808d-aece48254889',\n",
       " 'f4c588ae-f8d3-4d70-b24a-1aba1ac099f6',\n",
       " '5637ea72-c109-420c-9162-f8da48618796',\n",
       " '6db30ddd-b9bd-4913-a235-8c3231a80886',\n",
       " 'd7648487-88a1-47ac-81fd-ffc8bf5dfb89',\n",
       " '3c396165-d0f8-42db-83a0-61e032d98287',\n",
       " '9b51e7b8-2fda-46e2-ac73-877ad41d8ea3',\n",
       " '68453216-cbb4-460d-b2e8-2c60be354726',\n",
       " '25fb68aa-df7d-4135-aa6c-4983c824b415',\n",
       " '35aeaaa0-d59a-488f-a1a8-3f16361f9745',\n",
       " '613e1fb4-e100-430c-95d6-5eac090ba097',\n",
       " '8c51a219-8557-49a7-9bb1-d76ffa6e6912',\n",
       " '942a5f8a-0d05-476e-9d0d-467c6c89bb4b',\n",
       " '2d78e3fc-2674-4901-9838-5f293faa9d9e',\n",
       " 'a4a03bd7-ebbc-42c7-8b3d-a1d3140ca77b',\n",
       " '87e7334c-ec7f-4d08-9142-3a17cfc07bc7',\n",
       " '6bd9748e-ce56-4ee9-a5a0-a1753be96d56',\n",
       " '621579e6-df92-40ed-964b-32cecad6ecdd',\n",
       " '78be283b-901f-4c8d-9aec-5fa8f7c167b4',\n",
       " '0d29b062-fa7e-4494-8deb-cf770c11ea28',\n",
       " '3233eb9b-e66f-4ba9-8c7d-1a063ec6236c',\n",
       " 'bb36927c-3c26-496f-b20a-9f9047b47932',\n",
       " '71003a9a-4fb4-4073-bb5d-2e9b363a9f7e',\n",
       " 'c8a4f23e-9a29-4de9-952f-d3e935fa90d1',\n",
       " 'e1b8aa5a-85f9-48e7-adaa-4cf74cf04bc0',\n",
       " '8871d13c-5d92-4e2a-9635-680a300dce0f',\n",
       " '751bca69-7880-4440-9f49-4a8507f9c41b',\n",
       " 'a1d3d0ec-0cc2-462f-8d3c-e32f4368ccb7',\n",
       " '6e625e7e-7652-48ea-ab57-d51ef3db4550',\n",
       " '37876f41-e179-40ad-a0e7-8ad6944d8818',\n",
       " '531eab37-5e48-40b1-84f2-279d10989aa2',\n",
       " 'b2803773-3f98-47d4-b4ff-fb48b7c9574f',\n",
       " '5957dadc-d995-4f51-a2c0-8863ee24d1f9',\n",
       " 'c72259cd-3abe-400c-aa7c-ad8576af245a',\n",
       " '001cbcb5-3d96-41ce-8f1a-5a8e02402af0',\n",
       " '4f6a631c-dee4-4c09-b1b5-654679349b8e',\n",
       " 'bec8f056-abd9-4f08-be41-9ced6b6f7f18',\n",
       " '78edef13-c870-45ae-ad32-ca2de81a8486',\n",
       " '50f68294-632a-4cf7-8212-1f86f4be2a91',\n",
       " 'ca4a8e28-dcd8-40f1-8634-d2a2174473a0',\n",
       " 'c4e4045f-92a5-48f7-bbb7-899fc6c923b7',\n",
       " 'bf5a6cc3-ee8f-45b7-a07d-d6b7430698d2',\n",
       " 'c936f108-ecbc-4ab3-801f-b67fc19295ae',\n",
       " '8387ddeb-318b-4303-9337-d7b28dd66844',\n",
       " 'b6538d92-0a4b-4d1c-b160-b07aa87d4151',\n",
       " '869586d1-e995-46a1-8ec5-31eb5362d359',\n",
       " '2e2c0b95-d825-4fa4-b9d8-38103e19b38a',\n",
       " '02ef82d6-6e5c-4447-a83a-899a94bfb40c',\n",
       " 'c60c3778-4744-4d42-8f2b-62347226a4be',\n",
       " 'fcaf8bab-b733-4d94-99ce-85d7ae52d3f5',\n",
       " 'b8c8f4c9-a0d2-4552-a8e9-ba530adbba5b',\n",
       " '94016e3b-07a1-4c88-b10e-c46d2e5db6eb',\n",
       " 'fa2721c3-599a-4cff-a03f-9dbaf45e4ed6',\n",
       " 'd906690a-a74a-43c5-805a-8b14b08f62d3',\n",
       " 'ab43ebf4-c84d-4d44-b01e-8f9302dfb76d',\n",
       " '6b81f240-3fc7-4b6a-a99e-90de38ded8e2',\n",
       " 'd82a44e6-866b-4c28-b81d-fe1272866bce',\n",
       " '44f6dd0a-3dd1-427f-a3dc-6803a3fed2ea',\n",
       " '2e41df6f-f856-4d21-9967-8fcb8ee6d2b1',\n",
       " '13c1345b-ea38-467a-912e-71af7feb4d0d',\n",
       " 'fa2627b2-3295-4b5d-b2ce-56bd4fe6bac9',\n",
       " '06be95e4-f3dc-4674-aeba-def4fd224710',\n",
       " '5f3c81b1-7214-4ece-ae94-f67d98450968',\n",
       " 'f8614a06-913f-4fae-a6df-9fa96bcfa6cd',\n",
       " '9a28f608-7ef4-473c-af9c-7e158519a432',\n",
       " '823c8a04-a55e-455a-b3c2-be207d3e9ca7',\n",
       " '861a6d02-f7f3-4112-b99e-7491d1db583d',\n",
       " '0454fa61-5237-4bd8-b3bd-67689780bc5b',\n",
       " 'e8592bc4-1fd2-4b44-8182-1d0d4d3dbaaf',\n",
       " '9e371ad6-dd69-40e6-b34f-4433e818e2b1',\n",
       " '69425005-b025-4d6a-8ec9-c6c2581fa018',\n",
       " 'f5aa6e80-d97d-4bf2-99c2-1d215b31e2ec',\n",
       " '40097e35-e882-4117-82dd-5c3ad823a863',\n",
       " 'e41d2bdd-bc0a-4c7c-b4c8-a563a17c629a',\n",
       " '7f3b0f96-7f3f-4950-b2c8-c3279042cbc3',\n",
       " '0749de71-637f-452e-9033-cea3d2ce218a',\n",
       " '4d89ee83-4311-46d0-acc1-e9814fa4578c',\n",
       " '4052da72-f5dd-4cc5-9345-2f317f199d89',\n",
       " 'bb4004c0-3665-4ef8-aec4-73fecd9c09ab',\n",
       " '82fe3c19-722d-4c18-b824-35be3117f8ce',\n",
       " '474541b0-a541-45c7-a1af-1f5fcf4536fd',\n",
       " 'cee27c5d-a5f1-4511-ab5e-8bdce9a4fe7b',\n",
       " '73872846-58da-4e4b-8985-5a2b992b4774',\n",
       " '1a8d6101-0199-4d7a-9fbf-4ec6e8face45',\n",
       " 'c1ff6963-37cf-43f1-97c9-60b3ff5ee96c',\n",
       " '0661ec31-8748-43f1-99c5-5f47c122012c',\n",
       " '455199a4-3913-4307-b538-75c1862989cb',\n",
       " 'ec28306e-e961-4b42-9b75-f6579c0e72ae',\n",
       " '3a95ab12-7fd6-495e-9d7b-855eaeb0a609',\n",
       " '20f224cb-69b4-4c07-98f7-ee17c6cbec87',\n",
       " 'befa6fde-afec-4404-99bc-846c86e9ae08',\n",
       " '14690afb-561e-435a-ae53-46736bb2c610',\n",
       " '0394cf4f-ec45-4544-bed5-8c3357c3c0f8',\n",
       " '7aa2246f-b78d-4fd4-a227-a49d57e29aa8',\n",
       " 'd52f4ade-ff5d-4514-9743-794512bde262',\n",
       " '3e3b4bd6-92d3-41a6-ae9c-48b1134ac689',\n",
       " '42278c4b-d080-47ca-8c5d-1e0678f8e0b0',\n",
       " '92738f27-06f4-46cb-8cbe-30f5b7364e1f',\n",
       " 'beaedd5c-cdf9-4a99-a8c5-1810309bf1a8',\n",
       " '07729548-7d31-46d0-af2d-a44365f824a5',\n",
       " 'ceed1c4b-5c28-4326-9725-e27d0c1b20a5',\n",
       " '48d54430-032f-4e44-a916-f91efcbaf703',\n",
       " 'f01565ab-7c99-431d-9baa-4848e22e1906',\n",
       " '5a7c90f8-da27-441c-8cb3-04853c32ddc5',\n",
       " '8e308313-4b2d-4334-a99b-952a7a760763',\n",
       " '7e0a72b6-fcec-45cb-a123-901bd2183999',\n",
       " '7922ea23-8f8a-4a8d-962c-7b0992736d71',\n",
       " 'fef77649-8985-4ee3-bda8-c1a11026e075',\n",
       " '0d5c25bf-5fcc-4021-81a9-e8e8e68f4468',\n",
       " 'dbd9a8fe-0ed0-4fce-a892-19b3924c606c',\n",
       " 'c2de9566-b498-455a-be46-9b4cc88820ff',\n",
       " '7aa45a70-efb2-4261-8dbd-8100f747087f',\n",
       " '9291949b-2f14-44f6-9dd5-aa848c9b9f04',\n",
       " '55429a35-3a06-4f58-9dc0-a56622c017d6',\n",
       " '9631efe1-17c5-4e3c-b5aa-edd6e4ac5d91',\n",
       " '8d2f02b4-f363-4296-94f5-1c37b47e7c5a',\n",
       " '84643e64-9f0c-47a3-9091-58f42c917f41',\n",
       " '42cf7429-2ece-4be9-9e20-fa4291a2bc67',\n",
       " '18522fa9-8ce4-4361-ab4b-e1b876be4cb1',\n",
       " '5b5dc6fd-ba6a-44df-8980-bc5fb129edf6',\n",
       " '940f3b17-548f-4bb0-b21d-07f47efd5fcb',\n",
       " '59302795-2733-4992-b87b-701bc21b03ac',\n",
       " 'e494ac48-d402-4ba3-ac06-168129fd0e73',\n",
       " '9204e1f3-c773-4a40-9a2d-bf12af304bf6',\n",
       " '9cd59502-89b0-46a2-acc3-6b0f9551dfbe',\n",
       " 'aa47a5a1-919e-4757-a036-4713dde81176',\n",
       " '56c9493e-ffec-45d7-8c65-c9e077afc316',\n",
       " '2b859dd2-5fee-4700-abfc-c4ab0b92dc78',\n",
       " '4a357699-1272-4e9f-87c3-4bf524fd0bf3',\n",
       " 'e9899c20-896b-4c7b-8f40-81244b5bf533',\n",
       " '777859a0-3296-4e7a-a5c8-91355256a498',\n",
       " 'a9ea3874-dec7-46a7-a3dd-d1117fde088d',\n",
       " '0b1fd2d5-15c6-4eba-8382-07478dd4540c',\n",
       " '78ac6915-9227-4aac-bb34-a10ba3d3faee',\n",
       " '3eceae38-1548-40ea-a696-e3cbba2a443a',\n",
       " '6c9e8e0e-8778-43ac-8620-0abe1cf59a2f',\n",
       " '40a9ac11-790f-4c06-bf2d-c8b45292bf6a',\n",
       " '280e419e-e506-41b0-89b3-1a6ed945f674',\n",
       " '8720cf88-cf96-4952-abd5-1dedd68f2622',\n",
       " '71d3b40a-c6d7-4b80-9ca7-6c6b73c22261',\n",
       " '1998062b-54c9-4217-864c-66a3c136b8cc',\n",
       " '52804c34-f29c-4831-8338-d9d49beefba2',\n",
       " '91f5ad95-9e17-4def-b07e-05fd87d710b0',\n",
       " 'e0271cf0-fb66-4f7a-ad93-f735233bf7d6',\n",
       " '81204643-d747-4bdc-a179-00b17fcfce94',\n",
       " 'd748169d-4b2a-490a-aaef-eabeb5b713e5',\n",
       " '68819b28-e066-4440-9ccb-e7d6525fc656',\n",
       " '9c454267-4eb3-4990-8f2b-4e8ead2bbe15',\n",
       " '0dd8a409-91b7-43e6-85e6-8aef98c67cae',\n",
       " '6789ccb2-14c1-48f7-a38c-5e849e9bcdeb',\n",
       " '21fee994-b3ac-4792-b8aa-13a7825ad0e9',\n",
       " '326fed59-920a-4e34-ae07-58a5834a80b1',\n",
       " '8bc204c9-c343-4332-9731-6975fabb569c',\n",
       " '9e981f1a-ca7f-411f-a407-e2d5b783764f',\n",
       " '09a04cd6-2fdd-4ee2-a5e3-f29450d9b087',\n",
       " 'ca533faa-babb-4c67-95e7-7eade6e06758',\n",
       " 'a4a8b005-8a34-447b-8d88-c4a6c110da78',\n",
       " '4807e0a4-9c27-45cf-a50e-ba00f823d6d1',\n",
       " '448fb2b4-7793-47e8-a8e8-3d03ab30e3c9',\n",
       " 'ecebcd7f-9efd-41b6-9a9a-5fd702b34e77',\n",
       " 'de5ca400-47a8-4f1c-a3ff-d5b53bf329ae',\n",
       " '60e03d1d-7bfa-46ea-85e9-7c610feddecd',\n",
       " '660c4512-327d-40f7-818e-d2bd02a07ac3',\n",
       " 'bb0c9f9c-722c-4fc8-a190-e726a884cbb5',\n",
       " '060a53b5-1061-4fd9-a818-bc0a0a2eae25',\n",
       " '09ab7045-a452-4fa8-b84c-4f42981a865a',\n",
       " '74dc89c2-1d88-4b21-ae65-6d153ea9e04d',\n",
       " '943914a5-46bc-4866-85f0-4f57504e6da5',\n",
       " 'e6fd6fb4-ae74-477a-bb54-6d6a0302b990',\n",
       " '299d0818-7756-449b-af2e-41b4a29a3c68',\n",
       " '3600e427-2e09-4976-9ed4-1e77be2d16b9',\n",
       " '9adb0afb-f43b-4173-b8e9-eb7d46f15a07',\n",
       " '46984b1f-23ba-43b5-aaf4-7d8514fd8755',\n",
       " 'c7eee69b-95a0-48c6-91b9-9cf21c11affc',\n",
       " 'b146d19e-581e-48e3-b062-434d9fcc6b1c',\n",
       " 'aafd4680-2776-46fa-89e1-3075fa646758',\n",
       " '32648382-d5b3-4359-a6a8-c44ceacd1701',\n",
       " '0e2fc576-9e7c-478c-8de9-94beea745182',\n",
       " 'e4d6d1d1-6a87-4eea-b41e-b7bea1899ae8',\n",
       " '7a2ff077-0954-4dda-96f4-ea94fa480fc1',\n",
       " '16354953-5943-4f06-961b-a6390e9466e8',\n",
       " '67c14907-f463-400f-9691-1de6bfa3e3ff',\n",
       " 'f70a149e-88e4-4096-9a5d-5da1423801a4',\n",
       " 'cc5af561-3ec3-4f0d-9025-4b89f43c6c9a',\n",
       " '94a2b847-3d5d-4bef-ba57-aee74e332b2e',\n",
       " 'dcdbc2df-4be6-44d1-9058-122ae72fe573',\n",
       " '7b20178e-b1a1-42ad-9dfa-27f211929b0f',\n",
       " 'e7b81007-1b26-4085-b733-65200a920239',\n",
       " 'd12881fc-4961-4c7f-9b49-a1038b27339d',\n",
       " '63b6ad5b-6829-49c2-a00d-fdea49696592',\n",
       " '2f79f507-7c32-4c3b-8bc6-fbc4620b4791',\n",
       " '71d094f2-c04a-4dd0-bc7d-2c756466491e',\n",
       " '7c2be729-e07a-4e86-b89d-763adc891c3b',\n",
       " '85c5259f-0c0d-4e32-9d6e-cc3a8f0b89b6',\n",
       " 'df9ae422-d666-45db-8470-74e8dafa94dd',\n",
       " 'bb3954a2-4b5c-47af-af22-34cc8d864a08',\n",
       " '62cbae55-cc14-484e-850f-1d7148027453',\n",
       " 'efe5edb6-3891-4287-a481-0ac392b1a068',\n",
       " '875b4411-7cb1-48e1-a0fd-2dac757a2f7b',\n",
       " '0801190e-5e02-4d0a-83eb-649e06de78d0',\n",
       " 'e423019a-5dd7-44f7-895f-1ec7c5ff4699',\n",
       " '4dca3cd7-f919-472e-9eff-205d5f743f50',\n",
       " 'e9e010b4-0b18-411c-8fd8-6ad14b99d8f1',\n",
       " 'f6a486aa-daae-4c30-9c21-2cca754b599e',\n",
       " 'f291538c-6d3d-494f-b93c-3efb6236a3e5',\n",
       " '8a5d06f0-fca2-4697-81f7-4c6f1bb30be4',\n",
       " 'aeb782be-9501-4c7f-a1a6-df9b7410c1de',\n",
       " '74392e10-b740-4cbd-85ca-2c18ee5aca0a',\n",
       " 'efab0410-8e65-4ee0-96e9-29ece72a52d7',\n",
       " '102d1bf5-01cb-43f4-bcfc-1ca2a2bd1c57',\n",
       " 'd640c67b-6247-48ad-9186-fa6902fbc24d',\n",
       " '1fa7ba7f-f8ac-4a94-8e80-2bbc18e3bb49',\n",
       " '7e8626e1-3818-45c9-9fc8-f13e100a77a8',\n",
       " '56246b8e-1c21-4c51-b1e0-46e1d7f4c333',\n",
       " '9e764334-c6f3-4a44-b1aa-ff57444f5a72',\n",
       " 'f7edd23c-fbc3-4de6-9231-1c268e73ab66',\n",
       " 'ecba552f-0ebc-467f-b901-b8ad385a2843',\n",
       " 'cd21eee0-54a4-4cf5-bee8-cfd063364f5e',\n",
       " '42b52d0a-1f36-4956-8ef8-7be54a1911f4',\n",
       " '60317222-1ad6-467f-a60a-9a8aa1e632cb',\n",
       " 'b1bc50f2-5643-40d6-800d-6dc32678b166',\n",
       " '4153b138-43de-4a86-ad28-906a91407a2a',\n",
       " 'dfc893dd-4ae5-4827-964b-c57b7227d23a',\n",
       " '5026a17f-1a8d-4d2f-a575-16dd0b5081a7',\n",
       " 'a25745eb-41cc-42d2-9f90-20009651047b',\n",
       " '7b5e0699-27fe-46b2-8c06-4802f50f2341',\n",
       " '3c3a5960-e903-4359-a1b5-6362fd508e80',\n",
       " '40d3e7d7-0668-460f-9b8c-764e9dbf0501',\n",
       " 'fba2f7c8-9513-4ea4-8c2d-369b0f4b5b1f',\n",
       " 'bc8a14c8-0477-40af-bc1d-2fa1536825b9',\n",
       " '6b00fc0e-1781-4de1-89bb-545d87bca9f2',\n",
       " 'ba95915a-3f64-4068-8e44-bc08473cc862',\n",
       " '560fe555-04f6-4d10-940b-00107ca0d51d',\n",
       " '5cba92fe-0996-41e2-8498-937b0afc24b8',\n",
       " 'd7e01d24-d248-4a8d-9484-9c4ac0e2a87c',\n",
       " 'ca23bc17-a92e-40bd-9d81-163779c986d7',\n",
       " 'b808a1b7-1ca9-4c99-89bb-f715b0a173b0',\n",
       " '853e682c-9c9e-4c0d-ad27-d4c8966bfce8',\n",
       " 'a666daf0-0281-430c-9941-6e96f70ddcff',\n",
       " 'ebf15ebd-2975-4200-917e-b266af14e67e',\n",
       " '94970c1a-1838-4439-83c4-6c46fe6be7e7',\n",
       " '7ce1f065-9a10-4368-ad50-63199a7c2f0c',\n",
       " 'c41e930f-6883-4b98-a667-45d9c8cb6399',\n",
       " '0acaf938-e964-43a7-bbed-fc4a33fd3a9e',\n",
       " 'acd76949-c22f-471e-a606-175ba9b0be4c',\n",
       " '72975f90-377b-4d62-8fee-12446c35b88a',\n",
       " 'd5e70e83-07d7-4101-9d68-08e171879ec5',\n",
       " 'e29ed476-d3ef-46ee-81f1-742b4f2ffcb6',\n",
       " '5a236d33-eaa6-4cf0-92a5-d957ff06d9ec',\n",
       " 'd0585efc-84b6-40a8-a8b5-d531428b2787',\n",
       " 'e6e2fef7-8616-49f8-bcfc-67dd5bb2e9df',\n",
       " '0ec8411e-c8f5-436c-a28a-7acde51dc72b',\n",
       " 'e26f98db-e958-456f-8c70-720cd45a3433',\n",
       " 'b6faa67b-0780-4715-a8c3-cadedb4f845f',\n",
       " 'ecca5498-b12c-4149-a3d7-dc2b4af639fb',\n",
       " '2809772e-47c7-410e-bf62-ef4d54e31729',\n",
       " 'b7afe6a9-be9d-405c-a69c-e11a83b061d7',\n",
       " 'daf72ca1-fbec-4fe4-bd1d-d791f23194ac',\n",
       " '72a1bc56-87d2-41c6-a3ae-c692fc75a3b4',\n",
       " '5586ac33-15b4-4370-89c9-09c9b72ea1b6',\n",
       " '682447da-eb3b-4e1f-aaa5-6c90a44f7774',\n",
       " 'c0fef821-d90c-4f39-b04d-d800724c3075',\n",
       " 'fb3d8a2d-bf6a-4613-885e-b1f4cf53d54d',\n",
       " '7bb86030-106e-4e73-8278-dd43f6f0694d',\n",
       " '519fbfde-38bb-4d72-88ee-33c46b231957',\n",
       " '271f23fa-57d2-498a-be80-72c81060b4a2',\n",
       " '24d09fb0-5bda-4370-b241-7ca12f2a5a9f',\n",
       " '37450336-df6e-4d1b-ac2d-8d09073f997c',\n",
       " '8a3cdcd2-8170-4d13-819d-a713be61771c',\n",
       " '45af1da2-bf1a-4dc0-822f-a05d359e27cf',\n",
       " '42c7a62a-6489-43b3-b1a1-42375665ea4c',\n",
       " '95db5fa4-2f83-495e-8baa-945061a732cb',\n",
       " 'cb7db3b6-f9af-4846-86ea-2a375afb0769',\n",
       " '3c6ae287-6497-47f5-9d57-9d9cbea345d6',\n",
       " 'fd2db2d1-0e48-45d3-86a0-867b3b9b0862',\n",
       " '17c70deb-6308-4233-afd2-f38b92f08321',\n",
       " '2ed57927-e500-4845-b7c1-ef9c7289190c',\n",
       " '94eeca3a-9c6d-4034-95f6-5ea04cf50b42',\n",
       " '150cee18-9c2e-4586-b644-07f19d5039bd',\n",
       " 'e3a67ecc-d3ed-4a29-a4dd-922ddb7c6a01',\n",
       " 'd49bd30e-41b8-4079-b1f6-0f3bcfcee833',\n",
       " 'ed9439e0-9e79-46bb-8540-56ecf268d07b',\n",
       " 'e7cd7c44-df03-4288-9257-d676cfad1d83',\n",
       " 'd429819c-2e0f-4ccf-81c6-21c18c75b884',\n",
       " 'fa23ec98-c07c-4564-b36a-5e3398b0ed8e',\n",
       " '5b4077f1-4915-4c8b-810d-5c92e7759614',\n",
       " '45e5a8be-e69d-4367-a560-1efb03e40963',\n",
       " '58d9c132-47d6-42e0-bcdf-ce05c11f508b',\n",
       " '1d9979e0-3127-4862-b6ac-703b673bca94',\n",
       " '08d93ba2-665d-4e7f-8728-908dacc3a223',\n",
       " '2fb0cb8c-d314-4d75-8ec4-9a4273514135',\n",
       " 'c5b728b8-ea08-4ac1-9534-6d4735f38339',\n",
       " '80535f12-64b8-4be2-aa24-c80c45c1705a',\n",
       " '442dc35b-fe8a-4512-b0a2-73caf09e5f41',\n",
       " 'ef29dafd-1bcd-4131-be50-b259c2fca97c',\n",
       " '952c7d64-8040-48f7-b893-606148f808ff',\n",
       " '2a61a989-1f5c-4f32-8e1a-422bb1777af9',\n",
       " '987f1dab-1667-4dd8-931e-d223938b07e9',\n",
       " '051c20bf-7f15-47cc-93e8-21595ca78734',\n",
       " '744ede01-4d11-4bdf-b091-77128cd80e55',\n",
       " '04592eb5-7545-41ba-bda4-c45cd5e9acea',\n",
       " '10401bd9-e06a-45fd-9751-cee38b3faabd',\n",
       " 'f462352b-4b41-45c9-b3c4-fbb06ab07b32',\n",
       " 'b1e975d9-3fe5-4bf8-a0bf-ea08689506c8',\n",
       " '370b371a-d585-48e4-9ed9-02185c3bce87',\n",
       " 'fcf0463c-d0a8-4c2c-9ae9-5b017b176eac',\n",
       " '9d26a98f-2516-45f5-b40e-801f0f384eac',\n",
       " 'c7ea1d4f-5ea3-4eec-ae15-a2ce71321f75',\n",
       " '8bc62886-b905-409a-b192-0a17acd06179',\n",
       " '1d9754a5-0cc9-42a9-ae86-a318fcf46f32',\n",
       " '2440038f-4b0f-4ba4-a92b-8b9b8517c5bb',\n",
       " 'af48cb00-6925-4893-9845-2e338b8b3246',\n",
       " '5b437379-50be-4d27-be0e-a3a395159014',\n",
       " '0b685fcf-e059-49d2-a0ac-ee4acb4a9c74',\n",
       " '9fa26e18-fd2a-4461-a10f-8ce7f849c2bc',\n",
       " '01ba43ed-f2f9-4353-9bb1-0980817c1d34',\n",
       " 'a9671511-791f-4574-9bc2-a224cfb7afb7',\n",
       " '8fe93ce2-8478-4993-a269-a6204fe43e24',\n",
       " 'e403ae0b-99e0-454b-9a15-a659421695ed',\n",
       " 'c6a92f94-a415-42df-b9ee-0c58d7d0d853',\n",
       " '7d75503f-898c-462a-8d73-a5c003f073ab',\n",
       " 'a9367556-c90f-4104-9310-398d2ecf476e',\n",
       " '8b71832f-93a7-458c-9340-4ef921e7d819',\n",
       " '24746a33-fd60-442a-9ce8-5e068a8b0f61',\n",
       " '6ebc86df-997f-4143-bad0-fca42943d80a',\n",
       " 'b8a25c87-f049-4320-b8da-03d182b8e634',\n",
       " '4c229aba-e598-4b78-ac7e-95cfd6d6f059',\n",
       " 'ed35f685-37d4-4aa5-95bd-71b029fa3d41',\n",
       " 'd5ec4563-ca2f-4a60-845d-c8f8f1a732cf',\n",
       " '551e8e38-ba00-4f2e-8bef-31fa45d8be0a',\n",
       " 'e3c12c39-95c5-499e-acb4-17c4e17e7084',\n",
       " '413eec5b-369b-4607-a86b-98ac6592e84c',\n",
       " 'efd1524f-72d9-4b3a-b0a0-6b776fd74337',\n",
       " '2e4ee910-3c76-4c9e-bcc7-a5f760118616',\n",
       " 'd0a16f42-d497-4011-bbc3-6ce221ade94a',\n",
       " '68440790-cd50-4012-ac41-0bf5187c2bd1',\n",
       " 'aa55b4c8-5e0e-446c-9fed-4f057d0b8469',\n",
       " '796696e8-f3f3-4f14-b911-c9c68fbe5b0b',\n",
       " '4a6353ee-7959-4aa6-a602-e8149b3ad605',\n",
       " 'e9237bbf-328d-4933-9b03-5ce183275191',\n",
       " '05d03f86-0c7f-4e40-a52a-4903465ad129',\n",
       " 'd01cecfe-5378-4763-9636-32d52831fb1d',\n",
       " 'c807559b-0cb9-459e-b7b5-ccd37cb7e2a5',\n",
       " '9ae0c770-0e58-4b37-a855-4d37fe04f0af',\n",
       " '4857be22-47d9-47cb-b361-85a78d7756f6',\n",
       " 'e56caf7b-fcef-4a77-8231-cd3e18cb3f53',\n",
       " '84860fe2-cfb8-4a52-9780-459c81fba87f',\n",
       " '60f64746-b751-48d9-a628-9ab33387cc12',\n",
       " '0c9b9338-2664-4093-9001-09af97856890',\n",
       " '51507111-ff6b-4960-8a87-238515bac002',\n",
       " '291b5997-2f3b-4304-aba2-45c008ded2eb',\n",
       " '6b3ca987-de51-478c-b4cd-8d0fb391af2d',\n",
       " '0788a498-3770-4ab5-a88d-8fcca843d5f1',\n",
       " 'ba893b90-8665-45d9-9d76-5880da187efe',\n",
       " '2caa8b5f-2600-4ea2-92ef-8c5a31205fc6',\n",
       " '758649e1-269d-47a0-937b-84b60c5b7dd1',\n",
       " 'ff77ceda-635e-4fed-9ae1-bc2f6baad19e',\n",
       " '7492bf2b-46da-4d89-80fc-7a2d821cfd38',\n",
       " 'e987df3b-44df-4551-b220-3b0b5932b4b6',\n",
       " '59ecca8c-fa4b-4b3e-b940-629dc9dbb9b7',\n",
       " '2b9661ab-21c7-4d9e-867e-037baab789c7']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(docs_split)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
